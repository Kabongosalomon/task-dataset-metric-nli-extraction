<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Attentioned Memory Network for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhu</surname></persName>
							<email>jingzhu@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiao</forename><surname>Shi</surname></persName>
							<email>yunxiao.shi@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengwei</forename><surname>Ren</surname></persName>
							<email>mengwei.ren@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
							<email>yfang@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Chin</forename><surname>Lien</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junli</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Dhabi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xmotors</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Structure-Attentioned Memory Network for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimation is a challenging task that aims to predict a corresponding depth map from a given single RGB image. Recent deep learning models have been proposed to predict the depth from the image by learning the alignment of deep features between the RGB image and the depth domains. In this paper, we present a novel approach, named Structure-Attentioned Memory Network, to more effectively transfer domain features for monocular depth estimation by taking into account the common structure regularities (e.g., repetitive structure patterns, planar surfaces, symmetries) in domain adaptation. To this end, we introduce a new Structure-Oriented Memory (SOM) module to learn and memorize the structure-specific information between RGB image domain and the depth domain. More specifically, in the SOM module, we develop a Memorable Bank of Filters (MBF) unit to learn a set of filters that memorize the structure-aware image-depth residual pattern, and also an Attention Guided Controller (AGC) unit to control the filter selection in the MBF given image features queries. Given the query image feature, the trained SOM module is able to adaptively select the best customized filters for cross-domain feature transferring with an optimal structural disparity between image and depth. In summary, we focus on addressing this structurespecific domain adaption challenge by proposing a novel endto-end multi-scale memorable network for monocular depth estimation. The experiments show that our proposed model demonstrates the superior performance compared to the existing supervised monocular depth estimation approaches on the challenging KITTI and NYU Depth V2 benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Depth estimation is an important component in many 3D computer vision tasks like visual Simultaneous Localization and Mapping (visual SLAM). Traditional approaches have made significant progress in binocular or multi-view depth estimation by taking advantage of geometry constraints of either spatial (i.e. stereo camera) or temporal (i.e. video sequence) pairs. With the prevalence of deep convolutional neural networks, researchers have been trying to relax the constraints by tackling monocular depth estimation. Recent works ; <ref type="bibr" target="#b23">Roy and Todorovic (2016)</ref>; <ref type="figure">Figure 1</ref>: Different from the recent methods that directly align the features from different domains, we focus on the structure-specific domain adaption. <ref type="bibr" target="#b13">Kuznietsov, Stckler, and Leibe (2017)</ref>; <ref type="bibr" target="#b11">Kim et al. (2016)</ref>; <ref type="bibr" target="#b5">Fu et al. (2018)</ref>; <ref type="bibr" target="#b3">Eigen and Fergus (2015)</ref>) have demonstrated promising results using regression-based deep learning models. Their models are trained by minimizing imagelevel losses with supervised signal on predicted results. Nevertheless, the cross-modality variance between the RGB image and the depth map still makes monocular depth prediction an ill-posed problem. Based on this observation, some researchers have considered solving the problem with additional feature-level structural constraints by minimizing the cross-modality residual complexity between image features and depth features. Most existing methods either consider the pixel-wise or structure-wise alignment in this regard. For instance, several architectures utilize the micro discrepancy loss as similarity measures such like sum of squared differences, correlation coefficients <ref type="bibr" target="#b21">(Myronenko and Song (2010)</ref>) and maximum mean discrepancy <ref type="bibr" target="#b7">(Ghifary et al. (2015)</ref>; <ref type="bibr" target="#b19">Long et al. (2015)</ref>) to align the RGB images features with depth features from pixel to pixel independently without considering the spatial dependencies. Another line of work has tried to apply the adversarial adaptation methods <ref type="bibr" target="#b12">(Kundu et al. (2018)</ref>; <ref type="bibr" target="#b28">Tzeng et al. (2017)</ref>; <ref type="bibr" target="#b9">Hoffman et al. (2015)</ref>) in conjunction with task-specific losses that concentrate on macro spatial distribution similarity between the image features and depth ones. In this paper, we seek a way to address this domain adaption challenge on both pixel-wise discrepancies and the structure dependencies by extracting the structure-specific information between the two domains (as shown in <ref type="figure">Figure 1</ref>).</p><p>In order to explore the pixel-wise discrepancies as well as the structure dependencies between the image features and depth features, we propose a memorable domain adaptation network, with an image-encoder-depth-decoder regression network backbone, and a specifically designed Structure-Oriented Memory (SOM) module coupled with a crossmodality residual complexity loss to minimize the gap between latent distribution of the image and depth map from both the pixel-level and structure-level. Given the observation that similar type of scenes (e.g. roadside scenes) often share common structural regularities (e.g. repetitive structure patterns, planar surfaces, symmetries), a set of filters could be trained to learn a specific structural image-depth residual patterns. Therefore, in our SOM module, we build a Memorable Bank of Filters (MBF) to store and learn the structure-ware filters, then we construct an Attention Guided Controller (AGC) to learn to automatically select the appropriate filters (from the MBF) to capture the significant information from the given image features (generated by the image encoder) for the further depth estimation. Finally, the customized image features are fed into the depth decoder network to output the corresponding depth maps. Importantly, comparing to the direct alignment between the two domains features (e.g. direct applying L 1 loss between Z i and Z d ), our introduced SOM module not only improves the fitting ability, but also reduces the training burden of the image encoder simultaneously. The experiments conducted on two well-known large scale benchmarks KITTI and NYU Depth V2, demonstrate that our proposed model obtains the state-of-the-art performance on monocular depth estimation tasks. Moreover, the performance margin between model trained with SOM and the one trained with direct alignment, validate the effectiveness of our proposed SOM module. In summary, our contributions in this paper are as follows:</p><p>• We introduce memory strategies to address monocular depth estimation by designing a novel Structure-Oriented Memory (SOM) module with a Memorable Bank of Filters (MBF) and an Attention Guided Controller (AGC) for feature-level cross-modality domain adaptation.</p><p>• We propose a novel end-to-end deep learning Structure-Attentioned Memory Network, which seamlessly integrates a front-end regression network with the SOM module that operates at feature-level to substantially improve the depth prediction performance.</p><p>• We achieve state-of-the-art performance on two large scale benchmarks: KITTI and NYU Depth V2, which validates the effectiveness of the proposed method.</p><p>The remainder of our paper is organized as follows. We present a brief review of the related literature in Section Related Works, after which we introduce the proposed method in details in Section Proposed Method. In Section Experiments, we provide the qualitative and quantitative experimental results, as well as ablation studies that demonstrate the effectiveness of the proposed method. Finally, we conclude the paper in Section Conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Monocular depth estimation is a fundamental problem in computer vision which has widespread application in graphics, robotics and AR/VR. While previous works mainly tackle this using hand-crafted image features or probabilistic models such as Markov Random Fields (MRFs) (Saxena, <ref type="bibr" target="#b25">Sun, and Ng (2009)</ref>), recent success of deep learning based methods ; <ref type="bibr" target="#b23">Roy and Todorovic (2016)</ref>; <ref type="bibr" target="#b13">Kuznietsov, Stckler, and Leibe (2017)</ref>; <ref type="bibr" target="#b11">Kim et al. (2016)</ref>; <ref type="bibr" target="#b5">Fu et al. (2018)</ref>; <ref type="bibr" target="#b3">Eigen and Fergus (2015)</ref>) have inspired researchers to use deep learning techniques to address the challenging depth estimation problem. The learning based monocular depth estimation approaches can be mainly summarized into two categories, the supervised and the unsupervised/semi-supervised methods.</p><p>Supervised Methods A majority of works focus on supervised learning to use the learned features from CNNs to do accurate depth prediction. <ref type="bibr" target="#b4">Eigen, Puhrsch, and Fergus (2014)</ref> first brought CNNs to depth regression task by integrating coarse and refined features with a two-stage network. The multi-task learning strategies were also applied in depth estimation to boost the performance. <ref type="bibr" target="#b16">Liu, Gould, and Koller (2010)</ref> utilized the semantic segmentation as objectness cues for depth estimation. Furthermore, <ref type="bibr" target="#b26">Shi and Pollefeys (2014)</ref> and <ref type="bibr" target="#b30">Xu et al. (2018)</ref> performed joint prediction of the pixellevel semantic labels as well as the depth. Surface normal information was also adopted in many recent works <ref type="bibr" target="#b3">(Eigen and Fergus (2015)</ref>; <ref type="bibr" target="#b32">Zhou et al. (2017)</ref>; <ref type="bibr" target="#b29">Wang et al. (2015)</ref>; <ref type="bibr" target="#b22">Qi et al. (2018)</ref>). Besides, some research works also demonstrated the robustness of multi-scale feature fusion in pixellevel prediction tasks (e.g. semantic segmentation, depth estimation). <ref type="bibr" target="#b5">Fu et al. (2018)</ref> adopted the dilated convolution to enlarge the perceptive field without decreasing spatial resolution of the feature maps. In Buyssens, Elmoataz, and Lzoray (2012)'s work, inputs at different resolutions are utilized to build a multi-stream architecture. Instead of regression, there are also methods that discretize the depth range and transfer the regression problem to a classification problem. In the work of <ref type="bibr" target="#b5">Fu et al. (2018)</ref>, the space-increasing discretization is proposed to reduce the over-strengthened loss for the large depth values.</p><p>Unsupervised/Semi-supervised Methods Another line of methods on monocular image depth prediction goes along the unsupervised/semi-supervised direction which mostly takes advantage of geometry constraints (e.g. epipolar geometry) on either spatial (between left-right pairs) or temporal (forward-backward) relationship. <ref type="bibr" target="#b6">Garg et al. (2016)</ref> proposed to estimate the depth map from a pair of stereo images by imposing the left-right consistency loss. <ref type="bibr" target="#b31">Zhan et al. (2018)</ref> jointly learned a single view depth estimator and monocular odometry estimator using stereo video sequences, which enables the use of both spatial and temporal photometric warp constraints. Moreover, following the trend of adversarial learning, the generative adversarial networks (GANs) have been utilized in the depth estimation problem. <ref type="bibr" target="#b12">Kundu et al. (2018)</ref> proposed an unsupervised domain adaptation strategy for adapting depth predictions from synthetic RGB-D pairs to natural scenes in the depth estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modality Domain Adaptation</head><p>In addition to the recent depth estimation methods, research works focused on the cross-modality domain adaption are also highly relevant to ours. The existence of cross modality, or domain shift, is commonly seen in real-world application, which is the consequence of data captured by different sensors (e.g. optical camera, LiDAR or stereo camera), or varying conditions (i.e. background). In different domains, semantic labels are shared whereas the data distributions are usually different to a large extent. For example, Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) in biomedical image analysis <ref type="bibr" target="#b2">(Dou et al. (2018)</ref>); RGB images, depth maps and point clouds in 2D, 2.5D and 3D computer vision tasks. Numerous approaches have been proposed to address the domain adaptation needs in different visual tasks. Here we briefly review some domain adaption methods using deep learning techniques. Most deep domain adaptation methods utilize a siamese architecture with two streams for source and target models respectively, and the network is trained with a discrepancy loss to minimize the pixel-wise shift between domains. <ref type="bibr" target="#b19">Long et al. (2015)</ref> used maximum mean discrepancy together with a task-specific loss to adapt the source and target, while <ref type="bibr" target="#b27">Sun and Saenko (2016)</ref> proposed the deep correlation alignment algorithm to match the mean and covariance. <ref type="bibr" target="#b0">Bloesch et al. (2018)</ref> proposed to learn a dense representation using an auto-encoder. <ref type="bibr" target="#b20">Mandikal et al. (2018)</ref> trained the network with L 1 constrain in latent space to transfer feature from 2D to 3D in order to directly predict 3D point cloud from a single image. In our work, we aim to design a domain adaptive (SOM) module using memory mechanism, so that the image features can be automatically customized to obtain a better depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>The monocular depth estimation problem can be defined as a nonlinear mapping f : I → Y from the RGB image I to the geometric depth map Y , which can be learned in a supervised fashion given a training set X = {I t , Y t } N t=1 . To learn the mapping function, we propose Structure-Attentioned Memory Network as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, which is composed of a (pre-trained) depth auto-encoder, an image encoder and a depth predictor equipped with SOM module. All the components are trained into two stages. In the first stage, a se-</p><formula xml:id="formula_0">ries of 'target' depth features {Z t d } k t=1 ∈ R k are learned by training a depth map auto-encoder (E d , D d ).</formula><p>In the second stage, we train an image encoder E i , SOM modules M id and a depth predictor P d to map the 2D image to the depth map in an end-to-end manner. Particularly, E i encodes the RGB image to the 'source' image features {Z t i } k t=1 ∈ R k , which act as queries to obtain image-depth residual patterns from SOM module. The residual is then concatenated to the source feature to form a newly transferred feature set</p><formula xml:id="formula_1">{Z t id } k t=1 ∈ R k (which is expected to be aligned with the target feature {Z t d } k t=1</formula><p>with supervision) is fed to the predictor P d to estimate the output depth map. We will elaborate the network structures from two stages separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 1: Depth Auto-Encoder</head><p>In order to learn a strong and robust prior over the depth map as a reference in the latent matching process, we train a depth auto-encoder (E d , D d ) which takes a ground truth depth map Y d ∈ R M ×N as input, and outputs a reconstructed depth mapŶ d ∈ R M ×N . As shown in <ref type="figure" target="#fig_1">Figure  3</ref> (Stage 1), we use the DenseNet based encoder-decoder structure. Specifically, DenseNet-121 is utilized for constructing the depth encoder <ref type="figure" target="#fig_1">(Figure 3 (a)</ref>), in which four feature maps with cascading resolutions are extracted from different blocks (shallow to deep) for depth decoding. In order to make sure that the object contours as well as details are well preserved, we use a Feature Pyramid Network (FPN) to build the depth decoder, fusing multi-scale features in a pyramid structure. Specifically, as shown in <ref type="figure" target="#fig_1">Figure 3</ref> (b), four features with sizes 1/4, 1/8, 1/16 and 1/32 of the input are derived. Starting from the deepest feature, each feature map is first upsampled by a factor of 2, and element-wisely added to its following feature map. After the fusion process of the multi-scale feature maps, each of the newly generated feature maps is upsampled to size of 1/4 the original input (or the size of the shallowest feature map), and concatenated together to form a feature volume. Finally, the output depth map is predicted via extra CNN layers on the concatenated feature volume. The FPN decoder is able to preserve details in the depth map decoding process. We will show more experimental comparison between different decoder structure to demonstrate its effectiveness in the Experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2: Depth Prediction with SOM Module for Latent Space Adaptation</head><p>In the second stage, we aim to train the network in an end-toend manner to effectively transfer the features derived from image encoder E i from image domain to depth domain, as a strong prior over the ground truth depth, so as to better deduce the depth from the transferred prior. To this end, this stage contains three major components as shown in <ref type="figure" target="#fig_1">Figure 3</ref> (c), (d) and (e): the image encoder, the SOM module for latent space adaptation, and the depth predictor (E i , M id , P d ).</p><p>Each component of the network will be explained below.</p><p>Image Encoder and Depth Predictor as Regression Backbone In order to make sure that the network derive both depth features and image features at the same scale, we design the encoder-decoder based backbone ((c) and (d) in <ref type="figure" target="#fig_1">Figure 3</ref>) for stage 2 exactly the same as those of stage 1 but without weight sharing. Specifically, the structure of image encoder E i ((c) in <ref type="figure" target="#fig_1">Figure 3</ref>) is identical to that of depth encoder E d ((e) in <ref type="figure" target="#fig_1">Figure 3</ref>), and similarly for D d ((b)) and P d ((e)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOM Module for Latent Space Adaptation</head><p>In the latent space, we propose an additional structure oriented memory module consisting of two collaborative units: a Memorable Bank of Filters (MBF) that stores a bank of learned filters to detect the cross-modality residual complexity between the depth feature and the image feature, and an Attention Guided Controller (AGC) which controls the interaction between the image feature with the MBF. The image feature as a specific query feature selects filters from MBF with an attention guided read controller, and the MBF is updated through a write controller that is naturally integrated into the back propagation to make the network can be trained end-to-end. The proposed SOM reading and writing process are as follows.</p><p>SOM Reading Different from reading by 'addressing' in general memory concept, the proposed SOM module is reading by 'attention', which means each memory slot is assigned with a weight, and the whole memory is merged per weights as reading output. As demonstrated in <ref type="figure" target="#fig_2">Figure 4</ref>, given the query feature Z i , in order to obtain weights for each memory slot, we build a LSTM-based read controller to learn the weights. Specifically, each filter from the memory slot {M t } n t=1 is firstly convolved on the feature, and the intermediate outputs are denoted as {x t } n t=1 , where n is the memory size, and x t is formulated as:</p><formula xml:id="formula_2">x t = W t * Z i + b t , M t = (W t , b t ), W t is the kernel, b t</formula><p>is the bias, and * is the convolution operation. The intermediate outputs {x t } n t=1 could be thought of as the 'unweighted/unbiased' output that takes each filter/memory slot equally. Then in order to further add weighted attention on the result pool, a Bi-Directional Convolutional Long Short Term Memory is applied as the read controller on {x t } n t=1 to explore the correlation within the pool, so as to aggregate the memory slots with strong attention. Particularly, read controller processes {x t } n t=1 from two directions and computes the forward hidden sequence h f by iterating the input from t = 1 to n, and the backward hidden sequence h b by iterating the input from t = n to 1. The forward/backward flow of the LSTM cell is formulated as below:</p><formula xml:id="formula_3">i t = σ(W xi * x t + W hi * h t−1 + W ci • c t−1 + b i ) f t = σ(W xf * x t + W hf * h t−1 + W cf • c t−1 + b f ) c t = f t • c t−1 + i t • tanh(W xc * x t + W hc * h t−1 + b c ) o t = σ(W xo * x t + W ho * h t−1 + W co • c t + b o ) h t = o t • tanh(c t )</formula><p>where h is the hidden sequence, σ is the logistic sigmoid function, * is the convolution operator and • denotes the Hadamard product. i t , f t , o t , c t represent input gate, forget gate, output gate, and cell activation vector respectively, and W hi is the hidden-input gate matrix, while W xo is the input-output gate matrix. The final attention sequence α is computed with regard to both h f and h b as follows:</p><formula xml:id="formula_4">α t = sof tmax(W h f y h f (t) + W h b y h b(t) + b y ),</formula><p>where t = 1 to n, and each y after softmax operation in the output sequence is associated with the weight for each memory slot (refer to α value in <ref type="figure" target="#fig_2">Figure 4</ref>, the redder the color, the higher the attention), therefore k i=1 α i = 1. The memory output Z m is a combination of the output sequence that focuses more on the slot with higher attention, while less on lower attention value: Z m = n t=1 y t , y t = α t x t . Finally, Z m is concatenated with the query feature itself to reproduce a transferred feature Z id that is supposed to match the distribution of the depth feature Z d .</p><p>SOM Writing The proposed memory writer can be seamlessly integrated to network back propagation. The attention learned from the read controller will also operate in the memory writing process, and specifically, the slot with higher attention will be updated to a larger extent and vice versa. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, there are two backward flows that affect the writing of the memory (red arrows in <ref type="figure" target="#fig_0">Figure  2</ref>): one comes from the output branch, and the other comes from the latent matching branch. The update rule could be formulated (in a simplified form) as W t ← W t + α t η∆ Wt , where α t is the attention for each slot, η is the learning rate, and ∆ Wt is the total gradient from both branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning objectives</head><p>We design multiple objectives to constrain the joint training of the network with details as follows.</p><p>Depth Estimation Objective The depth estimation objective poses constraints on the front-end pipeline of the single image depth estimation. A common way for supervising regression tasks is to adopt L 1 or L 2 loss between the prediction and the ground truth, which means that larger values have much heavier influence on the loss. However, in depth estimation task, the larger the depth value is, the farther the object is to the camera, which means that the information is less rich for the estimator, leading to unnecessarily large loss <ref type="bibr" target="#b5">(Fu et al. (2018)</ref>). Therefore, in order to reduce the over-emphasized error on large depth values, we use the logarithm mean squared error (RMSE log )) loss to make the predictor focus more on closer objects which makes up the main portion in a depth map. The objective is formulated as</p><formula xml:id="formula_5">L depth = 1 N i∈N || log(d i ) − log(d * i )|| 2 ,</formula><p>where d is the ground truth depth map, while d * is the predicted depth map.</p><p>Auto-Encoder Objective The objective for the depth auto-encoder is utilized in the first training stage. To make sure that the depth features and the image features are in the same scale with same constraints, we also applied the RMSE log on the auto-encoder as L AE = 1 N i∈N || log(d i ) − log(d i )|| 2 , where d is the ground truth depth map, whiled is the reconstructed depth map.</p><p>Cross-Modality Residual Complexity Objective The latent adaptation objective is applied to constrain the SOM module to minimize feature distribution discrepancies. We use L 1 loss between the 'target' depth features (pretrained from stage 1) and the SOM transferred image features. The objective is a sum of feature alignment losses at different levels as L CM RC = k ||Z k id − Z k d || 1 , where k is the number of features involved in latent matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient and Surface Normal Constraints</head><p>To further strengthen the network by pulling out the model from local minima, we added extra constraints on the predicted depth map including the gradient loss and the surface normal loss to finetune the training following commonly used techniques. The gradient loss is defined as L gradient = 1 N N i=1 ||∇d i − ∇d * i || 1 , and specifically, we adopt Sobel filter to calculate the gradient both vertically and horizontally; ∇d is the image gradient of the ground truth depth map, while ∇d * is the image gradient of the predicted depth map. The surface normal loss is defined as the similarity between the surface normal of the ground truth depth map with the predicted depth map as</p><formula xml:id="formula_6">L normal = 1 N N i=1 (1 − &lt;∇di,∇d * i &gt; ||∇di||2||∇d * i ||2 )</formula><p>, formulated with the corresponding gradient.</p><p>In total, the training objectives are summarized as follows: (1) In training stage 1, the total loss is: L S1 = L AE ;</p><p>(2) In training stage 2, the total loss is a weighted sum of L depth , L CM RC , L gradient and L normal , which is formulated as: L S2 = λ depth L depth + λ CM RC L CM RC + λ gradient L gradient + λ normal L normal , where λ is the weight for each objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we present our experiments on two largescale datasets by introducing the implementation details, benchmark performance, and ablation studies validating the effectiveness of the proposed approach.</p><p>Implementation Details The proposed method is implemented using the TensorFlow 1.10 framework and runs on a single NVIDIA TITAN X GPU with 12 GB memory. The encoder-decoder structure from both stage 1 and stage 2 are identical but without weight sharing. The depth autoencoder is trained from scratch, while the image encoder is initialized with ImageNet <ref type="bibr" target="#b24">(Russakovsky et al. (2015)</ref>) pretrained parameters. For multi-scale feature fusion, we consider four levels of feature maps which are derived from different blocks of the DenseNet-121 backbone with the feature map sizes 1/4, 1/8, 1/16 and 1/32 of the input images. For instance, in NYU Depth V2 dataset, with the input resolution 480 × 640, four feature maps with cascading sizes 120 × 160, 60 × 80, 30 × 40, 15 × 20 are extracted. The   <ref type="formula" target="#formula_9">(2015)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error (lower is better) Accuracy (higher is better) Abs Rel Sq Rel RMSE RMSE log σ &lt; 1.25 σ &lt; 1.25 2 σ &lt; 1.25 3 <ref type="bibr" target="#b25">Saxena, Sun, and Ng (2009)</ref>  network is trained with initial learning rate 0.001, and decreased every 10 epochs. The weight decay and momentum set to 10 −6 and 0.9 respectively. We used the Adam optimizer and batch normalization during training, with normalization decay 0.97. We set the weights for each objective as λ depth = 1, λ gradient = 1, λ norm = 1, and λ CM RC = 2. The gradient loss is added after 4k steps of training, and the surface normal loss is added after 8k steps of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>We employ several data augmentation techniques on NYU Depth V2 dataset to prevent overfitting from limited amount of data, including: (i) Random Cropping by 0−10% of the image height/ width; (ii) Scaling the original image by the factor interval of [0.75, 1.25]; (iii) Random Flipping 50% of the images horizontally; (iii) Rotating the images randomly with the degree of [−10 • , 10 • ]; (iv) Color jitter of brightness (by -10 to 10 of original value), contrast (by a factor of 0.5 to 2.0), saturation and hue (by -20 to 20 of original value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>Below is a list of evaluation metrics the quantitative evaluation is performed: (1) the absolute mean relative error (Abs Rel): 1 N i∈N</p><formula xml:id="formula_7">|di−d * i | d * i ,</formula><p>(2) the squared relative error (Sq Rel): 1 N i∈N</p><formula xml:id="formula_8">||di−d * i || 2 d * i ,<label>(3)</label></formula><p>the root mean squared error (RMSE):</p><p>1 N i∈N ||d i − d * i || 2 , (4) log mean squared error (RMSE log ):</p><formula xml:id="formula_9">1 N i∈N || log(d i ) − log(d * i )|| 2 ,<label>(5)</label></formula><p>average log 10 error (Avg log 10 ): 1 N i∈N | log 10 (d i ) − log 10 (d * i )|, and (6) accuracy with threshold t (t=1.25, 1.25 2 , 1.25 3 ):</p><formula xml:id="formula_10">1 N i∈N 1 {δ=max( d * i d i , d i d * i )&lt;t} .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on KITTI Dataset (Eigen split)</head><p>The KITTI dataset is a large scale dataset for autonomous driving, which contains depth images captured with LiDAR sensor mounted on a driving vehicle. In our experiment, to compare the results at the same level, we follow the experimental protocol proposed by <ref type="bibr" target="#b3">Eigen and Fergus (2015)</ref>, in which around 22600 images (resolution 384×1280) from 32 scenes are utilized as training data, and around 800 images from 29 scenes are used for validation. Following the previous works, the depth value of the RGB image is scaled to 0-80m. During training, the depth maps are down-scaled to resolution 192×640, and up-sampled to the original size in evaluation process. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison with the state-ofthe-art methods on KITTI dataset. We compared with stateof-the-art methods <ref type="bibr" target="#b25">(Saxena, Sun, and Ng (2009)</ref>  <ref type="formula">(2017)</ref>). Particularly, the methods proposed by <ref type="bibr" target="#b25">Saxena, Sun, and Ng (2009)</ref>  Results on NYU Depth V2 Dataset The NYU Depth V2 dataset contains 120K pairs of RGB-D (resolution 480 × 640) captured by Kinect. The dataset is manually selected and annotated into 1449 RGB-D pairs, in which 795 images are used for training, and the rest for validation. The depth value ranges from 0 to 10m. In the training process, the depth maps are down-scaled to resolution 120 × 160, and in testing/ evaluation, the predicted depth map is upsampled to the original resolution. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison of the proposed method with state-of-the-art methods (official test split). We compare with both hand-crafted feature based approaches <ref type="bibr" target="#b25">(Saxena, Sun, and Ng (2009)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b10">Karsch, Liu, and Kang (2012)</ref>; <ref type="bibr" target="#b26">Shi and Pollefeys (2014)</ref>) and deep learning based ones <ref type="bibr" target="#b17">(Liu, Salzmann, and He (2014)</ref> Ablation Studies To further demonstrate the effectiveness of the proposed method, we conduct ablation studies from two aspects on NYU Depth V2 dataset. Firstly, we compare the performance of the depth estimation pipeline with different decoder structures: (1) The decoder that simply uses symmetric structure with the encoder that cascadingly upsample the feature map until the output size.</p><p>(2) The decoder that takes four different feature maps from the encoder and fuses them in a pyramid fashion (as described in Section ). The qualitative comparison are shown in <ref type="table" target="#tab_2">Table  2</ref> (E i + D pure and E i + D F P N ). As can be seen from the evaluation results, the decoder structure with pyramid multi-sacle feature fusion out-performs the one that only takes the latent feature as input by a large margin, especially in the δ 1 &lt; 1.25 metric. Therefore, it is obvious that the mixture of features from different levels are beneficial for the details compensation (i.e. contour, edges).</p><p>To validate the effectiveness of the proposed SOM module, we compare the performance of the proposed method with SOM settings against direct alignment and analyze the results. Firstly, we add the feature alignment loss for latent feature maps based on the E i + D F P N structure to test the performance of direct feature alignment (E i + D F P N + align). The quantitative results of direct alignment rarely improved compared with the one that is trained without feature alignment loss, reflecting the limited capability of the encoder for feature adaptation. Then, we add the SOM module at feature level (E i + D F P N + SOM ) and compare the results with the baseline structure that goes without memory. The large margin quantitative improvement in <ref type="table" target="#tab_2">Table 2</ref> implies that structure-specific feature alignment with memory mechanism (SOM) is superior to other approaches such as direct alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we developed a novel memory guided network named Structure-Attentioned Memory Network for monocular depth estimation, consisting of the encoder-decoder based structure, as well as the external SOM module which is trained to learn and memorize the structure attentioned image-depth-residual pattern in cross-modality latent alignment. The proposed method achieves state-of-the-art performance on challenging large-scale benchmarks, and each component is validated to be effective in the ablation study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of our proposed Structure-Attentioned Memory Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The network structure of Structure-Attentioned Memory Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The SOM reading process (of a single SOM module).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Results on KITTI validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>; Liu, Shen, and Lin (2015); Zhou et al. (2017); Eigen, Puhrsch, and Fergus (2014); Garg et al. (2016); Kundu et al. (2018); Zhan et al. (2018); Godard, Aodha, and Brostow (2017); Kuznietsov, Stckler, and Leibe</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>;<ref type="bibr" target="#b18">Liu, Shen, and Lin (2015)</ref>;<ref type="bibr" target="#b32">Zhou et al. (2017)</ref>;<ref type="bibr" target="#b4">Eigen, Puhrsch, and Fergus (2014)</ref>;<ref type="bibr" target="#b12">Kundu et al. (2018)</ref> only employ monocular images in both training and testing, while approaches in Zhan et al. (2018); Garg et al. (2016); Kuznietsov, Stckler, and Leibe (2017); Godard, Aodha, and Brostow (2017) are unsupervised methods that use stereo images in training and apply single image during testing. The proposed method outperforms all these methods by a large margin, and Figure 5 displays a few visualized prediction results on examples ran-Examples of predicted depth maps on NYU V2 Depth dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>; Zhuo et al. (2015); Li et al. (2015); Wang et al. (2015); Xu et al. (2018); Liu et al. (2016); Roy and Todorovic (2016)). Figure 6 shows examples of predicted depth maps on the NYU Depth V2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on KITTI validation set. All scores are evaluated on Eigen split (Eigen and Fergus</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on NYU Depth V2. δ 1 : σ &lt; 1.25, δ 2 : σ &lt; 1.25 2 , δ 3 : σ &lt; 1.25 3 . + D F P N + align) 0.148 0.627 0.075 0.802 0.944 0.986 Ours (E i + D F P N + SOM ) 0.136 0.604 0.067 0.814 0.959 0.990 domly chosen from the validation dataset.</figDesc><table><row><cell>Method</cell><cell>Rel</cell><cell cols="2">Error RMSE log 10</cell><cell>δ 1</cell><cell>Accuracy δ 2</cell><cell>δ 3</cell></row><row><cell>Saxena, Sun, and Ng (2009)</cell><cell cols="2">0.349 1.214</cell><cell>-</cell><cell cols="3">0.447 0.745 0.897</cell></row><row><cell>Karsch, Liu, and Kang (2012)</cell><cell>0.35</cell><cell>1.2</cell><cell>0.131</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Liu, Salzmann, and He (2014) 0.335</cell><cell>1.06</cell><cell>0.127</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Shi and Pollefeys (2014)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.542 0.829 0.941</cell></row><row><cell>Zhuo et al. (2015)</cell><cell>0.305</cell><cell>1.04</cell><cell>-</cell><cell cols="3">0.525 0.838 0.962</cell></row><row><cell>Li et al. (2015)</cell><cell cols="6">0.232 0.821 0.094 0.621 0.886 0.968</cell></row><row><cell>Wang et al. (2015)</cell><cell cols="2">0.220 0.745</cell><cell>-</cell><cell cols="3">0.605 0.890 0.970</cell></row><row><cell>Xu et al. (2018)</cell><cell cols="6">0.214 0.792 0.091 0.643 0.902 0.977</cell></row><row><cell>Liu et al. (2016)</cell><cell cols="6">0.213 0.759 0.087 0.650 0.906 0.976</cell></row><row><cell>Roy and Todorovic (2016)</cell><cell cols="2">0.187 0.744</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (E i + D pure )</cell><cell cols="6">0.231 0.828 0.095 0.631 0.889 0.968</cell></row><row><cell>Ours (E i + D F P N )</cell><cell cols="6">0.229 0.803 0.092 0.633 0.891 0.969</cell></row><row><cell>Ours (E i</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Codeslam learning a compact, optimisable representation for dense visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale convolutional neural networks for visionbased classification of cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buyssens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lzoray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="342" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adversarial loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B G</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2144</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unified depth prediction and intrinsic image decomposition from a single image via joint convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adadepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2215" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discretecontinuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">3d-lmnet: Latent embedding matching for accurate and diverse 3d point cloud reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intensity-based image registration by minimizing residual complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1882</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Geonet : Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Make3D: Learning 3D Scene Structure from a Single Still Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6612" to="6619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
