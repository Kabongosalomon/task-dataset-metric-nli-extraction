<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROXYLESSNAS: DIRECT NEURAL ARCHITECTURE SEARCH ON TARGET TASK AND HARDWARE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
							<email>hancai@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
							<email>ligeng@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
							<email>songhan@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PROXYLESSNAS: DIRECT NEURAL ARCHITECTURE SEARCH ON TARGET TASK AND HARDWARE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design. 1 1 Pretrained models and evaluation code are released at https://github.com/MIT-HAN-LAB/ProxylessNAS. Figure 1: ProxylessNAS directly optimizes neural network architectures on target task and hardware. Benefiting from the directness and specialization, ProxylessNAS can achieve remarkably better results than previous proxy-based approaches. On ImageNet, with only 200 GPU hours (200 × fewer than MnasNet (Tan et al., 2018)), our searched CNN model for mobile achieves the same level of top-1 accuracy as MobileNetV2 1.4 while being 1.8× faster.</p><p>proxy <ref type="figure">(Figure 1</ref>). We also remove the restriction of repeating blocks in previous NAS works <ref type="bibr" target="#b23">Liu et al., 2018c)</ref> and allow all of the blocks to be learned and specified. To achieve this, we reduce the computational cost (GPU hours and GPU memory) of architecture search to the same level of regular training in the following ways.</p><p>GPU hour-wise, inspired by recent works <ref type="bibr" target="#b23">(Liu et al., 2018c;</ref><ref type="bibr" target="#b0">Bender et al., 2018)</ref>, we formulate NAS as a path-level pruning process. Specifically, we directly train an over-parameterized network that contains all candidate paths ( <ref type="figure">Figure 2</ref>). During training, we explicitly introduce architecture parameters to learn which paths are redundant, while these redundant paths are pruned at the end of training to get a compact optimized architecture. In this way, we only need to train a single network without any meta-controller (or hypernetwork) during architecture search.</p><p>However, naively including all the candidate paths leads to GPU memory explosion <ref type="bibr" target="#b23">(Liu et al., 2018c;</ref><ref type="bibr" target="#b0">Bender et al., 2018)</ref>, as the memory consumption grows linearly w.r.t. the number of choices. Thus, GPU memory-wise, we binarize the architecture parameters (1 or 0) and force only one path to be active at run-time, which reduces the required memory to the same level of training a compact model. We propose a gradient-based approach to train these binarized parameters based on Bina-ryConnect (Courbariaux et al., 2015). Furthermore, to handle non-differentiable hardware objectives (using latency as an example) for learning specialized network architectures on target hardware, we model network latency as a continuous function and optimize it as regularization loss. Additionally, we also present a REINFORCE-based (Williams, 1992) algorithm as an alternative strategy to handle hardware metrics.</p><p>In our experiments on CIFAR-10 and ImageNet, benefiting from the directness and specialization, our method can achieve strong empirical results. On CIFAR-10, our model reaches 2.08% test error with only 5.7M parameters. On ImageNet, our model achieves 75.1% top-1 accuracy which is 3.1% higher than MobileNetV2 (Sandler et al., 2018) while being 1.2× faster. Our contributions can be summarized as follows:</p><p>• ProxylessNAS is the first NAS algorithm that directly learns architectures on the largescale dataset (e.g. ImageNet) without any proxy while still allowing a large candidate set and removing the restriction of repeating blocks. It effectively enlarged the search space and achieved better performance. • We provide a new path-level pruning perspective for NAS, showing a close connection between NAS and model compression . We save memory consumption by one order of magnitude by using path-level binarization. • We propose a novel gradient-based approach (latency regularization loss) for handling hardware objectives (e.g. latency). Given different hardware platforms: CPU/GPU/Mobile, ProxylessNAS enables hardware-aware neural network specialization that's exactly optimized for the target hardware. To our best knowledge, it is the first work to study specialized neural network architectures for different hardware architectures. • Extensive experiments showed the advantage of the directness property and the specialization property of ProxylessNAS. It achieved state-of-the-art accuracy performances on CIFAR-10 and ImageNet under latency constraints on different hardware platforms (GPU, CPU and mobile phone). We also analyze the insights of efficient CNN models specialized for different hardware platforms and raise the awareness that specialized neural network architecture is needed on different hardware architectures for efficient inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural architecture search (NAS) has demonstrated much success in automating neural network architecture design for various deep learning tasks, such as image recognition <ref type="bibr" target="#b2">Cai et al., 2018a;</ref><ref type="bibr" target="#b21">Liu et al., 2018a;</ref><ref type="bibr" target="#b34">Zhong et al., 2018)</ref> and language modeling <ref type="bibr" target="#b36">(Zoph &amp; Le, 2017)</ref>. Despite the remarkable results, conventional NAS algorithms are prohibitively computation-intensive, requiring to train thousands of models on the target task in a single experiment. Therefore, directly applying NAS to a large-scale task (e.g. ImageNet) is computationally expensive or impossible, which makes it difficult for making practical industry impact. As a trade-off,  propose to search for building blocks on proxy tasks, such as training for fewer epochs, starting with a smaller dataset (e.g. CIFAR-10), or learning with fewer blocks. Then top-performing blocks are stacked and transferred to the large-scale target task. This paradigm has been widely adopted in subsequent NAS algorithms <ref type="bibr" target="#b21">(Liu et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b28">Real et al., 2018;</ref><ref type="bibr" target="#b3">Cai et al., 2018b;</ref><ref type="bibr" target="#b23">Liu et al., 2018c;</ref><ref type="bibr" target="#b25">Luo et al., 2018)</ref>.</p><p>However, these blocks optimized on proxy tasks are not guaranteed to be optimal on the target task, especially when taking hardware metrics such as latency into consideration. More importantly, to enable transferability, such methods need to search for only a few architectural motifs and then repeatedly stack the same pattern, which restricts the block diversity and thereby harms performance.</p><p>In this work, we propose a simple and effective solution to the aforementioned limitations, called ProxylessNAS, which directly learns the architectures on the target task and hardware instead of with 2 RELATED WORK The use of machine learning techniques, such as reinforcement learning or neuro-evolution, to replace human experts in designing neural network architectures, usually referred to as neural architecture search, has drawn an increasing interest <ref type="bibr" target="#b36">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b21">Liu et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr">c;</ref><ref type="bibr" target="#b2">Cai et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b27">Pham et al., 2018;</ref><ref type="bibr" target="#b1">Brock et al., 2018;</ref><ref type="bibr" target="#b0">Bender et al., 2018;</ref><ref type="bibr" target="#b7">Elsken et al., 2017;</ref><ref type="bibr" target="#b20">Kamath et al., 2018)</ref>. In NAS, architecture search is typically considered as a meta-learning process, and a meta-controller (e.g. a recurrent neural network (RNN)), is introduced to explore a given architecture space with training a network in the inner loop to get an evaluation for guiding exploration. Consequently, such methods are computationally expensive to run, especially on large-scale tasks, e.g. ImageNet.</p><p>Some recent works <ref type="bibr" target="#b1">(Brock et al., 2018;</ref><ref type="bibr" target="#b27">Pham et al., 2018)</ref> try to improve the efficiency of this meta-learning process by reducing the cost of getting an evaluation. In <ref type="bibr" target="#b1">Brock et al. (2018)</ref>, a hypernetwork is utilized to generate weights for each sampled network and hence can evaluate the architecture without training it. Similarly, <ref type="bibr" target="#b27">Pham et al. (2018)</ref> propose to share weights among all sampled networks under the standard NAS framework <ref type="bibr" target="#b36">(Zoph &amp; Le, 2017)</ref>. These methods speed up architecture search by orders of magnitude, however, they require a hypernetwork or an RNN controller and mainly focus on small-scale tasks (e.g. CIFAR) rather than large-scale tasks (e.g. ImageNet).</p><p>Our work is most closely related to One-Shot <ref type="bibr" target="#b0">(Bender et al., 2018)</ref> and DARTS <ref type="bibr" target="#b23">(Liu et al., 2018c)</ref>, both of which get rid of the meta-controller (or hypernetwork) by modeling NAS as a single training process of an over-parameterized network that comprises all candidate paths. Specifically, One-Shot trains the over-parameterized network with DropPath  that drops out each path with some fixed probability. Then they use the pre-trained over-parameterized network to evaluate architectures, which are sampled by randomly zeroing out paths. DARTS additionally introduces a real-valued architecture parameter for each path and jointly train weight parameters and architecture parameters via standard gradient descent. However, they suffer from the large GPU memory consumption issue and hence still need to utilize proxy tasks. In this work, we address the large memory issue in these two methods through path binarization.</p><p>Another relevant topic is network pruning ) that aim to improve the efficiency of neural networks by removing insignificant neurons <ref type="bibr" target="#b11">(Han et al., 2015)</ref> or channels . Similar to these works, we start with an over-parameterized network and then prune the redundant parts to derive the optimized architecture. The distinction is that they focus on layer-level pruning that only modifies the filter (or units) number of a layer but can not change the topology of the network, while we focus on learning effective network architectures through path-level pruning. We also allow both pruning and growing the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We first describe the construction of the over-parameterized network with all candidate paths, then introduce how we leverage binarized architecture parameters to reduce the memory consumption of training the over-parameterized network to the same level as regular training. We propose a gradient-based algorithm to train these binarized architecture parameters. Finally, we present two techniques to handle non-differentiable objectives (e.g. latency) for specializing neural networks on target hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CONSTRUCTION OF OVER-PARAMETERIZED NETWORK</head><p>Denote a neural network as N (e, · · · , e n ) where e i represents a certain edge in the directed acyclic graph (DAG). Let O = {o i } be the set of N candidate primitive operations (e.g. convolution, pooling, identity, zero, etc). To construct the over-parameterized network that includes any architecture in the search space, instead of setting each edge to be a definite primitive operation, we set each edge to be a mixed operation that has N parallel paths ( <ref type="figure" target="#fig_4">Figure 2</ref>), denoted as m O . As such, the over-parameterized network can be expressed as N (e = m 1 O , · · · , e n = m n O ). Given input x, the output of a mixed operation m O is defined based on the outputs of its N paths. In </p><formula xml:id="formula_0">One-Shot, m O (x) is the sum of {o i (x)}, while in DARTS, m O (x) is weighted sum of {o i (x)} where</formula><formula xml:id="formula_1">2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g b</formula><p>J v a 5 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f / Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i u </p><formula xml:id="formula_2">S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J</formula><formula xml:id="formula_3">2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g b</formula><p>J v a 5 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f / Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i u </p><formula xml:id="formula_4">S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J</formula><formula xml:id="formula_5">2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g b</formula><p>J v a 5 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f / Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i u  the weights are calculated by applying softmax to N real-valued architecture parameters {α i }:</p><formula xml:id="formula_6">S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J</formula><formula xml:id="formula_7">2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g b J v a 5 5 &lt; / l a t e x i t &gt; Loss = Loss CE + 1 ||w|| 2 2 + 2 E[</formula><formula xml:id="formula_8">m One-Shot O (x) = N i=1 o i (x), m DARTS O (x) = N i=1 p i o i (x) = N i=1 exp(α i ) j exp(α j ) o i (x).<label>(1)</label></formula><p>As shown in Eq. (1), the output feature maps of all N paths are calculated and stored in the memory, while training a compact model only involves one path. Therefore, One-Shot and DARTS roughly need N times GPU memory and GPU hours compared to training a compact model. On largescale dataset, this can easily exceed the memory limits of hardware with large design space. In the following section, we solve this memory issue based on the idea of path binarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING BINARIZED PATH</head><p>To reduce memory footprint, we keep only one path when training the over-parameterized network. Unlike <ref type="bibr" target="#b4">Courbariaux et al. (2015)</ref> which binarize individual weights, we binarize entire paths. We introduce N real-valued architecture parameters {α i } and then transforms the real-valued path weights to binary gates:</p><formula xml:id="formula_9">g = binarize(p 1 , · · · , p N ) =   </formula><p>[1, 0, · · · , 0] with probability p 1 , · · · [0, 0, · · · , 1] with probability p N .</p><p>(2) Based on the binary gates g, the output of the mixed operation is given as:</p><formula xml:id="formula_10">m Binary O (x) = N i=1 g i o i (x) =    o 1 (x) with probability p 1 · · · o N (x) with probability p N .</formula><p>.</p><p>( <ref type="formula">3)</ref> As illustrated in Eq.</p><p>(3) and <ref type="figure" target="#fig_4">Figure 2</ref>, by using the binary gates rather than real-valued path weights <ref type="bibr" target="#b23">(Liu et al., 2018c)</ref>, only one path of activation is active in memory at run-time and the memory requirement of training the over-parameterized network is thus reduced to the same level of training a compact model. That's more than an order of magnitude memory saving. <ref type="figure" target="#fig_4">Figure 2</ref> illustrates the training procedure of the weight parameters and binarized architecture parameters in the over-parameterized network. When training weight parameters, we first freeze the architecture parameters and stochastically sample binary gates according to Eq. (2) for each batch of input data. Then the weight parameters of active paths are updated via standard gradient descent on the training set ( <ref type="figure" target="#fig_4">Figure 2 left)</ref>. When training architecture parameters, the weight parameters are frozen, then we reset the binary gates and update the architecture parameters on the validation set <ref type="figure" target="#fig_4">(Figure 2 right)</ref>. These two update steps are performed in an alternative manner. Once the training of architecture parameters is finished, we can then derive the compact architecture by pruning redundant paths. In this work, we simply choose the path with the highest path weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">TRAINING BINARIZED ARCHITECTURE PARAMETERS</head><p>Unlike weight parameters, the architecture parameters are not directly involved in the computation graph and thereby cannot be updated using the standard gradient descent. In this section, we introduce a gradient-based approach to learn the architecture parameters.</p><p>Published as a conference paper at ICLR 2019</p><p>In BinaryConnect <ref type="bibr" target="#b4">(Courbariaux et al., 2015)</ref>, the real-valued weight is updated using the gradient w.r.t. its corresponding binary gate. In our case, analogously, the gradient w.r.t. architecture parameters can be approximately estimated using ∂L/∂g i in replace of ∂L/∂p i :</p><formula xml:id="formula_11">∂L ∂α i = N j=1 ∂L ∂p j ∂p j ∂α i ≈ N j=1 ∂L ∂g j ∂p j ∂α i = N j=1 ∂L ∂g j ∂ exp(αj ) k exp(α k ) ∂α i = N j=1 ∂L ∂g j p j (δ ij − p i ), (4) where δ ij = 1 if i = j and δ ij = 0 if i = j.</formula><p>Since the binary gates g are involved in the computation graph, as shown in Eq. <ref type="formula">(3)</ref>, ∂L/∂g j can be calculated through backpropagation. However, computing ∂L/∂g j requires to calculate and store o j (x). Therefore, directly using Eq. (4) to update the architecture parameters would also require roughly N times GPU memory compared to training a compact model.</p><p>To address this issue, we consider factorizing the task of choosing one path out of N candidates into multiple binary selection tasks. The intuition is that if a path is the best choice at a particular position, it should be the better choice when solely compared to any other path. 2</p><p>Following this idea, within an update step of the architecture parameters, we first sample two paths according to the multinomial distribution (p 1 , · · · , p N ) and mask all the other paths as if they do not exist. As such the number of candidates temporarily decrease from N to 2, while the path weights {p i } and binary gates {g i } are reset accordingly. Then we update the architecture parameters of these two sampled paths using the gradients calculated via Eq. (4). Finally, as path weights are computed by applying softmax to the architecture parameters, we need to rescale the value of these two updated architecture parameters by multiplying a ratio to keep the path weights of unsampled paths unchanged. As such, in each update step, one of the sampled paths is enhanced (path weight increases) and the other sampled path is attenuated (path weight decreases) while all other paths keep unchanged. In this way, regardless of the value of N , only two paths are involved in each update step of the architecture parameters, and thereby the memory requirement is reduced to the same level of training a compact model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HANDLING NON-DIFFERENTIABLE HARDWARE METRICS</head><p>Besides accuracy, latency (not FLOPs) is another very important objective when designing efficient neural network architectures for hardware. Unfortunately, unlike accuracy that can be optimized using the gradient of the loss function, latency is non-differentiable. In this section, we present two algorithms to handle the non-differentiable objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">MAKING LATENCY DIFFERENTIABLE</head><p>To make latency differentiable, we model the latency of a network as a continuous function of the neural network dimensions 3 . Consider a mixed operation with a candidate set {o j } and each o j is associated with a path weight p j which represents the probability of choosing o j . As such, we have the expected latency of a mixed operation (i.e. a learnable block) as:</p><formula xml:id="formula_12">E[latency i ] = j p i j × F (o i j ),<label>(5)</label></formula><p>where E[latency i ] is the expected latency of the i th learnable block, F (·) denotes the latency prediction model and F (o i j ) is the predicted latency of o i j . The gradient of E[latency i ] w.r.t. architecture parameters can thereby be given as:</p><formula xml:id="formula_13">∂E[latency i ] / ∂p i j = F (o i j )</formula><p>. For the whole network with a sequence of mixed operations <ref type="figure" target="#fig_5">(Figure 3 left)</ref>, since these operations are executed sequentially during inference, the expected latency of the network can be expressed with the sum of these mixed operations' expected latencies:</p><formula xml:id="formula_14">E[latency] = i E[latency i ],<label>(6)</label></formula><p>Published as a conference paper at ICLR 2019  We incorporate the expected latency of the network into the normal loss function by multiplying a scaling factor λ 2 (&gt; 0) which controls the trade-off between accuracy and latency. The final loss function is given as (also shown in <ref type="figure" target="#fig_5">Figure 3</ref> right)</p><formula xml:id="formula_15">Loss = Loss CE + λ 1 ||w|| 2 2 + λ 2 E[latency],<label>(7)</label></formula><p>where Loss CE denotes the cross-entropy loss and λ 1 ||w|| 2 2 is the weight decay term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">REINFORCE-BASED APPROACH</head><p>As an alternative to BinaryConnect, we can utilize REINFORCE to train binarized weights as well. Consider a network that has binarized parameters α, the goal of updating binarized parameters is to find the optimal binary gates g that maximizes a certain reward, denoted as R(·). Here we assume the network only has one mixed operation for ease of illustration. Therefore, according to REINFORCE (Williams, 1992), we have the following updates for binarized parameters:</p><formula xml:id="formula_16">J(α) = E g∼α [R(N g )] = i p i R(N (e = o i )), ∇ α J(α) = i R(N (e = o i ))∇ α p i = i R(N (e = o i ))p i ∇ α log(p i ), = E g∼α [R(N g )∇ α log(p(g))] ≈ 1 M M i=1 R(N g i )∇ α log(p(g i )),<label>(8)</label></formula><p>where g i denotes the i th sampled binary gates, p(g i ) denotes the probability of sampling g i according to Eq. (2) and N g i is the compact network according to the binary gates g i . Since Eq. (8) does not require R(N g ) to be differentiable w.r.t. g, it can thus handle non-differentiable objectives. An interesting observation is that Eq. (8) has a similar form to the standard NAS <ref type="bibr" target="#b36">(Zoph &amp; Le, 2017)</ref>, while it is not a sequential decision-making process and no RNN meta-controller is used in our case. Furthermore, since both gradient-based updates and REINFORCE-based updates are essentially two different update rules to the same binarized architecture parameters, it is possible to combine them to form a new update rule for the architecture parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>We demonstrate the effectiveness of our proposed method on two benchmark datasets (CIFAR-10 and ImageNet) for the image classification task. Unlike previous NAS works <ref type="bibr" target="#b23">Liu et al., 2018c)</ref> that first learn CNN blocks on CIFAR-10 under small-scale setting (e.g. fewer blocks), then transfer the learned block to ImageNet or CIFAR-10 under large-scale setting by repeatedly stacking it, we directly learn the architectures on the target task (either CIFAR-10 or ImageNet) and target hardware (GPU, CPU and mobile phone) while allowing each block to be specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTS ON CIFAR-10</head><p>Architecture Space. For CIFAR-10 experiments, we use the tree-structured architecture space that is introduced by <ref type="bibr" target="#b3">Cai et al. (2018b)</ref> with PyramidNet <ref type="bibr" target="#b10">(Han et al., 2017)</ref> as the backbone 4 . Specifically,</p><p>Published as a conference paper at ICLR 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params Test error (%) DenseNet-BC  25.6M 3.46 PyramidNet <ref type="bibr" target="#b10">(Han et al., 2017)</ref> 26.0M 3.31 Shake-Shake + c/o (DeVries &amp; Taylor, 2017) 26.2M 2.56 PyramidNet + SD <ref type="bibr" target="#b33">(Yamada et al., 2018)</ref> 26.0M 2.31 ENAS + c/o <ref type="bibr" target="#b27">(Pham et al., 2018)</ref> 4.6M 2.89 DARTS + c/o <ref type="bibr" target="#b23">(Liu et al., 2018c)</ref> 3.4M 2.83 NASNet-A + c/o  27.6M 2.40 PathLevel EAS + c/o <ref type="bibr" target="#b3">(Cai et al., 2018b)</ref> 14.3M 2.30 AmoebaNet-B + c/o <ref type="bibr" target="#b28">(Real et al., 2018)</ref> 34.9M 2.13 Proxyless-R + c/o (ours) 5.8M 2.30 Proxyless-G + c/o (ours) 5.7M 2.08 <ref type="table">Table 1</ref>: ProxylessNAS achieves state-of-the-art performance on CIFAR-10.</p><p>we replace all 3 × 3 convolution layers in the residual blocks of a PyramidNet with tree-structured cells, each of which has a depth of 3 and the number of branches is set to be 2 at each node (except the leaf nodes). For further details about the tree-structured architecture space, we refer to the original paper <ref type="bibr" target="#b3">(Cai et al., 2018b)</ref>. Additionally, we use two hyperparameters to control the depth and width of a network in this architecture space, i.e. B and F , which respectively represents the number of blocks at each stage (totally 3 stages) and the number of output channels of the final block.</p><p>Training Details. We randomly sample 5,000 images from the training set as a validation set for learning architecture parameters which are updated using the Adam optimizer with an initial learning rate of 0.006 for the gradient-based algorithm (Section 3.2.1) and 0.01 for the REINFORCEbased algorithm (Section 3.3.2). In the following discussions, we refer to these two algorithms as Proxyless-G (gradient) and Proxyless-R (REINFORCE) respectively.</p><p>After the training process of the over-parameterized network completes, a compact network is derived according to the architecture parameters, as discussed in Section 3.2.1. Next, we train the compact network using the same training settings except that the number of training epochs increases from 200 to 300. Additionally, when the DropPath regularization <ref type="bibr" target="#b17">Huang et al., 2016</ref>) is adopted, we further increase the number of training epochs to 600 .</p><p>Results. We apply the proposed method to learn architectures in the tree-structured architecture space with B = 18 and F = 400. Since we do not repeat cells and each cell has 12 learnable edges, totally 12 × 18 × 3 = 648 decisions are required to fully determine the architecture.</p><p>The test error rate results of our proposed method and other state-of-the-art architectures on CIFAR-10 are summarized in <ref type="table">Table 1</ref>, where "c/o" indicates the use of Cutout <ref type="bibr" target="#b5">(DeVries &amp; Taylor, 2017)</ref>. Compared to these state-of-the-art architectures, our proposed method can achieve not only lower test error rate but also better parameter efficiency. Specifically, Proxyless-G reaches a test error rate of 2.08% which is slightly better than AmoebaNet-B <ref type="bibr" target="#b28">(Real et al., 2018)</ref> (the previous best architecture on CIFAR-10). Notably, AmoebaNet-B uses 34.9M parameters while our model only uses 5.7M parameters which is 6× fewer than AmoebaNet-B. Furthermore, compared with PathLevel EAS <ref type="bibr" target="#b3">(Cai et al., 2018b</ref>) that also explores the tree-structured architecture space, both Proxyless-G and Proxyless-R achieves similar or lower test error rate results with half fewer parameters. The strong empirical results of our ProxylessNAS demonstrate the benefits of directly exploring a large architecture space instead of repeatedly stacking the same block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTS ON IMAGENET</head><p>For ImageNet experiments, we focus on learning efficient CNN architectures <ref type="bibr" target="#b19">(Iandola et al., 2016;</ref><ref type="bibr" target="#b15">Howard et al., 2017;</ref><ref type="bibr" target="#b29">Sandler et al., 2018;</ref> that have not only high accuracy but also low latency on specific hardware platforms. Therefore, it is a multi-objective NAS task <ref type="bibr" target="#b16">(Hsu et al., 2018;</ref><ref type="bibr" target="#b6">Dong et al., 2018;</ref><ref type="bibr" target="#b8">Elsken et al., 2018a;</ref><ref type="bibr" target="#b14">He et al., 2018;</ref>, where one of the objectives is non-differentiable (i.e. latency). We use three different hardware platforms, including mobile phone, GPU and CPU, in our experiments. The GPU latency is measured on V100 GPU with a batch size of 8 (single batch makes GPU severely under-utilized). The CPU latency is measured under batch size 1 on a server with two 2.40GHz Intel(R) Xeon(R)</p><p>Published as a conference paper at ICLR 2019   Additionally, on mobile phone, we use the latency prediction model (Appendix B) during architecture search. As illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>, we observe a strong correlation between the predicted latency and real measured latency on the test set, suggesting that the latency prediction model can be used to replace the expensive mobile farm infrastructure  with little error introduced.</p><p>Architecture Space. We use MobileNetV2 <ref type="bibr" target="#b29">(Sandler et al., 2018)</ref> as the backbone to build the architecture space. Specifically, rather than repeating the same mobile inverted bottleneck convolution (MBConv), we allow a set of MBConv layers with various kernel sizes {3, 5, 7} and expansion ratios {3, 6}. To enable a direct trade-off between width and depth, we initiate a deeper over-parameterized network and allow a block with the residual connection to be skipped by adding the zero operation to the candidate set of its mixed operation. In this way, with a limited latency budget, the network can either choose to be shallower and wider by skipping more blocks and using larger MBConv layers or choose to be deeper and thinner by keeping more blocks and using smaller MBConv layers.</p><p>Training Details. We randomly sample 50,000 images from the training set as a validation set during the architecture search. The settings for updating architecture parameters are the same as CIFAR-10 experiments except the initial learning rate is 0.001. The over-parameterized network is trained on the remaining training images with batch size 256.</p><p>Published as a conference paper at ICLR 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Top-1 Top-5 GPU latency MobileNetV2 <ref type="bibr" target="#b29">(Sandler et al., 2018)</ref> 72.0 91.0 6.1ms ShuffleNetV2 (1.5) <ref type="bibr" target="#b26">(Ma et al., 2018)</ref> 72.6 -7.3ms ResNet-34 <ref type="bibr" target="#b13">(He et al., 2016)</ref> 73.3 91.4 8.0ms NASNet-A  74.0 91.3 38.3ms DARTS <ref type="bibr" target="#b23">(Liu et al., 2018c)</ref> 73.1 91.0 -MnasNet  74.0 91.8 6.1ms Proxyless (GPU) 75.1 92.5 5.1ms <ref type="table">Table 3</ref>: ImageNet Accuracy (%) and GPU latency (Tesla V100) on ImageNet.</p><p>ImageNet Classification Results. We first apply our ProxylessNAS to learn specialized CNN models on the mobile phone. The summarized results are reported in <ref type="table" target="#tab_2">Table 2</ref>. Compared to Mo-bileNetV2, our model improves the top-1 accuracy by 2.6% while maintaining a similar latency on the mobile phone. Furthermore, by rescaling the width of the networks using a multiplier <ref type="bibr" target="#b29">(Sandler et al., 2018;</ref>, it is shown in <ref type="figure">Figure 4</ref> that our model consistently outperforms MobileNetV2 by a significant margin under all latency settings. Specifically, to achieve the same level of top-1 accuracy performance (i.e. around 74.6%), MobileNetV2 has 143ms latency while our model only needs 78ms (1.83× faster). While compared with MnasNet , our model can achieve 0.6% higher top-1 accuracy with slightly lower mobile latency. More importantly, we are much more resource efficient: the GPU-hour is 200× fewer than MnasNet <ref type="table" target="#tab_2">(Table 2)</ref>.</p><p>Additionally, we also observe that Proxyless-G has no incentive to choose computation-cheap operations if were not for the latency regularization loss. Its resulting architecture initially has 158ms latency on Pixel 1. After rescaling the network using the multiplier, its latency reduces to 83ms. However, this model can only achieve 71.8% top-1 accuracy on ImageNet, which is 2.4% lower than the result given by Proxyless-G with latency regularization loss. Therefore, we conclude that it is essential to take latency as a direct objective when learning efficient neural networks.</p><p>Besides the mobile phone, we also apply our ProxylessNAS to learn specialized CNN models on GPU and CPU. <ref type="table">Table 3</ref> reports the results on GPU, where we find that our ProxylessNAS can still achieve superior performances compared to both human-designed and automatically searched architectures. Specifically, compared to MobileNetV2 and MnasNet, our model improves the top-1 accuracy by 3.1% and 1.1% respectively while being 1.2× faster. <ref type="table">Table 4</ref> shows the summarized results of our searched models on three different platforms. An interesting observation is that models optimized for GPU do not run fast on CPU and mobile phone, vice versa. Therefore, it is essential to learn specialized neural networks for different hardware architectures to achieve the best efficiency on different hardware.</p><p>Specialized Models for Different Hardware. <ref type="figure">Figure 6</ref> demonstrates the detailed architectures of our searched CNN models on three hardware platforms: GPU/CPU/Mobile. We notice that the architecture shows different preferences when targeting different platforms: (i) The GPU model is shallower and wider, especially in early stages where the feature map has higher resolution; (ii) The GPU model prefers large MBConv operations (e.g. 7 × 7 MBConv6), while the CPU model would go for smaller MBConv operations. This is because GPU has much higher parallelism than CPU so it can take advantage of large MBConv operations. Another interesting observation is that our searched models on all platforms prefer larger MBConv operations in the first block within each stage where the feature map is downsampled. We suppose it might because larger MBConv operations are beneficial for the network to preserve more information when downsampling. Notably, such kind of patterns cannot be captured in previous NAS methods as they force the blocks to share the same structure <ref type="bibr" target="#b21">Liu et al., 2018a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduced ProxylessNAS that can directly learn neural network architectures on the target task and target hardware without any proxy. We also reduced the search cost (GPU-hours and GPU memory) of NAS to the same level of normal training using path binarization. Benefiting from the direct search, we achieve strong empirical results on CIFAR-10 and ImageNet. Furthermore, (2) Efficient CPU architecture found by ProxylessNAS.</p><p>(3) Efficient GPU architecture found by ProxylessNAS.</p><p>(a) Efficient GPU model found by ProxylessNAS. (1) Efficient mobile architecture found by ProxylessNAS.</p><p>(2) Efficient CPU architecture found by ProxylessNAS.</p><p>(3) Efficient GPU architecture found by ProxylessNAS.</p><p>(b) Efficient CPU model found by ProxylessNAS. (1) Efficient mobile architecture found by ProxylessNAS.</p><p>(2) Efficient CPU architecture found by ProxylessNAS.</p><p>(3) Efficient GPU architecture found by ProxylessNAS.</p><p>(c) Efficient mobile model found by ProxylessNAS. <ref type="figure">Figure 6</ref>: Efficient models optimized for different hardware. "MBConv3" and "MBConv6" denote mobile inverted bottleneck convolution layer with an expansion ratio of 3 and 6 respectively. Insights: GPU prefers shallow and wide model with early pooling; CPU prefers deep and narrow model with late pooling. Pooling layers prefer large and wide kernel. Early layers prefer small kernel. Late layers prefer large kernel.  <ref type="table">Table 4</ref>: Hardware prefers specialized models. Models optimized for GPU does not run fast on CPU and mobile phone, vice versa. ProxylessNAS provides an efficient solution to search a specialized neural network architecture for a target hardware architecture, while cutting down the search cost by 200× compared with state-of-the-arts <ref type="bibr" target="#b36">(Zoph &amp; Le, 2017;</ref>.</p><p>we allow specializing network architectures for different platforms by directly incorporating the measured hardware latency into optimization objectives. We compared the optimized models on CPU/GPU/mobile and raised the awareness of the needs of specializing neural network architecture for different hardware architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THE LIST OF CANDIDATE OPERATIONS USED ON CIFAR-10</head><p>We adopt the following 7 operations in our CIFAR-10 experiments: There are two reasons: (i) Slow. As suggested in TensorFlow-Lite, we need to average hundreds of runs to produce a precise measurement, approximately 20 seconds. This is far more slower than a single forward / backward execution. (ii) Expensive. A lot of mobile devices and software engineering work are required to build an automatic pipeline to gather the latency from a mobile farm. Instead of direct measurement, we build a model to estimate the latency. We need only 1 phone rather than a farm of phones, which has only 0.75ms latency RMSE. We use the latency model to search, and we use the measured latency to report the final model's latency.</p><p>We sampled 5k architectures from our candidate space, where 4k architectures are used to build the latency model and the rest are used for test. We measured the latency on Google Pixel 1 phone using TensorFlow-Lite. The features include (i) type of the operator (ii) input and output feature map size (iii) other attributes like kernel size, stride for convolution and expansion ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILS OF MNASNET'S SEARCH COST</head><p>Mnas  trains 8,000 mobile-sized models on ImageNet, each of which is trained for 5 epochs for learning architectures. If these models are trained on V100 GPUs, as done in our experiments, the search cost is roughly 40,000 GPU hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D IMPLEMENTAION OF THE GRADIENT-BASED ALGORITHM</head><p>A naive implementation of the gradient-based algorithm (see Eq. <ref type="formula">(4)</ref>) is calculating and storing o j (x) in the forward step to later compute ∂L/∂g j in the backward step:</p><formula xml:id="formula_17">∂L/∂g j = reduce sum(∇ y L • o j (x)),<label>(9)</label></formula><p>where ∇ y L denotes the gradient w.r.t. the output of the mixed operation y, "•" denotes the elementwise product, and "reduce sum(·)" denotes the sum of all elements.</p><p>Notice that o j (x) is only used for calculating ∂L/∂g j when j th path is not active (i.e. not involved in calculating y). So we do not need to actually allocate GPU memory to store o j (x). Instead, we can calculate o j (x) after getting ∇ y L in the backward step, use o j (x) to compute ∂L/∂g j following Eq. (9), then release the occupied GPU memory. In this way, without the approximation discussed in Section 3.2.1, we can reduce the GPU memory cost to the same level of training a compact model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f / Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i uS i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X Vm t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&lt;Figure 2 :</head><label>2</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; E[Latency] = ↵ ⇥ F (conv 3x3) ⇥ F (conv 5x5) ⇥ F (identity)+ ...... ⇣ ⇥ F (pool 3x3) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; Learning both weight parameters and binarized architecture parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>&lt;Figure 3 :</head><label>3</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f / Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i u S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L 2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g b J v a 5 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f / Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s Ai u S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L 2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g bJ v a 5 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f /Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i u S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F Jm o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g bJ v a 5 5 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l n t r w t q g r P O 1 V S k 1 e a e y N u f W f /Q = " &gt; A A A C N X i c f V D L S s N A F J 3 4 r P U V d e l m s A i u S i K C b o S i C C 5 c V L A P a E K Y T C f t 0 J k k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J m o W 2 4 o G B w z n n M v c e P 2 Z U K s t 6 M R Y W l 5 Z X V m t r 9 f W N z a 1 t c 2 e 3 K 6 N E Y N L B E Y t E 3 0 e S M B q S j q K K k X 4 s C O I + I z 1 / c l n 4 v X s i J I 3 C O 5 X G x O V o F N K A Y q S 0 5 J k 3 D k d q 7 P v Z V T 7 I S i 5 4 x p A i I U 7 z 3 I X n 0 J E J 9 z K a w 3 + S h a / T n t m w m l Y J O E / s i j R A h b Z n P j n D C C e c h A o z J O X A t m L l Z k g o i h n J 6 0 4 i S Y z w B I 3 I Q N M Q c S L d r L w 6 h 4 d a G c I g E v q F C p b q z 4 k M c S l T 7 u t k s a 2 c 9 Q r x L 2 + Q q O D M z W g Y J 8 V x 0 4 + C h E E V w a J C O K S C Y M V S T R A W V O 8 K 8 R g J h J U u u q 5 L s G d P n i f d 4 6 Z t N e 3 b k 0 b r o q q j B v b B A T g C N j g F L X A N 2 q A D M H g A z + A N v B u P x q v x Y X x O o w t G N b M H f s H 4 + g b J v a 5 5 &lt; / l a t e x i t &gt; Loss = Loss CE + 1 ||w|| 2 2 + 2 E[latency]&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8+ g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a an V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B im a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 6 / 3 1 7 h 5 G W 9 5 M 8 4 n 9 U 6 2 5 e 9 C 6 t B w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v M 7 E 2 w 6 l X t W 5 0 L P l 1 8 + g D N 1 T 4 k = " &gt; A A A C S n i c b V B N S x x B E O 3 Z a G J W T T b J 0 U v j I g j C M r M E k k t A I o K H H B R c F X b H o a a n V h t 7 P u i u 0 S y 9 8 / t y 8 e T N H + H F Q 0 L w Y s + 4 g t E U d P N 4 V Y 9 6 9 e J C S U O + f + 2 1 X s 3 N v 3 6 z 8 L a 9 u L T 8 7 n 3 n w 8 c D k 5 d a 4 E D k K t d H M R h U M s M B S V J 4 V G i E N F Z 4 G J 9 t 1 f 3 D c 9 R G 5 t k + T Q o M U z j J 5 F g K I E d F H f i R G / O N 1 3 9 k t 7 Y r v s F H y s k T i A I + n V 5 M p 1 H / 2 P Y b n v A n N R u t x q S y j 3 N 9 P k q B T u P Y b l f D B u r U K i D M x K Q K q 6 j T 9 X t + U / w l C G a g y 2 a 1 G 3 W u R k k u y h Q z E g q M G Q Z + Q a E F T V I o r N q j 0 m A B 4 g x O c O h g B i m a 0 D b G K r 7 m m I S P c + 1 e R r x h n y o s p M Z M 0 t h N 1 k 7 N 8 1 5 N / q 8 3 L G n 8 N b Q y K 8 r 6 s I d F 4 1 J x y n m d K 0 + k R k F q 4 g A I L Z 1 X L k 5 B g y C X f t u F E D w / + S U 4 6 P c C v x f s f e 5 u f p / F s c B W 2 C p b Z w H 7 w j b Z D t t l A y b Y L 3 b D f r M / 3 q V 3 t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L m 2 s 0 3 u P U Y j M A P u 2 2 Q v 9 N r i X Y R M = " &gt; A A A D I H i c f V J N b 9 Q w E H X C V w l f W z h y s V i B i p B W S U u B C 1 I F A n H g U C S 2 r b S O V o 4 z m 7 X q 2 J H t V B u i 8 E + 4 8 F e 4 c A A h u M G v w U n D V 7 v L S J a e Z u Z 5 3 j w 7 K Q Q 3 N g y / e / 6 Z s + f O X 1 i 7 G F y 6 f O X q t c H 6 9 T 2 j S s 1 g z J R Q + i C h B g S X M L b c C j g o N N A 8 E b C f H D 5 t 6 / t H o A 1 X 8 r W t C o h z m k k + 4 4 x a l 5 q u e w 9 I A h m X N R U 8 k 5 A 2 A c m p n S d J / a y Z d F D n 9 U t q Q b K q i f F j f O c t s b C w 3 e Q 6 E S U 0 N a G i m N M G E 8 t z M P j 5 x i 8 e U / K I T L c W W 8 1 d f A 8 T E i w j J 2 A d d x V 5 e 7 H 9 P 7 L h W b 5 s M k 9 B O j e q P 9 x R F 6 v u e b N C R K G U 6 D c I C M j 0 t 0 3 T w T A c h V 3 g 0 y D q w R D 1 s T s d f C O p Y m X u d D F B j Z l E Y W H j m m r L m Q B n e 2 m g o O y Q Z j B x U F I n J K 4 7 k Q 2 + 7 T I p n i n t j r S 4 y / 7 N q G l u T J U n r r M V b k 7 W 2 u S y 2 q S 0 s 0 d x z W V R t i 9 8 P G h W C m w V b n 8 L T r k G Z k X l A G W a O 6 2 Y z a m m z L o / F T g T o p M r n w Z 7 m 6 M o H E W v 7 g 9 3 n v R 2 r K G b 6 B b a Q B F 6 i H b Q C 7 S L x o h 5 7 7 w P 3 i f v s / / e / + h / 8 b 8 e t / p e z 7 m B / g n / x 0 9 Y V g H C &lt; / l a t e x i t &gt; Making latency differentiable by introducing latency regularization loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FigureFigure 5 :</head><label>5</label><figDesc>Our mobile latency model is close to y = x. The latency RMSE is 0.75ms. CPU E5-2640 v4. The mobile latency is measured on Google Pixel 1 phone with a batch size of 1. For Proxyless-R, we use ACC(m) × [LAT (m)/T ] w as the optimization goal, where ACC(m) denotes the accuracy of model m, LAT (m) denotes the latency of m, T is the target latency and w is a hyperparameter for controlling the trade-off between accuracy and latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: ProxylessNAS achieves state-of-the art accuracy (%) on ImageNet (under mobile latency</cell></row><row><cell cols="3">constraint ≤ 80ms) with 200× less search cost in GPU hours. "LL" indicates latency regularization loss. Details of MnasNet's search cost are provided in appendix C.</cell></row><row><cell></cell><cell></cell><cell>76.7</cell></row><row><cell>74.6</cell><cell>1.83x faster</cell><cell>74.7</cell></row><row><cell>72.0</cell><cell></cell><cell></cell></row><row><cell>68.2</cell><cell></cell><cell></cell></row><row><cell>65.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>• 3 × 3 dilated depthwise-separable convolution • Identity • 3 × 3 depthwise-separable convolution • 5 × 5 depthwise-separable convolution • 7 × 7 depthwise-separable convolution • 3 × 3 average pooling • 3 × 3 max pooling B MOBILE LATENCY PREDICTIONMeasuring the latency on-device is accurate but not ideal for scalable neural architecture search.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In Appendix D, we provide another solution to this issue that does not require the approximation. 3 Details of the latency prediction model are provided in Appendix B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The list of operations in the candidate set is provided in the appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank MIT Quest for Intelligence, MIT-IBM Watson AI lab, SenseTime, Xilinx, Snap Research for supporting this work. We also thank AWS Cloud Credits for Research Program providing us the cloud computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Path-level network transformation for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dpp-net: Device-aware progressive search for pareto-optimal neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-Chieh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-objective architecture search for cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09081</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Huan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10332</idno>
		<title level="m">Monas: Multi-objective neural architecture search using reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushotham</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debo</forename><surname>Dutta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06744</idno>
		<title level="m">Neural architecture construction using envelopenets</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07233</idno>
		<title level="m">Neural architecture optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platformaware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Haq: Hardware-aware automated quantization. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02375</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Shakedrop regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparsely aggregated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
