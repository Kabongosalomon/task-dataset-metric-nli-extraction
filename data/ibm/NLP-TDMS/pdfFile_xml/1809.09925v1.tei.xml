<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Every Node Counts: Self-Ensembling Graph Convolutional Networks for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">JD.COM Silicon Valley Research Center</orgName>
								<address>
									<settlement>Big Data Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Every Node Counts: Self-Ensembling Graph Convolutional Networks for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional network (GCN) provides a powerful means for graph-based semi-supervised tasks. However, as a localized first-order approximation of spectral graph convolution, the classic GCN can not take full advantage of unlabeled data, especially when the unlabeled node is far from labeled ones. To capitalize on the information from unlabeled nodes to boost the training for GCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which marries GCN with Mean Teacher -another powerful model in semi-supervised learning. SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly classify the labeled nodes, but also tries to be consistent with the teacher on unlabeled nodes in more challenging situations, such as a high dropout rate and graph collapse. As a teacher, it averages the student model weights and generates more accurate predictions to lead the student. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for backpropagating effective gradients to train GCN. In three article classification tasks, i.e. Citeseer, Cora and Pubmed, we validate that the proposed method matches the state of the arts in the classification accuracy.</p><p>arXiv:1809.09925v1 [cs.LG]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semi-supervised learning (SSL) aims to build a better classifier, by utilizing huge amounts of unlabeled data which is readily accessible, together with a limited number of labeled data. Such line of work is of great significance because it achieves a high accuracy while requiring less human effort for data annotation. Recently, SSL has gained considerable attention when applied to deep learning-based methods, which are well known for their high demand on sizable and reliable labeled samples. Through distilling knowledge from unlabeled data, SSL boosts the deep learning-based methods to a new level in many tasks, e.g., speech recognition <ref type="bibr" target="#b2">(Dai and Le 2015)</ref>, image segmentation <ref type="bibr" target="#b13">(Papandreou et al. 2015)</ref> and video understanding <ref type="bibr" target="#b1">(Caelles et al. 2017)</ref>. Inspired by the great success of SSL on regular Euclideanbased data such as speech, images, or video, a surge of recent approaches seek to apply SSL to data in a more general form -graph. The motivation is natural: in many real prob-Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. , where student model operates under a perturbed setting and tries to be consistent with the predictions of teacher model. Our method marries (a) and (b), which utilizes both supervised classification loss and unsupervised consistency loss to train GCN. Such framework enables us to explore more unlabeled knowledge to boost the classification accuracy under the semi-supervised setting. lems, the data samples are on irregular grid or more generally in non-Euclidean domains, e.g. point cloud , chemical molecules ) and social networks <ref type="bibr" target="#b14">(Rahimi, Cohn, and Baldwin 2018)</ref>. Instead of regularly shaped tensors, those data are better to be structured as graph, which is capable of handling varying neighborhood vertex connectivity. Similar to the original goal on regular data, SSL on graph-structured data aims to classify the all the nodes in a graph using a small subset of labeled nodes and large amounts of unlabeled nodes. The recently developed graph convolutional neural network (GCNN) <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> and the following graph convolutional network (GCN) <ref type="bibr" target="#b7">(Kipf and Welling 2017)</ref> are successful attempts along this line, which general-ize the powerful convolutional neural network (CNN) in Euclidean data to modeling graph-structured data. This line of work capitalizes on the relation between nodes and enables the features to propagate between neighboring vertices. During the training, the supervised loss acts upon the confluent features of labeled node and then distribute the gradient information across other unlabeled adjacent nodes. Such mechanism makes the features of both labeled and unlabeled vertices in the same cluster similar, thus largely easing the classification task.</p><p>Although it has made great progresses, GCN fails to take full advantage of unlabeled data, especially when the unlabeled node is far away from labeled ones. This is due to a K-layer GCN only captures node information up to K-hop neighborhood, which cannot effectively propagate the information to the entire graph. Taking the two-layers GCN as an example. In such network, a vertex v i would directly aggregate and filter features in its 2-hop neighborhood N vi . Considering that an adjacent vertex v j ∈ N vi also aggregates features from its own 2-hop neighborhood N vj , the v i can indirectly discover farther vertices beyond N vi . However, such indirect link is de facto negligible which is represented with a tiny kernel weight. Consequently, any unlabeled vertex with shortest path distance &gt; 2 from labeled ones in graph would gain very limited attention and are prone to be underutilized during the training. A deeper network with more graph convolutional layers may help to discover information in such remote nodes. However, as mentioned in <ref type="bibr" target="#b10">(Li, Han, and Wu 2018)</ref>, GCN is essentially a special form of Laplacian smoothing. Therefore, a deeper GCN may bring potential concerns of over-smoothing, i.e. the output features may be over-smoothed and the vertices from different clusters may become indistinguishable. In summary, the utilization of unlabeled information beyond the K-hop neighborhood remains an open problem.</p><p>In this paper, we propose a new architecture that can discover much more information within unlabeled vertices and learn from the global graph topology. A key innovation is to marry Mean Teacher framework <ref type="bibr" target="#b17">(Tarvainen and Valpola 2017)</ref> into the classic GCN. Instead of merely supervising the propagated features in labeled nodes, we directly give chances to unlabeled nodes to "speak up for themselves". Specifically, SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly predict the labeled nodes, but also tries to be consistent with teacher output on unlabeled nodes in more challenging settings, such as high dropout rates and graph collapse. As a teacher, it updates itself by averaging the student model weights. Since the teacher model operates under better settings such as low dropout rates and lossless graph, it generates more accurate predictions on both labeled and unlabeled nodes, thus being able to lead the student to learn in the next epoch. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for back-propagating effective gradient to train GCN.</p><p>To the best of our knowledge, this is the first time to introduce Mean Teacher strategy in the GCN design. Precisely, the main contributions of this work are summarized below.</p><p>• By proposing to combine Mean Teacher with classic GCN, we emphasize the importance of exploitation of unlabeled nodes in graph-structured data classification.</p><p>• Analogy to the noise added to student model in regular data, we successfully adapt Mean Teacher to graph-based data by designing new perturbation strategies for student model. • Our results are on par with the state-of-the-art methods on three node classification benchmarks in terms of accuracy, i.e. Citeseer (69.9% → 73.4%), Core (80.4% → 83.5%) and Pubmed (78.6% → 78.9%). The rest of this paper is organized as follows. Section 2 discusses related work for GCN and Mean Teacher that provide the foundation for this paper. Then we propose the SEGCN model in Section 3. Section 4 presents an experimental study in which we compare our method with baseline and state-of-the-art results of benchmark datasets. Finally, we conclude with our contributions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head><p>We first provide a brief introduction to the required background. Then we review SSL with GCN and Mean Teacher, which provide fundamental theories for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spectral Graph Convolution</head><p>There are two means to define convolution on graph, either from a spatial approach or from a spectral approach. This paper focuses on the latter. Based on the theory of Chung et al. <ref type="bibr" target="#b1">(Chung and Graham 1997)</ref>, spectral GCNNs construct the convolution kernel on spectrum domain. They represent both the filter and the signal with the Fourier basis and multiply them, then transforms the result back into the discrete domain. However this model requires explicitly computing the eigenvectors of Laplacian matrix L, which is impractical for real large graphs. To circumvent this problem, it was suggested in <ref type="bibr" target="#b5">(Hammond, Vandergheynst, and Gribonval 2009</ref>) that approximate the spectral filter g θ with Chebyshev polynomials up to Kth order:</p><formula xml:id="formula_0">g θ x ≈ K k=0 θ k T k (L)x ,<label>(1)</label></formula><p>where θ ∈ R K is a vector of Chebyshev coefficients and T k (.) denotes the Kth item of Chebyshev polynomials.L = 2 λmax L−I N , where I N is a N order diagonal matrix and λ max denotes the largest eigenvalue of L.</p><p>Recent proposed GCN (Kipf and Welling 2017) further simplifies this model by limiting K = 1 and approximating λ max ≈ 2. Given the adjacent matrix A and the input feature X, the output of a single convolutional layer Z can be represented as</p><formula xml:id="formula_1">Z =D − 1 2ÃD − 1 2 XΘ ,<label>(2)</label></formula><p>whereÃ = A + I N ,D ii = jÃ ij and Θ denotes the trainable model parameters. Specifically, a two-layer GCN model can be defined as</p><formula xml:id="formula_2">Z = f (X, A) = softmax Â ReLU Â XΘ (0) Θ (1) .<label>(3)</label></formula><p>whereÂ =D − 1 2ÃD − 1 2 . This two-layer model forms the backbone of our SEGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Learning with GCNs</head><p>The above GCN model in Eq. 3 can be expediently used for SSL. However, as the analysis in Sec. 1, this method is limited to its small receptive field within few-hops neighborhood. Several recent methods are proposed to overcome such limitation <ref type="bibr" target="#b18">(Weston et al. 2012;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al. 2018;</ref><ref type="bibr" target="#b17">Verma and Boyer 2018;</ref><ref type="bibr" target="#b12">Monti et al. 2017</ref>). Among these attempts, Random Walk is proven to be very effective to discover more remote cues (Grover and Leskovec 2016; Perozzi, Al-Rfou, and Skiena 2014). Moreover, attention mechanisms are introduced to emphasize the useful unlabeled information <ref type="bibr" target="#b17">(Thekumparampil et al. 2018;</ref><ref type="bibr" target="#b17">Velikovi et al. 2018;</ref><ref type="bibr" target="#b15">Shang et al. 2018</ref>). To enable SSL on an extreme large graph, Liao et al. <ref type="bibr" target="#b11">(Liao et al. 2018</ref>) extendse GCN by graph partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semi-supervised Learning with Mean Teacher</head><p>Mean Teacher <ref type="bibr" target="#b17">(Tarvainen and Valpola 2017)</ref> is one of the self-ensembling methods. The idea of a teacher model training a student is related to model compression <ref type="bibr" target="#b1">(Bucilu, Caruana, and Niculescu-Mizil 2006)</ref> and distillation <ref type="bibr" target="#b6">(Hinton, Vinyals, and Dean 2015)</ref>. Apart from other variants (Bachman, Alsharif, and Precup 2014; Laine and Aila 2016), Mean Teacher averages model weights instead of predictions and achieves excellent results in SSL. Other lines of work in self-ensembling focus on designing effective perturbation, including <ref type="bibr">(Gastaldi 2017;</ref><ref type="bibr" target="#b6">Huang et al. 2016;</ref><ref type="bibr" target="#b17">Wan et al. 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Combining GCN with Mean Teacher</head><p>Graph Convolutional Network and Mean Teacher model are individually powerful. However, as we present in early sections, the former explores the unlabeled information by halves while the latter has only shown its ability on Euclidean data. In this section, we propose a new framework called SEGCN that combines the merits of both models while overcomes their limitations. Particularly, SEGCN contains a student model f (Θ s ) and a teacher model f (Θ t ), where Θ s and Θ t are the weights of the respective models. Given the labeled data</p><formula xml:id="formula_3">D L = {(x L i , y L i )} N L i=1 and unla- beled data D U = {x U i } N U i=1</formula><p>, we first construct a normalized adjacent matrix A upon these data according to their pairwise relations. Specific to our article classification task, the pairwise relations consist in a citation from one article to another.</p><p>We formulate the overall loss of SEGCN from two aspects. On the one hand, the student should learn to minimize the cross-entropy loss under the supervision of labeled data in a noise-free environment. In original complete graph, the propagation paths between vertices are abundant, e.g., the vertex 5 can propagate its feature to the labeled vertex 1 via four different routes. (b) On the premise of no isolated node appears, P A (.) randomly cuts off the redundant edges in order to partially block the information transmission between vertices. Our perturbation strategy enables the student to generate worse predictions while keeping the intrinsic BoW features unchanged.</p><formula xml:id="formula_4">CE (Θ s , A, x, y) = − C c=1 y c log f (A, x; Θ s ) c ,<label>(4)</label></formula><p>where x denotes a labeled sample. The f (A, x; Θ s ) c denotes the predicted probability from the student classifier on the class c. The y c denotes the ground truth probability of the class c.</p><p>On the other hand, the student classifier should be consistent with teacher's predictions when operates under small perturbations. In classic Mean Teacher on Euclidean-based data, the perturbation is usually added to original inputs x to construct student inputs. Differently, the input fed in SEGCN are the Bag-of-Words (BoW) features distilled from articles. As a result, the traditional data augmentation strategies such as scaling, inversion or distortion are not available in our task. Instead, we innovatively construct such perturbation on both the graph structure and the model of student network. For the perturbation on graph structure A, we generate a "collapsed graph" by A = P A (A) where P A (.) is our perturbation function on the normalized adjacent matrix A. Similarly, we add noise to the original student model f (.) by f (.) = P f (f (.)) where P f is the perturbation function on model. These two perturbation functions will be detailed discussed in Sec. 3.2. Given the disturbed setting {A , f (.)} for student and the original setting {A, f (.)} for teacher, the unsupervised consistency loss would penalizes the difference between the student's predicted probabilities f (A , x; Θ s ) and the teacher's f (A, x; Θ t ). In our paper, we formulate this loss as KL divergence.</p><formula xml:id="formula_5">cons (Θ t , Θ s , A, A , x) = KL(f (A, x; Θ t )||f (A , x; Θ s )) .<label>(5)</label></formula><p>With the above loss terms in Eq. 4 and Eq. 5, the overall loss function of our approach can be written as  Since the labeled feature is mainly aggregated from adjacent vertices, the near samples can be well classified. However, a remote unlabeled node (red circle) is prone to be ignored by assigning a very small GCN kernel weight. Consequently, the vanilla model may be under-constrained and cannot be well adapted to these distant samples. (b) SEGCN pays more attentions on the unlabeled nodes than vanilla GCN. Via a mutual-promoting process leading by the consistency losses (e.g., the inconsistency between the solid/dotted red circles), SEGCN gives the chances to the unlabeled nodes as well to backpropagate effective gradient to train the model. Once the mutual-promoting process converges, even a remote unlabeled node can be well classified.</p><formula xml:id="formula_6">L(Θ t , Θ s , A, A , x, y) = (x,y)∈D L CE LSup +λ x∈D L ∪D U cons LUnsup ,<label>(6)</label></formula><p>where the parameter λ &gt; 0 controls the relative importance of the consistency term in the overall loss. SEGCN is trained end-to-end to minimize the overall loss. In such scheme, the student can not only learn to distill knowledge from the labeled data under the supervised loss, but also pay much attention to the unlabeled vertices in order to come after the teacher. On the other side, through averaging the latest parameters in the student, the teacher is able to evolve itself since the gradient descent direction leading by consistency loss also applies to the update for the teacher model. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, such a joint evolution between the dual GCNs paves the way for more thoroughly exploration on unlabeled information, which is otherwise not possible if solved alone in their traditional frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Perturbation in Student Model</head><p>This subsection aims to design effective perturbations on graph-based data. We do not follow the traditional ways of adding noise to raw inputs <ref type="bibr" target="#b4">(French, Mackiewicz, and Fisher 2017)</ref>, which is popularly used on images and videos. The reason is two-fold. Firstly, the traditional data augmentation strategies on Euclidean data such as scaling, inversion or distortion are not suitable in article classification task. Secondly, some keywords are closely related to the article category. If we directly modify the BoW feature vectors, the intrinsic cues for classifying an article may be lost. As a result, we instead propose two perturbation strategies on the graph structure (represented by adjacent matrix A) and GCN model respectively.</p><p>The perturbation operation on graph structure, which is denoted as P A (.), is shown in <ref type="figure" target="#fig_1">Fig.2</ref>. On the premise of no isolated node appears, P A (.) randomly cuts off the redun-dant edges in order to partially block the information transmission between vertices. The decreased number of connection makes the feature aggregation from neighbor vertices becoming even harder, thus enabling the student to generate inconsistent predictions with the teacher.</p><p>Another function P f (.) denotes the perturbation on model itself. We implement P f (.) by appending a dropout layer (Srivastava et al. 2014) to each model. The dropout layer can drop different nodes in each time and obtaining two different output vectors from student and teacher. To force the student to operate in a harder environment, we give a higher dropout rate to the student than the teacher. In a nutshell, the two specifically designed perturbations enable the student to generate inconsistent predictions while keeping the intrinsic BoW features unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-training in Mean Teacher</head><p>Self-training is an effective scheme where the model is bootstrapped with additional labeled data obtained from its own highly confident predictions. This process is repeated until some termination conditions are reached. Although these methods are heuristic and have achieved much progress in semi-supervised learning, seeking for an appropriate threshold to select those high confident predictions is by no means easy. A strict threshold may reject most of the right predictions, thus leading to a degenerated self-training scheme. On the contrary, a loose threshold may bring about a poorer classifier since the wrongly imported pseudo labels can reinforce poor predictions. Traditionally, this threshold is set empirically and any prediction with a softmax score larger than the threshold will be selected as an extended labeled sample.</p><p>Different from the traditional methods which generate Add high confident pseudo labels to (X L , Y L ) 10: Until L Overall converges.</p><p>predictions merely from a single classifier, SEGCN is born with the dual classifiers and able to make predictions from two views, i.e. the student view and the teacher view. This property motivates us to combine the different views aiming to select more robust pseudo labels. Particularly, we regard a prediction as a high confident result only if both student and teacher give larger softmax scores than the threshold t on the same class. Otherwise, if the two classifiers give inconsistent predictions, it indicates a probably incorrect prediction which will be excluded from the additional labeled data in this epoch. In our experiment, we verify that combining teacher and student can achieve a better self-training performance compared with the single model-based variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training SEGCN</head><p>The training of SEGCN framework is essentially a mutual-promoting process. We detail the training step in Algorithm 1. The student weight Θ s is initialized from scratch while the teacher weight Θ t is initialized by copying from Θ s . Given the labeled BoW features (X L , Y L ) and the unlabeled ones X U , the student is firstly trained to minimize the supervised cross-entropy loss on X L using a complete graph A and a low dropout model f (.). Then the student is forced to be consistent with the teacher predictions on all vertices X L X U using a collapsed graph A and a high dropout model f (.). Finally, the teacher updates its own model weights from the latest student model. In each iteration, the student network is optimized using ADAM (Kingma and Ba 2014), while the weights of the teacher network are updated with an exponential moving average of those of the latest student, which is formulated as Eq. 7.</p><formula xml:id="formula_7">Θ e+1 t = αΘ e t + (1 − α)Θ e+1 s ,<label>(7)</label></formula><p>where α is a smoothing coefficient hyperparameter and e denotes the current epoch.</p><p>Since the student and the teacher are both inaccurate in early epochs, the mutual learning between each other could  <ref type="table" target="#tab_2">Node5  35  42  7  1433  Citeseer  3327  4732  6  3703  Cora  2708  5429  7  1433  Pubmed  19717  44338  3  500</ref> lead to unstable predictions, which may deviate from our original intention. Therefore, traditional solution utilizes a two-stage training scheme. Namely, in first stage the student is merely trained on labeled data until it converges. Then the mutual learning is started up in the second stage. Differently, to enable an end-to-end scheme, we approximate the classic two-stage training process using a hyperparameter trick. We initialize small λ and α in early epochs and progressively increase them during the training. In such one-stage scheme we can avoid the unwanted knowledge transmission between the immature partners in early epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We experiment on three public available citation graph datasets: Citeseer, Cora and Pubmed. <ref type="table" target="#tab_2">Table 1</ref> summarizes dataset statistics. A citation graph dataset consists of documents as nodes and citation links as directed edges. Each node has a human annotated topic from a finite set of classes and a feature vector. For Citeseer and Cora, the feature vector has binary entries indicating the presence/absence of the corresponding word from a dictionary. For the Pubmed dataset, the feature vector has real-values entries indicating Term Frequency-Inverse Document Frequency (TF-IDF) of the corresponding word from a dictionary. Although the networks are directed, we use undirected versions of the graphs for all experiments, which is common in all baseline approaches.</p><p>Besides the three benchmarks above, we construct a new toy dataset called Node5 in order to clearly showcase the effect of SEGCN on far-away nodes. It derives from a subset of Cora. For each class in Cora, we select 5 (1 labeled and 4 unlabeled) samples and link them as a complete graph structure presented in <ref type="figure" target="#fig_1">Fig. 2</ref>. The experiment on this toy dataset aims to verify the phenomenon we present in <ref type="figure" target="#fig_2">Fig. 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We use PyTorch for implementation. we train both baseline and SEGCN as two-layer networks described in GCN <ref type="bibr" target="#b7">(Kipf and Welling 2017)</ref>, where the first layer outputs 16 dimensions per node and the second layer outputs the number of classes. For the student model, we use ADAM optimizer (Kingma and Ba 2014) with β1 = 0.9, β2 = 0.999, weightdecay = 0.0005 and dropoutrate = 0.5. While for the teacher, no dropout is applied. We fix the learning rate to 0.01 and train SEGCN for 1,000 epochs. As mentioned in Sec. 3.4, hyperparameter λ in Eq. 6 gradually increases from 0 to 2 while α in Eq. 7 increases from 0 to 0.999 in our best model. To import more pseudo label candidates when  <ref type="bibr" target="#b19">(Yang et al. 2016)</ref> 64.7 75.7 77.2 ---GCN <ref type="bibr">(Kipf et al. 2016)</ref> 70.3 81.5 79.0 67.9 ± 0.5 80.1 ± 0.5 78.9 ± 0.7 Graph-CNN  -76.3 ----MoNet <ref type="bibr" target="#b12">(Monti et al. 2017)</ref> -81.7 ± 0.5 78.8 ± 0.3 ---Bootstrap <ref type="bibr" target="#b1">(Buchnik et al. 2017)</ref> 53.6 78.4 78.8 50.3 78.2 75.6 FeaStNet <ref type="bibr" target="#b17">(Verma et al. 2018)</ref> -81.6 79.0 ---GPNN <ref type="bibr" target="#b11">(Liao et al. 2018)</ref> 69.7 81.8 79.3 68.6 ± 1.7 79.9 ± 2.4 76.1 ± 2.0 N-GCN <ref type="bibr" target="#b0">(Abu-El-Haija et al. 2018)</ref> 71.0 81.8 79.4 ---F-GCN <ref type="bibr" target="#b17">(Vijayan et al. 2018)</ref> 72.3 79.0 ----GAT <ref type="bibr" target="#b17">(Velikovi et al. 2018)</ref> 72.5 ± 0.7 83.0 ± 0.7 79.0 ± 0.3 ---GCN * 69.9 80.4 78.6 66.8 ± 0.7 79.6 ± 0.6 78.3 ± 0.7 SEGCN 73.4 ± 0.7 83.5 ± 0.4 78.9 ± 0.7 69.0 ± 0.9 80.8 ± 1.0 78.0 ± 1.4</p><p>our model is stable, we gradually decrease the self-training threshold t from 0.9 to 0.7. In all experiments, we utilize 500 samples as a validation set and evaluate prediction accuracy on a test set of 1,000 examples. These validation samples are used for capturing the model parameters at peak validation accuracy to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative Study</head><p>Fixed splits. In the first experiment, we use the fixed data splits from the work of Yang et al. <ref type="bibr" target="#b19">(Yang, Cohen, and Salakhudinov 2016)</ref> as it is the standard benchmark data splits in literatures. Specifically, these experiments are run on the same fixed split of 20 labeled nodes for each class.</p><p>We present the classification accuracy on the mentioned three benchmarks in <ref type="table" target="#tab_3">Table 2</ref> with comparisons to our baseline as well as the state-of-the-art semi-supervised classification methods. Except our own implemented baseline GCN model, the accuracy of the other comparative methods are all taken from existing literature. On the one hand, we observe that SEGCN can significantly outperforms baseline GCN method, which brings 3.5%, 3.1% and 0.3% improvement on Citeseer, Cora and Pubmed respectively. It implies that Mean Teacher can actually boost the classifier training. On the other hand, we compare SEGCN with the state-of-the-art methods including Deepwalk <ref type="bibr" target="#b14">(Perozzi, Al-Rfou, and Skiena 2014)</ref>, node2vec (Grover and Leskovec 2016), DCNN (Atwood and Towsley 2016), Planetoid <ref type="bibr" target="#b19">(Yang, Cohen, and Salakhudinov 2016)</ref>, Monet <ref type="bibr" target="#b12">(Monti et al. 2017)</ref>, Bootstrap <ref type="bibr" target="#b1">(Buchnik and Cohen 2017)</ref>, Graph-CNN <ref type="bibr">(Such et al. 2017), FeaSt-Net (Verma and</ref><ref type="bibr" target="#b17">Boyer 2018)</ref>, GPNN <ref type="bibr" target="#b11">(Liao et al. 2018</ref><ref type="bibr">), N-GCN (Abu-El-Haija et al. 2018</ref><ref type="bibr">), F-GCN (Vijayan et al. 2018</ref> and <ref type="bibr">GAT (Velikovi et al. 2018)</ref>. As it can be seen SEGCN performs best on Citeseer and Cora which yields new state-of-the-art accuracies, while falling short by only 0.5% from the best on the Pubmed dataset. The great performance not only relies on the high-performance baseline GCN method, but also due to the proposed self-ensembling strategy applied in the training scheme.</p><p>Random splits. Next, following the setting of GCN (Kipf and Welling 2017) , we run experiments keeping the same size in labeled, validation, and test sets as in fixed splits, but now selecting those nodes uniformly at random. This, along with the fact that different topics have different numbers of nodes in it, means that the labels might not be spread evenly across the topics. For 20 such randomly drawn dataset splits, the average accuracy is shown in <ref type="table" target="#tab_3">Table 2</ref> with the standard error. As we do not force an equal number of labeled data for each class, we observe that the performance degrades for all methods compared to fixed splits except Deepwalk <ref type="bibr" target="#b14">(Perozzi, Al-Rfou, and Skiena 2014)</ref>. Besides, SEGCN achieves best results among the state-of-the-art methods on Citeseer and Cora, yielding 69.0% and 80.8% respectively in accuracy. We also note that the variances of the accuracies become larger and the performance falls short on Pubmed than baseline. We will discuss this observation in next subsection.</p><p>More labeled samples on Pubmed. We note that the improvement is relatively lower on Pubmed than that on Citeseer and Core. We suspect that the reason is due to the different label rates in the three benchmarks: the Pubmed dataset is relative large and the labeled rate is very low when we only select 20 training samples from each class. Consequently, the labeled nodes can only reach extremely limited scope in graph and can not give stable initial gradient directions to unlabeled vertices for the latter mutual-promoting process. Particularly, we observe that a few very low values appears under the random splits settings. These failure cases significantly decrease the average value and increase the variance. To clarify our hypothesis, we compare SEGCN with the state-of-the-art methods on Pubmed with more labeled samples over range {50, 100, 200}. As it can be seen SEGCN outperforms other methods in all cases with higher accuracies and lower variances. These results validate our hypothesis since a relative large labeled set can create stable  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To assess the importance of various aspects of the model, we run experiments on Cora under the setting of fixed splits, deactivating one or a few modules at a time while keeping the others activated. <ref type="table" target="#tab_5">Table 4</ref> reports the classification accuracies under different ablations. To begin with, we test the two proposed perturbations P A (.) and P f (.) respectively, comparing with randomly erasing (RE) (Zhong et al. 2017) the raw input features X. We observe that the direct perturbation on X would hurt the final accuracy, dropping 0.3% from the baseline and bringing about larger variance. While our proposed perturbations are helpful in improving the accuracy and P A is more effective than P f . When combining the P A and P f , SEGCN yields 82.4% accuracy, which is higher than any single perturbation. It implies that P A and P f are complimentary and can lead the student to learn information from teacher. Then we test the self-training strategy with P A and P f fixed. We observe a significant improvement when combining student and teacher over a single model scheme, which implies that by using Mean Teacher we can produce more correct labeled sample candidates in self-training. Finally, SEGCN can yield new state-of-the-art accuracies when employing all the proposed modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Feature Distribution Analysis</head><p>In this section, We aim to further prove the effectiveness of SEGCN via feature distribution analysis. To this end, we map the high-dimensional features distilled from the second layer into a 2-D space with t-SNE. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the t-SNE visualization, in which (a) and (b) represent the results of vanilla GCN and SEGCN respectively on Node5. As depicted in (a) and (b), GCN maps the feature of the remote vertex far from the cluster center while SEGCN can map it near. As a result, SEGCN significantly eases the classification task. The distribution distinction can be also observed on Cora shown in (c) and (d). These visualization results further validate the effectiveness of the mutual learning we described in <ref type="figure" target="#fig_2">Fig. 3</ref>. To sum up, SEGCN can capitalize on the every nodes' information to train a better classifier. It is effective for those remote unlabeled vertices, which are hard to be classified with vanilla GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a self-ensembling framework called SEGCN for semi-supervised learning on graph-based data. Apart from the vanilla GCN, SEGCN can directly explore unlabeled information via the mutual learning between student and teacher, thus enabling every labeled and unlabeled nodes to backpropagate effective gradient to train the model. To the best of our knowledge, this is the first work that integrates the Mean Teacher model into GCN to boost the semi-supervised node classification. The extensive experiments on toy and public datasets show that SEGCN precedes the baseline model significantly and is on par with the state-of-the-art methods in tasks of article classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Best viewed in color.) (a) Vanilla GCN, where the each node gathers features from its neighbors in limited scope and only labeled nodes are supervised under the classification loss. (b) Mean Teacher model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Perturbation on graph structure. The arrows indicate the feature aggregation directions. A deeper blue arrow represents a larger kernel weight. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(Best viewed in color.) (a) Vanilla GCN pushes decision boundaries away from the labeled sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Feature distribution analysis on Node5 ((a), (b)) and Cora ((c), (d)). We map the high-dimensional features outputted from the second layer to a 2-D space with t-SNE. (a)&amp;(c) are the result of vanilla GCN while (b)&amp;(d) are ours. Different color indicates different class. The triangles in (a)&amp;(b) denote the remote nodes (same as the red circles inFig. 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>BoW features and labels (X L , Y L ) &amp; X U 2: Initialization : High-noise model f (.) = P f (f (.))</figDesc><table><row><cell cols="2">Algorithm 1 GCN in Mean Teacher framework</cell></row><row><cell cols="2">1: Input :</cell></row><row><cell></cell><cell>Two-layer GCN model f (.)</cell></row><row><cell></cell><cell>Normalized adjacent matrix A</cell></row><row><cell></cell><cell>Collapsed graph A = P A (A)</cell></row><row><cell></cell><cell>Model weights Θ t = Θ s from scratch</cell></row><row><cell cols="2">3: Repeat :</cell></row><row><cell>4:</cell><cell>L Sup ← Eq. 4</cell></row><row><cell>5:</cell><cell>L U nsup ← Eq. 5</cell></row><row><cell>6:</cell><cell>L Overall ← Eq. 6</cell></row><row><cell>7:</cell><cell>Θ s ← ADAM</cell></row><row><cell>8:</cell><cell>Θ t ← Eq. 7</cell></row><row><cell>9:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statisticsDataset # Nodes # Edges # Classes # Feature Dim.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy comparison with the state-of-art-methods under the setting of Fixed / Random data splits. * indicates our own implemented baseline.</figDesc><table><row><cell>Fixed splits</cell><cell>Random splits</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy comparison with the state-of-art-methods on Pubmed with varying # labeled sample over range {50, 100, 200}. * indicates our own implemented baseline.</figDesc><table><row><cell></cell><cell cols="3"># Labeled samples per class</cell></row><row><cell>Method</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>DCNN</cell><cell>-</cell><cell>82.6 ± 0.3</cell><cell>-</cell></row><row><cell>N-GCN</cell><cell>-</cell><cell>83.0 ± 0.4</cell><cell>-</cell></row><row><cell>GCN  *</cell><cell cols="3">80.9 ± 0.3 81.8 ± 0.3 84.1 ± 0.2</cell></row><row><cell cols="4">SEGCN 81.7 ± 0.5 83.8 ± 0.4 84.7 ± 0.4</cell></row><row><cell cols="4">initializations for mutual learning and self-training, which</cell></row><row><cell cols="3">unlock the potential of SEGCN.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on Cora.</figDesc><table><row><cell>Perturbation</cell><cell cols="2">Self-training</cell></row><row><cell cols="2">RE(.) P A (.) P f (.) T √ √ √ √ √ √ √ √ √ √</cell><cell>S&amp;T √</cell><cell>Accuracy 80.1 ± 0.6 81.7 ± 0.2 80.9 ± 0.1 82.4 ± 0.2 83.1 ± 0.6 83.5 ± 0.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abu-El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08888</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02618</idno>
	</analytic>
	<monogr>
		<title level="m">Bootstrapped graph diffusions: Exposing the power of nonlinearity</title>
		<meeting><address><addrLine>Bucilu, Caruana, and Niculescu-Mizil</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR. Spectral graph theory. Number 92</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
	<note>and Le</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bresson</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vandergheynst ; Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mackiewicz</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05208</idno>
		<idno>arXiv:1705.07485</idno>
	</analytic>
	<monogr>
		<title level="m">node2vec: Scalable feature learning for networks</title>
		<meeting><address><addrLine>Leskovec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KDD. and Gribonval</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.3848</idno>
		<title level="m">Wavelets on graphs via spectral graph theory</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dean ; Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<idno>arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Distilling the knowledge in a neural network</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03226</idno>
		<title level="m">Adaptive graph convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph partition neural networks for semi-supervised classification</title>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly-and semisupervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papandreou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08049</idno>
	</analytic>
	<monogr>
		<title level="m">Semi-supervised user geolocation via graph convolutional networks</title>
		<meeting><address><addrLine>Baldwin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edge attention-based multirelational graph convolutional networks</title>
		<idno type="arXiv">arXiv:1802.04944</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dropout: a simple way to prevent neural networks from overfitting</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust spatial filtering with graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="884" to="896" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Such et al. 2017</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feastnet: Feature-steered graph convolutions for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thekumparampil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<idno>arXiv:1801.07829</idno>
	</analytic>
	<monogr>
		<title level="m">Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dynamic graph cnn for learning on point clouds</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
