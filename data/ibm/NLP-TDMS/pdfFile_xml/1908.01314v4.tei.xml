<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching Beyond MobileNetV3</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
							<email>chuxiangxiang@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<email>zhangbo11@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
							<email>xuruijun@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Searching Beyond MobileNetV3</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The evolution of MobileNets has laid a solid foundation for neural network applications on mobile end. With the latest MobileNetV3, neural architecture search again claimed its supremacy in network design. Unfortunately, till today all mobile methods mainly focus on CPU latencies instead of GPU, the latter, however, is much preferred in practice for it has faster speed, lower overhead and less interference. Bearing the target hardware in mind, we propose the first Mobile GPU-Aware (MoGA) neural architecture search in order to be precisely tailored for real-world applications. Further, the ultimate objective to devise a mobile network lies in achieving better performance by maximizing the utilization of bounded resources. Urging higher capability while restraining time consumption is not reconcilable. We alleviate the tension by weighted evolution techniques. Moreover, we encourage increasing the number of parameters for higher representational power. With 200× fewer GPU days than MnasNet, we obtain a series of models that outperform MobileNetV3 under the similar latency constraints, i.e., MoGA-A achieves 75.9% top-1 accuracy on ImageNet, MoGA-B meets 75.5% which costs only 0.5 ms more on mobile GPU. MoGA-C best attests GPU-awareness by reaching 75.3% and being slower on CPU but faster on GPU. The models and test code are made publicly here 12 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The MobileNets trilogy has opened a gate to on-device artificial intelligence for the mobile vision world <ref type="bibr" target="#b4">(Howard et al. 2017;</ref><ref type="bibr">Howard et al. 2019</ref>). In the meantime, neural architecture search becomes the new engine to empower the future architecture innovation <ref type="bibr" target="#b9">Tan et al. 2019;</ref><ref type="bibr" target="#b1">Cai, Zhu, and Han 2019;</ref><ref type="bibr" target="#b1">Chu et al. 2019a)</ref>. The guideline in designing mobile architecture is that not only should the high performance be concerned, but also we must strive for low latency in favor of rapid responsiveness and improved power efficiency to prolong battery life.</p><p>In this paper, we aim to bring forward the frontier of mobile neural architecture design by stretching out the repre-1 https://github.com/xiaomi-automl/MoGA 2 This is a preview version, subject to frequent changes. sentational space within the desired latency range. Our work can be summarized in following aspects. First, we make a shift in the search trend from mobile CPUs to mobile GPUs, with which we can gauge the speed of a model more accurately and provide a productionready solution. On this account, our overall search approach is named Mobile GPU-Aware neural architecture search (MoGA). Our results suggest that generated models show different behavior related to the targeted hardware as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Second, we replace traditional multi-objective optimization with a weighted fitness strategy. While considering accuracy, latency and the number of parameters as our objectives, particular care is required to abate these three contending forces. One important insight is that the number of parameters should be made reasonably large instead of as few as possible, this leverages performance but doesn't necessarily increase latency. At the mobile scale, this would be the proper choice as we try to avoid underfitting instead of overfitting. On top of that, we lay more attention on accuracy and latency than the number of parameters.</p><p>Third, as per search cost, we benefit from one-shot supernet training and an accurate latency look-up table. Actually, it only requires the same expense as training a standalone model. The overall pipeline costs 12 GPU days, about 200× less than <ref type="bibr">MnasNet (Tan et al. 2019</ref>). More importantly, to cater for various mobile devices, as we decouple search process from training, it only requires one more inexpensive search with a renewed latency table. In contrast, gradient descent and reinforced methods have to start all over for supernet training or incomplete training for multitudinous models <ref type="bibr" target="#b6">(Liu, Simonyan, and Yang 2019;</ref><ref type="bibr" target="#b9">Tan et al. 2019)</ref>.</p><p>Finally, we present our searched architectures that outperform MobileNetV3. MoGA-A that achieves 75.9% top-1 accuracy on ImageNet, MoGA-B 75.5% and MoGA-C 75.3%. MoGA-C is best comparable to MobileNetV3, with similar FLOPs and an equal number of parameters, which runs slower on mobile CPUs but faster on mobile GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>During the era of human craftsmanship, <ref type="bibr">MobileNetV1 and V2 (Howard et al. 2017;</ref>) have widely disseminated depthwise separable convolutions and inverted residuals with linear bottlenecks. Moreover, Squeeze and excitation blocks are later introduced in <ref type="bibr" target="#b5">(Hu, Shen, and Sun 2018)</ref> to enrich residual modules from ResNet <ref type="bibr" target="#b4">(He et al. 2016)</ref>.</p><p>In their aftermath, a series of automated architectures are searched based on these building blocks <ref type="bibr" target="#b9">(Tan et al. 2019;</ref><ref type="bibr" target="#b1">Cai, Zhu, and Han 2019;</ref><ref type="bibr" target="#b1">Chu et al. 2019a;</ref><ref type="bibr">Howard et al. 2019</ref>  <ref type="bibr" target="#b10">(Yang et al. 2018</ref>) and improved non-linearities <ref type="bibr">(Howard et al. 2019)</ref>.</p><p>As for search methods, recent attention has been drawn to the one-shot approaches initiated by <ref type="bibr" target="#b0">(Bender et al. 2018)</ref>, as they tremendously reduce computing resources and also offer state of the art results <ref type="bibr" target="#b1">(Cai, Zhu, and Han 2019;</ref><ref type="bibr" target="#b9">Stamoulis et al. 2019;</ref><ref type="bibr" target="#b3">Guo et al. 2019;</ref><ref type="bibr" target="#b1">Chu et al. 2019a)</ref>. Briefly, a one-shot approach embodies weightsharing across models by constructing a supernet where each step of training accounts for the final performance. Its single-path variations further cut down memory consumption by training a picked path at each step instead of the whole supernet, yielding more flexibility for architecture design <ref type="bibr" target="#b9">(Stamoulis et al. 2019;</ref><ref type="bibr" target="#b3">Guo et al. 2019;</ref><ref type="bibr" target="#b1">Chu et al. 2019a</ref>). Among them, FairNAS proved it is critical to maintaining strict fairness for training single-path nets so to reach a steady rank, which can reasonably facilitate the search process (Chu et al. 2019a). In practice, mobile neural networks are mostly deployed to run on GPUs, DSPs and recently also on specific Neural Processing Units (NPUs), while CPUs would be the last to choose. To further investigate the relationship of CPU latencies versus GPU ones, we measure 100 random models on both two platforms. The result is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We see that there is no obvious linear correspondence. Hence, To develop architectures with target hardware in mind is more than necessary. For this reason, we are driven to apply Mobile GPU awareness to the latest neural architecture search approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mobile GPU Awareness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Underfitting and Overfitting</head><p>As we try to tear apart two contradicting objectives, there isn't too much freedom left to increase accuracy with a constrained latency. Fortunately, we observe from the evolution of MobileNets as in <ref type="figure" target="#fig_2">Figure 3</ref>, the number of parameters has grown while the latencies and multiply-adds are kept low. Moreover, for the mobile end, models tend to be underfitted instead of overfitted since they carry fewer numbers of parameters , which means we are free to encourage representational power by enlarging its range of parameters. This intuition greatly expands our design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Problem Formulation</head><p>Most hardware-aware methods build the classification problem as follows,</p><formula xml:id="formula_0">max Accuracy(m) s.t. Latency(m) &lt; L model m ∈ Ω.</formula><p>(1)</p><p>where Ω is the whole search space and L is a given maximal acceptable latency 3 . Informally speaking, larger models have greater capacity and to achieve better accuracy. Therefore, NAS methods will prefer models which have large running time. As a result, when the requirement of L changes, the whole NAS pipeline must start over. To address the above problem, a recent popular approach formulate it as multi-objective problem (MOP) whose solution is called Pareto Front,</p><formula xml:id="formula_1">max {Accuracy(m), −Latency(m)} m ∈ Ω.<label>(2)</label></formula><p>One popular approach for Equation 2 is converting it into a customized objective of weighted product ACC × (LAT /T AR) w , which requires delicate manual tuning for a parameter w to balance between latency and accuracy <ref type="bibr" target="#b9">(Tan et al. 2019)</ref>.</p><p>Upon these previous attempts, as inspired by Section 3.2, we maximize the number of parameters in addition to the two objectives in Equation 2. More formally, we try to solve the following problem, max {Accuracy(m), −Latency(m), P arams(m)} m ∈ Ω.</p><p>As a matter of fact, these three objectives are not of equal importance in most cases. A typical case is like Equation 2. Therefore, we need to introduce some strategies to address the issue. Let w acc , w lat , w params denote customized preference for those objectives. Without loss of generality, the problem can be defined as,</p><formula xml:id="formula_3">min {−Accuracy(m), Latency(m), −P arams(m)} s.t. m ∈ Ω w acc + w lat + w params = 1 w acc , w lat , w params &gt;= 0.<label>(4)</label></formula><p>There are two basic subproblems to be solved in the next section. One is to instantly evaluate accuracy and latency of a model, the other is to solve Equation 4. We use NSGA-II, <ref type="bibr">3</ref> The following latency means mobile GPU latency. <ref type="table" target="#tab_4">Size SE   0  3  3  -1  3  3   2  3  5  -3  3  5   4  3  7  -5  3  7   6  6  3  -7  6  3   8  6  5  -9</ref> 6 5 10 6 7 -11 6 7 <ref type="table">Table 1</ref>: Each layer in our search space has 12 choices. SE: Squeeze-and-Excitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Expansion Kernel</head><p>which is one of the most powerful and widely used algorithms to solve such problems <ref type="bibr" target="#b1">(Deb et al. 2002)</ref>. First, it's efficient to solve MOPs, especially when the number of objectives is large, some variants can still work. Second, it's flexible to apply customized preferences for different objectives, as well as various constraints. We also benefit from its implicit objective scaling and normalization.</p><p>4 Solving it Using Weighted NSGA-II</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Space</head><p>Our search space is built layer by layer on inverted bottleneck blocks as <ref type="bibr" target="#b1">(Cai, Zhu, and Han 2019;</ref><ref type="bibr" target="#b1">Chu et al. 2019a</ref>). We keep the same number of layers and activation functions as MobilenetV3-large. For each layer, we search from three dimensions (see <ref type="table">Table 1</ref>):</p><p>• the convolution kernel size (3, 5, 7)</p><p>• the expansion ratio for the inverted bottleneck block (3, 6)</p><p>• whether the squeezing and excitation mechanism is enabled or not.</p><p>Therefore, the total search space has a volume of 12 14 , which needs efficient methods to differentiate better models from worse. To be simple, we search for the expansion rate instead of channels which is used by (Howard et al. 2019) based on NetAdapt <ref type="bibr" target="#b10">(Yang et al. 2018)</ref>. Besides, we utilize choice index to directly encode each model chromosome.</p><p>More formally, a model chromosome m can be written as m 1 = (x 1 1 , x 1 2 , ..., x 1 14 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Accuracy and Latency Prediction</head><p>The evaluation of model accuracy must be made immediate for searching efficiency. We take advantage of a variation of one-shot approaches FairNAS for fast evaluation with a stable ranking. Unlike their version, based on our previously defined search space, we construct a supernet with 12 choice blocks per layer. Then we train our supernet on the ImageNet dataset with the same fairness strategy. As for mobile GPU latency, we don't acquire real-time latency during the pipeline from a cell phone for two reasons. One is that while the performance can be rapidly predicted by the supernet which takes less than 1 minute, it will easily become the bottleneck when we use a mobile device to evaluate latency on the fly. The other is that latency measurement may become inaccurate as a result of overheating after long-time insistent testing.</p><p>Instead, since each choice block in our search space has a fixed input, we can efficiently approximate the latency for any sampled model. To do so, we benchmark the latency of each choice block under a given input and construct a layerwise lookup table. We can then accurately calculate the latency simply by accumulating time cost across all layers. We find that predicted GPU latency coincides with groundtruth values with a negligible RMSE, see <ref type="figure" target="#fig_4">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weighted NSGA-II</head><p>We comply with the standard NSGA-II procedure, and only state the difference if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Population Initialization</head><p>We initialize population to introduce various choice blocks to encourage exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crossover</head><p>We take a single-point crossover. Specifically, for two models m 1 = (x 1 1 , x 1 2 , ..., x 1 14 ) and m 2 = (x 2 1 , x 2 2 , ..., x 2 14 ), if the point position is k, the result after crossover is (x 1 1 , x 1 2 , ...x 2 k ..., x 1 14 ). Mutation We use hierarchical mutation and the same hyperparameters as FairNAS <ref type="bibr" target="#b1">(Chu et al. 2019a</ref>).</p><p>Non-dominated Sorting For a minimization problem with n objectives, we state that A dominates B means for</p><formula xml:id="formula_4">any objective O i , O i A ≤ O i B .</formula><p>For a given population P , A is not dominated if and only if A is not dominated by any other individuals.</p><p>The crowding distance is a key component to achieve better trade-off among various objectives. We use the customized weights to define the crowding distance for nonboundary individuals, Uniformly generate the populations P 0 and Q 0 until each has n individuals.</p><formula xml:id="formula_5">D(m j ) = n i=1 w i * O i neighbor+ − O i neighbor− O i max − O i min .<label>(5</label></formula><formula xml:id="formula_6">for i = 0 to N − 1 do R i = P i ∪ Q i F = non-dominated-sorting(R i )</formula><p>Pick n individuals to form P i+1 by ranks and the crowding distance weighted by w based on Equation <ref type="formula">5</ref>.</p><formula xml:id="formula_7">Q i+1 = ∅ while size(Q i+1 ) &lt; n do M = tournament-selection(P i+1 ) q i+1 = crossover(M ) ∪ hierarchical-mutation(M )</formula><p>Obtain fitness value across all objectives Evaluate model q i+1 's accuracy with S on D Regress model q i+1 's latency based on T Q i+1 = Q i+1 ∪ {q i+1 } end while end for Select K models at an equal distance near Pareto front from P N incorporated. If w acc = w lat = w params = 1 3 , it degrades as the standard NSGA-II. In our experiment, w acc = w lat = 0.4, w params = 0.2, whose requirement is from a practical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Our NAS Pipeline</head><p>Our search pipeline is an evolution process, drawn in Figure 5 and detailed in Algorithm 1. Specifically, we use a trained supernet as a fast evaluator, a GPU latency lookup table, and a statistic tool to compute the number of parameters. The initial random population propagates at significant speed. The pipeline evolves 120 generations with a population size 70, and it only takes about 1.5 GPU days to evaluate these 8400 models. We use the same hyperparameters as FairNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Population</head><p>Weighted NSGA-II GPU Look-up  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mobile GPU Latency</head><p>In practice, we employ SNPE (Qualcomm 2019) and Mobile AI Compute Engine (MACE) for mobile GPU bench-  marking (Xiaomi 2018). We randomly sample some models and report the differences between our predictions and on-device measurements, which are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. For instant latency prediction, we construct a latency lookup table based on MACE measurements on all 12 choices blocks for each cell (12 × 14). For the final comparison with stateof-the-art models, we also report mobile GPU latencies with SNPE (Qualcomm 2019), and CPU latencies with Tensorflow Lite <ref type="bibr" target="#b0">(Abadi et al. 2015)</ref>. Considering recent updates on Tensorflow speed up the inference time, we choose a version that can reproduce the result on MobileNetV2 . Unless otherwise noted, mobile CPU latencies are measured on a Google Pixel 1 using a single large core of CPU with a batch size of 1. Mobile GPU latencies are benchmarked on a Mi MIX 3. The input size is set to 224×224.</p><formula xml:id="formula_8">× 160 conv2d, 1 × 1 -960 - HS 1 7 2 × 960 avgpool, 7 × 7 -- - HS - 1 2 × 960 conv2d, 1 × 1 -1280 - HS 1 1 2 × 1280 conv2d, 1 × 1 -k - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training</head><p>Training of our Supernet We search proxylessly on the ImageNet <ref type="bibr" target="#b2">(Deng et al. 2009</ref>) classification dataset. We take out 50k images from the training set to form our validation set and use the official validation set as our test set to evaluate our models, which is on par with other methods. In particular, we train the supernet by SGD with momentum 0.9 for 32 epochs. The initial learning rate is 0.05 and is scheduled to arrive at zero within a single cosine cycle.</p><p>Training for Stand-Alone models To alleviate the training unfairness, we utilize the same training tricks and hyperparameters as <ref type="bibr">MobileNetV3 (Howard et al. 2019</ref>).6. By doing so, we singled out various training tricks in order to focus on the authentic model performance. In particular, We use a batch size of 4096 and RMSProp optimizer with 0.9 momentum. The initial learning rate is 0.1 and linear warmup <ref type="bibr" target="#b3">(Goyal et al. 2017</ref>) is applied for the first 5 epochs. We use a dropout rate of 0.2 before the last layer <ref type="bibr" target="#b8">(Srivastava et al. 2014</ref>) and L2 weight decay 1e − 5. Besides, we make use of NVIDIA's mixed precision library Apex to enable larger batch size 4 . All our experiments are performed on two Tesla-V100 machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with State-of-the-art Methods</head><p>We are mostly comparable to the latest version of MnasNet <ref type="bibr" target="#b9">(Tan et al. 2019)</ref> and <ref type="bibr">MobileNetV3 (Howard et al. 2019</ref>), as we share the similar search space. Also, we use the same training and data processing tricks as in <ref type="bibr" target="#b9">(Tan et al. 2019)</ref> for complete training of stand-alone models. Note that with latency considered as one of the objectives, our generated models pay more attention to increase the number of parameters in order to gain higher performance, see detailed comparison results in <ref type="table" target="#tab_6">Table 3</ref>. We list all layers of MoGA-A in <ref type="table" target="#tab_4">Table 2</ref>, and illustrate the whole MoGA family in <ref type="figure" target="#fig_10">Figure 8</ref>.</p><p>For a fair comparison, here we only consider single path models based on inverted bottleneck blocks. MoGA-A achieves a new state-of-the-art top-1 accuracy 75.9%, surpassing Proxyless-R Mobile (+1.3%), MnasNet-A1 (+0.7%), MnasNet-A2 (+0.3%) with fewer FLOPs. MoGA-B obtains 75.5%, excelling MobileNetV3 at similar GPU speed. MoGA-C hits a higher accuracy with faster GPU speed, note it is slower on CPU, which otherwise will be treated as inferior by CPU-aware methods. Therefore, it's beneficial to fit models for specific hardware, indicating that even latency on other computing units and FLOPs are not ideal proxies.</p><p>MoGA-A makes extensive use of large kernels (4 layers with 7 × 7), which helps to enlarge receptive fields. Moreover, it mostly places large kernels on the stages with 14×14 input to reduce the latency cost. It also utilizes a large expansion rate after each downsampling stage to retain and to extract more useful features.</p><p>Interestingly for MoGA-B, the expansion rates across various layers mimic a sine curve. Like MoGA-A, it utilizes five 7 × 7 kernels to obtain a large receptive field. To cut down the latency cost, it places most of them in the 14 × 14 stage. Like FairNAS-A, it selects larger expansion rates right before downsampling operations.</p><p>Coincidentally, both MoGA-C and MobileNetV3-large simply contain 3 × 3 and 5 × 5 kernels only, even with same amount of such layers. While MobileNetV3-large prefers 5 × 5 operations in the tail of the model, MoGA-C chooses 3 × 3 instead. Besides, MoGA-C places 5 × 5 kernels in the middle and uses less squeeze-and-excitation operations. In such way, it better balances accuracy and latency cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Mobile GPU Awareness Analysis</head><p>We benchmark the inference cost for our three models both on mobile CPUs and GPUs. The result is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As for mobile GPUs, all models spend most of the time on   2D convolutions. MoGA-A and B spend the second most of the time on depthwise convolutions because they make extensive use of large kernels and expansion rates, whereas MoGA-C pays more attention to elementwise operations instead. Note all MoGA series invest more time on depthwise convolutions, contributing for faster speed and better performance.</p><p>It is worth noticing that how models exhibit a different behavior on mobile GPUs than on CPUs. For instance, vanilla convolutions and depthwise convolutions generally share bigger percentages on CPUs than on GPUs, while elementwise operations have a smaller percentage, as seen from <ref type="figure" target="#fig_0">Figure 1</ref>. Additionally, there is a discrepancy when running the same model with the different inference frameworks as well, which could call for a framework-aware solution, see <ref type="table" target="#tab_6">Table 3</ref>. Apart from the mobile framework we use, CPUs and GPUs differ on inherent microarchitectures, which puts hardware-specific requirements a must for the design of neural architectures.  Given a target device, our overall search cost c all can be decomposed into two parts: c super for supernet training (10.5 GPU days) and c search for the NSGA-II pipeline. The latter estimates model accuracy by the supernet evalua-tor. Notably, there is no need to retrain the supernet when we design neural models for different mobile platforms. In contrast, the cost for most existing NAS methods, such as RL, EA and gradient descent, increases linearly with the number of platforms <ref type="bibr" target="#b9">(Tan et al. 2019;</ref><ref type="bibr" target="#b9">Wu et al. 2019;</ref><ref type="bibr" target="#b1">Cai, Zhu, and Han 2019;</ref><ref type="bibr" target="#b6">Liu, Simonyan, and Yang 2019)</ref>. For N platforms, our c super is amortized as csuper N . When N ≥ 22, the overall cost c all reduces to less than 2 GPU days per platform. This benefit is better depicted in <ref type="figure" target="#fig_8">Figure 6</ref>.    We compare the best elitists for Equation 2 and 4, which is shown in the upper part of <ref type="figure" target="#fig_9">Figure 7</ref>. The Pareto front formed by the two objectives is largely surrounded by those with three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">GPU Cost Analysis with More Mobile Devices</head><p>While FairNAS states that a fair training can boost the rank relationship between the supernet predictor and standalone training, it also points out that it can be affected by initialization techniques and suboptimal training hyperparameters. For the latter, we empirically maximize the number of parameters as a compensation bonus.  <ref type="figure">Figure 9</ref>: Histogram on numbers of parameters of models from the last generation of weighted NSGA-II with hierarchical mutator, compared with that of two objectives (accuracy, latency).</p><p>Does it matter to use parameters as an objective? Occam's Razor doesn't fit in this case, because in such mobile setting, a neural network is prone to underfitting instead of overfitting. If we consider minimizing the number of parameters, NSGA-II is then at the risk of excluding models with more parameters generation by generation. For evidence, we show the histograms of the number of parameters for the final elitists in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>To sum up, we have discussed several critical issues in mobile neural architecture design. First, we promote the first Mobile GPU-Aware (MoGA) solution, as in production, running networks on mobile GPUs are much preferred. Second, we adopt weighted fitness strategy to comfort more valuable objectives like accuracy and latency, other than the number of parameters. Third, our total search cost has been substantially reduced to 12 GPU days. Also, the trained supernet is once-for-all since the same supernet caters for all mobile contexts. It requires o(1) search cost when applying to a new mobile device. Last, we employ an automated search approach in the search space adapted from MnasNet and MobileNetV3, which generates a new set of state-of-theart architectures for mobile settings. In particular, MoGA-C hits 75.3% top-1 ImageNet accuracy, which outperforms MobileNetV3 with competing mobile GPU latency at similar FLOPs and an equal number of parameters. In the future, there will still be continuous interest to squeeze out better performance within limited hardware bounds, especially on targeted computing units. Also, balancing between architecture diversity and search space size will remain as a major topic, it also poses a challenge for searching algorithms when search space grows enormously.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Latency pie chart of MoGA-A/B/C, MobileNetV3 operations when run on mobile CPUs (inner circle with TFLite) vs. on mobile GPUs (outer circle with MACE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Latency relationship on mobile CPUs vs. on mobile GPUs. Recent NAS approaches have an increased emphasis on target platforms, primarily on mobile CPUs (Tan et al. 2019; Dong et al. 2018; Wu et al. 2019; Cai, Zhu, and Han 2019; Stamoulis et al. 2019; Howard et al. 2019). MnasNet has developed a reward ACC × (LAT /T AR) w , which requires delicate manual tuning for a parameter w to balance between latency and accuracy (Tan et al. 2019), in MobileNetV3, w is reduced from −0.07 to −0.15 (Howard et al. 2019) to compensate for accuracy drop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The evolution of MobileNets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Mobile GPU latency measured vs. predicted ones. The latency RMSE is 0.0571ms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The overall pipeline of MoGA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Overall search cost vs. the number of target platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Pareto Front of weighted NSGA-II with hierarchical mutator compared with that of a random mutator and of two objectives (accuracy, latency).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>The Architectures of MoGA-A, B, C. Note Ex Ky SE means an expansion rate of x for its expansion layer and a kernel size of y for its depthwise convolution layer, SE for squeeze-and-excitation. Grey thick lines refer to downsampling points. Dashed lines separate the stem and end layers from the backbone.compare it with the random mutation baseline, see inFigure 7. Plenty of models from the baseline are dominated by the hierarchical version, which attests that hierarchical mutation improves searching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and O i neighbor+ are the i-th objective value of the left and the right neighbor of model m j respectively, while O i max and O i min are the maximum and the minimum for the i-th objective in the current population. In Equation 5, customized preference can be flexibly Algorithm 1 The weighted NAS pipeline. Input: Supernet S, search space Ω, the number of generations N , population size n, validation dataset D, objective weights w Output: A set of K individuals on the Pareto front. Train supernet S by the FairNAS approach on Ω. Make gpu latency table T as section 4.2.</figDesc><table><row><cell>)</cell></row><row><cell>Note O i neighbour−</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table Supernet</head><label>Supernet</label><figDesc></figDesc><table><row><cell></cell><cell>Evaluator</cell></row><row><cell>Pareto-optimal Front</cell><cell>Statistic Tool</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The architecture of MoGA-A. Note t, c, s refer to expansion rate, output channel size and stride respectively. SE for squeeze-and-excitation, NL for non-linearity. k for the number of categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison of mobile models on ImageNet. : Our reimplementation. Numbers within the parentheses are reported by its authors, same for below.</figDesc><table /><note>† : Based on its published code.‡ : Samsung Galaxy S8.* : Samsung Note8.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Mobile GPU-Aware NAS Based on Multi-Objective OptimizationIn this section, to better formulate our design problem, we draw insights from the development of MobileNets and experiments on the mobile GPU/CPU relationship, as well as reviewing previous optimization approaches.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/NVIDIA/apex.git</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. tag: v1.14.0-rc0. Software available from tensorflow.org</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han ;</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han ;</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<idno>arXiv:1901.01074</idno>
	</analytic>
	<monogr>
		<title level="m">FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. arXiv preprint</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="182" to="197" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE Transactions on Evolutionary Computation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="517" to="531" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<idno>arXiv:1904.00420</idno>
	</analytic>
	<monogr>
		<title level="m">Accurate, Large Minibatch SGD: Training Im-ageNet in 1 Hour. arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Guo et al. 2019</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<idno>arXiv:1905.02244</idno>
	</analytic>
	<monogr>
		<title level="m">MobileNets: Efficient. Convolutional Neural Networks for Mobile Vision Applications. arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Howard et al. 2019. et al. 2019. Searching for MobileNetV3. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>Qualcomm 2019] Qualcomm. 2019</idno>
		<ptr target="https://developer.qualcomm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Snapdragon Neural Processing Engine SDK</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno>com/software/qualcomm-neural-processing-sdk, version: 1.27.1.382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Sandler et al. 2018</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02877</idno>
		<idno>Wu et al. 2019</idno>
		<ptr target="https://github.com/XiaoMi/mace,commithashtag" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3362" to="3362" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The IEEE Conference on Computer Vision and Pattern Recognition. Xiaomi 2018] Xiaomi. 2018. Mobile AI Compute Engine</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NetAdapt: Platform-Aware Neural Network. Adaptation for Mobile Applications</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
	<note>The IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
