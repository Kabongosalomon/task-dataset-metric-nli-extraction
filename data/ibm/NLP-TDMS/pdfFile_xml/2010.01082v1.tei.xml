<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Modal Open-Domain Dialogue</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
							<email>kshuster@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
							<email>daju@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
						</author>
						<title level="a" type="main">Multi-Modal Open-Domain Dialogue</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work in open-domain conversational agents has demonstrated that significant improvements in model engagingness and humanness metrics can be achieved via massive scaling in both pre-training data and model size <ref type="bibr" target="#b0">(Adiwardana et al., 2020;</ref><ref type="bibr" target="#b40">Roller et al., 2020)</ref>. However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of engaging humans in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot <ref type="bibr" target="#b40">(Roller et al., 2020)</ref> in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to engagingness metrics. * Joint First Authors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important goal of artificial intelligence is the construction of open-domain conversational agents that can engage humans in discourse. Indeed, the future of human interaction with AI is predicated on models that can exhibit a number of different conversational skills over the course of rich dialogue. Much recent work has explored building and training dialogue agents that can blend such skills throughout natural conversation, with the ultimate goal of providing an engaging and interesting Figure 1: Paper author (right speaker) conversing with our MMB DegenPos model (left speaker). This example was cherry picked. We show more sample conversations in Section 6.2. experience for humans <ref type="bibr" target="#b45">Shuster et al., 2019b)</ref>. Coupled with the advancement of large-scale model training schemes, such models are becoming increasingly engaging and humanlike when compared to humans <ref type="bibr" target="#b0">Adiwardana et al., 2020;</ref><ref type="bibr" target="#b40">Roller et al., 2020)</ref>.</p><p>In order to better approach human-like ability, however, it is necessary that agents can converse with both textual and visual context, similarly to how humans interact in the real world; indeed, communication grounded in images is naturally engag-ing to humans <ref type="bibr" target="#b19">(Hu et al., 2014)</ref>. Recent efforts have gone beyond classical, fact-based tasks such as image captioning or visual question answering <ref type="bibr" target="#b2">(Antol et al., 2015;</ref><ref type="bibr" target="#b7">Das et al., 2017a)</ref> to produce models that can respond and communicate about images in the flow of natural conversation <ref type="bibr" target="#b45">(Shuster et al., , 2019b</ref>.</p><p>In this work, we explore the extension of largescale conversational agents to image-based dialogue. We combine representations from imagebased models that have been trained on object detection tasks <ref type="bibr" target="#b29">(Lu et al., , 2019</ref> with representations from Transformers with billions of parameters pre-trained on massive (text-only) dialogue datasets, to produce responses conditioned on both visual and textual context. To ensure that our model retains the ability to engage in regular, text-based conversation, we include in our training procedure multi-tasking with datasets expressly designed to instill conversational skills in the model .</p><p>We find that our best resulting models are as proficient in text-only conversation as the current best reported dialogue models, with respect to both automated metrics measuring performance on the relevant datasets and human evaluations of engagingness. Simultaneously, our model significantly outperforms recent strong multi-modal dialogue models when in an image-dialogue regime; we measure several metrics via pairwise human judgments using ACUTE-Eval <ref type="bibr" target="#b27">(Li et al., 2019b)</ref> to show that our model is not only more engaging but can also discuss and reference visual context throughout a conversation. See <ref type="figure">Figure 1</ref> for one sample cherrypicked conversation with our model, with random and lemon-picked conversations in Figures 2 and 3.</p><p>One important avenue that we explore with our best models is safety -that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area <ref type="bibr" target="#b28">Liu et al., 2019;</ref><ref type="bibr" target="#b10">Dinan et al., 2019a;</ref><ref type="bibr" target="#b4">Blodgett et al., 2020;</ref><ref type="bibr" target="#b21">Khatri et al., 2018;</ref><ref type="bibr" target="#b42">Schäfer and Burtenshaw, 2019;</ref><ref type="bibr" target="#b56">Zhang et al., 2018a</ref>), yet we note that safety in the context of image-dialogue is relatively less explored. In this work we examine gender bias and toxicity of text generations in the context of various styles from the Image-Chat dataset . Notably, after tuning the model to reduce toxicity and gender bias, we find that model engagingness does not diminish.</p><p>We make publicly available the training procedure, initial pre-trained model weights, and datasets in ParlAI 1 to allow for fully reproducible results.</p><p>2 Related Work 2.1 Multi-Modal Models and Tasks Rich Representations Modeling multi-modal inputs, i.e. in the visual + textual context, is a well-researched area. Much of the existing literature explores similar architectures to our setup, i.e., using standard Transformer-based models to jointly encode text and images <ref type="bibr" target="#b26">(Li et al., 2019a;</ref><ref type="bibr" target="#b22">Kiela et al., 2019)</ref>. Others have explored modifications to the standard self-attention scheme in Transformers by incorporating additional co-attention <ref type="bibr" target="#b29">(Lu et al., 2019;</ref><ref type="bibr" target="#b50">Tan and Bansal, 2019)</ref> or crossattention <ref type="bibr" target="#b49">(Stefanini et al., 2020)</ref> layers. These models have primarily been used for generating rich joint representations of images and text for use in downstream tasks, and they primarily focus on the encoding aspect.</p><p>Visual Dialogue/Caption Generation Many tasks have been designed to measure the ability of a model to produce text in the context of images. Specifically, COCO Captions <ref type="bibr" target="#b5">(Chen et al., 2015)</ref> and Flickr30k <ref type="bibr" target="#b55">(Young et al., 2014)</ref> require a model to produce a caption for a given image. A variety of sequence-to-sequence <ref type="bibr" target="#b52">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b54">Xu et al., 2015;</ref><ref type="bibr" target="#b1">Anderson et al., 2018)</ref> and retrievalbased <ref type="bibr" target="#b17">(Gu et al., 2018;</ref><ref type="bibr" target="#b15">Faghri et al., 2018;</ref><ref type="bibr" target="#b34">Nam et al., 2016)</ref> models have been applied to these tasks, however they do not go beyond the one-turn text generation expected for captioning an image. Other recent architectures have explored text generation <ref type="bibr" target="#b53">(Wang et al., 2020;</ref><ref type="bibr" target="#b35">Park et al., 2020)</ref> in the context of the Visual Dialog <ref type="bibr" target="#b8">(Das et al., 2017b)</ref> task; however, this task is primarily used to measure a model's ability to answer questions about an image in the flow of a natural conversation, which differs somewhat from the open-domain dialogue task. Further still, there have been recent forays into open-domain natural dialogue in the context of images, e.g. in the Image-Chat  and Image-grounded Conversations <ref type="bibr" target="#b33">(Mostafazadeh et al., 2017)</ref> tasks. Again, retrievalbased <ref type="bibr" target="#b20">Ju et al., 2019)</ref> and sequence-to-sequence <ref type="bibr" target="#b45">(Shuster et al., 2019b</ref> models have been used to conduct dialogue in this regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Task Training / Using Pre-Trained Representations</head><p>Our multi-modal model is constructed from models pre-trained in other, related domains; specifically, we seek to fuse the resulting weights of large-scale, uni-modal pre-training to achieve good performance on downstream, multi-modal tasks. Adapting pre-trained representations to later downstream tasks has been shown to be successful in NLP <ref type="bibr" target="#b36">(Peters et al., 2019;</ref><ref type="bibr" target="#b9">Devlin et al., 2019)</ref> and dialogue in particular <ref type="bibr" target="#b40">(Roller et al., 2020;</ref><ref type="bibr" target="#b32">Mazaré et al., 2018)</ref>. Additionally, large-scale multi-modal pre-training has been shown to be effective in other downstream multi-modal tasks <ref type="bibr" target="#b47">Singh et al., 2020b)</ref>. Our work does not contain multi-modal pre-training in itself, but rather we explore what some have deemed "domain-adaptive pre-training" <ref type="bibr" target="#b18">(Gururangan et al., 2020)</ref> or "intermediate task transfer" <ref type="bibr" target="#b37">(Pruksachatkun et al., 2020)</ref>, in which pre-trained representations are "adapted" to a certain domain via an intermediate training step, before training/evaluating on the requisite downstream tasks. It is also worth noting that we employ multi-task training, to both help generalize the applicability of the model and improve its performance on downstream tasks/evaluations; this has been shown recently to help in both image-based <ref type="bibr" target="#b47">(Singh et al., 2020b;</ref><ref type="bibr" target="#b20">Ju et al., 2019;</ref> and text-based <ref type="bibr" target="#b45">(Shuster et al., 2019b;</ref><ref type="bibr" target="#b40">Roller et al., 2020)</ref> tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison to Existing Models</head><p>In this work, we compare our best resulting model to the following existing models in the literature:</p><p>• The 2.7-billion-parameter Transformer sequence-to-sequence model from <ref type="bibr" target="#b40">Roller et al. (2020)</ref>, known as "BST Generative 2.7B model" in that work, pre-trained on 1.5B comments from a third-party Reddit dump hosted by pushshift.io <ref type="bibr" target="#b3">(Baumgartner et al., 2020)</ref>. We refer to this model as "BlenderBot".</p><p>• DialoGPT, a GPT-2-based model trained on 147M exchanges from public-domain socialmedia conversations  • Meena, a 2.6B-parameter Transformer sequence-to-sequence model trained on 341GB of conversations <ref type="bibr" target="#b0">(Adiwardana et al., 2020)</ref> • The Image+Seq2Seq model from dodecaDialogue <ref type="bibr" target="#b45">(Shuster et al., 2019b)</ref>, a Transformer sequence-to-sequence model in which the encoder is passed pre-trained image features from the ResNeXt-IG-3.5B model <ref type="bibr" target="#b31">(Mahajan et al., 2018)</ref>. We use the dodecaDialogue model fine-tuned on Image-Chat (and we refer to this model as "Dodeca").</p><p>• 2AMMC <ref type="bibr" target="#b20">(Ju et al., 2019)</ref>, in which multiple Transformers are attended over in order to make use of a combination of ResNeXt-IG-3.5B and Faster R-CNN image features . We specifically use the 2AMMC model from <ref type="bibr" target="#b20">Ju et al. (2019)</ref> because that model has the best test-set performance on Image-Chat in that work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architectures</head><p>The inputs to our models are visual and/or textual context, where applicable. We explore different ways to encode images from their pixels to vector representations, and we additionally compare ways of combining (fusing) the image and text representations before outputting a response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Encoders</head><p>Converting an image from pixels to a vector representation is a well-researched problem, and thus we explore using two different image encoders to determine the best fit for our tasks.</p><p>• ResNeXt WSL We first experiment with image representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion public images <ref type="bibr" target="#b31">(Mahajan et al., 2018)</ref>, with subsequent fine-tuning on the ImageNet1K dataset <ref type="bibr" target="#b41">(Russakovsky et al., 2015)</ref> 2 . The output of this model is a 2048-dimensional vector, and we refer to these representations as "ResNeXt WSL" features.</p><p>• ResNeXt WSL Spatial One can also take the output of the image encoder prior to its final fully-connected layer to obtain "spatial" image features, resulting in a 2048×7×7dimensional vector. We explore results with these features as well, and refer to them as "ResNeXt WSL Spatial".</p><p>• Faster R-CNN Finally, we consider Faster R-CNN features <ref type="bibr" target="#b39">(Ren et al., 2017)</ref>, using models trained in the Detectron framework ; specifically, we use a ResNeXt-152 backbone trained on the Visual Genome dataset <ref type="bibr" target="#b24">(Krishna et al., 2016)</ref> with the attribute head <ref type="bibr">(Singh et al., 2020a)</ref> 3 . The Faster R-CNN features are 2048×100-dimensional representations, and we refer to these features as "Faster R-CNN".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Modal Architecture</head><p>To jointly encode visual and textual context, we use a modification of a standard Transformer sequenceto-sequence architecture <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref>, whereby we experiment with different ways of combining (fusing) the image and text representations to generate an output sequence. Our Transformer model has 2 encoder layers, 24 decoder layers, 2560-dimensional embeddings, and 32 attention heads, and the weights are initialized from a 2.7-billion parameter model pre-trained on 1.5B comments from a third-party Reddit dump hosted by pushshift.io <ref type="bibr" target="#b3">(Baumgartner et al., 2020)</ref> to generate a comment conditioned on the full thread leading up to the comment <ref type="bibr" target="#b40">(Roller et al., 2020)</ref>. From this base model, we explore two possible fusion schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late Fusion</head><p>The late fusion method is the same as used in <ref type="bibr" target="#b45">Shuster et al. (2019b)</ref>, whereby the encoded image is projected to the same dimension as the text encoding of the Transformer encoder, concatenated with this output as an extra "token" output, and finally fed together as input to the decoder.</p><p>Early Fusion We additionally experiment with an earlier fusion scheme to allow greater interaction between the image and text in the sequenceto-sequence architecture. In a similar fashion to VisualBERT <ref type="bibr" target="#b26">(Li et al., 2019a)</ref> and multi-modal Bitransformers <ref type="bibr" target="#b22">(Kiela et al., 2019)</ref>, we concatenate the projected image encoding from the visual input with the token embeddings from the textual input, assign each a different segment embedding, and jointly encode the text and image in the encoder. The encoder thus performs full self-attention across the textual and visual context, with the entire output used as normal in the sequence-to-sequence architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Details</head><p>When training the model, we fix the weights of the pre-trained image encoders, except the linear projection to the Transformer output dimension, and fine-tune all of the weights of the Transformer encoder/decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domain-Adaptive Pre-Training</head><p>During training, the vast majority of trainable model weights are initialized from a large, 2.7B parameter Transformer pre-trained solely on textual input. As our end goal is to achieve improved performance on multi-modal tasks, we found that training first on domain-specific/related data was helpful in order to adapt the Transformer model to an image setting. Following <ref type="bibr" target="#b47">(Singh et al., 2020b)</ref>, we experimented with pre-training on COCO Captions <ref type="bibr" target="#b5">(Chen et al., 2015</ref>) -a dataset of over 120k images with 5 captions each, resulting in over 600k utterances -in which the model is trained to generate a caption solely from image input. We additionally explored multi-tasked training with COCO Captions and on the same third-party Reddit dump hosted by pushshift.io <ref type="bibr" target="#b3">(Baumgartner et al., 2020)</ref> as the one used in pre-training the Transformer model, to see whether it was necessary to ensure the model did not stray too far from its ability to handle pure textual input. During domain-adaptive pre-training, we trained the model on 8 GPUs for 10k-30k SGD updates, using early-stopping on the validation set. The models were optimized using Adam (Kingma and Ba, 2014), with sweeps over a learning rate between 5e-6 and 3e-5, using 100 warmup steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-tuning Datasets</head><p>The goal of our resulting model is to perform well in a multi-modal dialogue setting; thus, we fine-tune the model on both dialogue and imagedialogue datasets. For dialogue-based datasets, we consider the same four as in <ref type="bibr" target="#b40">Roller et al. (2020)</ref>: ConvAI2 <ref type="bibr" target="#b13">(Dinan et al., 2020b)</ref>, <ref type="bibr">EmpatheticDialogues (Rashkin et al., 2019)</ref>, Wizard of Wikipedia <ref type="bibr" target="#b14">(Dinan et al., 2019c)</ref>, and BlendedSkillTalk . To model image-dialogue, we consider the Image-Chat dataset .</p><p>We give a brief description below of the five datasets; more information can be found in <ref type="bibr" target="#b40">Roller et al. (2020)</ref> and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvAI2</head><p>The ConvAI2 dataset <ref type="bibr" target="#b13">(Dinan et al., 2020b)</ref> is based on the Persona-Chat <ref type="bibr" target="#b57">(Zhang et al., 2018b)</ref> dataset, and contains 140k training utterances in which crowdworkers were given prepared "persona" lines, e.g. "I like dogs" or "I play basketball", and then paired up and asked to get to know each other through conversation.</p><p>EmpatheticDialogues (ED) The EmpatheticDialogues dataset (Rashkin et al., 2019) was created via crowdworkers as well, and involves two speakers playing different roles in a conversation. One is a "listener", who displays empathy in a conversation while conversing with someone who is describing a personal situation. The model is trained to act like the "listener". The resulting dataset contains 50k utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wizard of Wikipedia (WoW)</head><p>The Wizard of Wikipedia dataset <ref type="bibr" target="#b14">(Dinan et al., 2019c)</ref> involves two speakers discussing a given topic in depth, comprising 194k utterances. One speaker (the "apprentice") attempts to dive deep on and learn about a chosen topic; the other (the "wizard") has access to a retrieval system over Wikipedia, and is tasked with teaching their conversational partner about a topic via grounding their responses in a knowledge source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BlendedSkillTalk</head><p>(BST) BlendedSkillTalk  is a dataset that essentially combines the three above. That is, crowdworkers are paired up similarly to the three previous datasets, but now all three "skills" (personalization, empathy, and knowledge) are at play throughout the dialogue: the speakers are tasked with blending the skills while engaging their partners in conversation. The resulting dataset contains 74k utterances.</p><p>Image-Chat (IC) The Image-Chat dataset (Shuster et al., 2020) contains 200k dialogues over 200k images: crowdworkers were tasked with discussing an image in the context of a given style, e.g. "Happy", "Cheerful", or "Sad", in order to hold an engaging conversation. The resulting dataset contains over 400k utterances. For each conversation in the dataset, the two speakers are each assigned a style in which that speaker responds, and these styles are optionally fed into models as part of the input, alongside the dialogue context. There are 215 styles in total, and styles are divided into 3 categories, "positive", "neutral", and "negative". 4</p><p>In the fine-tuning stage, we consider two different regimes: one in which we multi-task train on the five datasets together, and one in which we train on Image-Chat alone. While the latter regime is useful in exploring upper bounds of model performance, our main goal is to build a model that can display the requisite skills of an engaging conversationalist (empathy, personalization, knowledge) while also having the ability to respond to and converse about images; thus, we are more interested in the former training setup. In this stage, we train the models on 8 GPUs for around 10k train updates using a similar optimization setup as in the domain-adaptive pre-training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Results on Pre-Training Datasets</head><p>To fully understand the effects of various training data and image features, as well as multi-modal fusion schemes, we measure model perplexity on the COCO and pushshift.io Reddit validation sets. We are primarily interested in performance on COCO Captions, as the model has already been extensively pre-trained on the pushshift.io Reddit data. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Training Data We first note that, regardless of image fusion and image feature choices, we see the best performance on COCO Captions by simply fine-tuning exclusively on that data. This is an expected result, though we do see that in nearly every scenario the decrease in perplexity is not large (e.g. 5.23 for Faster R-CNN early fusion multi-tasking, down to 4.83 with just COCO Captions).  Image Fusion Finally, holding all other variables constant, we find that using our early fusion scheme yields improvements over using a late fusion scheme. E.g., with Faster-R-CNN features in the COCO-only setup, we see a decrease in perplexity from 5.21 to 4.83; with ResNeXt WSL Spatial image features, we see perplexity differences ranging from 0.3 to 0.9 depending on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results on Fine-Tuned Datasets</head><p>We conduct the same ablation setups for training on the dialogue and image-and-dialogue datasets as we did in the domain-adapative pre-training setup; the results for multi-tasking all of the datasets are in    <ref type="table">Table 5</ref>: Test performance of existing models on the datasets considered, compared to MMB (specifically, the "MMB Style" model discussed in Section 5.2.2), in terms of F1, BLEU-4 (B), and ROUGE-L (R) scores. * indicates that gold knowledge was utilized in the WoW task.</p><p>Chat alone are in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>From these extensive ablations, we note some interesting conclusions.</p><p>Text-Only Datasets First, we look at the performance of our models on the text-only datasets. The second-to-last column in <ref type="table" target="#tab_1">Table 2</ref> shows the average perplexity across the text-only datasets. If we compare the model that performs best on Image-Chat across all sets of image features (Faster-R-CNN features with BST + + IC + COCO + Reddit training data with early fusion) to the model in row 2, which is trained both without images and without Image-Chat on the text-only datasets, we see that the perplexity differences are quite small: that is, including training on an image-dialogue dataset, and overloading the Transformer encoder/decoder to incorporate image features, does not hinder dialogue performance.</p><p>Training Data Across all image-feature choices, we see that the choice of training data indeed makes a difference in performance on Image-Chat. Examining the early fusion model in <ref type="table" target="#tab_1">Table 2</ref>, by including COCO Captions (and, in some cases, pushshift.io Reddit) in the training data we see drops in perplexity from 12.99 to 12.85, 13.02 to 12.87, and 12.43 to 12.36 with ResNeXt WSL, ResNeXt WSL Spatial, and Faster R-CNN features respectively. The decrease in perplexity indicates that domain-adaptive pre-training indeed improves performance on Image-Chat. This difference is highlighted even more when we measure performance on the first turn of Image-Chat, in which the model must generate a response given no textual context: 15.16 to 14.64, 15.34 to 14.76, and 13.66 to 13.51. We note a similar trend in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Image Features Again, we see that using Faster R-CNN features leads to dramatic improvements compared to using the ResNeXt WSL features (spatial or otherwise), yielding 12.36 perplexity on Image-Chat compared to 12.85 and 12.87 perplexity with ResNeXt WSL (non-spatial and spatial respectively) during multi-tasking, and 12.29 perplexity on Image-Chat compared to 12.92 and 12.87 respectively for single-task training on Image-Chat (see <ref type="table" target="#tab_3">Table 3</ref>).</p><p>Image Fusion Finally, we note as before that using our early fusion technique improves performance on Image-Chat across all ablation regimes. While the average perplexity across the dialogue datasets is best when using late image fusion, we obtain the best image chat perplexity when performing early image fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Test Results</head><p>Following the ablation analyses, we decide to compare our best multi-tasked and single-tasked trained model (with respect to the fine-tuning datasets), where we use Faster R-CNN image features and an early fusion scheme, to existing models in the literature. For this comparison, we consider additional metrics that can be computed on the actual model generations: F1, BLEU-4 and ROUGE-L. We generate model responses during inference with the same generation scheme as in <ref type="bibr" target="#b40">Roller et al. (2020)</ref> -beam search with beam size of 10, minimum beam length of 20, and tri-gram blocking within the current generation and within the full textual context. The test performance of our best multitask model on the various datasets is shown in <ref type="table" target="#tab_4">Table 4</ref>, with comparisons to existing models from Section 2.3 in <ref type="table">Table 5</ref>; all evaluations are performed in ParlAI 5 .</p><p>We first note that the Dodeca model performs well across the board, and indeed has the highest ROUGE-L, BLEU-4, and F1 scores for the three text-only datasets. Higher BLEU-4 scores can be attributed to specifying a smaller minimum generation length, as forcing the BlenderBot models to generate no less than 20 tokens hurts precision when compared to reference labels -this was verified as we tried generating with a smaller minimum length (5 tokens) and saw a 20% increase in BLEU-4 on Image-Chat for Multi-Modal Blender-Bot. Higher ROUGE-L scores can additionally be attributed to specifying a larger minimum generation length; this was also verified by generating with a higher minimum length (50 tokens) where we saw nearly a 40% increase in ROUGE-L score. Nevertheless, we do not report an exhaustive search over parameters here for our model, and instead compare it to BlenderBot with the same settings next.</p><p>When compared to its predecessor, text-only BlenderBot, MMB performs nearly the same on all four text-only datasets, indicating that MMB has not lost its proficiency in text-only dialogue. Additionally, when comparing performance on Image-Chat to models trained on multi-modal data, MMB outperforms Dodeca in terms of F1 score (13.1 vs. 12.9) and outperforms 2AMMC on all three metrics. For the 2AMMC model, these metrics are computed under the assumption that the model's chosen response (from a set of candidate responses collated from the Image-Chat training set) is the "generated" response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Summary of Human Evaluations</head><p>Since our model must demonstrate compelling performance both in general chit-chat dialogue and when responding to an image when conversing with humans, we present several types of human evaluations in this section. Section 5.    performs when talking to a human about an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Human/Model Chats Without Images</head><p>We compare MMB Style, our model exposed to Image-Chat styles during training, to BlenderBot by having crowdsourced workers chat with our models, over 50 conversations per model. Each conversation consists of 7 turns per speaker, with the human speaking first by saying "Hi!", following the convention of <ref type="bibr" target="#b0">Adiwardana et al. (2020)</ref>. After every model response, the human records if the response contains any one of a number of different issues. Finally, at the end of the conversation, the human gives a 1-to-5 Likert-scale rating of the model's overall engagingness. No Image-Chat style is shown to MMB Style at the beginning of these conversations, matching its training setup in which no style was given when training on dialogue datasets. <ref type="table" target="#tab_6">Table 6</ref> shows that humans flag the models' responses at comparable rates for most categories of issues, with BlenderBot being flagged slightly more often for contradictions and repetitiveness and MMB Style flagged more often for being nonsensical; however, the mean engagingness rating of the two models across conversations is the same (both 4.7 out of 5).</p><p>We then perform ACUTE-Evals <ref type="bibr" target="#b27">(Li et al., 2019b</ref>) on the collected conversations of MMB Style and BlenderBot in order for crowdsourced raters to directly compare conversations from different models in an A/B setting. For each comparison, we ask each rater to compare conversations on one of two metrics, following <ref type="bibr" target="#b27">Li et al. (2019b)</ref>:</p><p>• (Engaging) "Who would you prefer to talk to for a long conversation?"</p><p>• (Human) "Which speaker sounds more human?"</p><p>Results are shown in <ref type="table" target="#tab_7">Table 7</ref>: raters choose conversations from one model over the other roughly equally, with no statistically significant differences among models.</p><p>In <ref type="table" target="#tab_8">Table 8</ref>, we also compare MMB Style to two other baseline models, DialoGPT and Meena. Raters are significantly more likely to prefer MMB Style over both of these models with respect to both the engagingness and humanness metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Initial Responses to Images</head><p>We measure MMB Style's ability to communicate about what it perceives visually by generating responses of the model to 100 images in the test set of Image-Chat. 6 Only Image-Chat images for which the first speaker speaks with a style in the list of "positive" or 'neutral" styles are included when creating this set of 100 images, and all images for which the first speaker has a style in the "negative" list are filtered out. As a comparison, we also generate responses from two previous models trained on Image-Chat data, Dodeca and 2AMMC. (Style) Narcissistic (Self-centered, Egotistical) MMB I would never go to a beach like this. My beach is so much nicer. Dodeca I could take a better picture than that.</p><p>2AMMC I could swim out to those rocks. The current is strong but I am a stronger swimmer.</p><p>(Style) Boisterous (Rowdy, Loud) MMB I would love to jump in that puddle and play in the mud! It would be so fun! Dodeca This looks like a great place to party! 2AMMC I would ride through here on a loud bike and throw a party! (Style) Happy MMB I can't wait to go to the beach and fly a kite with my best friend! Dodeca What a beautiful day! 2AMMC Face full of wind, smell of salt water, cool of the ocean, and fantastic memories being made!    Among the three models, 2AMMC alone is a retrieval model: it retrieves its response from the set of utterances in the Image-Chat training set. Examples of models' responses to images are in <ref type="table" target="#tab_10">Table 9</ref>. We run ACUTE-Evals to ask raters to compare these models' responses on the following metric (henceforth referred to as the Image-response metric): "Who talks about the image better?" The same image is used for both sides of each A/B comparison between model responses.</p><p>We find that raters choose both the MMB Style and 2AMMC models' responses significantly more often than those of Dodeca <ref type="table" target="#tab_0">(Table 10)</ref>. We also find no significant difference in the rate at which MMB Style image responses are chosen compared to the same model fine-tuned only on Image-Chat and not on dialogue datasets <ref type="table" target="#tab_0">(Table 11)</ref>, which implies that multitasking on dialogue datasets does not degrade the ability to effectively respond to an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Human/Model Chats About Images</head><p>In order to most meaningfully assess MMB Style's ability to simultaneously engage in general chitchat and talk about an image, we perform ACUTE-Evals where we ask raters to evaluate model performance through the span of an entire conversation about an image. For each conversation, an image from the subset of Image-Chat test set images discussed in Section 5.2.3 is first shown to both the human and the model. Then, the model responds to the image, and the human responds to the model to carry the conversation forward. The conversation continues for 6 human utterances and 7 model utterances total.</p><p>Ratings are shown in     performs significantly better than Dodeca and 2AMMC on the engagingness and humanness metrics, and it performs significantly better than Dodeca on the image-response metric.</p><p>6 Analysis of Safety and Gender Bias</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Degendering Models</head><p>We would like to reduce the ways in which the MMB Style model could potentially display gender bias: for instance, there is no safeguard against it misgendering a person in an image, and many common text datasets are known to contain gender bias <ref type="bibr" target="#b10">(Dinan et al., 2019a</ref><ref type="bibr">(Dinan et al., , 2020a</ref>, which may lead to bias in models trained on them. To remedy this, we train a version of the MMB Style model in which the label of each training example is run through a classifier that identifies whether it contains female or male words, and then a string representing that classification is appended to the example's context string <ref type="bibr" target="#b10">(Dinan et al., 2019a)</ref>, for input to the model. At inference time, the string representing a classification of "no female or male words" is appended to the context, nudging the model to generate a response containing no gendered words. The fraction of utterances produced by this model that still contain gendered words is shown in <ref type="table" target="#tab_0">Table 13</ref>. Compared to the gold response, the original Blender-Bot, and MMB Style, this degendered MMB model (which we call "MMB Degendered") reduces the likelihood of producing an utterance with male word(s) by roughly a factor of 9 and of producing an utterance with female word(s) by roughly a factor of 4, given a context from the ConvAI2 validation set. ACUTE-Evals in <ref type="table" target="#tab_7">Table 7</ref> show that this degendering does not lead to a significant drop in the humanness or engagingness of the model's responses during a conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Removing Dependence on Style</head><p>Since each of the images that MMB Style saw during training was associated with an Image-Chat style, it relies on an input style during inference in order to be able to discuss an image. However, this results in a model whose utterances will necessarily strongly exhibit a particular style. (For example, see the "Playful" MMB Style response in <ref type="table" target="#tab_10">Table 9</ref>: constricting the model to respond playfully to all images could seem rather contrived and perhaps unlike typical human speech.) To avoid this, we train a version of MMB Style where, for 75% of all images seen during training, the accompanying style is replaced with the string "positive/neutral" or "negative", depending on which list the style was a part of. Thus, during inference, the string "positive/neutral" can be used in lieu of a specific style string in order to produce responses that are unlikely to be negative and that do not consistently display strong adherence to a specific style. We refer to this model as the "MMB Positive" model, or "MMB DegenPos" if it was trained with degendering in addition as in Section 6.1. <ref type="table" target="#tab_0">Table 14</ref> shows that these models exhibit little increase in perplexity, and what little increase exists is likely due to the loss of specificity provided by a concrete style. The MMB DegenPos model exhibits the same level of degendering as the base MMB Degendered model <ref type="table" target="#tab_0">(Table 13)</ref>, and ACUTE-Evals show that these models exhibit no detectable loss of ability to talk about an image <ref type="table" target="#tab_0">(Table 15</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analyzing Dependence on Image</head><p>We also train a no-image ablation model, otherwise equivalent to MMB Positive, for which Image-Chat images are removed during both training and inference: crowdsource workers prefer the image responses of MMB Positive to those of this ablation model 80% to 20% <ref type="table" target="#tab_0">(Table 16</ref>). For this ablation, style was removed from the context (and replaced with the string "positive/neutral") in order to prevent the ablation model from being aided by this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Safety</head><p>The MMB models may demonstrate offensiveness beyond gender bias for several reasons: (1) its generative nature makes it rather difficult to define a limited set of utterances;</p><p>(2) the model's training data contains real-world conversations from the Internet; and (3) the Image-Chat dataset has negative styles to better capture the range of human styles. All of these factors could lead to an unsafe response given a multi-modal context. To mitigate this problem, we first measure our models' toxicity using an openly available blocklist 7 and an offensive language classifier presented in . We define the term "toxicity" to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validation set, with a fixed style trait to control the generation, presenting results for different choices of fixed trait. We first evaluate our model in the first round of the Image-Chat validation set. The results in <ref type="table" target="#tab_0">Table 17</ref> indicate that positive styles reduce the level of toxicity by a large margin for both metrics (classifier and blocklist). The results also align well   <ref type="table" target="#tab_0">Table 18</ref>: Toxicity of MMB variants as assessed with different control variables. We evaluate on the second round of the Image-Chat validation set. Column "Pos C" shows the safety classifier metric when conditioning on a positive style for the round-1 utterance, and "Pos B" shows the same thing for the blocklist metric. The following two columns show the same metrics when the round-1 utterance has a negative style.</p><p>with our previous experiments on degendering, as toxicity is reduced across all styles after applying the degendering process. After degendering, we can considerably improve our model's safety by enforcing that it uses positive styles. We also evaluate our model in the second round of the conversation and collect the statistics based on the first round style, as shown in <ref type="table" target="#tab_0">Table 18</ref>. This result suggests that even if the model is controlled with a positive style, it is less safe when responding to negative conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Example Conversations and Failure Cases</head><p>We show several handpicked examples of conversations with our MMB DegenPos model in <ref type="figure" target="#fig_0">Figures   1, 2</ref>, and 3. <ref type="figure">Figure 1</ref> in particular demonstrates a successful conversation: the model is clearly able to interpret what is in the image (a teddy bear and a road), and it is able to engagingly and creatively combine these two subjects in the conversation for several turns. <ref type="figure" target="#fig_0">Figure 2</ref> provides several more example conversations: in all of these, the model is able to both discuss the image and use it as a catalyst for further conversation, although occasionally with contradiction and forgetfulness issues as seen in <ref type="bibr" target="#b40">Roller et al. (2020)</ref>. (For instance, the model contradicts itself on whether it has any pets and forgets who is planning to make a fancy dinner.) Last, we show a few hand-picked examples of poor conversations in <ref type="figure" target="#fig_1">Figure 3</ref>: in these, the model fails to identify the contents of the images, identifying them both as buildings, although this may reflect a difference in the prevalence of (for example) buildings vs. roller coasters in the training sets. Despite the human nudging the model about what the images actually convey, the model does not demonstrate that it has corrected its initial misidentification in later turns. This could perhaps be remedied by an increase in image training data, by further advancements in the integration of image features with this BlenderBot-based sequence-to-sequence model, or perhaps by training specifically on data in which one partner learns about the contents of an image over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we explored a necessary component of engaging open-domain dialogue models: the ability to perceive and converse in the context of what is seen. We showed that we can match prior work in text-only dialogue in both automated metrics and engagingness metrics, and our best model surpasses existing models in multi-modal dialogue. Finally, we demonstrated that we do not sacrifice engagingness by incorporating safety components into the model.</p><p>We leave to future work the exploration of full multi-modal dialogue pre-training, as opposed to combining two models pre-trained solely in their own domain, which we believe could lead to improved performance on both types of tasks. Additionally, we believe that implementing further rigorous safety measures into multi-modal dialogue models is an important avenue of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>For ACUTE-Evals comparing pairs of human/model conversations from different models, crowdsource workers are asked to select among 10 checkboxes to explain their preference for one conversation over another. Workers are able to select multiple checkboxes. Results for ACUTE-Evals on the engagingness metric are shown in Tables 19, 20, and 21.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Randomly picked author examples. Paper author (right speaker) talking to the MMB DegenPos model (left speaker). Conversations are mostly fluent, with occasional contradictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Lemon-picked author examples. Paper author (right speaker) talking to the MMB DegenPos model (left speaker): misidentifying the subject of the image (top); misidentifying the subject of the image and not being able to learn from the chat partner's feedback (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Across all training setups, we see that using spatially-based image features (ResNeXt WSL Spatial, Faster R-CNN) yields better performance than just a single vector image representation (ResNeXt WSL). This difference is particularly noticeable when training with COCO and pushshift.io Reddit, where with Faster R-CNN features the model obtains an average ppl of 9.13 over Model performance, measured via perplexity on validation data, on domain-adaptive pre-training datasets, comparing various image features and image fusion techniques. The top three rows involve multi-task training on COCO Captions and pushshift.io Reddit, while the bottom three rows involve single task training on COCO Captions only. We note that early fusion with Faster R-CNN features yields the best performance on COCO Captions.</figDesc><table><row><cell cols="2">Image Features</cell><cell cols="9">Image Fusion COCO (ppl) pushshift.io Reddit (ppl) Average</cell></row><row><cell></cell><cell></cell><cell cols="6">COCO &amp; pushshift.io Reddit training data</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNeXt WSL</cell><cell>Late</cell><cell></cell><cell></cell><cell>11.11</cell><cell></cell><cell></cell><cell>13.80</cell><cell></cell><cell>12.45</cell></row><row><cell></cell><cell></cell><cell>Early</cell><cell></cell><cell></cell><cell>6.69</cell><cell></cell><cell></cell><cell>13.50</cell><cell></cell><cell>10.10</cell></row><row><cell cols="3">ResNeXt WSL Spatial Late</cell><cell></cell><cell></cell><cell>7.43</cell><cell></cell><cell></cell><cell>13.00</cell><cell></cell><cell>10.22</cell></row><row><cell></cell><cell></cell><cell>Early</cell><cell></cell><cell></cell><cell>6.53</cell><cell></cell><cell></cell><cell>13.46</cell><cell></cell><cell>10.00</cell></row><row><cell cols="2">Faster R-CNN</cell><cell>Late</cell><cell></cell><cell></cell><cell>5.26</cell><cell></cell><cell></cell><cell>13.17</cell><cell></cell><cell>9.21</cell></row><row><cell></cell><cell></cell><cell>Early</cell><cell></cell><cell></cell><cell>5.23</cell><cell></cell><cell></cell><cell>13.15</cell><cell></cell><cell>9.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">COCO training data only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNeXt WSL</cell><cell>Late</cell><cell></cell><cell></cell><cell>5.82</cell><cell></cell><cell></cell><cell>19.52</cell><cell></cell><cell>12.67</cell></row><row><cell></cell><cell></cell><cell>Early</cell><cell></cell><cell></cell><cell>6.21</cell><cell></cell><cell></cell><cell>21.30</cell><cell></cell><cell>13.76</cell></row><row><cell cols="3">ResNeXt WSL Spatial Late</cell><cell></cell><cell></cell><cell>6.51</cell><cell></cell><cell></cell><cell>16.50</cell><cell></cell><cell>11.51</cell></row><row><cell></cell><cell></cell><cell>Early</cell><cell></cell><cell></cell><cell>6.19</cell><cell></cell><cell></cell><cell>18.77</cell><cell></cell><cell>12.48</cell></row><row><cell cols="2">Faster R-CNN</cell><cell>Late</cell><cell></cell><cell></cell><cell>5.21</cell><cell></cell><cell></cell><cell>17.88</cell><cell></cell><cell>11.55</cell></row><row><cell></cell><cell></cell><cell>Early</cell><cell></cell><cell></cell><cell>4.83</cell><cell></cell><cell></cell><cell>18.81</cell><cell></cell><cell>11.82</cell></row><row><cell>Image</cell><cell>Training</cell><cell></cell><cell>Image</cell><cell>ConvAI2</cell><cell>ED</cell><cell>WoW</cell><cell>BST</cell><cell>IC 1st</cell><cell>IC</cell><cell>Text</cell><cell>All</cell></row><row><cell>Features</cell><cell>Data</cell><cell></cell><cell>Fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Turn</cell><cell></cell><cell>Avg.</cell><cell>Avg.</cell></row><row><cell></cell><cell>None</cell><cell></cell><cell></cell><cell>12.31</cell><cell>10.21</cell><cell>13.00</cell><cell>12.41</cell><cell>32.36</cell><cell>21.48</cell><cell>11.98</cell><cell>13.88</cell></row><row><cell>None</cell><cell>BST +</cell><cell></cell><cell>None</cell><cell>8.74</cell><cell>8.32</cell><cell>8.78</cell><cell>10.08</cell><cell>38.94</cell><cell>23.13</cell><cell>8.98</cell><cell>14.76</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell></cell><cell>8.72</cell><cell>8.24</cell><cell>8.81</cell><cell>10.03</cell><cell>16.03</cell><cell>13.21</cell><cell>8.95</cell><cell>9.83</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell>Late</cell><cell>8.71</cell><cell>8.25</cell><cell>8.87</cell><cell>10.09</cell><cell>16.20</cell><cell>13.27</cell><cell>8.98</cell><cell>9.84</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell>Early</cell><cell>8.80</cell><cell>8.32</cell><cell>8.79</cell><cell>10.17</cell><cell>15.16</cell><cell>12.99</cell><cell>9.02</cell><cell>9.81</cell></row><row><cell>ResNeXt WSL</cell><cell cols="2">BST + + IC + COCO + Reddit</cell><cell>Late</cell><cell>9.27</cell><cell>8.87</cell><cell>9.45</cell><cell>10.74</cell><cell>17.56</cell><cell>14.44</cell><cell>9.58</cell><cell>10.56</cell></row><row><cell></cell><cell cols="2">BST + + IC + COCO + Reddit</cell><cell>Early</cell><cell>9.34</cell><cell>8.90</cell><cell>9.48</cell><cell>10.78</cell><cell>15.87</cell><cell>13.88</cell><cell>9.62</cell><cell>10.48</cell></row><row><cell></cell><cell>BST + + IC + COCO</cell><cell></cell><cell>Late</cell><cell>8.79</cell><cell>8.36</cell><cell>9.00</cell><cell>10.21</cell><cell>16.00</cell><cell>13.31</cell><cell>9.09</cell><cell>9.93</cell></row><row><cell></cell><cell>BST + + IC + COCO</cell><cell></cell><cell>Early</cell><cell>8.91</cell><cell>8.38</cell><cell>8.99</cell><cell>10.29</cell><cell>14.64</cell><cell>12.85</cell><cell>9.14</cell><cell>9.88</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell>Late</cell><cell>8.71</cell><cell>8.24</cell><cell>8.88</cell><cell>10.10</cell><cell>15.39</cell><cell>13.02</cell><cell>8.98</cell><cell>9.78</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell>Early</cell><cell>8.79</cell><cell>8.29</cell><cell>8.92</cell><cell>10.15</cell><cell>15.34</cell><cell>13.02</cell><cell>9.04</cell><cell>9.83</cell></row><row><cell>ResNeXt WSL</cell><cell cols="2">BST + + IC + COCO + Reddit</cell><cell>Late</cell><cell>8.76</cell><cell>8.31</cell><cell>8.88</cell><cell>10.14</cell><cell>15.20</cell><cell>13.04</cell><cell>9.02</cell><cell>9.83</cell></row><row><cell>Spatial</cell><cell cols="2">BST + + IC + COCO + Reddit</cell><cell>Early</cell><cell>9.30</cell><cell>8.82</cell><cell>9.46</cell><cell>10.76</cell><cell>15.67</cell><cell>13.79</cell><cell>9.56</cell><cell>10.43</cell></row><row><cell></cell><cell>BST + + IC + COCO</cell><cell></cell><cell>Late</cell><cell>8.73</cell><cell>8.31</cell><cell>8.87</cell><cell>10.13</cell><cell>15.04</cell><cell>12.98</cell><cell>9.01</cell><cell>9.84</cell></row><row><cell></cell><cell>BST + + IC + COCO</cell><cell></cell><cell>Early</cell><cell>8.81</cell><cell>8.34</cell><cell>8.99</cell><cell>10.22</cell><cell>14.76</cell><cell>12.87</cell><cell>9.09</cell><cell>9.80</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell>Late</cell><cell>8.70</cell><cell>8,24</cell><cell>8.92</cell><cell>10.07</cell><cell>13.97</cell><cell>12.48</cell><cell>8.98</cell><cell>9.68</cell></row><row><cell></cell><cell>BST + + IC</cell><cell></cell><cell>Early</cell><cell>8.81</cell><cell>8.33</cell><cell>8.81</cell><cell>10.15</cell><cell>13.66</cell><cell>12.43</cell><cell>9.03</cell><cell>9.71</cell></row><row><cell></cell><cell cols="2">BST + + IC + COCO + Reddit</cell><cell>Late</cell><cell>8.75</cell><cell>8.31</cell><cell>8.93</cell><cell>10.14</cell><cell>13.83</cell><cell>12.49</cell><cell>9.03</cell><cell>9.73</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">BST + + IC + COCO + Reddit</cell><cell>Early</cell><cell>8.78</cell><cell>8.31</cell><cell>8.85</cell><cell>10.15</cell><cell>13.51</cell><cell>12.36</cell><cell>9.02</cell><cell>9.69</cell></row><row><cell></cell><cell>BST + + IC + COCO</cell><cell></cell><cell>Late</cell><cell>8.74</cell><cell>8.33</cell><cell>8.87</cell><cell>10.13</cell><cell>13.85</cell><cell>12.51</cell><cell>9.02</cell><cell>9.72</cell></row><row><cell></cell><cell>BST + + IC + COCO</cell><cell></cell><cell>Early</cell><cell>8.81</cell><cell>8.34</cell><cell>8.93</cell><cell>10.19</cell><cell>13.57</cell><cell>12.39</cell><cell>9.07</cell><cell>9.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation analysis of the impact of various image features, training data (including domain-adaptive pretraining), and image fusion techniques on the datasets described in Section 4.2, where BST + refers to the four text-only dialogue datasets (ConvAI2, ED, WoW, and BST). The numbers shown are model perplexities measured on each of the datasets' validation data. Performance on the first turn of Image-Chat is also measured to highlight model performance when only given visual context. We note that using Faster R-CNN image features results in the best average performance, as well as the best performance on Image-Chat. the two datasets, while with ResNeXt WSL features the model only obtains 10.1 ppl. We find that using Faster R-CNN features additionally outperforms using ResNeXt WSL Spatial features, where using the latter obtains an average of 10.0 ppl over the two datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Image</cell><cell>Training</cell><cell>Image</cell><cell>IC First Turn</cell><cell>IC</cell></row><row><cell>Features</cell><cell>Data</cell><cell>Fusion</cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>None</cell><cell>None</cell><cell cols="2">32.36 21.48</cell></row><row><cell></cell><cell>Image Chat</cell><cell></cell><cell cols="2">28.71 13.17</cell></row><row><cell></cell><cell>IC</cell><cell>Late</cell><cell cols="2">14.80 12.83</cell></row><row><cell></cell><cell>IC</cell><cell>Early</cell><cell cols="2">16.00 13.21</cell></row><row><cell></cell><cell cols="2">IC + COCO + Reddit Late</cell><cell cols="2">16.73 13.92</cell></row><row><cell cols="3">ResNeXt WSL IC + COCO + Reddit Early</cell><cell cols="2">15.71 13.53</cell></row><row><cell></cell><cell>IC + COCO</cell><cell>Late</cell><cell cols="2">14.70 12.95</cell></row><row><cell></cell><cell>IC + COCO</cell><cell>Early</cell><cell cols="2">14.62 12.92</cell></row><row><cell></cell><cell>IC</cell><cell>Late</cell><cell cols="2">15.34 13.01</cell></row><row><cell></cell><cell>IC</cell><cell>Early</cell><cell cols="2">15.27 13.00</cell></row><row><cell cols="3">ResNeXt WSL IC + COCO + Reddit Late</cell><cell cols="2">15.09 12.95</cell></row><row><cell>Spatial</cell><cell cols="2">IC + COCO + Reddit Early</cell><cell cols="2">15.55 13.50</cell></row><row><cell></cell><cell>IC + COCO</cell><cell>Late</cell><cell cols="2">15.02 12.95</cell></row><row><cell></cell><cell>IC + COCO</cell><cell>Early</cell><cell cols="2">14.62 12.87</cell></row><row><cell></cell><cell>IC</cell><cell>Late</cell><cell cols="2">13.99 12.51</cell></row><row><cell></cell><cell>IC</cell><cell>Early</cell><cell cols="2">13.76 12.42</cell></row><row><cell></cell><cell cols="2">IC + COCO + Reddit Late</cell><cell cols="2">13.75 12.43</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">IC + COCO + Reddit Early</cell><cell cols="2">13.44 12.29</cell></row><row><cell></cell><cell>IC + COCO</cell><cell>Late</cell><cell cols="2">13.82 12.48</cell></row><row><cell></cell><cell>IC + COCO</cell><cell>Early</cell><cell cols="2">13.56 12.37</cell></row></table><note>, while results for fine-tuning on Image-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>PPL</cell><cell>F1</cell><cell cols="2">BLEU-4 ROUGE-L</cell></row><row><cell>Image Chat (First Round)</cell><cell cols="2">13.56 11.96</cell><cell>0.411</cell><cell>16.72</cell></row><row><cell>Image Chat</cell><cell cols="2">12.64 13.14</cell><cell>0.418</cell><cell>18.00</cell></row><row><cell>BlendedSkillTalk</cell><cell cols="2">9.98 17.84</cell><cell>0.980</cell><cell>19.25</cell></row><row><cell>Wizard of Wikipedia (Seen)</cell><cell cols="2">8.82 18.63</cell><cell>2.224</cell><cell>17.39</cell></row><row><cell>ConvAI2</cell><cell cols="2">8.78 18.41</cell><cell>1.080</cell><cell>22.64</cell></row><row><cell>EmpatheticDialogues</cell><cell cols="2">8.46 19.23</cell><cell>1.448</cell><cell>24.46</cell></row></table><note>Ablation analysis of the impacts of various image features, training data (including domain-adaptive pre- training), and image fusion techniques when training on the Image-Chat dataset alone (i.e., ignoring the text-only dialogue datasets). As in Table 2, we note that Faster R-CNN features yield the best results on Image-Chat.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test results of best multi-task model on BST + and Image Chat datasets, measured via perplexity (ppl), F1, BLEU-4, and ROUGE-L scores. ConvAI2 results are reported on the validation set, as the test set is hidden.</figDesc><table><row><cell>Model</cell><cell></cell><cell>ConvAI2</cell><cell></cell><cell></cell><cell>ED</cell><cell></cell><cell></cell><cell>WoW Seen</cell><cell></cell><cell></cell><cell>BST</cell><cell></cell><cell></cell><cell>IC</cell><cell></cell></row><row><cell></cell><cell>F1</cell><cell>B</cell><cell>R</cell><cell>F1</cell><cell>B</cell><cell>R</cell><cell>F1</cell><cell>B</cell><cell>R</cell><cell>F1</cell><cell>B</cell><cell>R</cell><cell>F1</cell><cell>B</cell><cell>R</cell></row><row><cell>DialoGPT</cell><cell>11.4</cell><cell>0.1</cell><cell>8.5</cell><cell>10.8</cell><cell>0.3</cell><cell>8.2</cell><cell>8.6</cell><cell>0.1</cell><cell>5.9</cell><cell>10.5</cell><cell>0.1</cell><cell>7.6</cell><cell>6.2</cell><cell>0.1</cell><cell>5.2</cell></row><row><cell>(Zhang et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dodeca</cell><cell>21.7</cell><cell>5.5</cell><cell>33.7</cell><cell>19.3</cell><cell>3.7</cell><cell>31.4</cell><cell>38.4*</cell><cell>21.0*</cell><cell>45.4*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.9</cell><cell>2.1</cell><cell>24.6</cell></row><row><cell>(Shuster et al., 2019b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2AMMC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.3</cell><cell>0.1</cell><cell>11.0</cell></row><row><cell>(Ju et al., 2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BlenderBot</cell><cell>18.4</cell><cell>1.1</cell><cell>22.7</cell><cell>19.1</cell><cell>1.4</cell><cell>24.2</cell><cell>18.8</cell><cell>2.3</cell><cell>17.5</cell><cell>17.8</cell><cell>1.0</cell><cell>19.2</cell><cell>9.2</cell><cell>0.1</cell><cell>12.3</cell></row><row><cell>(Roller et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-Modal BlenderBot</cell><cell>18.4</cell><cell>1.1</cell><cell>22.6</cell><cell>19.2</cell><cell>1.5</cell><cell>24.5</cell><cell>18.6</cell><cell>2.2</cell><cell>17.4</cell><cell>17.8</cell><cell>1.0</cell><cell>19.3</cell><cell>13.1</cell><cell>0.4</cell><cell>18.0</cell></row><row><cell>(ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Per-turn annotations and mean engaging-</cell></row><row><cell cols="6">ness ratings of human/model conversations with MMB</cell></row><row><cell cols="6">Style and BlenderBot. Both models perform roughly</cell></row><row><cell cols="6">equivalently on these metrics. Ranges given are</cell></row><row><cell cols="4">plus/minus one standard deviation.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Loss %</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MMB Style MMB Degen BB</cell></row><row><cell>Win %</cell><cell>Engaging Human</cell><cell>MMB Style MMB Degen BlenderBot MMB Style MMB Degen BlenderBot</cell><cell>50 55 48 47</cell><cell>50 57 52 47</cell><cell>45 43 53 53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">: ACUTE-Evals (engagingness and human-</cell></row><row><cell cols="2">ness) on human/model conversations with MMB Style,</cell></row><row><cell cols="2">MMB Degendered, and BlenderBot. No ratings are sta-</cell></row><row><cell>tistically significant (&gt;100 ratings per matchup).</cell><cell></cell></row><row><cell cols="2">Baseline vs MMB</cell></row><row><cell>DialoGPT std. beam DialoGPT min beam 20 DialoGPT std. beam 29  Human 17  *  Engaging 33  *  DialoGPT min beam 20 40</cell><cell>83  *  67  *</cell></row></table><note>* 71* Meena 37* 63** 60* Meena 36* 64*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: ACUTE-Evals (engagingness and humanness)</cell></row><row><cell>show that MMB Style outperforms DialoGPT with</cell></row><row><cell>standard generation parameters (GPT-2 medium, beam</cell></row><row><cell>search with beam width 10), DialoGPT with the same</cell></row><row><cell>parameters but a min beam length of 20 (to match</cell></row><row><cell>BlenderBot's setting), and Meena. Asterisk indicates</cell></row><row><cell>significance (two-tailed binomial test, p &lt; 0.05).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Sweet MMB I would love to take my wife here for our anniversary. It would be so romantic. Dodeca What a beautiful view! 2AMMC Oh what a great honeymoon spot with the lovely view of the mountains.(Style) Maternal (Mother-like) MMB I would love to take my children here to show them the beauty of the earth and its inhabitants. Dodeca I would love to take my kids here. 2AMMC I would like to hide my kids safe in that cavern from a storm. These flowers remind me of my grandmother's garden. I miss her so much. Dodeca I feel bad for the flowers. 2AMMC I can't imagine how difficult it is for the farmer to round them all up. He works so hard.</figDesc><table><row><cell>Image</cell><cell>Output</cell></row><row><cell cols="2">(Style) (Style) Fearful</cell></row><row><cell cols="2">MMB I hope that dog doesn't bite me. I don't want to get bitten by a dog.</cell></row><row><cell cols="2">Dodeca I hope that dog doesn't bite me.</cell></row><row><cell cols="2">2AMMC This dog looks ready to attack. It's focused on something, maybe</cell></row><row><cell></cell><cell>another animal? I hope it doesn't go after it or anyone around!</cell></row><row><cell cols="2">(Style) Playful</cell></row><row><cell cols="2">MMB I would love to climb to the top of that sculpture and play in the fountain.</cell></row><row><cell cols="2">Dodeca I would love to visit this place!</cell></row><row><cell cols="2">2AMMC Who wants to put bubble bath in the fountain with me?</cell></row><row><cell cols="2">(Style) Pompous (Self-important, Arrogant)</cell></row><row><cell cols="2">MMB I have a statue just like this one in my backyard, but it's much nicer.</cell></row><row><cell cols="2">Dodeca I could take a better picture than that.</cell></row><row><cell cols="2">2AMMC I could make a fence that looks way better</cell></row><row><cell cols="2">(Style) Empathetic</cell></row><row><cell>MMB</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Example outputs from MMB Style, Dodeca fine-tuned on Image-Chat, and 2AMMC, on images from. Image-Chat styles fed to the models are shown above the models' responses.</figDesc><table><row><cell></cell><cell></cell><cell>Loss %</cell><cell></cell></row><row><cell></cell><cell cols="3">MMB Dodeca 2AMMC</cell></row><row><cell>Win %</cell><cell>MMB Style Dodeca 35  *  2AMMC 51</cell><cell>65  *  61  *</cell><cell>49 39  *</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>MMB Style</cell><cell></cell></row><row><cell cols="2">Multi-task vs. FT Image-Chat</cell></row><row><cell>48</cell><cell>52</cell></row><row><cell>: ACUTE-Evals on the image-response metric</cell><cell></cell></row><row><cell>show that MMB Style and 2AMMC significantly out-</cell><cell></cell></row><row><cell>perform Dodeca fine-tuned on Image-Chat. ACUTE-</cell><cell></cell></row><row><cell>Evals are measured on the models' first response to an</cell><cell></cell></row><row><cell>image only.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: ACUTE-Evals show no significant difference</cell></row><row><cell>on the image-response metric for MMB Style vs. an</cell></row><row><cell>equivalent model only fine-tuned on Image-Chat and</cell></row><row><cell>no dialogue datasets. ACUTE-Evals are measured on</cell></row><row><cell>the models' first response to an image.</cell></row></table><note>* 66* Dodeca 30* 38* 2AMMC 34* 62*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: ACUTE-Evals show that MMB Style signifi-</cell></row><row><cell>cantly outperforms Dodeca and often 2AMMC on vari-</cell></row><row><cell>ous metrics on human/model conversation about an im-</cell></row><row><cell>age.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: MMB Style</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="4">: The frequency of utterances containing gen-</cell></row><row><cell cols="4">dered words is greatly reduced for degendered models</cell></row><row><cell cols="4">(MMB Degendered, MMB DegenPos), given contexts</cell></row><row><cell cols="4">from ConvAI2 and the same generation parameters as</cell></row><row><cell cols="2">in Roller et al. (2020).</cell><cell></cell></row><row><cell cols="2">BST Conv</cell><cell cols="2">ED WoW</cell><cell>IC Avg</cell></row><row><cell>Style 10.15</cell><cell cols="2">8.78 8.31</cell><cell>8.88 12.36 9.70</cell></row><row><cell>Degen 10.14</cell><cell cols="2">8.76 8.21</cell><cell>9.01 12.58 9.74</cell></row><row><cell>Pos 10.15</cell><cell cols="2">8.76 8.27</cell><cell>8.95 12.55 9.74</cell></row><row><cell>DP 10.36</cell><cell cols="2">8.97 8.34</cell><cell>9.41 12.65 9.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: Perplexities of MMB Style, MMB Degen-</cell></row><row><cell cols="4">dered, MMB Positive, and MMB DegenPos on the val-</cell></row><row><cell cols="4">idation set. For Image-Chat, styles are used in the con-</cell></row><row><cell cols="4">text for all models, for consistency. (MMB Positive</cell></row><row><cell cols="4">and MMB DegenPos observed styles for 25% of Image-</cell></row><row><cell cols="2">Chat examples during training.)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Loss %</cell></row><row><cell></cell><cell cols="3">Style Degen Pos DP</cell></row><row><cell></cell><cell>MMB Style</cell><cell>54</cell><cell>49 56</cell></row><row><cell>Win %</cell><cell>MMB Degendered 46 MMB Positive 51</cell><cell>52</cell><cell>48 52 41</cell></row><row><cell></cell><cell>MMB DegenPos 44</cell><cell>48</cell><cell>59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15</head><label>15</label><figDesc></figDesc><table><row><cell>: ACUTE-Evals on the models' first response</cell></row><row><cell>to an image show no significant differences in how well</cell></row><row><cell>MMB models can respond to the image, even if the</cell></row><row><cell>model is degendered or was trained to not require con-</cell></row><row><cell>crete Image-Chat styles.</cell></row><row><cell>MMB Positive</cell></row><row><cell>With image vs. Without image</cell></row><row><cell>80</cell></row></table><note>* 20*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16</head><label>16</label><figDesc></figDesc><table><row><cell>: ACUTE-Evals show that the MMB Positive</cell></row><row><cell>model is significantly better at responding to an image</cell></row><row><cell>than an equivalent model not shown any images during</cell></row><row><cell>training or inference.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17</head><label>17</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Toxicity of human baseline (top row) and</cell></row><row><cell cols="6">MMB variants as assessed with different control vari-</cell></row><row><cell cols="6">ables. The human baseline is set by evaluating gold</cell></row><row><cell cols="6">labels from the first rounds (turns) of the Image-Chat</cell></row><row><cell cols="2">validation set.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Style</cell><cell cols="4">Pos C Pos B Neg C Neg B</cell></row><row><cell></cell><cell>Cheerful</cell><cell>2.41</cell><cell>0.00</cell><cell>3.81</cell><cell>0.09</cell></row><row><cell>Style</cell><cell>Relaxed Angry</cell><cell>3.87 67.07</cell><cell>0.00 0.22</cell><cell>6.47 62.62</cell><cell>0.09 0.27</cell></row><row><cell></cell><cell>Cruel</cell><cell>77.57</cell><cell>1.42</cell><cell>73.67</cell><cell>0.83</cell></row><row><cell></cell><cell>Cheerful</cell><cell>1.50</cell><cell>0.00</cell><cell>3.19</cell><cell>0.09</cell></row><row><cell>Dgen</cell><cell>Relaxed Angry</cell><cell>2.55 53.90</cell><cell>0.00 0.33</cell><cell>4.43 51.64</cell><cell>0.04 0.31</cell></row><row><cell></cell><cell>Cruel</cell><cell>58.28</cell><cell>0.95</cell><cell>57.00</cell><cell>0.84</cell></row><row><cell>Pos</cell><cell cols="2">Pos/Neu Negative 30.96 7.00</cell><cell>0.00 0.22</cell><cell>12.98 31.05</cell><cell>0.22 0.09</cell></row><row><cell>Dgen</cell><cell cols="2">Pos/Neu Negative 25.86 4.56</cell><cell>0.04 0.26</cell><cell>8.86 25.42</cell><cell>0.18 0.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19 :</head><label>19</label><figDesc>Fraction of the time that crowdsource workers select a particular reason for choosing one human/model conversation over another when comparing MMB variants with BlenderBot during ACUTE-Evals on the engagingness metric. Conversations do not include images.</figDesc><table><row><cell></cell><cell cols="4">MMB Style DialoGPT std. beam DialoGPT min beam 20 Meena</cell></row><row><cell>Contradicts themselves less</cell><cell>8%</cell><cell>9%</cell><cell>14%</cell><cell>11%</cell></row><row><cell>Better English</cell><cell>32%</cell><cell>53%</cell><cell>46%</cell><cell>37%</cell></row><row><cell>Repeats themselves less</cell><cell>13%</cell><cell>3%</cell><cell>13%</cell><cell>13%</cell></row><row><cell>More on-topic</cell><cell>37%</cell><cell>33%</cell><cell>43%</cell><cell>32%</cell></row><row><cell>Makes more sense</cell><cell>47%</cell><cell>38%</cell><cell>47%</cell><cell>43%</cell></row><row><cell>More detailed / less vague</cell><cell>35%</cell><cell>17%</cell><cell>34%</cell><cell>34%</cell></row><row><cell>More knowledgeable</cell><cell>33%</cell><cell>28%</cell><cell>30%</cell><cell>25%</cell></row><row><cell>Better listener / more inquisitive</cell><cell>34%</cell><cell>17%</cell><cell>25%</cell><cell>29%</cell></row><row><cell>More entertaining/witty/thoughtful</cell><cell>17%</cell><cell>14%</cell><cell>21%</cell><cell>20%</cell></row><row><cell>Other</cell><cell>1%</cell><cell>2%</cell><cell>1%</cell><cell>1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 20 :</head><label>20</label><figDesc>Fraction of the time that crowdsource workers select a particular reason for choosing one human/model conversation over another when comparing MMB Style to existing text-only models during ACUTE-Evals on the engagingness metric. Conversations do not include images. Models and generation parameters are as inTable 8.</figDesc><table><row><cell></cell><cell cols="3">MMB Style Dodeca 2AMMC</cell></row><row><cell>Contradicts themselves less</cell><cell>9%</cell><cell>8%</cell><cell>11%</cell></row><row><cell>Better English</cell><cell>30%</cell><cell>38%</cell><cell>33%</cell></row><row><cell>Repeats themselves less</cell><cell>10%</cell><cell>16%</cell><cell>8%</cell></row><row><cell>More on-topic</cell><cell>33%</cell><cell>31%</cell><cell>31%</cell></row><row><cell>Makes more sense</cell><cell>45%</cell><cell>31%</cell><cell>54%</cell></row><row><cell>More detailed / less vague</cell><cell>32%</cell><cell>22%</cell><cell>16%</cell></row><row><cell>More knowledgeable</cell><cell>30%</cell><cell>28%</cell><cell>20%</cell></row><row><cell>Better listener / more inquisitive</cell><cell>25%</cell><cell>18%</cell><cell>24%</cell></row><row><cell>More entertaining/witty/thoughtful</cell><cell>20%</cell><cell>17%</cell><cell>18%</cell></row><row><cell>Other</cell><cell>1%</cell><cell>1%</cell><cell>1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 21 :</head><label>21</label><figDesc>Fraction of the time that crowdsource workers select a particular reason for choosing one human/model conversation over another when comparing MMB Style to other multi-modal models during ACUTE-Evals on the engagingness metric. Conversations are started by the model responding to an image. Models and generation parameters are as inTable 12.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/ ParlAI/tree/master/projects/multimodal_ blenderbot</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/hub/ facebookresearch_WSL-Images_resnext/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/ vilbert-multi-task As our resulting model can be seen as a multimodal extension to the BlenderBot model<ref type="bibr" target="#b40">(Roller et al., 2020)</ref>, we refer to it as "Multi-Modal BlenderBot" (MMB).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Lists of positive, neutral, and negative styles are from http://ideonomy.mit.edu/essays/traits. html, following<ref type="bibr" target="#b44">Shuster et al. (2019a)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://parl.ai</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We select only images that fall under a CC-BY license and do not contain recognizable people.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/LDNOOBW</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pushshift reddit dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savvas</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="830" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Queens are powerful too: Mitigating gender bias in dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jason Weston</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00614</idno>
	</analytic>
	<monogr>
		<title level="m">Douwe Kiela, and Adina Williams. 2020a. Multi-dimensional gender bias classification</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Build it break it fix it for dialogue safety: Robustness from adversarial human attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Chintagunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4537" to="4546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The second conversational intelligence challenge (Con-vAI2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NeurIPS &apos;18 Competition</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="187" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wizard of Wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vse++: Improving visualsemantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual crossmodal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00750</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What we instagram: A first analysis of instagram photo content and user types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lydia</forename><surname>Manikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">All-in-one image-grounded conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12394</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Detecting offensive content in open-domain conversations using two stage semisupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Anushree Venkatesh, Raefer Gabriel, and Arindam Mandal</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Supervised multimodal bitransformers for classifying images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrat</forename><surname>Bhooshan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Hamed Firooz, and Davide Testuggine</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6795</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ACUTE-EVAL: Improved dialogue evaluation with optimized questions and multi-turn comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop on Conversational AI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Does gender matter? towards fairness in dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamell</forename><surname>Dacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training millions of personalized dialogue agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2775" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-grounded conversations: Multimodal context for natural question and response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="462" to="472" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2156" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-view attention networks for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeochan</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueiseok</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w19-4302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
		<meeting>the 4th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Intermediate-task transfer learning with pretrained language models: When and why does it work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phu</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.467</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5231" to="5247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards empathetic opendomain conversation models: A new benchmark and dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5370" to="5381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13637</idno>
		<title level="m">Recipes for building an open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Offence in dialogues: A corpus-based study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Burtenshaw</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-056-4_125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1085" to="1093" />
		</imprint>
	</monogr>
	<note>RANLP 2019. INCOMA Ltd</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image-chat: Engaging grounded conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2414" to="2429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Engaging image captioning via personality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/mmf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Are we pretraining it right? digging deeper into visio-linguistic pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can you put it all together: Evaluating conversational agents&apos; ability to blend skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. ACL</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A novel attention-based aggregation function to combine vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5100" to="5111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Vdbert: A unified vision and dialog transformer with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Conversations gone awry: Detecting early signs of conversational failure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Taraborelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1350" to="1361" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dialogpt: Large-scale generative pre-training for conversational response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, system demonstration</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
