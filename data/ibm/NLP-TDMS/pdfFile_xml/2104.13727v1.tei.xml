<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ♢ ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ♢ ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♢</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ♢ ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ♢ ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrasestructure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols. We further use neural parameterization for the new form to improve unsupervised parsing performance. We evaluate our model across ten languages and empirically demonstrate the effectiveness of using more symbols.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised constituency parsing is the task of inducing phrase-structure grammars from raw text without using parse tree annotations. Early work induces probabilistic context-free grammars (PCFGs) via the Expectation Maximation algorithm and finds the result unsatisfactory <ref type="bibr" target="#b19">(Lari and Young, 1990;</ref><ref type="bibr" target="#b3">Carroll and Charniak, 1992)</ref>. Recently, PCFGs with neural parameterization (i.e., using neural networks to generate rule probabilities) have been shown to achieve good results in unsupervised constituency parsing <ref type="bibr" target="#b15">(Kim et al., 2019a;</ref><ref type="bibr" target="#b14">Jin et al., 2019;</ref><ref type="bibr" target="#b40">Zhu et al., 2020)</ref>. However, due to the cubic computational complexity of PCFG representation and parsing, these approaches learn PCFGs with relatively small numbers of nonterminals and preterminals. For example, <ref type="bibr" target="#b14">Jin et al. (2019)</ref> use 30 * Corresponding Author 1 Our code: https://github.com/sustcsonglin/TN-PCFG nonterminals (with no distinction between preterminals and other nonterminals) and <ref type="bibr" target="#b15">Kim et al. (2019a)</ref> use 30 nonterminals and 60 preterminals.</p><p>In this paper, we study PCFG induction with a much larger number of nonterminal and preterminal symbols. We are partly motivated by the classic work of latent variable grammars in supervised constituency parsing <ref type="bibr" target="#b22">(Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b23">Petrov et al., 2006;</ref><ref type="bibr" target="#b20">Liang et al., 2007;</ref><ref type="bibr" target="#b6">Cohen et al., 2012;</ref><ref type="bibr" target="#b39">Zhao et al., 2018)</ref>. While the Penn treebank grammar contains only tens of nonterminals and preterminals, it has been found that dividing them into subtypes could significantly improves the parsing accuracy of the grammar. For example, the best model from <ref type="bibr" target="#b23">Petrov et al. (2006)</ref> contains over 1000 nonterminal and preterminal symbols. We are also motivated by the recent work of <ref type="bibr" target="#b1">Buhai et al. (2019)</ref> who show that when learning latent variable models, increasing the number of hidden states is often helpful; and by <ref type="bibr" target="#b4">Chiu and Rush (2020)</ref> who show that a neural hidden Markov model with up to 2 16 hidden states can achieve surprisingly good performance in language modeling. A major challenge in employing a large number of nonterminal and preterminal symbols is that representing and parsing with a PCFG requires a computational complexity that is cubic in its symbol number. To resolve the issue, we rely on a new parameterization form of PCFGs based on tensor decomposition, which reduces the computational complexity from cubic to at most quadratic. Furthermore, we apply neural parameterization to the new form, which is crucial for boosting unsupervised parsing performance of PCFGs as shown by <ref type="bibr" target="#b15">Kim et al. (2019a)</ref>.</p><p>We empirically evaluate our approach across ten languages. On English WSJ, our best model with 500 preterminals and 250 nonterminals improves over the model with 60 preterminals and 30 nonter-minals by 6.3% mean F1 score, and we also observe consistent decrease in perplexity and overall increase in F1 score with more symbols in our model, thus confirming the effectiveness of using more symbols. Our best model also surpasses the strong baseline Compound PCFGs <ref type="bibr" target="#b15">(Kim et al., 2019a</ref>) by 1.4% mean F1. We further conduct multilingual evaluation on nine additional languages. The evaluation results suggest good generalizability of our approach on languages beyond English.</p><p>Our key contributions can be summarized as follows: (1) We propose a new parameterization form of PCFGs based on tensor decomposition, which enables us to use a large number of symbols in PCFGs.</p><p>(2) We further apply neural parameterization to improve unsupervised parsing performance.</p><p>(3) We evaluate our model across ten languages and empirically show the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar induction using neural networks:</head><p>There is a recent resurgence of interest in unsupervised constituency parsing, mostly driven by neural network based methods <ref type="bibr" target="#b27">(Shen et al., 2018a</ref><ref type="bibr" target="#b28">(Shen et al., , 2019</ref><ref type="bibr" target="#b8">Drozdov et al., 2019</ref><ref type="bibr" target="#b7">Drozdov et al., , 2020</ref><ref type="bibr">Kim et al., 2019a,b;</ref><ref type="bibr" target="#b14">Jin et al., 2019;</ref><ref type="bibr" target="#b40">Zhu et al., 2020)</ref>. These methods can be categorized into two major groups: those built on top of a generative grammar and those without a grammar component. The approaches most related to ours belong to the first category, which use neural networks to produce grammar rule probabilities. <ref type="bibr" target="#b14">Jin et al. (2019)</ref> use an invertible neural projection network (a.k.a. normalizing flow (Rezende and Mohamed, 2015)) to parameterize the preterminal rules of a PCFG. <ref type="bibr" target="#b15">Kim et al. (2019a)</ref> use neural networks to parameterize all the PCFG rules. <ref type="bibr" target="#b40">Zhu et al. (2020)</ref> extend their work to lexicalized PCFGs, which are more expressive than PCFGs and can model both dependency and constituency parse trees simultaneously.</p><p>In other unsupervised syntactic induction tasks, there is also a trend to use neural networks to produce grammar rule probabilities. In unsupervised dependency parsing, the Dependency Model with Valence (DMV) <ref type="bibr" target="#b18">(Klein and Manning, 2004)</ref> has been parameterized neurally to achieve higher induction accuracy <ref type="bibr" target="#b13">(Jiang et al., 2016;</ref><ref type="bibr" target="#b36">Yang et al., 2020)</ref>. In part-of-speech (POS) induction, neurally parameterized Hidden Markov Models (HMM) also achieve state-of-the-art results <ref type="bibr" target="#b33">(Tran et al., 2016;</ref><ref type="bibr" target="#b12">He et al., 2018)</ref>.</p><p>Tensor decomposition on PCFGs: Our work is closely related to <ref type="bibr" target="#b5">Cohen et al. (2013)</ref> in that both use tensor decomposition to parameterize the probabilities of binary rules for the purpose of reducing the time complexity of the inside algorithm. However, <ref type="bibr" target="#b5">Cohen et al. (2013)</ref> use this technique to speed up inference of an existing PCFG, and they need to actually perform tensor decomposition on the rule probability tensor of the PCFG. In contrast, we draw inspiration from this technique to design a new parameterization form of PCFG that can be directly learned from data. Since we do not have a probability tensor to start with, additional tricks have to be inserted in order to ensure validity of the parameterization, as will be discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tensor form of PCFGs</head><p>PCFGs build upon context-free grammars (CFGs). We start by introducing CFGs and establishing notations. A CFG is defined as a 5-tuple G = (S, N , P, Σ, R) where S is the start symbol, N is a finite set of nonterminal symbols, P is a finite set of preterminal symbols, 2 Σ is a finite set of terminal symbols, and R is a set of rules in the following form:</p><formula xml:id="formula_0">S → A A ∈ N A → BC, A ∈ N , B, C ∈ N ∪ P T → w, T ∈ P, w ∈ Σ</formula><p>PCFGs extend CFGs by associating each rule r ∈ R with a probability π r . Denote n, p, and q as the number of symbols in N , P, and Σ, respectively. It is convenient to represent the probabilities of the binary rules in the tensor form:</p><formula xml:id="formula_1">T h A ,h B ,h C = π A→BC , T ∈ R n×m×m ,</formula><p>where T is an order-3 tensor, m = n + p, and h A ∈ [0, n) and h B , h C ∈ [0, m) are symbol indices. For the convenience of computation, we assign indices [0, n) to nonterminals in N and [n, m) to preterminals in P. Similarly, for a preterminal rule we define</p><formula xml:id="formula_2">Q h T ,h w = π T →w , Q ∈ R p×q .</formula><p>Again, h T and h w are the preterminal index and the terminal index, respectively. Finally, for a start rule we define</p><formula xml:id="formula_3">r h A = π S→A , r ∈ R n .</formula><p>Generative learning of PCFGs involves maximizing the log-likelihood of every observed sentence w = w 1 , . . . , w l :</p><formula xml:id="formula_4">log p θ (w) = log t∈T G (w) p(t) ,</formula><p>where T G (w) contains all the parse trees of the sentence w under a PCFG G. The probability of a parse tree t ∈ T G is defined as p(t) = ∏ r∈t R π r , where t R is the set of rules used in the derivation of t. log p θ (w) can be estimated efficiently through the inside algorithm, which is fully differentiable and amenable to gradient optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tensor form of the inside algorithm</head><p>We first pad T, Q, and r with zeros such that T ∈ R m×m×m , Q ∈ R m×q , r ∈ R m , and all of them can be indexed by both nonterminals and preterminals.</p><p>The inside algorithm computes the probability of a symbol A spanning a substring w i,j = w i , . . . , w j in a recursive manner (0 ≤ i &lt; j &lt; l):</p><formula xml:id="formula_5">s A i,j = j−1 k=i B,C π A→BC ⋅ s B i,k ⋅ s C k+1,j . (1) Base Case: s T i,i = π T →w i , 0 ≤ i &lt; l .</formula><p>We use the tensor form of PCFGs to rewrite Equation 1 as:</p><formula xml:id="formula_6">s h A i,j = j−1 k=i h B ,h C T h A ,h B ,h C ⋅ s h B i,k ⋅ s h C k+1,j = j−1 k=i T h A ⋅ s k+1,j ⋅ s i,k ,<label>(2)</label></formula><p>where s i,j , s i,k , and s k+1,j are all m-dimensional vectors; the dimension h A corresponds to the symbol A. Thus</p><formula xml:id="formula_7">s i,j = j−1 k=i T ⋅ s k+1,j ⋅ s i,k .<label>(3)</label></formula><p>Equation <ref type="formula" target="#formula_7">3</ref> represents the core computation of the inside algorithm as tensor-vector dot product. It is amenable to be accelerated on a parallel computing device such as GPUs. However, the time and space complexity is cubic in m, which makes it impractical to use a large number of nonterminals and preterminals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parameterizing PCFGs based on tensor decomposition</head><p>The tensor form of the inside algorithm has a high computational complexity of O(m 3 l 3 ). It hinders the algorithm from scaling to a large m. To resolve the issue, we resort to a new parameterization form of PCFGs based on tensor decomposition (TD-PCFGs) <ref type="bibr" target="#b5">(Cohen et al., 2013)</ref>. As discussed in Section 2, while Cohen et al. <ref type="formula" target="#formula_6">(2013)</ref> use a TD-PCFG to approximate an existing PCFG for speedup in parsing, we regard a TD-PCFG as a stand-alone model and learn it directly from data. The basic idea behind TD-PCFGs is using Kruskal decomposition of the order-3 tensor T. Specifically, we require T to be in the Kruskal form,</p><formula xml:id="formula_8">T = d l=1 T (l) , T (l) = u (l) ⊗ v (l) ⊗ w (l) , (4) where u (l) ∈ R n is a column vector of a matrix U ∈ R n×d ; v (l) , w (l) ∈ R m are column vectors of matrices V, W ∈ R m×d , respectively; ⊗ indicates Kronecker product. Thus T (l) ∈ R n×m×m is an order-3 tensor and T (l) i,j,k = u (l) i ⋅ v (l) j ⋅ w (l) k .</formula><p>The Kruskal form of the tensor T is crucial for reducing the computation of Equation 3. To show this, we let x = s i,k , y = s k+1,j , and z be any summand in the right-hand side of Equation 3, so we have:</p><formula xml:id="formula_9">z = (T ⋅ y) ⋅ x .<label>(5)</label></formula><p>Substitute T in Equation 4 into Equation 5 and consider the i-th dimension of z:</p><formula xml:id="formula_10">z i = (T i ⋅ y) ⋅ x = m j=1 m k=1 d l=1 T (l) i,j,k ⋅ x j ⋅ y k = m j=1 m k=1 d l=1 u (l) i ⋅ v (l) j ⋅ w (l) k ⋅ x j ⋅ y k = d l=1 u (l) i ⋅ ⎛ ⎜ ⎝ m j=1 v (l) j ⋅ x j ⎞ ⎟ ⎠ ⋅ ⎛ ⎜ ⎝ m k=1 w (l) k ⋅ y k ⎞ ⎟ ⎠ = d l=1 u (l) i ⋅ x T v (l) ⋅ y T w (l) = e T i U ⋅ V T x ⊙ W T y ,<label>(6)</label></formula><p>where ⊙ indicates Hadamard (element-wise) product; e i ∈ R m is a one-hot vector that selects the i-th row of U. We have padded U with zeros such that U ∈ R m×d and the last m − n rows are all zeros. Thus</p><formula xml:id="formula_11">z = U ⋅ V T x ⊙ W T y ,<label>(7)</label></formula><p>and accordingly,</p><formula xml:id="formula_12">s i,j = U ⋅ j−1 k=i V T s i,k ⊙ W T s k+1,j . (8)</formula><p>Equation <ref type="formula">8</ref>   <ref type="bibr" target="#b32">(Socher et al., 2013)</ref> if we treat inside score vectors as span embeddings.</p><p>One problem with TD-PCFGs is that, since we use three matrices U, V and W to represent tensor T of binary rule probabilities, how we can ensure that T is non-negative and properly normalized, i.e., for a given left-hand side symbol A, ∑ j,k T h A ,j,k = 1. Simply reconstructing T with U, V and W and then performing normalization would take O(m 3 ) time, thus defeating the purpose of TD-PCFGs. Our solution is to require that the three matrices are non-negative and meanwhile U is row-normalized and V and W are columnnormalized <ref type="bibr" target="#b29">(Shen et al., 2018b)</ref>. </p><formula xml:id="formula_13">T ∈ R n×m×m where T i,j,k ∈ [0, 1] and T i is normalized such that ∑ j,k T i,j,k = 1. Proof. m j=1 m k=1 T i,j,k = m j=1 m k=1 d l=1 u (l) i ⋅ v (l) j ⋅ w (l) k = d l=1 u (l) i ⋅ ( m j=1 v (l) j ) ⋅ ( m k=1 w (l) j ) = d l=1 u (l) i ⋅ 1 ⋅ 1 = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural parameterization of TD-PCFGs</head><p>We use neural parameterization for TD-PCFGs as it has demonstrated its effectiveness in inducing PCFGs <ref type="bibr" target="#b15">(Kim et al., 2019a)</ref>. In a neurally parameterized TD-PCFGs, the original TD-PCFG parameters are generated by neural networks, rather than being learned directly; parameters of the neural network will thus be the parameters to be optimized. This modeling approach breaks the parameter number limit of the original TD-PCFG, so we can control the total number of parameters flexibly. When the total number of symbols is small, we can overparameterize the model as over-parameterization has been shown to ease optimization <ref type="bibr" target="#b0">(Arora et al., 2018;</ref><ref type="bibr" target="#b34">Xu et al., 2018;</ref><ref type="bibr" target="#b9">Du et al., 2019)</ref>. On the other hand, when the total number of symbols is huge, we can decrease the number of parameters to save GPU memories and speed up training. Specifically, we use neural networks to generate the following set of parameters of a TD-PCFG:</p><formula xml:id="formula_14">Θ = {U , V , W , Q , r} .</formula><p>The resulting model is referred to as neural PCFGs based on tensor decomposition (TN-PCFGs).</p><p>We start with the neural parameterization of U ∈ R n×d and V, W ∈ R m×d . We use shared symbol embeddings E s ∈ R m×k (k is the symbol embedding dimension) in which each row is the embedding of a nonterminal or preterminal. We first compute an unnormalizedŨ by applying a neural network f u (⋅) to symbol embeddings E s :</p><formula xml:id="formula_15">U = f u (E s ) = ReLU E s M (1) u M (2) u , where M (1) u ∈ R k×k and M<label>(2)</label></formula><p>u ∈ R k×d are learnable parameters of f u (⋅). For simplicity, we omit the learnable bias terms. We compute unnormal-izedṼ andW in a similar way. Note that only E s is shared in computing the three unnormalized matrices. Then we apply the Softmax activation function to each row ofŨ and to each column of V andW, and obtain normalized U, V, and W. For preterminal-rule probabilities Q ∈ R p×q and start-rule probabilities r ∈ R n , we follow <ref type="bibr" target="#b15">(Kim et al., 2019a)</ref> and define them as:</p><formula xml:id="formula_16">Q h T ,h w = π T →w = exp(u T w f t (w T )) ∑ w ′ ∈Σ exp(u T w ′ f t (w T ))</formula><p>,</p><formula xml:id="formula_17">r h A = π S→A = exp(u T A f s (w S )) ∑ A ′ ∈N exp(u T A ′ f s (w S ))</formula><p>, where w and u are symbol embeddings; f s (⋅) and f t (⋅) are neural networks that encode the input into a vector (see details in <ref type="bibr" target="#b15">Kim et al. (2019a)</ref>). Note that the symbol embeddings are not shared between preterminal rules and start rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parsing with TD-PCFGs</head><p>Parsing seeks the most probable parse t ⋆ from all the parses T G (w) of a sentence w:</p><formula xml:id="formula_18">t ⋆ = arg max t∈T G (w) p(t|w) .<label>(9)</label></formula><p>Typically, the CYK algorithm 3 can be directly used to solve this problem exactly: it first computes the score of the most likely parse; and then automatic differentiation is applied to recover the best tree structure t ⋆ <ref type="bibr" target="#b10">(Eisner, 2016;</ref><ref type="bibr" target="#b25">Rush, 2020</ref> become unaffordable when m is large. Therefore, we resort to Minimum Bayes-Risk (MBR) style decoding because we can compute the inside probabilities efficiently. Our decoding method consists of two stages. The first stage computes the conditional probability of a substring w i,j being a constituent in a given sentence w (a.k.a. posteriors of spans being a constituent):</p><formula xml:id="formula_19">p(w i,j |w) = 1 p(w) t∈T G (w) p(t) ⋅ 1 {w i,j ∈t} .</formula><p>We can estimate the posteriors efficiently by using automatic differentiation after obtaining all the inside probabilities. This has the same time complexity as our improved inside algorithm, which is O(dl 3 + mdl 2 ). The second stage uses the CYK algorithm to find the parse tree that has the highest expected number of constituents <ref type="bibr" target="#b31">(Smith and Eisner, 2006)</ref>: We evaluate TN-PCFGs across ten languages. We use the Wall Street Journal (WSJ) corpus of the Penn Treebank <ref type="bibr" target="#b21">(Marcus et al., 1994)</ref> for English, the Penn Chinese Treebank 5.1 (CTB) <ref type="bibr" target="#b35">(Xue et al., 2005)</ref> for Chinese, and the SPRML dataset <ref type="bibr" target="#b26">(Seddah et al., 2014)</ref> for the other eight morphologyrich languages. We use a unified data preprocessing pipeline 5 provided by <ref type="bibr" target="#b37">Zhao (2020)</ref>. The same pipeline has been used in several recent papers <ref type="bibr" target="#b27">(Shen et al., 2018a</ref><ref type="bibr" target="#b28">(Shen et al., , 2019</ref><ref type="bibr" target="#b15">Kim et al., 2019a;</ref><ref type="bibr" target="#b38">Zhao and Titov, 2020)</ref>. Specifically, for every treebank, punctuation is removed from all data splits and the top 10,000 frequent words in the training data are used as the vocabulary.</p><formula xml:id="formula_20">t ⋆ = arg max t∈T G (w) w i,j ∈t p(w i,j |w) .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Settings and hyperparameters</head><p>For baseline models we use the best configurations reported by the authors. For example, we use 30 nonterminals and 60 preterminals for N-PCFGs and C-PCFGs. We implement TN-PCFGs and reimplement N-PCFGs and C-PCFGs using automatic differentiation <ref type="bibr" target="#b10">(Eisner, 2016)</ref> and we borrow the idea of ? to batchify the inside algorithm. Inspired by <ref type="bibr" target="#b15">Kim et al. (2019a)</ref>, for TN-PCFGs we set n/p, the ratio of the nonterminal number to the preterminal number, to 1/2. For U ∈ R n×d and V, W ∈ R m×d we set d = p when there are more than 200 preterminals and d = 200 otherwise. The symbol embedding dimension k is set to 256. We optimize TN-PCFGs using the Adam optimizer (Kingma and Ba, 2015) with β 1 = 0.75, β 2 = 0.999, and learning rate 0.001 with batch size 4. We use the unit Gaussian distribution to initialize embedding parameters. We do not use the curriculum learning strategy that is used by <ref type="bibr" target="#b15">Kim et al. (2019a)</ref>  Systems with pretrained word embeddings DIORA <ref type="bibr" target="#b8">(Drozdov et al., 2019)</ref> 56.8 S-DIORA <ref type="bibr">(Drozdov et al., 2020) 57.6</ref> 64.0 CT <ref type="bibr" target="#b2">(Cao et al., 2020)</ref> 62.8 65.9</p><p>Oracle Trees 84.3 time. Early stopping is performed based on the perplexity on the development data. The best model in each run is selected according to the perplexity on the development data. We tune model hyperparameters only on the development data of WSJ and use the same model configurations on the other treebanks. <ref type="bibr">6</ref> We report average sentence-level F1 score 7 as well as their biased standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experimental results</head><p>We evaluate our models mainly on WSJ (Section 8.1-8.3). We first give an overview of model <ref type="bibr">6 Shi et al. (2020)</ref> suggest not using the gold parses of the development data for hyperparameter tuning and model selection in unsupervised parsing. Here we still use the gold parses of the WSJ development set for the English experiments in order to conduct fair comparison with previous work. No gold parse is used in the experiments of any other language. 7 Following <ref type="bibr" target="#b15">Kim et al. (2019a)</ref>, we remove all trivial spans (single-word spans and sentence-level spans). Sentence-level means that we compute F1 for each sentence and then average over all sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Time <ref type="formula">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Main results</head><p>Our best TN-PCFG model uses 500 preterminals (p = 500). We compare it with a wide range of recent unsupervised parsing models (see the top section of <ref type="table" target="#tab_5">Table 1</ref>). Since we use MBR decoding for TN-PCFGs, which produces higher F1-measure than the CYK decoding <ref type="bibr" target="#b11">(Goodman, 1996)</ref>, for fair comparison we also use MBR decoding for our reimplemented N-PCFGs and C-PCFGs (see the middle section of <ref type="table" target="#tab_5">Table 1</ref>). We draw three key observations from <ref type="table" target="#tab_5">Table 1</ref>: (1) TN-PCFG (p = 500) achieves the best mean and max F1 score. Notebly, it outperforms the strong baseline model C-PCFG by 1.4% mean F1. Compared with TN-PCFG (p = 60), TN-PCFG (p = 500) brings a 6.3% mean F1 improvement, demonstrating the effectiveness of using more symbols.</p><p>(2) Our reimplementations of N-PCFGs and C-PCFGs are comparable to those of <ref type="bibr" target="#b15">Kim et al. (2019a)</ref>, (3) MBR decoding indeed gives higher F1 scores (+1.4% mean F1 for N-PCFG and +0.9% mean F1 for C-PCFG).</p><p>In <ref type="table" target="#tab_5">Table 1</ref> we also show the results of Constituent test (CT) <ref type="bibr" target="#b2">(Cao et al., 2020)</ref> and DIORA <ref type="bibr" target="#b8">(Drozdov et al., 2019</ref><ref type="bibr" target="#b7">(Drozdov et al., , 2020</ref>, two recent state-of-the-art approaches. However, our work is not directly comparable to these approaches. CT relies on pretrained language models (RoBERTa) and DIORA relies on pretrained word embeddings (context insensitive ELMo). In contrast, our model and the other approaches do not use pretrained word embeddings and instead learn word embeddings from scratch. We are also aware of URNNG <ref type="bibr" target="#b16">(Kim et al., 2019b)</ref>, which has a max F1 score of 45.4%, but it uses punctuation and hence is not directly comparable to the models listed in the table. Recall that the nonterminal number n is set to half of p.</p><p>We report the average running time 8 per epoch and the parameter numbers of different models in <ref type="table" target="#tab_7">Table 2</ref>. We can see that TN-PCFG (p = 500), which uses a much larger number of symbols, has even fewer parameters and is not significantly slower than N-PCFG. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the change of F1 scores and perplexities as the number of nonterminals and preterminals increase. We can see that, as the symbol number increases, the perplexities decrease while F1 scores tend to increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Influence of symbol number</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Analysis on constituent labels</head><p>We analyze model performance by breaking down recall numbers by constituent labels <ref type="table" target="#tab_9">(Table 3)</ref>. We use the top six frequent constituent labels in the WSJ test data (NP, VP, PP, SBAR, ADJP, and ADVP). We first observe that the right-branching baseline remains competitive. It achieves the highest recall on VPs and SBARs. TN-PCFG (p = 500) displays a relatively even performance across the six labels. Specifically, it performs best on NPs and PPs among all the labels and it beats all the other models on ADJPs. Compared with TN-PCFG (p = 60), TN-PCFG (p = 500) results in the largest improvement on VPs (+19.5% recall), which are usually long (with an average length of 11) in comparison with the other types of constituents. As NPs and VPs cover about 54% of the total constituents in the WSJ test data, it is not surprising that models which are accurate on these labels have high F1 scores (e.g., C-PCFGs and TN-PCFGs (p = 500)).</p><p>We further analyze the correspondence between the nonterminals of trained models and gold constituent labels. For each model, we look at all the correctly-predicted constituents in the test set and estimate the empirical posterior distribution of nonterminals assigned to a constituent given the gold label of the constituent (see <ref type="figure">Figure 2</ref>). Compared with the other three models, in TN-PCFG (p = 500), the most frequent nonterminals are more likely to correspond to a single gold label. One possible explanation is that it contains much more nonterminals and therefore constituents of different labels are less likely to compete for the same nonterminal. <ref type="figure">Figure 2d</ref> (TN-PCFG (p = 500)) also illustrates that a gold label may correspond to multiple nonterminals. A natural question that follows is: do these nonterminals capture different subtypes of the gold label? We find it is indeed the case for some nonterminals. Take the gold label NPs (noun phrases), while not all the nonterminals have clear interpretation, we find that NT-3 corresponds to constituents which represent a company name; NT-99 corresponds to constituents which contain a possessive affix (e.g., "'s" in "the market 's decline"); NT-94 represents constituents preceded by an indefinite article. We further look into the gold label PPs (preposition phrases). Interestingly, NT-108, NT-175, and NT-218 roughly divided preposition phrases into three groups starting with 'with, by, from, to', 'in, on, for', and 'of', respectively. See Appendix for more examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Multilingual evaluation</head><p>In order to understand the generalizability of TD-PCFGs on languages beyond English, we conduct a multilingual evaluation of TD-PCFGs on CTB and SPMRL. We use the best model configurations obtained on the English development data and do not perform any further tuning on CTB and SPMRL. We compare TN-PCFGs with N-PCFGs and C-PCFGs and use MBR decoding by default. The results are shown in <ref type="table" target="#tab_11">Table 4</ref>. In terms of the average F1 over the nine languages, all the three models beat trivial left-and right-branching baselines by a large margin, which suggests they have good generalizability on languages beyond English. Among the three models, TN-PCFG (p = 500) fares best. It achieves the highest F1 score on six out of nine treebanks. On Swedish, N-PCFG is worse than the right-branching baseline <ref type="bibr">(-13</ref>   <ref type="figure">Figure 2</ref>: Correspondence between nonterminals and gold constituent labels. For each gold label, we visualize the proportion of correctly-predicted constituents that correspond to each nonterminal. Nonterminals NT-# are listed in descending order of the prediction frequency (e.g., NT-21 is the most predicted nonterminal in TN-PCFG (p = 500). Only the top 30 frequent nonterminals are listed. We show the seven most frequent gold labels and aggregate the rest (denoted by OTHER).  TN-PCFG (p = 500) surpasses the right-branching baseline by 9.6% F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussions</head><p>In our experiments, we do not find it beneficial to use the compound trick <ref type="bibr" target="#b15">(Kim et al., 2019a)</ref> in TN-PCFGs, which is commonly used in previous work of PCFG induction <ref type="bibr" target="#b15">(Kim et al., 2019a;</ref><ref type="bibr" target="#b38">Zhao and Titov, 2020;</ref><ref type="bibr" target="#b40">Zhu et al., 2020)</ref>. We speculate that the additional expressiveness brought by compound parameterization may not be necessary for a TN-PCFG with many symbols which is already sufficiently expressive; on the other hand, compound parameterization makes learning harder when we use more symbols. We also find neural parameterization and the choice of nonlinear activation functions greatly influence the performance. Without using neural parameterization, TD-PCFGs have only around 30% S-F1 scores on WSJ, which are even worse than the right-branching baseline. Activation functions other than ReLU (such as tanh and sigmoid) result in much worse performance. It is an interesting open question why ReLU and neural parameterization are crucial in PCFG induction. When evaluating our model with a large number of symbols, we find that only a small fraction of the symbols are predicted in the parse trees (For example, when we use 250 nonterminals, only tens of nonterminals are used). We expect that our models can benefit from regularization techniques such as state dropout <ref type="bibr" target="#b4">(Chiu and Rush, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We have presented TD-PCFGs, a new parameterization form of PCFGs based on tensor decomposition. TD-PCFGs rely on Kruskal decomposition of the binary-rule probability tensor to reduce the computational complexity of PCFG representation and parsing from cubic to at most quadratic in the symbol number, which allows us to scale up TD-PCFGs to a much larger number of (nonterminal and preterminal) symbols. We further propose neurally parameterized TD-PCFGs (TN-PCFGs) and learn neural networks to produce the parameters of TD-PCFGs. On WSJ test data, TN-PCFGs outperform strong baseline models; we empirically show that using more nonterminal and preterminal symbols contributes to the high unsupervised parsing performance of TN-PCFGs. Our multiligual evaluation on nine additional languages further reveals the capability of TN-PCFGs to generalize to languages beyond English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>F1 scores and perplexities w.r.t. the preterminal number p of TN-PCFGs on the WSJ test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Theorem 1. Given non-negative matrices U ∈ R n×d and V, W ∈ R m×d , if U is row-normalized and V and W are column-normalized, then U, V, and W are a Kruskal decomposition of a tensor</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Unlabeled sentence-level F1 scores on the WSJ test data.</figDesc><table><row><cell></cell><cell>indicates numbers reported by Kim</cell></row><row><cell>et al. (2019a).</cell><cell>⋆ indicates our reimplementations of N-</cell></row><row><cell cols="2">PCFGs and C-PCFGs. p denotes the preterminal num-</cell></row><row><cell>ber.</cell><cell></cell></row></table><note>†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Average running time per epoch and the parameter number of each model.</figDesc><table><row><cell>performance in Section 8.1 and then conduct abla-</cell></row><row><cell>tion study of TN-PCFGs in Section 8.2. We quan-</cell></row><row><cell>titatively and qualitatively analyze constituent la-</cell></row><row><cell>bels induced by TN-PCFGs in Section 8.3. In Sec-</cell></row><row><cell>tion 8.4, we conduct a multilingual evaluation over</cell></row><row><cell>nine additional languages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Recall on the six frequent constituent labels in the WSJ test data, † denotes results reported byKim et al.    </figDesc><table><row><cell cols="4">(2019a), S-F1 represents sentence-level F1 measure.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBAR NP VP PP ADJP ADVP S OTHER NT-7 NT-13 NT-2 NT-16 NT-19 NT-1 NT-10 NT-8 NT-20 NT-9 NT-22 NT-24 NT-11 NT-28 NT-25 NT-15 NT-3 NT-17 NT-5 NT-4 NT-27 NT-0 NT-26 NT-18 NT-6 NT-14 NT-12 NT-23 NT-21 NT-29</cell><cell>0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40</cell><cell>SBAR NP VP PP ADJP ADVP S OTHER NT-0 NT-16 NT-14 NT-1 NT-5 NT-13 NT-27 NT-12 NT-20 NT-11 NT-10 NT-2 NT-3 NT-23 NT-19 NT-4 NT-15 NT-18 NT-22 NT-6 NT-28 NT-8 NT-26 NT-17 NT-25 NT-21 NT-7 NT-9 NT-24 NT-29</cell><cell>0.0 0.1 0.2 0.3 0.4 0.5</cell><cell>SBAR NP VP PP ADJP ADVP S OTHER NT-12 NT-0 NT-9 NT-24 NT-3 NT-10 NT-20 NT-15 NT-26 NT-14 NT-11 NT-28 NT-5 NT-6 NT-16 NT-4 NT-7 NT-8 NT-23 NT-2 NT-25 NT-19 NT-21 NT-27 NT-18 NT-22 NT-29 NT-13 NT-17 NT-1</cell><cell>0.0 0.1 0.2 0.3 0.4 0.5 0.6</cell><cell>SBAR NP VP PP ADJP ADVP S OTHER NT-21 NT-178 NT-218 NT-166 NT-40 NT-118 NT-78 NT-146 NT-197 NT-59 NT-99 NT-225 NT-142 NT-219 NT-153 NT-3 NT-158 NT-18 NT-137 NT-94 NT-127 NT-108 NT-119 NT-224 NT-175 NT-10 NT-131 NT-187 NT-81 NT-195</cell><cell>0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40</cell></row><row><cell>(a) N-PCFG</cell><cell></cell><cell>(b) C-PCFG</cell><cell></cell><cell cols="2">(c) TN-PCFG (p = 60)</cell><cell cols="2">(d) TN-PCFG (p = 500)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>PCFG w/ MBR 26.3 ±2.5 35.1 ±2.0 42.3 ±1.6 45.0 ±2.0 45.7 ±2.2 43.5 ±1.2 28.4 ±6.5 43.2 ±0.8 17.0 ±9.9 36.3 C-PCFG w/ MBR 38.7 ±6.6 36.0 ±1.2 43.5 ±1.2 45.0 ±1.1 45.2 ±0.5 44.9 ±1.5 30.5 ±4.2 43.8 ±1.3 33.0 ±15.4 40.1 TN-PCFG p = 500 39.2 ±5.0 36.0 ±3.0 47.1 ±1.7 39.1 ±4.1 39.2 ±10.7 43.1 ±1.1 35.4 ±2.8 48.6 ±3.1 40.0 ±4.8 40.9</figDesc><table><row><cell cols="9">Model Chinese Basque German French Hebrew Hungarian Korean Polish</cell><cell cols="2">Swedish Mean</cell></row><row><cell>Left Branching  †</cell><cell>7.2</cell><cell>17.9</cell><cell>10.0</cell><cell>5.7</cell><cell>8.5</cell><cell>13.3</cell><cell>18.5</cell><cell>10.9</cell><cell>8.4</cell><cell>11.2</cell></row><row><cell>Right Branching  †</cell><cell>25.5</cell><cell>15.4</cell><cell>14.7</cell><cell>26.4</cell><cell>30.0</cell><cell>12.7</cell><cell>19.2</cell><cell>34.2</cell><cell>30.4</cell><cell>23.2</cell></row><row><cell>Random Trees  †</cell><cell>15.2</cell><cell>19.5</cell><cell>13.9</cell><cell>16.2</cell><cell>19.7</cell><cell>14.1</cell><cell>22.2</cell><cell>21.4</cell><cell>16.4</cell><cell>17.6</cell></row><row><cell>N-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Sentence-level F1 scores on CTB and SPMRL.</figDesc><table /><note>† denotes results reported by Zhao (2020).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Strictly, CFGs do not distinguish nonterminals N (constituent labels) from preterminals P (part-of-speech tags). They are both treated as nonterminals. N , P, Σ satisfy N ∩ P = ∅ and (N ∪ P) ∩ Σ = ∅.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/zhaoyanpeng/xcfg.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We measure the running time on a single Titan V GPU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (61976139).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Empirical study of the benefits of overparameterization in learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares-Darius</forename><surname>Buhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00030</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Andrej Risteski, and David Sontag</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised parsing via constituency tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4798" to="4808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Two experiments on learning probabilistic dependency grammars from corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Univ</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling hidden Markov language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1341" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate PCFG parsing using tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
	<note>Georgia</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">O</forename><surname>&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4832" to="4845" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised latent tree induction with deep inside-outside recursive auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long and Short Papers; Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1129" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inside-outside and forwardbackward algorithms are just backprop (tutorial paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5901</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.3115/981863.981887</idno>
	</analytic>
	<monogr>
		<title level="m">34th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Santa Cruz, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of syntactic structure with invertible neural projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="763" to="771" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of PCFGs with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compound probabilistic context-free grammars for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1105" to="1117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1219016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The estimation of stochastic context-free grammars using the insideoutside algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Lari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer speech &amp; language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The infinite PCFG using hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Penn Treebank: Annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Plainsboro, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219850</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.3115/1220175.1220230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Torch-struct: Deep structured prediction library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages<address><addrLine>Dublin, Ireland. Dublin City University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural language modeling by jointly learning syntax and lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Wei Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243</idno>
		<title level="m">Efficient attention: Attention with linear complexities</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the role of supervision in unsupervised constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7611" to="7621" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised neural hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5907</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Benefits of over-parameterization with EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arian</forename><surname>Maleki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10685" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
		<idno type="DOI">10.1017/S135132490400364X</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Second-order unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An empirical study of compound pcfgs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://github.com/zhaoyanpeng/cpcfg" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visually grounded compound PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4369" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gaussian mixture latent vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1181" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The return of lexical dependencies: Neural lexicalized pcfgs. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
