<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungmin</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National Univ</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Seoul National Univ. Namil Kim NAVER LABS Seong-Gyun Jeong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The advent of deep neural networks (DNNs) has shown exceptional performances on various visual recognition tasks using large-scale datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13]</ref>. Training a DNN model begins with curating data and its associated label. In general, the annotation process is expensive and time-consuming. Moreover, we are unable to collect appropriate data in some cases, if events are rarely encountered or related to dangerous situations. Hence, researchers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> are paying attention to leverage synthetic data in a simulation environment, where annotating labels is effortless to a wide range of scenarios.</p><p>To take full advantage of synthetic datasets, domain adaptation has become an active research area. In the domain adaptation setting, we leverage rich annotations on a source domain to achieve strong performance on a target domain regardless of poor annotations. Nevertheless, a model trained only on the source domain provides disappointing * denotes equal contribution. This work was done while the authors were at NAVER LABS. Correspondence to {profile2697, dongwan123}@gmail.com  <ref type="figure">Figure 1</ref>. We illustrate the domain adaptation process with adversarial dropout (AdD). We depict the source and target domains as solid and dashed lines, respectively. Decision boundary of a model only trained on the source domain easily violates the cluster assumption in that it passes through target feature-dense regions (a). We can apply AdD on both the feature extractor (c) and classifier (d). When AdD is used on the feature extractor, the decision boundary is pushed away from feature dense regions. On the contrary, AdD on the classifier pushes features away from the decision boundary. Eventually, our domain adapted model draws a robust decision boundary that avoids clusters (b).</p><p>outcomes when the target domain shows inherently different characteristics. This issue is known as domain shift and is one of the main reasons for performance drops on the target domain. Therefore, we propose a novel method that can reduce the domain shift for domain adaptation.</p><p>In this paper, we tackle unsupervised domain adaptation (UDA), where the target domain is completely unlabelled. Recent works have proposed to align source and target domain distributions through domain adversarial training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12]</ref>. These methods employ an auxiliary domain discriminator to obtain domain-invariant feature representation. The main assumption in domain adversarial training is that if the feature representation is domaininvariant, a classifier trained on the source domain's features will operate on the target domain as well. However, the weaknesses of domain adversarial methods have been pointed out in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Since the domain discriminator simply aligns source and target features without considering the class labels, it is likely that the resulting features will not only be domain-invariant, but also non-discriminative with respect to class labels. Consequently, it is hard to reach the optimal performance on classification.</p><p>Our approach is based on the cluster assumption, which states that decision boundaries should be placed in low density regions in the feature space <ref type="bibr" target="#b4">[5]</ref>. Without model adaptation, the feature extractor generates indiscriminate features for unseen data from the target domain, and the classifier may draw decision boundaries that pass through feature-dense regions on the target domain. Thus, we learn a domain adapted model by pushing the decision boundary away from the target domain's features. Our method, Drop to Adapt (DTA), employs adversarial dropout <ref type="bibr" target="#b30">[31]</ref> to enforce the cluster assumption on the target domain. More precisely, to support various tasks, we introduce element-wise and channel-wise adversarial dropout operations for fully-connected and convolutional layers, respectively. <ref type="figure">Fig. 1</ref> overviews our method, and we design the associated loss functions in Section 3.3.</p><p>We summarize our contributions as follows: 1) We propose a generalized framework in UDA, which is built upon adversarial dropout <ref type="bibr" target="#b30">[31]</ref>. Our implementation supports both convolutional and fully connected layers; 2) We test on various domain adaptation benchmarks for image classification, and achieve competitive results compared to state-of-the-art methods; and 3) We extend the proposed method to a semantic segmentation task in UDA, where we perform adaptation from the simulation to real-world environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain adaptation has been studied extensively. Ben-David et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> examined various divergence metrics between two domains, and defined an upper bound for the target domain error. Based on these studies, image-translation methods minimize the discrepancy between the two domains at an image-level <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>On the other hand, feature alignment methods have attempted to match feature distributions between the source and target domains <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24]</ref>. In particular, Ganin et al. <ref type="bibr" target="#b10">[11]</ref> proposed a domain adversarial training method that aims to generate domain-invariant features by deceiving a domain discriminator. Many recent works use domain adversarial training as a key component in their adaptation procedure <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47]</ref>. However, the domain classifier cannot consider class labels; thus, the generated features tend to be sub-optimal for classification.</p><p>To overcome the weaknesses of domain adversarial training, more recent works directly deal with the relationship between the decision boundary and feature representations based on the cluster assumption <ref type="bibr" target="#b4">[5]</ref>. Several works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref> exploit semi-supervised learning for domain adaptation. Besides, MCD <ref type="bibr" target="#b37">[38]</ref> and ADR <ref type="bibr" target="#b36">[37]</ref> use a minimax training method to push target feature distributions away from the decision boundary, where both methods are composed of the feature extractor and the classifiers. More precisely, in <ref type="bibr" target="#b36">[37]</ref>, two different classifiers are sampled via stochastic dropout. Then, for the same target data sample, the classifiers are updated to maximize the discrepancy between the two predictions. Lastly, the feature extractor is updated multiple times to minimize this discrepancy. The minimax training process leaves the classifier in a noise sensitive state. Therefore, it must be newly trained for optimal performance.</p><p>Though our work is partly inspired by ADR, the proposed method is more efficient and simpler to train compared to the prior arts <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Instead of updating the classifier for maximizing discrepancy, we employ adversarial dropout <ref type="bibr" target="#b30">[31]</ref> on the classifier to achieve a similar effect. Furthermore, this adversarial dropout can be applied to the feature extractor as well. Without the need of a minimax training scheme, DTA has a straightforward and reliable adaptation process.</p><p>Dropout is a simple yet effective regularization method that randomly drops a fraction of the neurons during the training process <ref type="bibr" target="#b41">[42]</ref>. According to Srivastava el al. <ref type="bibr" target="#b41">[42]</ref>, dropout has the effect of ensembling multiple subsets of a network. Park et al. <ref type="bibr" target="#b29">[30]</ref> spotlighted the efficacy of the dropout on convolutional layers. Tompson el al. <ref type="bibr" target="#b43">[44]</ref> pointed out that activations of convolutional layers are usually surrounded by similar activations within the same feature map; thus, dropping individual neurons does not have a strong effect in convolution layers. Instead, they proposed spatial dropout, which drops entire feature maps instead of individual neurons. Building on spatial dropout, Hou el al. <ref type="bibr" target="#b15">[16]</ref> proposed a weighted channel dropout that uses variable drop rates for individual channels, where the drop rates depend on the channel's averaged activation value. The weighted channel dropout is only applied to deep layers of the network, where activations are known to have high specificity <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49]</ref>. Similarly, for channel-wise adversarial dropout, we remove entire feature maps in an adversarial way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Unsupervised Domain Adaptation</head><p>We first define the unsupervised domain adaptation (UDA) problem in general, and relevant notations to our work. In the UDA setting, we use data from two distinctive domains: the source domain S = {X s , Y s } and the target domain T = {X t }. A data point from the source domain x s ∈ X s has an associated label y s ∈ Y s , whereas one from the target domain x t ∈ X t has no paired groundtruth label. We employ a feature extractor f (x; m f ), where m f represents a dropout mask which can be applied at an arbitrary layer of the feature extractor. The feature extractor takes a data point from two domains x ∼ S ∪ T and creates a latent vector, which is fed into a classifier c(·; m c ). The classifier applies a dropout mask m c at an arbitrary layer. We denote the entire neural network as a composition of the feature extractor and the classifier:</p><formula xml:id="formula_0">h(x; m f , m c ) = c(f (x; m f ); m c ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adversarial Dropout</head><p>We leverage a non-stochastic dropout mechanism, Adversarial Dropout (AdD) <ref type="bibr" target="#b30">[31]</ref>, for unsupervised domain adaptation. Adversarial dropout was originally proposed as an effective regularization method for supervised and semi-supervised learning. More specifically, Park et al. <ref type="bibr" target="#b30">[31]</ref> define two types of Adversarial Dropout: Supervised Adversarial Dropout (SAdD), and Virtual Adversarial Dropout (VAdD). With access to ground truth labels, SAdD is used to maximize the divergence between a model's prediction and ground truth label. Without labels, on the other hand, VAdD is used to maximize the divergence between two independent predictions to an input. Due to the lack of target domain labels, SAdD cannot be employed for our purpose. Thus, we exclusively work with VAdD, which is referred to as AdD for the sake of convenience.</p><p>AdD provides a simple and efficient mechanism of generating two divergent predictions for an input. Ultimately, our goal is to enforce the cluster assumption on target data by minimizing the divergence between predictions. To this end, we introduce element-wise AdD (EAdD) and propose its variant, channel-wise AdD (CAdD).</p><p>We first define a dropout mask m applied to an intermediate layer of a network h. For simplicity, we decompose a network h into the subsequent sub-networks h l and h u by the layer applied dropout m, such as:</p><formula xml:id="formula_1">h(x; m) = h u (m h l (x)),<label>(1)</label></formula><p>where represents the element-wise multiplication. Note that m has the same dimensions to the output of h l (x). Let D[p, p ] ≥ 0 measure the divergence between two distributions p and p . Then, the divergence between the predictions of x with different dropout masks, m and m s , is defined as:</p><formula xml:id="formula_2">D [h(x; m s ), h(x; m)] (2) = D [h u (m s h l (x)), h u (m h l (x))] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Element-wise Adversarial Dropout</head><p>The element-wise adversarial dropout (EAdD) mask m adv is defined with respect to a stochastic dropout mask m s as:</p><formula xml:id="formula_3">m adv = argmax m D [h(x; m s ), h(x; m)] where m s − m ≤ δ e L,<label>(3)</label></formula><p>where L denotes the dimension of m ∈ R L , and δ e is a hyper parameter to control the perturbation magnitude with respect to m s . The objective is to find a minimally modified adversarial mask m adv that maximizes the output divergence D between two independent forward passes of x.</p><p>To find m adv , Park et al. <ref type="bibr" target="#b30">[31]</ref> optimize a 0/1 knapsack problem with appropriate relaxations in the process. Their optimization process can be simplified into the following steps. First, an impact value is approximated for each element in h l (x), which is directly proportional to the element's contribution for increasing the divergence. When negative, the element has a decreasing effect on the divergence. Then, without breaching the boundary condition, the elements of m s are adjusted to maximize divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Channel-wise Adversarial Dropout</head><p>To use DTA in a wider range of tasks, we extend EAdD to convolutional layers. In these layers, however, standard dropout is relatively ineffective due to the strong spatial correlation between individual activations of a feature map <ref type="bibr" target="#b43">[44]</ref>. EAdD dropout suffers from the same issues when naively applied to convolutional layers.</p><p>Hence, we formulate CAdD, which adversarially drops entire feature maps rather than individual activations. While the general procedure is similar to that of EAdD, we impose certain constraints on the mask to represent spatial dropout <ref type="bibr" target="#b43">[44]</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> highlights the difference between EAdD and CAdD.</p><p>Consider the activation of a convolutional layer, h l (x) ∈ R C×H×W , where C, H, and W denote the channel, height, and width dimensions of the activation, respectively. We define a channel-wise dropout mask m(i) ∈ R H×W , with the following constraints:</p><formula xml:id="formula_4">m(i) = 0 or 1, ∀i ∈ {1, · · · , C}.<label>(4)</label></formula><p>Here, m(i) corresponds to the i-th activation map of h l (x), 0 ∈ R H×W denotes a matrix of zeros, and 1 ∈ R H×W denotes a matrix of ones, respectively. Then, the channelwise adversarial dropout mask is defined as:</p><formula xml:id="formula_5">m adv = argmax m D [h(x; m s ), h(x; m)] , where 1 HW C i=1 m s (i) − m(i) ≤ δ c C.<label>(5)</label></formula><p>As before, δ c is the hyper parameter that controls degree of the perturbation. The process of finding the channel-wise adversarial dropout mask m adv is similar to those of element-wise adversarial dropout. For CAdD, however, the impact value is approximated for each activation map of h l (x) due to the constraints in Eq. (4). We provide the further details about the approximation in Appendix A of our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Drop to Adapt</head><p>Unlike the prior arts <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>, the proposed algorithm leverages a unified objective function to optimize all network parameters. The overall loss function is defined as a weighted sum of four objective functions:</p><formula xml:id="formula_6">L(S, T ) = L T (S) + λ 1 L DT A (T ) + λ 2 L E (T ) + λ 3 L V (T ),<label>(6)</label></formula><p>where L T , L DT A , L E , and L V represent the objectives for task-specific, domain adaptation, entropy minimization and Virtual Adversarial Training (VAT) <ref type="bibr" target="#b27">[28]</ref>, respectively. Also, the associated hyper-parameters, λ 1 , λ 2 , and λ 3 , control the relative importance of the terms.</p><p>Task-specific objective. We define the task-specific objective function L T regarding the source domain S. In practice, this objective function can be replaced according to the given task. As an example, we present the cross entropy which is widely used for classification:</p><formula xml:id="formula_7">L T (S) = −E xs,ys∼S [y T s log h(x s )],<label>(7)</label></formula><p>where y s is one-hot encoded vector of y s .</p><p>Domain adaptation objective. As the main component, we present the objective function for the domain adaptation first. The objective consists of two parts to affect on the feature extractor L f DT A and the classifier L cDT A :</p><formula xml:id="formula_8">L DT A (T ) = L f DT A (T ) + L cDT A (T ).<label>(8)</label></formula><p>We aim to minimize the divergence between two predicted distribution regarding to an input x: one with a random dropout mask m s f and another with an adversarial dropout mask m adv f . Among the various divergence measures, we choose the Kullback-Leibler (KL) divergence in this work. Assuming that the feature extractor consists of convolutional layers, we employ channel-wise adversarial dropout for m adv f :</p><formula xml:id="formula_9">L f DT A (T ) = E xt∼T D h(x t ; m s f ), h(x t ; m adv f ) = E xt∼T D KL h(x t ; m s f ) h(x t ; m adv f )) .<label>(9)</label></formula><p>We illustrate the effects of L f DT A in <ref type="figure">Fig. 1(c)</ref>. Initially, the decision boundary crosses high density regions in the feature space ( <ref type="figure">Fig. 1(a)</ref>), which is in violation of the cluster assumption. By applying adversarial dropout on the feature extractor, we cause certain features to cross the decision boundary <ref type="figure">(Fig. 1(c)</ref>, left). Then, to enforce consistent predictions, the model parameter are updated to push the decision boundary away from these features ( <ref type="figure">Fig. 1(c)</ref>, right).</p><p>Similarly, we apply AdD to the classifier, where the classifier is defined as a series of fully connected layers. Thus, we perform the element-wise adversarial dropout m adv c and compute the divergence:</p><formula xml:id="formula_10">L cDT A (T ) = E xt∼T D KL h(x t ; m s c ) h(x t ; m adv c )) .<label>(10)</label></formula><p>When adversarial dropout is applied on the classifier, we determine the most volatile areas in the feature space. These volatile regions are in the vicinity of the decision boundary, and predictions in these regions can be changed even by a small perturbation. <ref type="figure">(Fig. 1(d)</ref>, left). Therefore, minimizing L cDT A lets the features avoid falling into such volatile regions ( <ref type="figure">Fig. 1(d)</ref>, right).</p><p>Entropy minimization objective. We introduce the entropy minimization objective to enforce the cluster assumption further. This loss penalizes target samples for being close to the decision boundary, and thus, causes the model to learn more discriminative features:</p><formula xml:id="formula_11">L E (T ) = −E xt∼T [h(x t ) T log h(x t )].<label>(11)</label></formula><p>VAT objective. Lastly, we exploit VAT, which adversarially perturbs the target data at the input level. The VAT minimization objective is defined as:</p><formula xml:id="formula_12">L V (T ) = E xt∼T max r ≤ D KL [h(x t ) h(x t + r)] ,<label>(12)</label></formula><p>where r represents the virtual adversarial perturbation on input x t . While DTA and VAT are similarly motivated, they regularize the network with different forms of perturbations: network parameter perturbations (DTA) and input perturbations (VAT). Thus, VAT provides an orthogonal regularization to DTA, leading to complementary effects.</p><p>Interpretation of DTA. <ref type="figure" target="#fig_2">Fig. 3</ref> visualizes the effects of adversarial dropout using Grad-GAM <ref type="bibr" target="#b39">[40]</ref>, which accentuates the most discriminative regions for a prediction. As a baseline, we present Grad-CAM visualizations of a model trained only on the source domain (SO, see <ref type="figure" target="#fig_2">Fig. 3(b)</ref>). We apply AdD on the source only model (SO + AdD), and see that semantically meaningful areas are deactivated. In contrast, our domain adapted model (DTA, see <ref type="figure" target="#fig_2">Fig. 3(d)</ref>) stays relatively unaffected by AdD, as it keeps seeing the same discriminative regions (see <ref type="figure" target="#fig_2">Fig. 3</ref>(e)) regardless of AdD. The visualizations imply that AdD promotes activations on more hidden units, and lends to robust decision boundary across the domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we evaluate the proposed method on small and large DA benchmarks. To demonstrate the generality of our model, we conduct the experiments in two major recognition tasks: classification and segmentation. In each experiment, we select one domain as the source domain, and another as the target domain. We denote "Source only" as the target domain performance of a model trained on the source domain, and "Target only" as that of a model trained on the target domain. These two serve as baselines for the lower and upper bound performance in domain adaptation. We do not tune a set of data augmentation schemes nor do we report performance with ensemble predictions, as in French el al. <ref type="bibr" target="#b8">[9]</ref>. Rather, all evaluation results are based on the same data augmentation strategy with a single model prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DA on Small Datasets</head><p>To evaluate the influence of DTA model, we first perform experiments on small datasets. We use MNIST <ref type="bibr" target="#b20">[21]</ref>, USPS <ref type="bibr" target="#b16">[17]</ref>, and Street View House Numbers (SVHN) <ref type="bibr" target="#b28">[29]</ref> for adaptation on digits recognition. For object recognition, we use CIFAR10 (CIFAR) <ref type="bibr" target="#b18">[19]</ref> and STL10 (STL) <ref type="bibr" target="#b5">[6]</ref>. For fair comparison against recent state-of-the-art methods such as Self-Ensembling (SE) <ref type="bibr" target="#b8">[9]</ref>, VADA <ref type="bibr" target="#b40">[41]</ref>, and DIRT-T <ref type="bibr" target="#b40">[41]</ref>, we conduct experiments on the same network architecture as in SE. Not that while VADA/DIRT-T use a slightly differernet architecture, the total number of parameters are comparable. The results can be found in <ref type="table" target="#tab_0">Table 1</ref>, and a full list of hyperparameter settings can be found in Appendix B.</p><p>SVHN → MNIST. SVHN and MNIST are two digit classification datasets with a drastic distributional shift between the two. While MNIST consists of binary handwritten digit images, SVHN consists of colored images of street house numbers. Since MNIST has a significantly lower image dimensionality than SVHN, we adopt the dimension of MNIST to 32 × 32 of SVHN, with three channels. When the proposed DTA is applied, our approach demonstrates a significant improvement over previous works, and achieves a performance similar to the "Target only" performance on MNIST.</p><p>MNIST ↔ USPS. MNIST and USPS contain grayscale images, so the domain shift between these two datasets is relatively smaller compared to that of the SVHN → MNIST setting. In both adaptation directions, we achieve an accuracy close to the performance of fully supervised learning on the target domain. In fact, we obtain higher accuracy on USPS when adapting from MNIST, than when trained directly on USPS. This is because the USPS training is relatively small, allowing us to achieve improved performance by adapting from MNIST, using DTA.</p><p>CIFAR ↔ STL. CIFAR and STL are 10-class object recognition datasets with colored images. We remove the non-overlapping classes and redefine the task as a 9-class classification task. Furthermore, we downscale the 96 × 96 image dimesion of STL to match the 32 × 32 dimension of CIFAR. In the CIFAR → STL setting, our method's performance surpasses others by a comfortable margin. For the same reasons presented in the MNIST → USPS setting, our adapted model outperforms the target only model on this dataset pair. In STL → CIFAR, however, our method is slightly weak. This is because STL contains a very small dataset, with only 50 images per class. Since DTA regularizes the decision boundary of the model, the inherent assumption is that the model can achieve low generalization error on the source domain. This assumption holds in most cases, but breaks down when STL is the source domain. To summarize, we achieve a substantial margin of improvement over the source only model across all domain configurations. In four of the five configurations, our method outperforms the recent state-of-the-art results. Next, we evaluate our method on more practical settings that embody real-life domain adaptation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">DA on Large Datasets</head><p>We apply our method to adaptation on large-scale, largeimage datasets. In particular, we evaluate on VisDA-2017 <ref type="bibr" target="#b32">[33]</ref> image classification and VisDA-2017 image segmentation tasks.</p><p>Classification. The VisDA-2017 image classification is a 12-class domain adaptation problem. The source domain consists of 152,397 synthetic images, where 3D CAD models are rendered from various conditions. The target domain consists of 55,388 real images taken from the MS-COCO dataset <ref type="bibr" target="#b21">[22]</ref>. Since the objective is to learn from labeled synthetic images and correctly predict the class of real images, this dataset has been frequently used in many domain adaptation works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9]</ref>. For fair comparison with recent works, we follow the protocol of ADR <ref type="bibr" target="#b36">[37]</ref> in our experiments. Specifically, we apply the EAdD after the second fully connected layer, and CAdD within the last convolution layer of ResNet-50 <ref type="bibr" target="#b13">[14]</ref> and ResNet-101 models. Both models are initialized with weights from an ImageNet <ref type="bibr" target="#b7">[8]</ref> pre-trained model. For more details on implementation, we refer our readers to Appendix B. The per-class adaptation performance with a ResNet-101 backbone can be found in <ref type="table" target="#tab_1">Table 2</ref>. The table clearly shows that our proposed method surpasses previous methods by a large margin. Note that all methods in this table use the same ResNet-101 backbone. Compared to the performance of a source only model, we achieve a 30.7% improvement (or 60.4% relative improvement) on the average accuracy. Furthermore, DTA shows a significant improvement across all categories; in fact, it achieves the best per-class performance in all classes, except the "truck" class, where it falls behind ADR by a mere 0.2%. Although our source only model is slightly lower than that of both MCD <ref type="bibr" target="#b37">[38]</ref> and ADR, our proposed method effectively generalizes a model from the source to target domain, with stronger adaptation performance over MCD and ADR by margins of 9.6% and 6.7%, respectively.</p><p>In <ref type="table">Table 4</ref>, we show that it is feasible to apply DTA on a different backbone network with success. Similarly to  <ref type="bibr" target="#b23">[24]</ref> 53.0 RTN <ref type="bibr" target="#b25">[26]</ref> 53.6 DANN <ref type="bibr" target="#b10">[11]</ref> 55.0 JAN-A <ref type="bibr" target="#b26">[27]</ref> 61.6 GTA <ref type="bibr" target="#b38">[39]</ref> 69.5 SimNet <ref type="bibr" target="#b33">[34]</ref> 69.6 CDAN-E <ref type="bibr" target="#b24">[25]</ref> 70.0 Ours 76.2 SE* <ref type="bibr" target="#b8">[9]</ref> 82.8</p><p>DTA on ResNet-101, our model outperforms recent previous methods, and demonstrates a significance improvement over the source only model. While SE reports the best overall performance, we do not consider it to be comparable to other methods -including ours -because the reported accuracy is a result of 16 ensembled predictions. For qualitative analysis, <ref type="figure" target="#fig_3">Figure 4</ref> visualizes the feature representations of VisDA-2017 classification with t-SNE <ref type="bibr" target="#b45">[46]</ref>. The source only model shows strong clustering of the source domain's synthetic image samples (blue), but fails to have similar influence on the target domain's real image samples (red). During training, DTA constantly enforces the clustering of target samples by stimulating the feature representations and decision boundary of the model. Therefore, we can clearly see an improved separation of target features with DTA, resulting in the best performance in VisDA-2017.</p><p>Segmentation. To further demonstrate our method's applicability to real-world adaptation settings, we evaluate DTA in the challenging VisDA-2017 semantic segmentation task. For the source domain, we use the synthetic GTA5 <ref type="bibr" target="#b34">[35]</ref> dataset which consists of 24966 labeled images. As the target domain, we use the real-world Cityscapes <ref type="bibr" target="#b6">[7]</ref>, consisting of 5000 images. Both datasets are evaluated on the same category of 19 classes, with the mean Intersectionover-Union (mIoU) metric. For fair comparison with recent methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, we follow the procedure of ADR and use a modified version of Fully Convolutional Networks (FCN) <ref type="bibr" target="#b22">[23]</ref> on a ResNet-50 backbone. We apply CAdD within the last convolutional layer of ResNet-50.</p><p>We report our results in <ref type="table" target="#tab_2">Table 3</ref>, alongside results of existing methods. Our method clearly improves upon the mIoU of not only the source only model, but also competing methods. Even with the same training procedure and settings as in the classification experiments, DTA is extremely effective at adapting the most common classes in the dataset. This conclusion is supported in <ref type="figure">Figure 5</ref>, where we display examples of input images, ground truths, and the corresponding outputs of source only and DTA model. While the source only predictions are erroneous in most classes, DTA's predictions are relatively clean and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Although the proposed DTA shows significant improvements on multiple visual tasks, we would like to understand the role of each component in DTA and how their combination operates in practice. We perform a series of ablation experiments and present the results in <ref type="table" target="#tab_3">Table 5</ref>. All ablations are conducted on VisDA-2017 image classification dataset. To verify the effectiveness and generality, we use ResNet-50 and ResNet-101 models for all experiments in this ablation. The modified ResNet-based models consist of the original convolutional layers with FAdD after the second fully connected layer, and CAdD within the last convolutional layer. The entropy loss term in Eq. (11) is applied on all ablations except the "Source Only" setting.</p><p>To assess whether each module of DTA (VAT, fDTA, cDTA) plays an important role in the performance, we first experiment with individual modules. Overall, all three modules improve the performance over a source only model. We observe that the three components contribute to the accuracy of each category differently. In ResNet-101, while fDTA has a great impact on the "knife" category, VAT significantly boosts the performance of the "skakteboard" class. Theoretically, VAT <ref type="bibr" target="#b27">[28]</ref> can be seen as the regularization by perturbing the input image, while the proposed methods can be seen as perturbations on the feature space of the model. Therefore, we can see that two combinations (fDTA + VAT), (cDTA + VAT) shows increased performance compared to the individually regularized model (i.e. 73.2% (VAT) / 77.0% (cDTA) → 81.2% in ResNet-  <ref type="figure">Figure 5</ref>. Semantic segmentation. Qualitative results of the semantic segmentation task on GTA → Cityscapes, before and after adaptation with DTA. We use a modified FCN architecture with ResNet-50 as the base model. These results suggest that it is beneficial to use VAT <ref type="bibr" target="#b27">[28]</ref> with the proposed method. More specifically, both methods exhibit complementary effects for adaptation on a large domain shift. This advantage can also be observed in the comparison of fDTA + cDTA to a final version of the proposed method (VAT + fDTA + cDTA). One interesting point is that all these trends are mostly maintained in both backbone models; the only difference is the amount of margin between the performance of source only and individual models. From this fact, we conclude that the proposed method can act as a general regularization technique for adaptation, regardless of the model's capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a simple yet effective method for unsupervised domain adaptation despite large domain shifts. With two types of proposed adversarial dropout modules, EAdD and CAdD, we enforced the cluster assumption on the tar-get domain. The proposed methods are easily integrated into existing deep learning architectures. Through extensive experiments on various small and large datasets, we demonstrated the effectiveness of the proposed method on two domain adaptation tasks, and in all cases we achieved significant improvement as compared to the source-only model and the state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Before adaptation (b) Adapted model (c) AdD on feature extractor (d) AdD on classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Element-wise AdD (EAdD) (b) Channel-wise AdD (CAdD) Comparison of EAdD and CAdD. EAdD drops units individually, regardless of spatial correlation. CAdD, on the other hand, drops entire feature maps, making it more suitable for convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of adversarial dropout. We visualize class activation maps on target domain images using GradCAM<ref type="bibr" target="#b39">[40]</ref>. Adversarial dropout (c) effectively deactivates semantically meaningful regions for a prediction compared to its baseline model only trained on source domain (b). Our domain adapted model (DTA) produces reasonable predictions (d), even though 10% of units are eliminated by AdD (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE. t-SNE visualization of VisDA-2017 classification dataset using ResNet-101, before and after adaptation with DTA. t-SNE hyperparameters are consistent in both visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of experiment on small image datasets. *We compare with the MT+CT+TF for SE.</figDesc><table><row><cell>Source</cell><cell cols="2">SVHN MNIST</cell><cell>USPS</cell><cell>STL</cell><cell>CIFAR</cell></row><row><cell>Target</cell><cell>MNIST</cell><cell>USPS</cell><cell cols="2">MNIST CIFAR</cell><cell>STL</cell></row><row><cell>Source only (Ours)</cell><cell>76.5</cell><cell>96.3</cell><cell>76.9</cell><cell>60.1</cell><cell>78.2</cell></row><row><cell>SE* [9]</cell><cell>98.6</cell><cell>98.1</cell><cell>97.3</cell><cell>74.2</cell><cell>79.7</cell></row><row><cell>VADA [41]</cell><cell>94.5</cell><cell>-</cell><cell>-</cell><cell>73.5</cell><cell>80.0</cell></row><row><cell>DIRT-T [41]</cell><cell>99.4</cell><cell>-</cell><cell>-</cell><cell>75.5</cell><cell>-</cell></row><row><cell>Co-DA [20]</cell><cell>98.3</cell><cell>-</cell><cell>-</cell><cell>76.4</cell><cell>81.1</cell></row><row><cell>Co-DA+DIRT-T [20]</cell><cell>99.4</cell><cell>-</cell><cell>-</cell><cell>76.3</cell><cell>-</cell></row><row><cell>Ours</cell><cell>99.4</cell><cell>99.5</cell><cell>99.1</cell><cell>72.8</cell><cell>82.6</cell></row><row><cell>Target only (Ours)</cell><cell>99.6</cell><cell>97.8</cell><cell>99.6</cell><cell>90.4</cell><cell>70.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on VisDA-2017 classification using ResNet-101. aero. bike bus car horse knife moto. person plant sktb. train truck avg. Source Only 46.2 27.6 31.4 78.1 71.8</figDesc><table><row><cell>1.3</cell><cell>71.7</cell><cell>14.3</cell><cell>63.5 31.0 93.7</cell><cell>3.2</cell><cell>50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on GTA → Cityscapes, using a modified FCN with ResNet-50 as the base network. Source only 25.3 13.7 56.8 2.7 17.2 21.2 20.0 8.7 75.3 11.2 72.0 45.7 4.9 42.2 14.2 20.2 0.4 19.5 0.0 24.76.9 20.9 15.4 19.6 21.8 7.9 82.9 26.7 76.1 51.7 9.4 76.1 22.4 28.9 1.7 15.2 0.0 35.8</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t light</cell><cell>t sign</cell><cell>veg</cell><cell>terrian</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>DANN</cell><cell cols="4">72.4 19.1 73.0 3.9</cell><cell cols="16">9.3 17.3 13.1 5.5 71.0 20.1 62.2 32.6 5.2 68.4 12.1 9.9 0.0 5.8 0.0 26.4</cell></row><row><cell>ADR</cell><cell cols="20">87.8 15.6 77.4 20.6 9.7 19.0 19.9 7.7 82.0 31.5 74.3 43.5 9.0 77.8 17.5 27.7 1.8 9.7 0.0 33.3</cell></row><row><cell cols="9">Ours 88.8 36.9 Table 4. Results on VisDA-2017 classification using ResNet-50.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">*SE report ensemble of multiple predictions. All other methods,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">including ours, report the average achieved by a single prediction.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Source Only (Ours) 45.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>DAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation Studies on VisDA-2017 Classification Dataset Methods aero. bike bus car horse knife moto. person plant sktb. train truck avg. 1% (VAT) / 72.5% (fDTA) → 73.1% in ResNet-50).</figDesc><table><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Only</cell><cell>54.2 27.7 17.6 57.1 48.4</cell><cell>4.0</cell><cell>86.4</cell><cell>11.0</cell><cell>69.1 15.6 95.7</cell><cell>7.3</cell><cell>46.0</cell></row><row><cell>VAT</cell><cell>83.1 62.5 70.5 53.0 81.8</cell><cell>13.2</cell><cell>89.9</cell><cell>74.4</cell><cell cols="3">88.5 41.1 89.0 38.2 67.1</cell></row><row><cell>fDTA</cell><cell>88.8 58.2 82.8 82.3 90.4</cell><cell>0.1</cell><cell>92.8</cell><cell>77.3</cell><cell>94.2 78.5 86.9</cell><cell>0.2</cell><cell>72.5</cell></row><row><cell>fDTA + VAT</cell><cell>91.3 66.3 77.7 77.5 91.0</cell><cell>13.1</cell><cell>92.6</cell><cell>83.0</cell><cell cols="3">94.2 58.0 85.9 12.0 73.1</cell></row><row><cell>cDTA</cell><cell>92.4 72.9 75.1 72.6 92.8</cell><cell>7.4</cell><cell>90.8</cell><cell>82.1</cell><cell cols="3">95.0 66.6 87.8 31.6 74.7</cell></row><row><cell>cDTA + VAT</cell><cell>90.0 72.7 83.7 79.3 92.0</cell><cell>6.8</cell><cell>91.4</cell><cell>82.6</cell><cell cols="3">92.2 70.4 86.3 22.9 75.4</cell></row><row><cell cols="2">cDTA + fDTA 88.2 68.8 87.2 82.8 92.3</cell><cell>5.8</cell><cell>89.4</cell><cell>78.4</cell><cell cols="3">95.5 74.8 82.4 16.1 75.0</cell></row><row><cell>Ours</cell><cell>93.1 70.5 83.8 87.0 92.3</cell><cell>3.3</cell><cell>91.9</cell><cell>86.4</cell><cell cols="3">93.1 71.0 82.0 15.3 76.2</cell></row><row><cell>ResNet-101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Only</cell><cell>46.2 27.6 31.4 78.1 71.8</cell><cell>1.4</cell><cell>71.6</cell><cell>14.3</cell><cell>63.5 31.0 93.7</cell><cell>3.2</cell><cell>50.8</cell></row><row><cell>VAT</cell><cell>90.1 43.9 83.9 85.6 90.9</cell><cell>1.4</cell><cell>95.0</cell><cell>78.6</cell><cell cols="3">93.8 57.9 86.2 13.4 73.2</cell></row><row><cell>fDTA</cell><cell>89.1 75.5 84.6 87.2 92.3</cell><cell>72.9</cell><cell>89.7</cell><cell>78.5</cell><cell cols="3">91.8 39.5 84.1 10.8 76.4</cell></row><row><cell>fDTA + VAT</cell><cell>93.0 84.8 81.8 78.1 93.2</cell><cell>70.1</cell><cell>88.8</cell><cell>82.0</cell><cell cols="3">94.0 81.5 87.4 39.6 80.5</cell></row><row><cell>cDTA</cell><cell>91.8 81.5 78.7 67.0 91.3</cell><cell>71.6</cell><cell>85.3</cell><cell>76.9</cell><cell cols="3">93.5 72.5 86.7 44.1 77.0</cell></row><row><cell>cDTA + VAT</cell><cell>93.8 86.1 82.9 78.3 92.2</cell><cell>83.9</cell><cell>88.2</cell><cell>80.6</cell><cell cols="3">94.1 82.2 88.0 40.0 81.2</cell></row><row><cell cols="2">cDTA + fDTA 91.7 77.7 78.8 75.2 91.0</cell><cell>73.2</cell><cell>88.4</cell><cell>78.8</cell><cell cols="3">93.2 56.6 88.7 35.6 77.4</cell></row><row><cell>Ours</cell><cell>93.7 82.2 85.6 83.8 93.0</cell><cell>81.0</cell><cell>90.7</cell><cell>82.0</cell><cell cols="3">95.1 78.1 86.4 32.1 81.5</cell></row><row><cell>101, 67.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We derive an approximation of the channel-wise adversarial dropout ( § Appendix A) and provide implementation details of the experiments ( § Appendix B). Lastly, we provide additional GradCAM visualizations ( § Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Approximation of Channel-wise Adversarial Dropout</head><p>Without loss of generality, the dropout mask m is vectorized to v = vec(m) ∈ R CHW . Similarly, v 0 and v s represent vectorized forms of m 0 and m s , respectively. After vectorization of m, we refer to the elements of m(i) with a set of indices πi, and impose the channel-wise dropout constraints as follows:</p><p>(a-13)</p><p>as the divergence between two outputs using different dropout masks for convenience sake. Assuming d is a differentiable function with respect to v, it can be approximated by a first-order Taylor expansion:</p><p>This equation shows that the Jacobian is proportional to the divergence. In other words,</p><p>(a-14)</p><p>We now see that the elements of J correspond to the impact values, which indicate the contribution of each activation over the divergence metric. Thus, for the given Jacobian, we can systematically modify the elements of v to maximize the divergence. However, due to the channel-wise dropout constraint from Eq. (a-13), we cannot modify each element individually. Instead, we reformulate the above relationship as:</p><p>The impact value s of the i-th activation map in h l (x) can be defined as:</p><p>Consequently, after computing the impact values s, we solve 0/1 Knapsack problem as proposed in <ref type="bibr" target="#b30">[31]</ref> while holding the constraints (a-13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Implementation Details</head><p>Training with DTA Loss</p><p>We apply a ramp-up factor on DTA loss function LDT A to stabilize the training process. Instead of directly modulating the weight term λ1, we gradually increase the perturbation magnitudes δe and δc which decide the number of hidden units to be eliminated. It allows us to regulate the consistency term, and to train the network being robust to various levels of perturbation generated by the adversarial dropout. We update the ramp-up factors with the following schedule:</p><p>where Tr represents the ramp-up period, and β (t) denotes the ramp up factor at the current epoch t. Finally, the perturbation magnitude is defined as:</p><p>whereδ denotes the maximum level of perturbation. In practice, the same ramp-up period Tr is applied for both δe and δc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head><p>Table A-1 presents the hyperparameters used in our experiments. We followed a similar hyperparameter search protocol as Shu et al. <ref type="bibr" target="#b40">[41]</ref>, where we sample a very small subset of labels from the target domain training set. For each objective function, we limit the hyperparameter search to a predefined set of values: λ1 = {2}, λ2 = {0, 0.01, 0.02}, λ3 = {0, 0.1, 0.2}, δe = {0, 0.1}, δc = {0, 0.01, 0.02, 0.05}, and = {0, 3.5, 15}. Furthermore, we provide the rest of parameters related to network training for each experimental set up.</p><p>Small dataset. All small dataset experiments were trained for 90 epochs, using Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with an initial learning rate of 0.001, decaying by a factor of 0.1 every 30 epochs. Large dataset. We conducted the VisDA-2017 classification experiments on ResNet-50 and ResNet-101. We trained the networks for 20 epochs using Stochastic Gradient Descent (SGD) with a momentum value of 0.9 and an initial learning rate of 0.001, which decays by a factor of 0.1 after 10th epoch.</p><p>Semantic segmentation. The semantic segmentation task for domain adaptation from GTA5 to Cityscapes was trained for 5 epochs using SGD with a momentum of 0.9. Since FCN <ref type="bibr" target="#b22">[23]</ref> has no fully-connected layers,δe was automatically set to 0. In addition, we used the maximumδc value from the beginning because the task-specific objective were dominant in the early stages of training. In this experiment, we turned off VAT objective which hinders from learning the segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Additional GradCAM visualizations</head><p>In <ref type="figure">Figure A-1</ref>, we provide additional GradCAM visualizations to highlight the effects of adversarial dropout. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of single layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted channel dropout for regularization of deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diedrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahini</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pattrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial dropout for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkeon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-Jin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">VisDA: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A DIRT-T approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial feature augmentation for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<title level="m">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
