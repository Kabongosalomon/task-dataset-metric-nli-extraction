<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Reasoning Strategies in End-to-End Differentiable Proving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt√§schel</surname></persName>
						</author>
						<title level="a" type="main">Learning Reasoning Strategies in End-to-End Differentiable Proving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. We present Conditional Theorem Provers (CTPs), an extension to NTPs that learns an optimal rule selection strategy via gradient-based optimisation. We show that CTPs are scalable and yield stateof-the-art results on the CLUTRR dataset, which tests systematic generalisation of neural models by learning to reason over smaller graphs and evaluating on larger ones. Finally, CTPs show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable. All source code and datasets are available online. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Natural Language Understanding (NLU) systemswherein a deep neural network is used as a function approximator <ref type="bibr" target="#b34">(LeCun et al., 2015;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2016</ref>)-have been extremely successful at various natural language tasks, such as Question Answering (QA) and Natural Language Inference (NLI) <ref type="bibr" target="#b13">(Goldberg, 2017)</ref>, achieving strong generalisation results on datasets available for these tasks <ref type="bibr" target="#b47">(Seo et al., 2017;</ref><ref type="bibr" target="#b20">Hu et al., 2018;</ref><ref type="bibr" target="#b48">Shen et al., 2016;</ref><ref type="bibr">Huang et al., 1</ref> UCL Centre for Artificial Intelligence, University College London 2 Facebook AI Research. Correspondence to: Pasquale Minervini &lt;p.minervini@ucl.ac.uk&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). <ref type="bibr">1</ref> At https://github.com/uclnlp/ctp 2018). Even strong performance on NLU problems have been recently achieved with advent of large models pretrained via self-supervision, such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b56">(Yang et al., 2019)</ref>, and RoBERTa <ref type="bibr" target="#b36">(Liu et al., 2019)</ref>.</p><p>Generalisation in Neural Models However, there are growing concerns about the ability of NLU systems, and neural networks more generally, to generalise in a systematic and robust way <ref type="bibr" target="#b1">(Bahdanau et al., 2019;</ref><ref type="bibr" target="#b33">Lake &amp; Baroni, 2018;</ref><ref type="bibr" target="#b24">Johnson et al., 2017;</ref><ref type="bibr" target="#b49">Sinha et al., 2019)</ref>. For instance, <ref type="bibr" target="#b22">Jia &amp; Liang (2017)</ref> highlight the brittleness of NLU systems to adversarial examples, while <ref type="bibr" target="#b18">Gururangan et al. (2018)</ref>; <ref type="bibr" target="#b27">Kaushik &amp; Lipton (2018)</ref> show that neural NLU models tend to exploit annotation artefacts and spurious correlations in the data. Furthermore, analysing and supervising the inner workings of such models is not trivial, due to their inherent black-box nature <ref type="bibr" target="#b10">(Doshi-Velez &amp; Kim, 2017;</ref><ref type="bibr" target="#b35">Lipton, 2018)</ref>.</p><p>More generally, <ref type="bibr" target="#b12">Garnelo &amp; Shanahan (2019)</ref> emphasise several limitations of neural models, in terms of i) data inefficiency and high sample complexity-the need of high volumes of training data in order to be effective, ii) poor generalisation-modern neural models may not produce the correct predictions when exposed to data outside the training distribution, and iii) lack of interpretability-such models are black boxes where internal representations and computations are hardly interpretable by humans.</p><p>In this vein, <ref type="bibr" target="#b49">Sinha et al. (2019)</ref> measured and compared the systematic generalisation abilities of several neural models (including very strong baselines such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and Graph Attention Networks (GATs) <ref type="bibr" target="#b53">(Velickovic et al., 2018)</ref>) on the task of answering questions about family relationship graphs, by evaluating on held-out combinations of reasoning patterns and by adding curated distracting noisy facts. Interestingly, they found that performance degrades monotonically for every model in their pool as they increase the complexity of the relational graph, highlighting the challenge of systematic generalisation <ref type="bibr" target="#b33">(Lake &amp; Baroni, 2018;</ref><ref type="bibr" target="#b50">Sodhani et al., 2018)</ref>.</p><p>Neuro-Symbolic Reasoning A promising direction for overcoming these issues consists in combining neural models and symbolic reasoning given their complementary strengths and weaknesses (d'Avila <ref type="bibr" target="#b6">Garcez et al., 2015;</ref><ref type="bibr"></ref> arXiv:2007.06477v3 [cs.AI] 24 Aug 2020 <ref type="bibr" target="#b11">Evans &amp; Grefenstette, 2018;</ref><ref type="bibr" target="#b12">Garnelo &amp; Shanahan, 2019)</ref>. We focus on NTPs , a family of neuro-symbolic reasoning models: NTPs are continuous relaxations of the backward-chaining reasoning algorithm that replace discrete symbols with their continuous embedding representations.</p><p>NTPs have interesting properties: they can jointly learn representations and interpretable rules from data via backpropagation, and can potentially combine such rules in ways that may have not been observed during training. However, a major limitation in NTPs is that, during training, they need to consider all rules for explaining a given goal or sub-goal. This quickly renders them ineffective in settings requiring a large number of rules or reasoning steps.</p><p>Conditional Theorem Provers For addressing limitations of NTPs, we propose CTPs, an extension that is able to learn an adaptive strategy for selecting subsets of rules to consider at each step of the reasoning process. This is achieved by a select module that, given a goal, produce the rules needed for proving it. Predicates and constants in the produced rules lie in a continuous embedding space. Hence, the select module is end-to-end differentiable, and can be trained jointly with the other modules via gradient-based optimisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">End-to-End Differentiable Proving</head><p>NTPs ) are a continuous relaxation of the backward chaining algorithm <ref type="bibr" target="#b44">(Russell &amp; Norvig, 2010)</ref>: this algorithm works backward from the goal, chaining through rules to find known facts supporting the proof.</p><p>Given a query (or goal) G, backward chaining first attempts to unify it with the facts available in a given Knowledge Base (KB). If no matching fact is available, it considers all rules H :-B, where H denotes the head (or consequence) and B the body (or premise), and H can be unified with the query G resulting in a substitution for the variables contained in H. Then, the backward chaining algorithm applies the substitution to the body B, and recursively attempts to prove the atoms contained therein.</p><p>Backward chaining can be seen as a type of and/or search: or because the goal can be proven by any rule in the KB, and and because all the conjuncts in the premise of a rule must be proven.</p><p>Example 2.1 (Backward Chaining). Consider a KB composed by the facts p(RICK, BETH) and p(BETH, MORTY), and by the rule g(X, Y) :p(X, Z), p(Z, Y), where p and g denote the relationships parent and grandparent, respectively. The goal G = g(RICK, MORTY) can be proven by unifying G with the head of the rule g(X, Y), with the substitution {X/RICK, Y/MORTY}, and then by re-cursively proving the subgoals p(RICK, Z), p(Z, MORTY), which hold true for the substitution {Z/BETH}.</p><p>NTPs make this reasoning process more flexible and endto-end differentiable by replacing the comparison between symbols with a soft matching of their respective trainable dense vector representations. NTPs recursively build a neural network enumerating all possible proof paths for proving a query given KB, and aggregate all proof scores via max pooling. They do so by relying on three modules: a unification module, which compares sub-symbolic representations of logic atoms, and mutually recursive or and and modules, which jointly enumerate all possible proof paths, before the final aggregation selects the highest scoring one. The whole process is outlined in Algorithm 1.   propose replacing comparisons between symbols with a differentiable similarity measure K : R k √ó R k ‚Üí [0, 1], such as a Gaussian kernel, between their embeddings. Their model enumerates all possible proofs for a goal G, and generates a proof score for each of them, given by the minimum of all embedding similarities. For instance, if G = grandPa(RICK, MORTY) = [grandPa, RICK, MORTY], one candidate proof consists in using the facts F = p(RICK, BETH) and F = p(BETH, MORTY) and the rule g(X, Y) :p(X, Z), p(Z, Y) from the KB, yielding the score K(Œ∏ grandPa , Œ∏ g ). It is important to mention that NTPs allow unifying symbols like grandPa and g (which, in this case, share the same semantics), even though they are lexically different. The score for G is given by the maximum of all proof scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conditional Proving Strategies</head><p>The NTPs proposed by  use a fixed set of rules, either specified by the user or learned from data via provided rule templates. In this model, given a goal, there is no hard decision mechanism for deciding which rules can be used for reformulating a given goal into subgoals: all rules in the KB need to be considered when proving each goal. For this reason, NTPs were shown not to scale to large datasets .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differentiable Goal Reformulation</head><p>In order to explicitly learn which rule to consider at each step, we propose the following solution. Rather than relying on a fixed, potentially very large set of rules, we propose to dynamically generate a minimal set of rules via a neural network architecture conditioned on the goal to prove.</p><p>Example 3.1 (Conditional Theorem Proving). Assume that Algorithm 1 Overview of the neural backward chaining algorithm proposed by  -intuitively, it recursively proves each goal with all rules in the KB (OR module) and, for each rule, it proves its premise (AND module), up to d recursion steps. </p><formula xml:id="formula_0">(S œà , S œÅ )) 2: S œà = S œà i T i with T i = Ô£± Ô£¥ Ô£≤ Ô£¥ Ô£≥ {H i /G i } if H i ‚àà V {G i /H i } if G i ‚àà V, H i ‚àà V ‚àÖ otherwise 3: S œÅ = min {S œÅ } Hi,Gi ‚ààV {K(Œ∏ Hi , Œ∏ Gi )} 4: return (S œà , S œÅ )</formula><p>Algorithm 2 In Conditional Theorem Provers, the set of rules is conditioned on the goal G. yield S the goal to prove is G = g(RICK, MORTY), we want the model to be able to only consider the best rules for proving G, such as g(X, Y) :p(X, Z), p(Z, Y), rather than all rules in the KB. Remember that, in NTPs, relations and constants in the KB are represented by their embedding vectors, and the aforementioned rule selection can be implemented via a mapping from Œ∏ g: to [Œ∏ p: , Œ∏ p: ].</p><p>Consider the or module in Algorithm 1. Selecting which rule to use during the proving process for a given goal G can be implemented by rewriting the or module as in Algorithm 2, where the set of clauses is produced by a module that, given the goal, generates a set of rules for proving G.</p><p>The core difference between NTPs and CTPs is that, when proving a goal G, rather than iterating through a possibly very large set of clauses in the KB K (see Line 2 in the or module definition in Algorithm 1), the conditional or module in Algorithm 2 only iterates through a small set of generated clauses, whose generation is conditioned on G (see Line 2 in Algorithm 2). Given a goal G, the select module with parameters Œ∏ in Algorithm 2 produces a set of clauses, each specifying which sub-goals to prove in order to produce a proof for G.</p><p>Note that the select module can be implemented by an end-to-end differentiable parametrised function f (¬∑) that, given a goal G, produces a finite sequence of corresponding subgoals:</p><formula xml:id="formula_1">select Œ∏ (G) : A ‚Üí [A :-A * ] ,<label>(1)</label></formula><p>where V is a set of variables, and</p><formula xml:id="formula_2">A R k √ó (R k ‚à™ V) √ó (R k ‚à™ V)</formula><p>denotes the embedding representation of a goal, such as g(RICK, MORTY).</p><p>For instance, the select module in Eq. 1 can be implemented by a neural network that, given a goal such as</p><formula xml:id="formula_3">G = [Œ∏ g: , Œ∏ RICK: , Œ∏ MORTY: ], generates H :-B with H = [Œ∏ g: , X, Y] and B = [[Œ∏ p: , X, Z], [Œ∏ p: , Z, Y]], correspond- ing to the symbolic rule g(X, Y) :-p(X, Z), p(Z, Y).</formula><p>If the positions of the variables in the rule are fixed, the whole module is end-to-end differentiable with respect to its parameters Œ∏.</p><p>Neural Goal Reformulation Here, we define select as a linear function of the goal predicate:</p><formula xml:id="formula_4">select Œ∏ (G) [F H (G) :-F B1 (G), F B2 (G)] ,<label>(2)</label></formula><p>where the head and body of the resulting rule are given by</p><formula xml:id="formula_5">F H (G) = [f H (Œ∏ G1 ), X, Y], F B1 (G) = [f B1 (Œ∏ G1 ), X, Z], and F B2 (G) = [f B2 (Œ∏ G1 ), Z, Y]. Every f i : R k ‚Üí R k is a differentiable function, such as the linear projection f i (x) = W i x + b, with W i ‚àà R k√ók and b ‚àà R k .</formula><p>Thus, instead of iterating through a possibly very large set of rules in the KB K, we can generate a significantly smaller set of rules, whose generation is conditioned on the goal G and can be trained end-to-end on downstream reasoning tasks.</p><p>Attentive Goal Reformulation We can incorporate a useful prior in the select module architecture-namely that predicate symbols in the rule already exist in the KB, among the available relations R. A method for incorporating this prior consists in using the given goal G for generating a distribution over the set of relations R:</p><formula xml:id="formula_6">f i (x) = Œ± E R Œ± = softmax (W i x) ‚àà ‚àÜ |R|‚àí1 ,<label>(3)</label></formula><p>where E R ‚àà R |R|√ók denotes the predicate embedding matrix, W i ‚àà R k√ó|R| , and Œ± is an attention distribution Œ± ‚àà ‚àÜ |R|‚àí1 over the predicates in R, where ‚àÜ n denotes  <ref type="figure">Figure 1</ref>: Example of a train and test instance in CLUTRR -the training instance (upper left) is composed by a graph with three edges, while the test instance is composed by ten edges; the task consists in identifying the relationships between the green nodes.</p><p>the standard n-simplex. 2 This is especially helpful if the embedding size is larger than the number of relationships, i.e. k |R|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory-Based Goal Reformulation</head><p>A problem with using black-box neural networks for reformulating goals into sub-goals is that it can be difficult to inspect the rules by analysing the model parameters, as in . For this reason, we propose an alternative goal reformulation module, where rules are stored in a differentiable memory.</p><p>More precisely, n rules are stored as memory matrices [M 1 , . . . , M m ], where each M i ‚àà R n√ók denotes the kdimensional embeddings of the i-th predicates in the n rules. Then, the goal G is used to compute an attention distribution over rules Œ± ‚àà ‚àÜ n‚àí1 , where each Œ± i denotes the attention weight on the i-th rule. The attention distribution can be formalised as follows:</p><formula xml:id="formula_7">f i (x) = Œ± M i Œ± = softmax (Wx) ‚àà ‚àÜ n‚àí1 ,<label>(4)</label></formula><p>where each f i : R k ‚Üí R k is a differentiable function that, given the goal, produces an attention distribution Œ± ‚àà ‚àÜ n‚àí1 over the rules and for indexing a memory M i , analogous to a key-value memory network <ref type="bibr" target="#b38">(Miller et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Memory-Augmented Networks Memory-augmented neural architectures aim at improving the generalisation and reasoning abilities in neural networks by disentangling representations from computations. By enriching neural networks with a differentiable external memory, these models were able to multi-hop reason over texts <ref type="bibr">(Sukhbaatar et al.,</ref><ref type="bibr">2</ref> The standard n-simplex ‚àÜ n is defined as </p><formula xml:id="formula_8">‚àÜ n = {(Œ±0, . . . , Œ±n) ‚àà R n+1 | n i=0 Œ±i = 1 ‚àß ‚àÄi : Œ±i ‚â• 0} CLUTRR -Sample of Learned Rules child(X, Y) ‚áê child(X, Z), sibling(Z, Y) child(X, Y) ‚áê SO(X, Z), child(Z, Y) grand(X, Y) ‚áê child(X, Z), child(Z, Y) grand(X, Y) ‚áê grand(X, Z), sibling(Z, Y) grand(X, Y) ‚áê SO(X, Z), grand(Z, Y) in-law(X, Y) ‚áê child(X, Z), SO(Z, Y) in-law(X, Y) ‚áê sibling-in-law(X, Z), child(Z, Y) sibling(X, Y) ‚áê sibling(X, Z), sibling(Z, Y) sibling(X, Y) ‚áê child(X, Z), uncle(Y, Z) sibling(X, Y) ‚áê child(X, Z), child(Y, Z)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2015)</head><p>, induce algorithmic behaviours <ref type="bibr" target="#b15">(Graves et al., 2014;</ref><ref type="bibr" target="#b25">Joulin &amp; Mikolov, 2015;</ref><ref type="bibr" target="#b16">Grefenstette et al., 2015;</ref><ref type="bibr" target="#b26">Kaiser &amp; Sutskever, 2016)</ref>, and rapidly assimilate new data <ref type="bibr" target="#b46">(Santoro et al., 2016)</ref>.</p><p>Neuro-Symbolic Models Differentiable interpreters enable translating declarative or procedural knowledge into neural networks exhibiting strong inductive biases of that knowledge <ref type="bibr" target="#b3">(Bo≈°njak et al., 2017;</ref><ref type="bibr" target="#b11">Evans &amp; Grefenstette, 2018)</ref>. <ref type="bibr" target="#b3">Bo≈°njak et al. (2017)</ref> propose ‚àÇ4, a differentiable abstract machine for the Forth programming language.  propose a differentiable implementation for the backward chaining algorithm, effectively implementing a differentiable Datalog interpreter. <ref type="bibr" target="#b11">Evans &amp; Grefenstette (2018)</ref> propose a differentiable forwardchaining reasoning process, while <ref type="bibr" target="#b9">Donadello et al. (2017)</ref> propose a continuous generalisation of the semantics of first-order logic. <ref type="bibr" target="#b55">Yang et al. (2017)</ref> and <ref type="bibr" target="#b45">Sadeghian et al. (2019)</ref> propose an approach for learning function-free Datalog clauses from KBs by means of a differentiable graph traversal operator, while <ref type="bibr" target="#b5">Das et al. (2018)</ref> propose learning policies for navigating a KB via reinforcement learning.</p><p>A major problem with these approaches is their computational complexity, which renders them unusable for largerscale learning problems. In order to address this issue, <ref type="bibr" target="#b41">Minervini et al. (2020)</ref> propose Greedy NTPs (GNTPs), an extension to NTPs where, for each goal, only the top-k facts and rules are considered during the differentiable reasoning process.</p><p>Neural Module Networks <ref type="bibr" target="#b0">Andreas et al. (2016)</ref> introduce Neural Module Networks (NMNs), an end-to-end differentiable composition of jointly trained neural modules. Analogously, NTPs can be seen as a recursive differentiable composition of or and and modules, jointly trained on downstream reasoning tasks. NMNs allow defining and training end-to-end differentiable composable models, and interpret and execute their compositions as simple programs. This is especially useful when dealing with reasoning tasks from visual and natural language inputs, such as reasoning over text with arithmetic modules <ref type="bibr" target="#b17">(Gupta et al., 2019)</ref> or visual question answering <ref type="bibr" target="#b0">(Andreas et al., 2016)</ref>. Interestingly, in this work the structure of the composition is statically drawn from the data, while <ref type="bibr" target="#b23">Jiang &amp; Bansal (2019)</ref> propose a way of learning the model composition via a coordination model <ref type="bibr" target="#b23">(Jiang &amp; Bansal, 2019)</ref>.</p><p>Incorporating Knowledge via Regularisation Another branch of works uses symbolic background knowledge to learn better representations for entities and relationships in a KB. An early work in this space is <ref type="bibr" target="#b43">Rockt√§schel et al. (2015)</ref>, which regularise a relation extraction model by penalising inconsistency with respect to a set of logical constraints and sampled entities. <ref type="bibr" target="#b39">Minervini et al. (2017a)</ref> regularise relation representations to incorporate equivalence and inversion axioms for a set of neural link prediction models, while <ref type="bibr" target="#b7">Demeester et al. (2016)</ref> focus on simple implication axioms. <ref type="bibr" target="#b40">Minervini et al. (2017b)</ref> propose adversarially regularising neural models by identifying, during training, inputs that violate a given set of constraints, and regularising the model to decrease the degree of such violations. <ref type="bibr" target="#b54">Xu et al. (2018)</ref> propose a similar idea, by using a semantic loss measuring to which extent a model matches a set of given constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate CTPs on two datasets: systematic generalisation on the CLUTRR dataset, and link prediction in Knowledge Graphs (KGs). Datasets are introduced in Section 5.1, while baselines are described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Tasks</head><p>Systematic Generalisation CLUTRR-Compositional Language Understanding and Text-based Relational Reasoning <ref type="bibr" target="#b49">(Sinha et al., 2019)</ref>-contains a large set of graphs modelling hypothetical family relationships. Given a set of family relations, encoded as a graph with a variable number of nodes and edges, the goal is to infer the relationship between two family members, whose relationship is not explicitly mentioned. To solve this task, a learning agent should be able to induce the logical rules governing the kinship relationships, such as the parent of a parent is a grandparent, and use these rules to infer the relationship between a given pair of entities.</p><p>CLUTRR allows testing a learning agent's ability for systematic generalisation, by testing on graphs containing combinations of logical rules that were not seen during training. Each edge in the graph is labelled with one out of nine family relation type from R = { child, grand, in-law, inv-child, inv-grand, inv-in-law, inv-un, sibling, un }, and the task consists in inferring the relationship between two of the nodes in the graph. During training, a model is trained to infer such relationship by traversing a limited number of edges (such as two, three, and four edges), and during evaluation the model has to traverse up to ten edges. <ref type="figure">Fig. 1</ref> shows an example of a training instance and a test instance in CLUTRR: the training instances consists in a graph modelling a set of family relations of only three edges, while the test instance is composed by a graph with ten edges. In both cases, the task consists in inferring the relationships between two of the nodes in the graph.</p><p>Link Prediction Furthermore, we evaluate CTPs on neural link prediction tasks, following the same evaluation protocols as  on the Countries <ref type="bibr" target="#b2">(Bouchard et al., 2015)</ref>, Nations, UMLS, and Kinship <ref type="bibr" target="#b28">(Kemp et al., 2006)</ref> datasets.</p><p>The Countries dataset contains countries, regions, and subregions as entities, and it is carefully designed to test the logical reasoning and learning capabilities of neural link prediction models: queries have the form locatedIn(C, ¬∑ ), where the answers are regions. This dataset comes with three tasks (S1, S2, and S3) each requiring reasoning skills of increasing complexity; we refer to  for more details about this dataset.</p><p>The Unified Medical Language System (UMLS) dataset is from bio-medicine: entities are biomedical concepts, and relations include treatments and diagnoses. The Kinship dataset contains kinship relationships among members of the Alyawarra tribe from Central Australia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Models and Baselines</head><p>Models We consider the following three CTP model variants: i) CTP L , where the mappings f i from goal predicates to rule predicates are implemented via a linear projection, ii) CTP A , where it is implemented via attentive goal reformulation, as described in Section 3, and iii) CTP M , where it is implemented via memory-based goal reformulation, also described in Section 3.</p><p>Baselines We consider two classes of baselines: graphbased and sequence-based. Graph-based baselines consist in neural architectures specifically designed for graph data and. We consider GNTPs, a neuro-symbolic reasoning model, and two Graph Neural Network (GNN) architectures, namely GATs <ref type="bibr" target="#b53">(Velickovic et al., 2018)</ref> and Graph Convolutional Networks (GCNs) <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>.</p><p>Sequence-based baselines are neural architectures originally proposed for encoding sequences: by linearising the relational graphs into sequences of subject-predicate-object triples, we can use such models for encoding graphs. We  <ref type="figure">Figure 3</ref>: Results on the CLUTRR dataset after training on stories of lengths {2, 3} and evaluating on stories of length {4, 5, . . . , 10}hyperparameters were fine-tuned on either short stories (left) and long stories (right). Significance testing was assessed via a unequal variances t-test in comparison with CTPL: (resp. ) represents a p-value lower than 10 ‚àí4 (resp. 10 ‚àí2 ).</p><p>consider several sequence encoding models, namely Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs) <ref type="bibr" target="#b19">(Hochreiter &amp; Schmidhuber, 1997)</ref>, Gated Recurrent Units (GRUs) <ref type="bibr" target="#b4">(Cho et al., 2014)</ref>, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b29">(Kim, 2014)</ref>, CNN with Highway Encoders (CNNHs) <ref type="bibr" target="#b30">(Kim et al., 2016)</ref>, Multi-Headed Attention Networks (MHAs) <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref>.</p><p>Encoding For both graph-based and sequence-based baselines, we considered two approaches: i) encoding the the KB and goal independently, as in <ref type="bibr" target="#b49">Sinha et al. (2019)</ref>, and ii) conditioning the KB encoding on the goal. Let enc Œ∏e denote an encoder that, given a set of ground facts (such as a KB or a goal), produces a continuous k-dimensional representation, and≈∑ denote a conditional distribution over the candidate relationship types. The encoder in item (i), where the goal G and the KB K are encoded independently, can be summarised as≈∑ = softmax(W [enc Œ∏e (K); enc Œ∏e (G)]). The encoder in item (ii), where G and K are encoded jointly, can be summarised as≈∑ = softmax(W enc Œ∏e ([G; K])).</p><p>For model selection, we generate a CLUTRR-like dataset using the code published by <ref type="bibr" target="#b49">Sinha et al. (2019)</ref> composed of training set graphs with {2, 3} edges, and two validation sets, one with graphs with three edges, and another with graphs with nine edges. We then select two sets of hyperparameters for each of the baselines: one that maximises the validation accuracy on graphs with three edges, and another that maximises the test accuracy on graphs with nine edges. All details on the hyperparameter selection process can be found in Appendix A.  <ref type="figure">Figure 4</ref>: Results on the CLUTRR dataset after training on stories of lengths {2, 3, 4} and evaluating on stories of length {5, . . . , 10}hyperparameters were fine-tuned on either short stories (left) and long stories (right). Significance testing was assessed via a unequal variances t-test in comparison with CTPL: (resp. ) represents a p-value lower than 10 ‚àí4 (resp. 10 ‚àí2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>CLUTRR We evaluated three CTP variants and all considered baselines on two datasets published by <ref type="bibr" target="#b49">Sinha et al. (2019)</ref> under the identifiers 089907f8 and db9b8f04we refer to these datasets as CLUTRR G (k = 2, 3) and CLUTRR G <ref type="figure" target="#fig_2">(k = 2, 3, 4)</ref>, where k denotes the number of edges in the training graphs. Results for CLUTRR G (k = 2, 3) are summarised in <ref type="figure">Fig. 3</ref>, while results for CLUTRR G (k = 2, 3, 4) are summarised in <ref type="figure">Fig. 4</ref>.</p><p>In <ref type="figure">Fig. 3</ref>, we observe that, after training on graphs with two and three edges, baseline models tend to be able to generalise correctly to slightly longer stories (such as graphs with four and five edges), but that predictive accuracy quickly decreases with increasing graph sizes and this phenomenon happens when tuning hyper-parameters either on graphs with three edges or on graph with nine edges.</p><p>In our experiments, LSTMs had a strikingly different be-haviour in comparison with other baselines: for graphs with nine edges, the accuracy decrease caused by using the LSTMs baseline is only significant with p ‚â§ 10 ‚àí2 (for all other baselines this change is significant with p ‚â§ 10 ‚àí4 ), with a drop in significance for smaller graphs.</p><p>The phenomenon that LSTMs yield surprisingly accurate results on the CLUTRR dataset can be seen across every experiment in our empirical evaluation, while other recurrent models such as RNNs and GRUs do not show this.</p><p>Model Analysis A great feature of CTPs is that we can analyse the goal reformulation process to understand the reasoning process underlying a given prediction, and extract explainable rules. In <ref type="figure" target="#fig_2">Fig. 2</ref>, we show a sample of the rules and common-sense reasoning patterns learned by CTPs on the CLUTRR dataset.</p><p>We can see that, for example, CTPs successfully identify that e.g. the child of a child is a grandchild, the child of <ref type="table">Table 1</ref>: Comparison of CTPs, with GNTPs <ref type="bibr" target="#b41">(Minervini et al., 2020)</ref>, NeuralLP <ref type="bibr" target="#b55">(Yang et al., 2017)</ref> and MINERVA <ref type="bibr" target="#b5">(Das et al., 2018)</ref> (from <ref type="bibr" target="#b41">Minervini et al. (2020)</ref>) on benchmark datasets: hyperparameters were selected based on the validation MRR, and we report the mean and standard deviation over 10 random seeds.  one's significant other is also one's child, and the parent of a significant other is an in-law.</p><p>Training Dynamics We analyse the training dynamics of CTP L , CTP A , CTP M , and GNTPs <ref type="bibr" target="#b41">(Minervini et al., 2020)</ref> on CLUTRR G <ref type="figure" target="#fig_2">(k=2,3,4)</ref>.</p><p>The CTP variants consisted of 5 goal reformulators, each implemented by an independent select module, while GNTP has a KB of 32 rules and k ‚àà {5, 7, 9, 11}. For all models, the embedding size for entities and relations was set to 50.</p><p>Figure 5 demonstrates how the training accuracy of such models evolves during training. We can see that, while the three CTP variants get to a nearly-perfect training set accuracy in less than 300 iterations, GNTPs is unable to match this result, even after careful hyperparameter tuning. A possible explanation is that, in order to scale to large rule sets, GNTPs only considers the top-k rules, based on the similarity between the goal and the head of the rules. This is equivalent to a hard attention mask, which is known to be problematic to train via gradient-based optimisation <ref type="bibr" target="#b37">(Luong et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction</head><p>In <ref type="table">Table 1</ref>, we show link prediction results in comparison with three other neuro-symbolic reasoning methods, namely GNTPs <ref type="bibr" target="#b41">(Minervini et al., 2020)</ref>, NeuralLP <ref type="bibr" target="#b55">(Yang et al., 2017)</ref> and MINERVA <ref type="bibr" target="#b5">(Das et al., 2018)</ref>. GNTPs are an extension of NTPs where rules are heuristically selected by search for the rules where the head predicate is closest to the sub-goal predicate in embedding space.</p><p>Our experiments show that CTPs produce significantly more accurate or very competitive link prediction results, while controlling the complexity of the reasoning process via the goal-conditioned rule selection. For instance, in the Nations dataset, only two rules were generated by CTPs for each goal, while in  NTPs were required to iterate over sixty rules.</p><p>Furthermore, in this case, CTPs were able to produce explanations for each of their predictions. For instance, in the Nations dataset, CTPs successfully extracted logical patterns such as commonbloc1(X, Y) :relngo(Y, X), timesincewar(X, Y) :independence(X, Y), unweightedunvote(X, Y) :relngo(X, Y), and ngo(X, Y) :independence(Y, X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduced CTPs, an extension to NTPs for learning the optimal rule selection strategy via gradient-based optimisation. For each sub-goal, a select module produces a smaller set of rules, which is then used during the proving mechanism. Furthermore, we proposed three variants of the rule selection mechanism, where the sub-goal reformulations are obtained by linear projections of the sub-goal predicate, attention distributions over predicate embeddings, and a key-value memory lookup over a set of rules.</p><p>We showed that CTPs are scalable and yield state-of-the-art results on the CLUTRR dataset, which explicitly tests the systematic generalisation of neural models, in comparison with a wide set of neural baselines. Finally, we demonstrated that CTPs yield competitive results in standard link prediction benchmark in comparison with other neuro-symbolic approaches.</p><p>Future Work An open problem is how CTPs can be able to process CLUTRR instances where family relationships are not directly provided as a labelled graph, but rather as free-form text. A possible solution, proposed by <ref type="bibr" target="#b41">Minervini et al. (2020)</ref>, consists in having an end-to-end differentiable encoder for producing the fact embeddings conditioned on the text, and we are currently analysing several options in this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameter Selection</head><p>For model selection in baseline models, we generated a CLUTRR-like dataset using the code published by <ref type="bibr" target="#b49">Sinha et al. (2019)</ref> composed by a training set of graphs with {2, 3} edges, and two test sets, one with graphs with three edges, and another with graphs with nine edges. We then selected two sets of hyperparameters for each of the baselines: one that maximises the validation accuracy on graphs with three edges, and another that maximises the test accuracy on graphs with nine edges. For each of the baselines, we considered a wide range of hyperparameters: the dimensionalities of node and edge embeddings were varied in {10, 50, 100, 200, 500}, the number of attention heads in attention-based architectures in {1, 2, . . . , 10}, the number of filters in convolutional architectures in {1, 2, . . . , 10}, and the number of hidden units in recurrent architectures in {32, 64, 128, 256, 512}.</p><p>To assess the statistical significance of our results, we ran each of the experiments 10 times, each time with a different seed, and compared the resulting accuracy values using an unequal variances t-test, or Welch's t-test. This is motivated by the observation that accuracy values to be Gaussiandistributed, as they approach a normal distribution for large numbers of re-runs, due to the Central Limit Theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Optimal Hyperparameters</head><p>Note that all recurrent architectures are bidirectional.</p><p>Graph Attention Networks: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are h = 10 for the number of attention heads, k = 50 for the dimensionality of node embeddings, k e = 200 for the dimensionality of edge embeddings. For validation graphs with 9 edges, k = 50, k e = 500, and h = 10.</p><p>Graph Convolutional Networks: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 50 for the dimensionality of node embeddings, k e = 500 for the dimensionality of edge embeddings. For validation graphs with 9 edges, k = 50, and k e = 50.</p><p>Convolutional Neural Networks: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 50 for the dimensionality of node and edge embeddings, f = 8 convolutional filters, and conditional encoding. For validation graphs with 9 edges, k = 200, f = 4, and conditional encoding.</p><p>Recurrent Neural Networks: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 50 for the dimensionality of node and edge embeddings, h = 64 for the size of the hidden state representations, and conditional encoding. For validation graphs with 9 edges, k = 500, h = 512, and conditional encoding.</p><p>Long Short-Memory Networks: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 50 for the dimensionality of node and edge embeddings, h = 64 for the size of the hidden state representations, and conditional encoding. For validations graphs with 9 edges, k = 100, h = 512, and independent encoding.</p><p>Gated Recurrent Units: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 50 for the dimensionality of node and edge embeddings, h = 64 for the size of the hidden state representations, and conditional encoding. For validation graphs with 9 edges, k = 200, h = 512, and conditional encoding.</p><p>CNN with Highway Encoder: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 200 for the dimensionality of node and edge embeddings, h = 2 highway layers, and conditional encoding. For validation graphs with 9 edges, k = 200, h = 1, and conditional encoding.</p><p>Multi-Head Attention: the hyperparameters that maximise the accuracy on validation graphs with 3 edges are k = 500 for the dimensionality of node and edge embeddings, h = 10 for the number of attention heads, h k for the size of the hidden state representation of the top LSTM layer, and conditional encoding. For validation graphs with 9 edges, k = 500, h = 10, h k = 128, and conditional encoding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1:</head><label></label><figDesc>function or(G, d, S) 2: for H :-B ‚àà K do 3: for S ‚àà and (B, d, unify(H, G, S)) do 4: yield S 1: function and(B, d, S) 2: if B = [] or d = 0 then yield S else 3: for S ‚àà or (sub(B 0 , S œà ), d ‚àí 1, S) do 4: for S ‚àà and(B 1: , d, S ) do 5: yield S 1: function unify(H, G, S =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1: function or(G, d, S) 2: for H :-B ‚àà select Œ∏ (G) do 3:for S ‚àà and (B, d, unify(H, G, S)) do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Rules learned on Compositional Language Understanding and Text-based Relational Reasoning (CLUTRR) by CTPssymbols were obtained by decoding the goal reformulations with the nearest predicate in embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Training dynamics of CTPs and GNTPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>CTPL .98 ¬± .02 .98 ¬± .03 .97 ¬± .05 .96 ¬± .04 .94 ¬± .05 .89 ¬± .07 .89 ¬± .07 CTPA .99 ¬± .02 .99 ¬± .01 .99 ¬± .02 .96 ¬± .04 .94 ¬± .05 .89 ¬± .08 .90 ¬± .07 CTPM .97 ¬± .03 .97 ¬± .03 .96 ¬± .06 .95 ¬± .06 .93 ¬± .06 .90 ¬± .06 .89 ¬± .06 GNTP .49 ¬± .18 .45 ¬± .21 .38 ¬± .23 .37 ¬± .21 .32 ¬± .20 .31 ¬± .19 .31 ¬± .22 GAT .92 ¬± .01 .73 ¬± .04 .56 ¬± .04 .55 ¬± .04 .54 ¬± .03 .55 ¬± .04 .50 ¬± .04 GCN .84 ¬± .04 .61 ¬± .03 .51 ¬± .02 .48 ¬± .02 .45 ¬± .03 .47 ¬± .05 .41 ¬± .04 RNN .93 ¬± .03 .91 ¬± .03 .79 ¬± .08 .82 ¬± .06 .75 ¬± .11 .68 ¬± .07 .64 ¬± .07 LSTM 1.0 ¬± .00 1.0 ¬± .00 .95 ¬± .03 .94 ¬± .04 .87 ¬± .08 .86 ¬± .08 .84 ¬± .09 GRU .92 ¬± .05 .88 ¬± .06 .78 ¬± .09 .77 ¬± .09 .74 ¬± .08 .66 ¬± .10 .65 ¬± .08 CNNH .94 ¬± .03 .86 ¬± .06 .77 ¬± .08 .72 ¬± .08 .64 ¬± .09 .59 ¬± .10 .59 ¬± .09 CNN .93 ¬± .04 .86 ¬± .07 .84 ¬± .09 .79 ¬± .08 .77 ¬± .10 .69 ¬± .09 .70 ¬± .11 MHA .81 ¬± .04 .76 ¬± .04 .74 ¬± .05 .70 ¬± .04 .69 ¬± .03 .64 ¬± .05 .67 ¬± .02</figDesc><table><row><cell></cell><cell>1.0</cell><cell></cell><cell cols="6">CLUTRR G (k=2,3), baselines tuned on k = 3</cell><cell>1.0</cell><cell></cell><cell cols="4">CLUTRR G (k=2,3), baselines tuned on k = 9</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell>Model</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell>CTP L CTP A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>CTP L CTP A</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.6 0.7</cell><cell></cell><cell>CTP M GAT GCN RNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.6 0.7</cell><cell></cell><cell>CTP M GAT GCN RNN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSTM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell>GRU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CNNH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNNH</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MHA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MHA</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell>4</cell><cell>5</cell><cell cols="3">6 Test Story Length 7 8</cell><cell>9</cell><cell>10</cell><cell>0.3</cell><cell>4</cell><cell>5</cell><cell cols="3">6 Test Story Length 7 8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell>4 Hops</cell><cell>5 Hops</cell><cell>6 Hops</cell><cell>7 Hops</cell><cell>8 Hops</cell><cell>9 Hops</cell><cell>10 Hops</cell><cell></cell><cell>4 Hops</cell><cell>5 Hops</cell><cell>6 Hops</cell><cell>7 Hops</cell><cell>8 Hops</cell><cell>9 Hops</cell><cell>10 Hops</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>CTPL .98 ¬± .02 .98 ¬± .03 .97 ¬± .05 .96 ¬± .04 .94 ¬± .05 .89 ¬± .07 .89 ¬± .07 CTPA .99 ¬± .02 .99 ¬± .01 .99 ¬± .02 .96 ¬± .04 .94 ¬± .05 .89 ¬± .08 .90 ¬± .07 CTPM .97 ¬± .03 .97 ¬± .03 .96 ¬± .06 .95 ¬± .06 .93 ¬± .06 .90 ¬± .06 .89 ¬± .06 GNTP .49 ¬± .18 .45 ¬± .21 .38 ¬± .23 .37 ¬± .21 .32 ¬± .20 .31 ¬± .19 .31 ¬± .22 GAT .91 ¬± .02 .76 ¬± .06 .54 ¬± .03 .56 ¬± .04 .54 ¬± .03 .55 ¬± .05 .45 ¬± .06 GCN .84 ¬± .03 .68 ¬± .02 .53 ¬± .03 .47 ¬± .04 .42 ¬± .03 .45 ¬± .03 .39 ¬± .02 RNN .86 ¬± .06 .76 ¬± .08 .67 ¬± .08 .66 ¬± .08 .56 ¬± .10 .55 ¬± .10 .48 ¬± .07 LSTM .98 ¬± .04 .95 ¬± .03 .88 ¬± .05 .87 ¬± .04 .81 ¬± .07 .75 ¬± .10 .75 ¬± .09 GRU .89 ¬± .05 .83 ¬± .06 .74 ¬± .12 .72 ¬± .09 .67 ¬± .12 .62 ¬± .10 .60 ¬± .12 CNNH .90 ¬± .04 .81 ¬± .05 .69 ¬± .10 .64 ¬± .08 .56 ¬± .13 .52 ¬± .12 .50 ¬± .12 CNN .95 ¬± .02 .90 ¬± .03 .89 ¬± .04 .80 ¬± .05 .76 ¬± .08 .69 ¬± .07 .70 ¬± .08 MHA .81 ¬± .04 .76 ¬± .04 .74 ¬± .05 .70 ¬± .04 .69 ¬± .03 .64 ¬± .05 .67 ¬± .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CTP L .99 ¬± .02 .98 ¬± .04 .97 ¬± .04 .98 ¬± .03 .97 ¬± .04 .95 ¬± .04 CTP A .99 ¬± .04 .99 ¬± .03 .97 ¬± .03 .95 ¬± .06 .93 ¬± .07 .91 ¬± .05 CTP M .98 ¬± .04 .97 ¬± .06 .95 ¬± .06 .94 ¬± .08 .93 ¬± .08 .90 ¬± .09 GNTP .68 ¬± .28 .63 ¬± .34 .62 ¬± .31 .59 ¬± .32 .57 ¬± .34 .52 ¬± .32 GAT .99 ¬± .00 .85 ¬± .04 .80 ¬± .03 .71 ¬± .03 .70 ¬± .03 .68 ¬± .02 GCN .94 ¬± .03 .79 ¬± .02 .61 ¬± .03 .53 ¬± .04 .53 ¬± .04 .41 ¬± .04 RNN .93 ¬± .06 .87 ¬± .07 .79 ¬± .11 .73 ¬± .12 .65 ¬± .16 .64 ¬± .16 LSTM .98 ¬± .03 .95 ¬± .04 .89 ¬± .10 .84 ¬± .07 .77 ¬± .11 .78 ¬± .11 GRU .95 ¬± .04 .94 ¬± .03 .87 ¬± .08 .81 ¬± .13 .74 ¬± .15 .75 ¬± .15 CNNH .99 ¬± .01 .97 ¬± .02 .94 ¬± .03 .88 ¬± .04 .86 ¬± .05 .84 ¬± .06 CNN 1.0 ¬± .00 1.0 ¬± .01 .98 ¬± .01 .95 ¬± .03 .93 ¬± .03 .92 ¬± .04 MHA .88 ¬± .03 .83 ¬± .05 .76 ¬± .04 .72 ¬± .04 .74 ¬± .05 .70 ¬± .03 CTP L .99 ¬± .02 .98 ¬± .04 .97 ¬± .04 .98 ¬± .03 .97 ¬± .04 .95 ¬± .04 CTP A .99 ¬± .04 .99 ¬± .03 .97 ¬± .03 .95 ¬± .06 .93 ¬± .07 .91 ¬± .05 CTP M .98 ¬± .04 .97 ¬± .06 .95 ¬± .06 .94 ¬± .08 .93 ¬± .08 .90 ¬± .09 GNTP .68 ¬± .28 .63 ¬± .34 .62 ¬± .31 .59 ¬± .32 .57 ¬± .34 .52 ¬± .32 GAT .98 ¬± .01 .86 ¬± .04 .79 ¬± .02 .75 ¬± .03 .73 ¬± .02 .72 ¬± .03 GCN .88 ¬± .01 .78 ¬± .02 .60 ¬± .02 .57 ¬± .02 .59 ¬± .04 .51 ¬± .02 RNN .96 ¬± .03 .87 ¬± .09 .82 ¬± .09 .73 ¬± .09 .65 ¬± .15 .67 ¬± .16 LSTM 1.0 ¬± .01 .99 ¬± .02 .96 ¬± .04 .96 ¬± .04 .94 ¬± .06 .92 ¬± .07 GRU .96 ¬± .02 .88 ¬± .03 .84 ¬± .04 .79 ¬± .06 .75 ¬± .08 .78 ¬± .04 CNNH 1.0 ¬± .00 .99 ¬± .01 .96 ¬± .02 .91 ¬± .04 .89 ¬± .04 .87 ¬± .04 CNN 1.0 ¬± .00 .98 ¬± .01 .97 ¬± .02 .92 ¬± .03 .89 ¬± .03 .87 ¬± .04 MHA .88 ¬± .03 .83 ¬± .05 .76 ¬± .03 .74 ¬± .04 .75 ¬± .04 .70 ¬± .03</figDesc><table><row><cell></cell><cell>1.0</cell><cell cols="6">CLUTRR G (k=2,3,4), baselines tuned on k = 3</cell><cell>1.0</cell><cell cols="4">CLUTRR G (k=2,3,4), baselines tuned on k = 9</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell>Model CTP L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell>Model CTP L</cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.7 0.8</cell><cell></cell><cell>CTP A CTP M GAT GCN RNN</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.7 0.8</cell><cell></cell><cell>CTP A CTP M GAT GCN RNN</cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell>LSTM GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell>LSTM GRU</cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell>CNNH CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell>CNNH CNN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MHA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MHA</cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell>5</cell><cell>6</cell><cell cols="2">7 Test Story Length 8</cell><cell>9</cell><cell>10</cell><cell>0.4</cell><cell>5</cell><cell>6</cell><cell cols="2">7 Test Story Length 8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell>5 Hops</cell><cell>6 Hops</cell><cell>7 Hops</cell><cell>8 Hops</cell><cell>9 Hops</cell><cell>10 Hops</cell><cell></cell><cell>5 Hops</cell><cell>6 Hops</cell><cell>7 Hops</cell><cell>8 Hops</cell><cell>9 Hops</cell><cell>10 Hops</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">To assess the statistical significance of our results, we ran</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">each of the experiments 10 times, each time with a different</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">seed, and compared the resulting accuracy values using an</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">unequal variances t-test, or Welch's t-test. 3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We assume accuracy values to be Gaussian-distributed, as they approach a normal distribution for large numbers of re-runs, due to the Central Limit Theorem.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the EU Horizon 2020 Research and Innovation Programme under the grant 875160. We thank Yihong Chen, Joe Stacey, and all the amazing folks in the UCL NLP group for the enlightening discussions and support. Finally, we thank NVIDIA for GPU donations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Systematic generalization: What is required and can it be learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noukhovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposia</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Programming with a Differentiable Forth Interpreter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bo≈°njak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">√á</forename><surname>G√ºl√ßehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural-symbolic learning and reasoning: Contributions and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>F√∂ldi√°k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>K√ºhnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposia</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lifted rule injection for relation embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1389" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Logic tensor networks for semantic image interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Donadello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Garcez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1596" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1702.08608</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Explanatory Rules from Noisy Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconciling deep learning with symbolic artificial intelligence: representing objects and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno>2352-1546. SI: 29</idno>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Network Methods for Natural Language Processing. Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Learning. Adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural Turing Machines. CoRR, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5401</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Transduce with Unbounded Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural module networks for reasoning over text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<idno>abs/1912.04971</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reinforced mnemonic reader for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fusionnet: Fusing via fully-aware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-assembling modular networks for interpretable multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4473" to="4483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural GPUs Learn Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How much reading does reading comprehension require? A critical investigation of popular benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5015" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Systems of Concepts with an Infinite Relational Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openreview</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-tosequence recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2879" to="2888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The mythos of model interpretability. Commun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<title level="m">A robustly optimized BERT pretraining approach. CoRR, abs</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularizing knowledge graph embeddings via equivalence and inversion axioms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mu√±oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nov√°cek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandenbussche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10534</biblScope>
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial sets for regularising neural link predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Differentiable reasoning on large knowledge bases and natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5182" to="5190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-to-end differentiable proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3788" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
	<note>HLT-NAACL</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Artificial Intelligence -A Modern Approach, Third International Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DRUM: end-to-end differentiable rule mining on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15321" to="15331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<idno>abs/1609.05284</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CLUTRR: A diagnostic benchmark for inductive reasoning from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4505" to="4514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On training recurrent neural networks for lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1811.07017</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Cortes, C. et al.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A semantic loss function for deep learning with symbolic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Broeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5498" to="5507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Guyon, I. et al.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Generalized autoregressive pretraining for language understanding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
