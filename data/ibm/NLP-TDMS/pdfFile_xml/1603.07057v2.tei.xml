<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do We Really Need to Collect Millions of Faces for Effective Face Recognition?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuãn</forename><surname>Trãn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatuporn</forename><forename type="middle">Toy</forename><surname>Leksut</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Open University of Israel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Do We Really Need to Collect Millions of Faces for Effective Face Recognition?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition capabilities have recently made extraordinary leaps. Though this progress is at least partially due to ballooning training set sizes -huge numbers of face images downloaded and labeled for identity -it is not clear if the formidable task of collecting so many images is truly necessary. We propose a far more accessible means of increasing training data sizes for face recognition systems. Rather than manually harvesting and labeling more faces, we simply synthesize them. We describe novel methods of enriching an existing dataset with important facial appearance variations by manipulating the faces it contains. We further apply this synthesis approach when matching query images represented using a standard convolutional neural network. The effect of training and testing with synthesized images is extensively tested on the LFW and IJB-A (verification and identification) benchmarks and Janus CS2. The performances obtained by our approach match state of the art results reported by systems trained on millions of downloaded images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent impact of deep Convolutional Neural Network (CNN) based methods on machine face recognition capabilities has been nothing short of revolutionary. The conditions under which faces can now be recognized and the numbers of faces which systems can now learn to identify improved to the point where some consider machines to be better than humans at this task. This remarkable advancement is partially due to the gradual improvement of new network designs which offer better performance. However, alongside developments in network architectures, it is also the underlying ability of CNNs to learn from massive training sets that allows these techniques to be so effective.</p><p>Realizing that effective CNNs can be made even more effective by increasing their training data, many begun focusing efforts on harvesting and labeling large image collections to better train their networks. In <ref type="bibr" target="#b34">[35]</ref>, a standard CNN was trained by Facebook using 4.4 million labeled faces and shown to achieve what was, at the time, state of the art performance on the Labeled Faces in the Wild (LFW) benchmark <ref type="bibr" target="#b10">[11]</ref>. Some time later, <ref type="bibr" target="#b23">[24]</ref> proposed the VGG-Face representation, trained on 2.6 million faces, and Face++ proposed its Megvii arXiv:1603.07057v2 [cs.CV] 11 Apr 2016 <ref type="bibr">Dataset</ref> #ID #Img #Img/#ID Google <ref type="bibr" target="#b28">[29]</ref> 8M 200M 25 Facebook <ref type="bibr" target="#b34">[35]</ref> 4,030 4.4M 1K VGG Face <ref type="bibr" target="#b23">[24]</ref> 2,622 2.6M 1K MegaFace <ref type="bibr" target="#b11">[12]</ref> 690,572 1.02M 1.5 CASIA <ref type="bibr" target="#b43">[44]</ref> 10,575 494,414 46 Aug. pose+shape 10,575 1,977,656 187 Aug. pose+shape+expr 10,575 2,472,070 234 (a) Face set statistics   <ref type="bibr" target="#b43">[44]</ref> (also shown in the last two rows of (a)).</p><p>System <ref type="bibr" target="#b44">[45]</ref>, trained on 5 million faces. All, however, pale in comparison to the Google FaceNet <ref type="bibr" target="#b28">[29]</ref> which used 200 million labeled faces for its training. Making networks better by collecting and labeling huge training sets is, unfortunately, not an easy game to play. The effort required to download, process and label millions of images from the Internet with reliable subject names is daunting. To emphasize this, the bigger sets, <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b28">[29]</ref>, required the efforts of large scale commercial organizations to assemble (Facebook and Google, resp.) and none of these sets was publicly released by its owners. By comparison, the largest face recognition training set which is publicly available is the CASIA WebFace collection <ref type="bibr" target="#b43">[44]</ref> weighing in at a mere 495K images, several orders of magnitudes smaller than the two bigger commercial sets <ref type="bibr" target="#b0">1</ref> .</p><p>But downloading and labeling so many faces is more than just financially challenging. <ref type="figure" target="#fig_1">Fig. 1a</ref> provides some statistical information on the larger face sets. Evidently, set sizes increase far faster than the numbers of images per-subject. This may imply that finding many images verified as belonging to the same subjects is difficult even when resources are abundant. Regardless of the reason, this is a serious problem: face recognition systems should learn to model not just inter-class appearance variations (differences between different people) but also intra-class variations (differences of appearance that do not change subject label) and so far this has been a challenge for data collection efforts.</p><p>In light of these challenges, it is natural to ask: is there no alternative to this labor intensive, data download and labeling approach to pushing recognition performance? Beyond potentially mitigating the challenges of data collection, this question touches a more fundamental issue. Namely, should images be processed with domain specific tools before being analyzed by CNNs, and if so, how?</p><p>In answer to these questions, we make the following contributions. (1) We propose synthesizing data in addition to collecting it. We inflate the size of an existing training set, the CASIA WebFace collection <ref type="bibr" target="#b43">[44]</ref>, to several times its size using domain specific methods designed for face image synthesis ( <ref type="figure" target="#fig_1">Fig. 1b</ref>). We generate images which introduce new intra-class facial appearance variations, including pose (Sec. 3.1), shape (Sec. 3.2) and expression (Sec. <ref type="bibr" target="#b2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.3). (2)</head><p>We describe a novel matching pipeline which uses similar synthesis methods at test time when processing query images. Finally, (3), we rigorously test our approach on the LFW <ref type="bibr" target="#b10">[11]</ref>, IJB-A (verification and identification) and CS2 benchmarks <ref type="bibr" target="#b13">[14]</ref>. Our results show that a CNN trained using these generated faces matches state of the art performance reported by systems trained on millions of faces downloaded from the web and manually processed 2 .</p><p>We note that our approach can be considered a novel face data augmentation method (Sec. 2): A domain specific data augmentation approach. Curiously, despite the success of existing generic augmentation methods, we are unaware of previous reports of applying the easily accessible approach described here to generate face image training data, or indeed training for any other class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Face recognition: Face recognition is one of the central problems in computer vision and, as such, work on this problem is extensive. As with many other computer vision problems, face recognition performances sky rocketed with the introduction of deep learning techniques and in particular CNNs. Though CNNs have been used for face recognition as far back as <ref type="bibr" target="#b16">[17]</ref>, only when massive amounts of data became available did their performance soar. This was originally demonstrated by the Facebook DeepFace system <ref type="bibr" target="#b34">[35]</ref>, which used an architecture not unlike the one used by <ref type="bibr" target="#b16">[17]</ref>, but with over 4 million images used for training they obtained far more impressive results.</p><p>Since then, CNN based recognition systems continuously cross performance barriers with some notable examples including the Deep-ID 1-3 systems <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. They and many others since, developed and trained their systems using far fewer training images, at the cost of somewhat more elaborate network architectures.</p><p>Though novel network architecture designs can lead to better performance, further improvement can be achieved by collecting more training data. This has been demonstrated by the Google FaceNet team <ref type="bibr" target="#b28">[29]</ref>, who developed and trained their system on 200 million images. Besides improving results, they also offered a fascinating analysis of the consequences of adding more data: apparently, there is a significant diminishing returns effect when training with increasing image numbers. Thus, the leap in performance obtained by going from thousands of images to millions is substantial but increasing the numbers further provides smaller and smaller benefits. One way to explain this is that the data they and others used suffers from a long tail phenomenon <ref type="bibr" target="#b43">[44]</ref>, where most subjects in these huge datasets have very few images available for the network to learn intra-subject appearance variations from.</p><p>These methods were all evaluated on the LFW dataset, which has been for some time a standard de facto for measuring face recognition performances. Many of these LFW results, however, are already reaching near-perfect performances, suggesting that LFW is no longer a challenging benchmark for today's systems. Another relevant benchmark, also frequently used to report performances, is the YouTube Faces (YTF) set <ref type="bibr" target="#b37">[38]</ref>. It contains unconstrained face videos rather than images, but it too is quickly being saturated.</p><p>Recently, a new benchmark was released in order to again push machine face recognition capabilities, the Janus set <ref type="bibr" target="#b13">[14]</ref>. It offers several novelties compared to existing sets, including template based, rather than image based, recognition and a mix of both images and videos. It is also tougher than previous collections. Not surprisingly, dominating performance on Janus are CNN methods such as <ref type="bibr" target="#b3">[4]</ref>.</p><p>Data augmentation: Data augmentation techniques are transformations applied to the images used for training or testing, but without altering their labels. Such methods are well known to improve the performance of CNN based methods and prevent overfitting <ref type="bibr" target="#b2">[3]</ref>. These methods, however, typically involved generic image processing operations which do not exploit knowledge of the underlying problem domain to synthesize new appearance variations.</p><p>Popular augmentation methods include simple, geometric transformations such as oversampling (multiple, translated versions of the input image obtained by cropping at different offsets) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>, mirroring (horizontal flipping) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref>, rotating <ref type="bibr" target="#b38">[39]</ref> the images as well as various photometric transformations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7]</ref>. Surprisingly, despite being widely recognized as highly beneficial to the training of CNN systems, we are unaware of previous attempts to go beyond these simple transformations as we proposed doing. One notable exception is the recent work of <ref type="bibr" target="#b20">[21]</ref> which proposes to augment training data for a person reidentification network by replacing image backgrounds. We propose a far more elaborate, yet easily accessible means of data augmentation.</p><p>Finally, we note that the recent work of <ref type="bibr" target="#b39">[40]</ref> describes a so-called task-specific data augmentation method. They, as well as <ref type="bibr" target="#b40">[41]</ref>, do not synthesize new data as we propose to do here, but rather offer additional means of collecting images from the Internet to improve learning in fine grained recognition tasks. This is, of course, very different from our own approach.</p><p>Face synthesis for face recognition: The idea that face images can be synthetically generated in order to aid face recognition systems is not new. To our knowledge, it was originally proposed in <ref type="bibr" target="#b8">[9]</ref> and then effectively used by <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b9">[10]</ref>. Contrary to us, they all produced frontal faces which are presumably better aligned and easier to compare. They did not use other transformations to generate new images (e.g., other poses, facial expressions). More importantly, their images were used to reduce appearance variability, whereas we propose the opposite: to dramatically increase it to improve both training and testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Synthesizing faces</head><p>In this section we detail our approach to augmenting a generic face dataset. We use the CASIA WebFace collection <ref type="bibr" target="#b43">[44]</ref>, enriching it with substantially more per-subject appearance variations, yet without changing subject labels or losing meaningful information. Specifically, we propose to generate (synthesize) new face images, by introducing the following face specific appearance variations:</p><p>1. Pose: Simulating face image appearances across unseen 3D viewpoints. 2. Shape: Producing facial appearances using different 3D generic face shapes. 3. Expression: Specifically, simulating closed mouth expressions.</p><p>As previously mentioned, (1) can be considered an extension of frontalization techniques <ref type="bibr" target="#b9">[10]</ref> to multiple views. Conceptually, they rendered new views to reduce variability for better alignment whereas we do this to increase variability to better capture intra-subject appearance variations. Also noteworthy is that (2) explicitly contradicts previous assumptions on the importance of 3D facial shape in recognizing faces (e.g., <ref type="bibr" target="#b34">[35]</ref>): Contrary to these claims -that shape carries important subject related information -we ignore these cues by rendering the same face using different underlying shapes. As we later show, this introduces subtle appearance variations which provide meaningful information at training, but by no means change perceived subject identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pose variations</head><p>In order to generate unseen viewpoints given a face image I, we use a technique similar to the frontalization proposed by <ref type="bibr" target="#b9">[10]</ref>. We begin by applying the facial landmark detector from <ref type="bibr" target="#b1">[2]</ref>. Given these detected landmarks we estimate the six degrees of freedom pose for the face in I using correspondences between the detected landmarks p i ∈ R 2 and points P i . = S(i) ∈ R 3 , labeled on a 3D generic face model S. Here, i indexes specific facial landmarks in I and the 3D shape S.</p><p>As mentioned earlier, we use CASIA faces for augmentation. These faces are roughly centered in their images, and so detecting face bounding boxes was unnecessary. Instead, we used a fixed bounding box determined once beforehand.</p><p>Given the corresponding landmarks p i ↔ P i we use PnP <ref type="bibr" target="#b7">[8]</ref> to estimate extrinsic camera parameters, assuming the principal point is in the image center and then refining the focal length by minimizing landmark re-projection errors. This process gives us a perspective camera model mapping the generic 3D shape S on the image such as</p><formula xml:id="formula_0">p i ∼ M P i where M = K [R t] is the camera matrix.</formula><p>Given the estimated pose M, we decompose it to obtain a rotation matrix R ∈ R 3×3 containing rotation angles for the 3D head shape with respect to the image. We then create new rotation matrices R θ ∈ R 3×3 for unseen (novel) viewpoints by sampling different yaw angles θ. In particular, since CASIA images are biased towards frontal faces, given an image I we render it at the fixed yaw values θ = {0 • , ±40 • , ±75 • }. Rendering itself is derived from <ref type="bibr" target="#b9">[10]</ref>, including softsymmetry. <ref type="figure" target="#fig_2">Fig. 2</ref> shows viewpoint (pose) synthesis results for a training subject in CASIA, illustrating the 3D pose estimation process.</p><p>Note that in practice, faces are rendered with a uniform black background not shown here (original background from the image was not preserved in rendering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D shape variations</head><p>In the past, some argued that to truthfully capture the appearance of a subject's face under different viewpoints, its true 3D form must be used for the rendering. They therefore attempted to estimate this 3D shape from the image directly prior to frontalization <ref type="bibr" target="#b34">[35]</ref>. Because this reconstruction process is unstable, particularly for challenging, unconstrained images, Hassner et al. <ref type="bibr" target="#b9">[10]</ref> instead used a single generic 3D face to frontalize all face images. We propose the following simple compromise between these two approaches.</p><p>Rather than using a single generic 3D shape or estimating it from the image directly, we extend the procedure described in Sec. 3.1 to multiple generic 3D faces. In particular we add the set of generic 3D shapes S = {S j } 10 j=1 . We then simply repeat the pose synthesis procedure with these ten shapes rather than using only a single 3D shape. We used 3D generic shapes from the publicly available Basel 3D face set <ref type="bibr" target="#b24">[25]</ref>. It includes 10 high quality 3D face scans captured from different people with different face shapes. The subjects vary in gender, age, and weight. The models are further well aligned to each other, hence requiring 3D landmarks to be selected only once, on one of these 3D faces, and then directly transferring them to the other nine models. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the ten generic models used here, along with images rendered to near profile view using each of these shapes. Clearly, subjects in these images remain identifiable, despite the different underlying 3D shape, meeting the augmentation requirement of not changing subject labels. Yet each image is slightly but noticeably different from the rest, introducing appearance variations to this subject's image set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expression variations</head><p>In addition to pose and shape, we also synthesize expression variations, specifically reducing deformations around the mouth. Given a face image I and its 2D detected landmarks p i , and following pose estimation (Sec. 3.1) we estimate facial expression by fitting a 3D expression Blendshape, similarly to <ref type="bibr" target="#b18">[19]</ref>. This is a linear combination of 3D generic face models with various basis expressions, including mouth-closed, mouth-opened and smile. Following alignment of the 3D face model and the 2D face image in both pose and expression, we perform image-based texture mapping to register the face texture onto the model. This is useful to quickly assign texture to our face model given that only one image is available. To synthesize expression, we manipulate the 3D textured face model to exhibit new expressions and render it back to the original image. This technique allows us to render a normalized expression where other image details, including hair and background, remain unchanged. In our experiments we do this to produce images with closed mouths. Some example synthesis results are provided in <ref type="figure" target="#fig_4">Fig. 4</ref>. Though slight artifacts are sometimes introduced by this process (some can be seen in <ref type="figure" target="#fig_4">Fig. 4</ref>) these typically do not alter the general facial appearances and are less pronounced than the noise often present in unconstrained images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Face recognition pipeline</head><p>Data augmentation techniques are not restricted to training and are often also applied at test time. Our augmentations provide opportunities to modify the matching process by using different augmented versions of the input image. We next describe our recognition pipeline including these and other novel aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN training with our augmented data</head><p>Augmented training data: Our pipeline employs a single CNN trained on both real and augmented data generated as described in Sec . This process raises the total number of images available for training from 494,414 in the original CASIA WebFace set to a total of 2,472,070 images in our complete (pose+shape+expression) augmented dataset. Note that this process leaves the number of CASIA WebFace subjects unchanged, inflating only the number of images per subject <ref type="figure" target="#fig_1">(Fig. 1b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN fine-tuning:</head><p>We use the very deep VGGNet <ref type="bibr" target="#b29">[30]</ref> CNN with 19 layers, trained on the large scale image recognition benchmark (ILSVRC) <ref type="bibr" target="#b25">[26]</ref>. We fine tune this network using our augmented data. To this end, we keep all layers</p><formula xml:id="formula_1">{W k , b k } 19</formula><p>k=1 of VGGNet except for the last linear layer (FC8) which we train from scratch. This layer produces a mapping from the embedded feature x ∈ R D (FC7) to the subject labels N = 10, 575 of the augmented dataset. It computes y = W 19 x + b 19 , where y ∈ R N is the linear response of FC8. Fine-tuning is performed by minimizing the soft-max loss:</p><formula xml:id="formula_2">L({W k , b k }) = − t ln e y l N g=1 e yg<label>(1)</label></formula><p>where l is the ground-truth index over N subjects and t indexes all training images. Eq. (1) is optimized using Stochastic Gradient Descent (SGC) with standard L2 norm over the learned weights. When performing back-propagation, we learn FC8 faster since it is trained from scratch while other network weights are updated with a learning rate an order of magnitude lower than FC8. Specifically, we initialize FC8 with parameters drawn from a Gaussian distribution with zero mean and standard deviation 0.01. Bias is initialized with zero.</p><p>The overall learning rate µ for the entire CNN is set to 0.001, except FC8 which uses learning rate of µ × 10. We decrease learning rate by an order of magnitude once validation accuracy for the fine tuned network saturates. Meanwhile, biases are learned twice as fast as the other weights. For all the other parameter settings we use the same values as originally described in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Face recognition with synthesized faces</head><p>General matching process: After training the CNN, we use the embedded feature vector x = f (I; {W k , b k }) from each image I as a face representation. Given two input images I p and I q , their similarity, s(x p , x q ) is simply the normalized cross correlation (NCC) of their feature vectors.</p><p>The value s(x p , x q ) is the recognition score at the image level. In some cases a subject is represented by multiple images (e.g., a template, as in the Janus benchmark <ref type="bibr" target="#b13">[14]</ref>). This plurality of images can be exploited to improve recognition at test time. In such cases, image sets are defined by P = {x 1 , ..., x P } and Q = {x 1 , ..., x Q } and a similarity score is defined between them: s(P, Q).</p><p>Specifically, we compute the pair-wise image level similarity scores, s(x p , x q ), for all x p ∈ P and x q ∈ Q, and pool these scores using a SoftMax operator, s β (P, Q) (Eq.(2), below). Though the use of SoftMax here is inspired by the SoftMax loss often used by CNNs, its aim is to get a robust score regression instead of a distribution over the subjects. SoftMax for set fusion can be seen as a weighted average in which the weight depends on the score when performing recognition. It is interesting to note here that the SoftMax hyper-parameter β controls the trade-off between averaging the scores or taking the maximum (or minimum). That is: .</p><formula xml:id="formula_3">s β (·, ·) =      max(·) if β → ∞ avg(·) if β = 0 min(·) if β → −∞</formula><p>(2) Pair-wise scores are pooled using Eq. </p><p>The use of positive values for β is motivated by the fact what we are using a score for recognition, so the higher the value, the better. In our experiments we found that the SoftMax operator reaches a remarkable trade-off between averaging the scores and taking the maximum. The improvement given by the proposed SoftMax fusion is shown in Tab. 1: we can see how the proposed method largely outperforms standard fusion techniques on IJB-A, in which subjects are described by templates. Exploiting pose augmentation at test time: The Achilles heel of many face recognition systems is cross pose face matching; particularly when one of the two images is viewed at an extreme, near profile angle <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6]</ref>. Directly matching two images viewed from extremely different viewpoints often leads to poor accuracy as the difference in viewpoints affects the similarity more than subject identities. To mitigate this problem, we suggest rendering both images from the same view: one that is close enough to the viewpoint of both images.</p><p>To this end, we leverage our pose synthesis method of Sec. 3.1 to produce images in poses better suited for recognition and matching. Cross pose rendering can, however, come at a price: Synthesizing novel views for faces runs the risk of producing meaningless images whenever facial landmarks are not accurately localized and the pose estimate is wrong. Even if pose was correctly estimated, warping images across poses involves interpolating intensities, which leads to smoothing artifacts and information loss. Though this may affect training, it is far more serious at test time where we have few images to compare and ruining one or both can directly affect recognition accuracy.</p><p>Rather than commit to pose synthesis or its standard alternative, simple yet robust in-plane alignment, we propose to use both: We found that pose synthesis and in-plane alignment are complimentary and by combining the two alignment techniques recognition performance improves. For an image pair (I p , I q ) we compute two similarity scores. One score is produced using in-plane aligned images and the other using images rendered to a mutually convenient view. This view is determined as follows: If the two images are near frontal then we render them to frontal view (essentially frontalizing them <ref type="bibr" target="#b9">[10]</ref>), if they are both near profile we render to 75 • , otherwise we render both to 40 • .</p><p>When matching templates (image sets), (P, Q), scores computed for in-plane aligned image pairs and pose synthesized pairs are pooled separately using Eq. (2). This is equivalent to comparing the two sets P and Q twice, once for each alignment method. These two similarities are then averaged for the final template level score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We tested our approach extensively on the recently released IARPA Janus benchmarks <ref type="bibr" target="#b13">[14]</ref> and LFW <ref type="bibr" target="#b10">[11]</ref>. We perform a minimum of database specific training, using the training images prescribed by each benchmark protocol. Specifically,   we perform Principal Component Analysis (PCA) on the training images of the target dataset with the features x extracted from the CNN trained on augmented data. This did not include dimensionality reduction; we did not cut any component after PCA projection. Following this, we apply root normalization to the new projected feature, i.e., x → x c , as previously proposed for the Fisher Vector encoding in <ref type="bibr" target="#b26">[27]</ref>. We found that a value of c = 0.65 provides a good baseline across all the experiments. For each dataset we report the contribution of each augmentation technique compared with state-of-the-art methods which use millions of images to train their deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on the IJB-A benchmarks</head><p>IJB-A is a new publicly available benchmark released by NIST 3 to raise the challenges of unconstrained face identification and verification methods. Both IJB-A and the Janus CS2 benchmark share the same subject identities, represented by images viewed in extreme conditions, including pose, expression and illumination variations, with IJB-A splits generally considered more difficult than those in CS2. The IJB-A benchmarks consist of face verification (1:1) and face identification (1:N) tests. Contrary to LFW, Janus subjects are described using templates containing mixtures of still-images and video frames.</p><p>It is important to note that the Janus set has some overlap with the images in the CASIA WebFace collection. In order to provide fair comparisons, our CNNs were fine tuned on CASIA subjects that are not included in Janus (Sec. 4.1). Face detections: Our pipeline uses the facial landmark detector of <ref type="bibr" target="#b1">[2]</ref> for head pose estimation and alignment. Although we found this detector quite robust, it failed to detect landmarks on some of the more challenging Janus faces. Whenever the detector failed on all the images in the same template, we use the images cropped to their facial bounding boxes as provided in the Janus data. Video pooling: We note that whenever face templates include multiple frames from a single video, we pool together CNN features extracted from the same  <ref type="table">Table 2</ref>: Effect of each augmentation on IJB-A performance on verification (ROC) and identification (CMC), resp. Only in-plane aligned images used in these tests.</p><p>video: this, by simple element wise average over all the features extracted from that video's frames. We emphasize that features are not pooled across videos but only within each video. Similar pooling techniques were very recently demonstrated to provide substantial performance enhancements (e.g., <ref type="bibr" target="#b30">[31]</ref>) but, to our knowledge, never for faces or in the manner suggested here. We refer to this technique as video pooling and report its influence on our system, and, whenever possible, for our baselines. In all our IJB-A and Janus CS2 results this method provided noticeable performance boosts: we compare video pooling to pair-wise single image comparisons (referred as without video pooling in our results).  <ref type="table">Table 3</ref>: Effect of in-plane alignment and pose synthesis at test-time (matching) on IJB-A dataset respectively for verification (ROC) and identification (CMC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study:</head><p>We provide a detailed analysis of each augmentation technique on the challenging IJB-A dataset. Clearly, the biggest contribution is given by pose augmentation (red curve) over the baseline (blue curve) in <ref type="figure" target="#fig_10">Fig. 5a</ref>. The improvement is especially noticeable in the rank-1 recognition rate for the identification protocol. The effect of video pooling along with each data augmentation method is provided in Tab. 2. We next evaluate the effect of pose synthesis at test time combined with the standard in-plane alignment (Sec. 4.2), in Tab 3 and in <ref type="figure" target="#fig_10">Fig. 5b</ref>. Evidently, these methods combined contribute to achieving state-of-the-art performance on the IJB-A benchmark. We conjecture that this is mainly due to three contributions: domain-specific augmentation when training the CNN, combination of SoftMax operator, video pooling and finally pose synthesis at test time.  Comparison with the state-of-the-art: Our proposed method achieves state of the art results in the IJB-A benchmark and Janus CS2 dataset. In particular, it largely improves over the off the shelf commercial systems COTS and GOTS <ref type="bibr" target="#b13">[14]</ref> and Fisher Vector encoding using frontalization <ref type="bibr" target="#b4">[5]</ref>. This gap can be explained by the use of deep learning alone. Even compared with deep learning based methods, however, our approach achieves superior performance and with very wide margins. This is true even comparing our results to <ref type="bibr" target="#b36">[37]</ref>, who use seven networks and fuse their output with the COTS system. Moreover, our method improves in IJB-A verification over <ref type="bibr" target="#b36">[37]</ref> in 15% TAR at FAR=0.01 and ∼20% TAR at FAR=0.001, also showing a better rank-1 recognition rate. It is interesting to compare our results to those reported by <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b27">[28]</ref>. Both fine tuned their deep networks on the ten training splits of each benchmark, at substantial computational costs. Some idea of the impact this fine tuning can have on performance is available by considering the huge performance gap between results reported before and after fine tuning in <ref type="bibr" target="#b3">[4]</ref> 4 . Our own results, obtained by training our CNN once on augmented data, far outperform those of <ref type="bibr" target="#b27">[28]</ref> also largely outperforming those reported by <ref type="bibr" target="#b3">[4]</ref>. We conjecture that by training the CNN with augmented data we avoid further specializing all the parameters of the network on the target dataset. Tuning deep models on indomain data is computationally expensive and thus, avoiding overfitting the network at training time is preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Labeled Faces in the Wild</head><p>For many years LFW <ref type="bibr" target="#b10">[11]</ref> was the standard benchmark for unconstrained face verification. Recent methods dominating LFW scores use millions of images collected and labeled by hand in order to obtain their remarkable performances. To test our approach, we follow the standard protocol for unrestricted, labeled outside data and report the mean classification accuracy as well as the 100%  -EER (Equal Error Rate). We prefer to use 100% -EER in general because it is not dependent on the selected classification threshold but we still report verification accuracy to be comparable with the other methods. Improvement for each augmentation: <ref type="figure" target="#fig_11">Fig. 6a</ref> provides ROC curves for each augmentation technique used in our approach. The green curve represents our baseline, that is the CNN trained on in-plane aligned images with respect to a frontal template. The ROC improves by a good margin when we inject unseen rendered images across poses into each subject. Indeed the 100% -EER improves by +1.67%. Moreover, by adding both shapes and expressions, performance improves even more, reaching 100% -EER rate of 98.00% (red curve). See Tab. 6b for a comparison with methods trained on millions of downloaded images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Result summary</head><p>It is not easy to compare our results with those reported by others using millions of training images: Their system designs and implementation details are different from our own and it is difficult to assess how different system components contribute to their overall performance. In particular, the reluctance of commercial groups to release their code and data makes it hard to say exactly how much performance our augmentation buys in comparison to their harvesting and curating millions of face images. Nevertheless, the results throughout this section clearly show that synthesizing training images using domain tools and knowledge leads to dramatic increase in recognition accuracy. This may be attributed to the potential of domain specific augmentation to infuse training data with important intra-subject appearance variations; the very variations that seem hardest to obtain by simply downloading more images. As a bonus, it is a more accessible means of increasing training set sizes than downloading and labeling millions of additional faces.</p><p>Finally, a comparison of our results on LFW to those reported by methods trained on millions of images ( <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b23">24]</ref> and <ref type="bibr" target="#b28">[29]</ref>), shows that with the initial set of less than 500K publicly available images from <ref type="bibr" target="#b43">[44]</ref>, our method surpasses those of <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b23">[24]</ref> (without their metric learning, which was not applied here), falling only slightly behind the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper makes several important contributions. First, we show how domain specific data augmentation can be used to generate (synthesize) valuable additional data to train effective face recognition systems, as an alternative to expensive data collection and labeling. Second, we describe a face recognition pipeline with several novel details. In particular, its use of our data augmentation for matching across poses in a natural manner. Finally, in answer to the question in the title, our extensive analysis shows that though there is certainly a benefit to downloading increasingly larger training sets, much of this effort can be substituted by simply synthesizing more face images.</p><p>There are several compelling directions of extending this work. Primarily, the underlying idea of domain specific data augmentation can be extended in more ways (more facial transformations) to provide additional intra subject appearance variations. Appealing potential augmentation techniques, not used here, are facial age synthesis <ref type="bibr" target="#b12">[13]</ref> or facial hair manipulations <ref type="bibr" target="#b21">[22]</ref>. Finally, beyond faces there may be other domains where such approach is relevant and where the introduction of synthetically generated training data can help mitigate the many problems of data collection for CNN training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Comparison of our augmented dataset with other face datasets along with the average number of images per subject. (b) Our improvement by augmentation (Aug.) in the distribution of per-subject image numbers in order to avoid the long-tail effect of the CASIA set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Adding pose variations by synthesizing novel viewpoints. Left: Original image, detected landmarks, and 3D pose estimation. Right: rendered novel views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Top: The ten generic 3D face shapes used for rendering. Bottom: Faces rendered with the generic appearing right above them. Different shapes induce subtle appearance variations yet do not change the perceived identity of the face in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Expression synthesis examples. Top: Example face images from the CASIA WebFace dataset. Bottom: Synthesized images with closed mouths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. 3. Specifically, training data is produced from the original CASIA WebFace images. It consists of the following types of images: (i) original CASIA images following alignment by a simple, in-plane similarity transform to two coordinate systems: roughly frontal facing faces (face yaw estimates in [−30 • ... 30 • ]) are aligned using nine landmarks on an ideal frontal template, while profile images (all other yaw angles) are aligned using the visible eye and the tip of the nose. (ii) Each image in CASIA is rendered from three novel views in yaw angles {0 • , ±40 • , ±75 • }, as described in Sec. 3.1. (iii) Synthesized views are produced by randomly selecting a 3D generic face model from S as the underlying face shape (see Sec. 3.2), thereby adding shape variations. (iv) Finally, a mouth neutralized version of each image is also added to the training (Sec. 3.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and s β (P, Q) = p∈P,q∈Q s(x p , x q )e β s(xp,xq) p∈P,q∈Q e β s(xp,xq)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 2 )</head><label>2</label><figDesc>and we finally average the SoftMax responses over multiple values of β = [0...20] to get the final similarity score:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 :</head><label>5</label><figDesc>Ablation study of our data synthesis and test time matching methods on IJB-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 :</head><label>6</label><figDesc>LFW verification results. (a) Break-down of the influence of different training data augmentation methods. (b) Performance comparison with state of the art methods, showing the numbers of real (original) and synthesized training images, number of CNNs used by each system, accuracy and 100%-EER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>SoftMax template fusion for score pooling vs. other standard fusion techniques on the IJB-A benchmark for verification (ROC) and identification (CMC) resp.</figDesc><table><row><cell cols="6">Fusion↓ IJB-A Ver. (TAR) IJB-A Id. (Rec. Rate)</cell></row><row><cell cols="6">Metrics → FAR0.01 FAR0.001 Rank-1 Rank-5 Rank-10</cell></row><row><cell>Min</cell><cell>26.3</cell><cell>11.2</cell><cell>33.1</cell><cell>56.1</cell><cell>66.8</cell></row><row><cell>Max</cell><cell>77.6</cell><cell>46.4</cell><cell>84.8</cell><cell>93.3</cell><cell>95.6</cell></row><row><cell>Mean</cell><cell>79.9</cell><cell>53.0</cell><cell>84.6</cell><cell>94.7</cell><cell>96.6</cell></row><row><cell>SoftMax</cell><cell>86.6</cell><cell>63.6</cell><cell cols="2">87.2 94.9</cell><cell>96.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparative performance analysis on JANUS CS2 and IJB-A respectively for verification (ROC) and identification (CMC). f.t. denotes fine tuning a deep network multiple times for each training split. A network trained once with our augmented data achieves mostly superior results, without this effort.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">MegaFace<ref type="bibr" target="#b11">[12]</ref> is larger than CASIA, but was designed as a testing set and so provides few images per subject. It was consequently never used for training CNN systems.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code and data will be publicly available. Please see www.openu.ac.il/home/ hassner/projects/augmented_faces for updates.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">IJB-A data and splits are available under request at http://www.nist.gov/itl/ iad/ig/facechallenges.cfm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The results reported in<ref type="bibr" target="#b3">[4]</ref> with fine tuning on the training sets include system components not evaluated without fine tuning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank Jongmoo Choi for all his help in this project. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon. Moreover, we gratefully acknowledge the support of NVIDIA Corporation with the donation of the NVIDIA Titan X GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using deep multi-pose representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conf. on App. of Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Constrained local neural fields for robust facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conf. on App. of Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using fisher vectors computed from frontalized faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Biometrics: Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="980" to="993" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision. Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<pubPlace>UMass, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The MegaFace benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open source biometric recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klontz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Biometrics: Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neural-network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Practice and Theory of Blendshape Facial Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anjyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic elastic matching for pose variant face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3499" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-augmentation for reducing dataset bias in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image-based shaving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A compact and discriminative face track descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance, 2009. AVSS &apos;09. Sixth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Triplet similarity embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03418</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">Deepid3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective unconstrained face recognition by combining multiple descriptors and learned background statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1978" to="1990" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hyper-class augmented and regularized deep learning for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2645" to="2654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Augmenting strong supervision using web data for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2524" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mirror, mirror on the wall, tell me, is the error small?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards pose robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3539" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<ptr target="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html.2,3" />
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<title level="m">Naive-deep face recognition: Touching the limit of LFW benchmark or not? arXiv preprint</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
