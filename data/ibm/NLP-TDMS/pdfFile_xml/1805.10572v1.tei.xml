<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BRITS: Bidirectional Recurrent Imputation for Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Toutiao AILab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Toutiao AILab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Toutiao AILab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Toutiao AILab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BRITS: Bidirectional Recurrent Imputation for Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data. We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multivariate time series data are abundant in many application areas, such as financial marketing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>, health-care <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>, meteorology <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>, and traffic engineering <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. Time series are widely used as signals for classification/regression in various applications of corresponding areas. However, missing values in time series are very common, due to unexpected accidents, such as equipment damage or communication error, and may significantly harm the performance of downstream applications.</p><p>Much prior work proposed to fix the missing data problem with statistics and machine learning approaches. However most of them require fairly strong assumptions on missing values. We can fill the missing values using classical statistical time series models such as ARMA or ARIMA (e.g., <ref type="bibr" target="#b0">[1]</ref>). But these models are essentially linear (after differencing). Kreindler et al. <ref type="bibr" target="#b18">[19]</ref> assume that the data are smoothable, i.e., there is no sudden wave in the periods of missing values, hence imputing missing values can be done by smoothing over nearby values. Matrix completion has also been used to address missing value problems (e.g., <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>). But it typically applies to only static data and requires strong assumptions such as low-rankness. We can also predict missing values by fitting a parametric data-generating model with the observations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>, which assumes that data of time series follow the distribution of hypothetical models. These assumptions make corresponding imputation algorithms less general, and the performance less desirable when the assumptions do not hold.</p><p>In this paper, we propose BRITS, a novel method for filling the missing values for multiple correlated time series. Internally, BRITS adapts recurrent neural networks (RNN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11]</ref> for imputing missing values, without any specific assumption over the data. Much prior work uses non-linear dynamical systems for time series prediction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>. In our method, we instantiate the dynamical system as a bidirectional RNN, i.e., imputing missing values with bidirectional recurrent dynamics. In particular, we make the following technical contributions:</p><p>• We design a bidirectional RNN model for imputing missing values. We directly use RNN for predicting missing values, instead of tuning weights for smoothing as in <ref type="bibr" target="#b9">[10]</ref>. Our method does not impose specific assumption, hence works more generally than previous methods.</p><p>• We regard missing values as variables of the bidirectional RNN graph, which are involved in the backpropagation process. In such case, missing values get delayed gradients in both forward and backward directions with consistency constraints, which makes the estimation of missing values more accurate.</p><p>• We simultaneously perform missing value imputation and classification/regression of applications jointly in one neural graph. This alleviates the error propagation problem from imputation to classification/regression. Additionally, the supervision of classification/regression makes the estimation of missing values more accurate.</p><p>• We evaluate our model on three real-world datasets, including an air quality dataset, a health-care dataset and a localization dataset of human activities. Experimental results show that our model outperforms the state-of-the-art models for both imputation and classification/regression accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large body of literature on the imputation of missing values in time series. We only mention a few closely related ones. The interpolate method tries to fit a "smooth curve" to the observations and thus reconstruct the missing values by the local interpolations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref>. Such method discards any relationships between the variables over time. The autoregressive method, including ARIMA, SARIMA etc., eliminates the non-stationary parts in the time series data and fit a parameterized stationary model. The state space model further combines ARIMA and Kalman Filter <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which provides more accurate results. Multivariate Imputation by Chained Equations (MICE) <ref type="bibr" target="#b1">[2]</ref> first initializes the missing values arbitrarily and then estimates each missing variable based on the chain equations. The graphical model <ref type="bibr" target="#b19">[20]</ref> introduces a latent variable for each missing value, and finds the latent variables by learning their transition matrix. There are also some data-driven methods for missing value imputation. Yi et al. <ref type="bibr" target="#b31">[32]</ref> imputed the missing values in air quality data with geographical features. Wang et al. <ref type="bibr" target="#b29">[30]</ref> imputed missing values in recommendation system with collaborative filtering. Yu et al. <ref type="bibr" target="#b33">[34]</ref> utilized matrix factorization with temporal regularization to impute the missing values in regularly sampled time series data.</p><p>Recently, some researchers attempted to impute the missing values with recurrent neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref>. The recurrent components are trained together with the classification/regression component, which significantly boosts the accuracy. Che et al. <ref type="bibr" target="#b9">[10]</ref> proposed GRU-D, which imputes missing values in health-care data with a smooth fashion. It assumes that a missing variable can be represented as the combination of its corresponding last observed value and the global mean. GRU-D achieves remarkable performance on health-care data. However, it has many limitations on general datasets <ref type="bibr" target="#b9">[10]</ref>. A closely related work is M-RNN proposed by Yoon et al. <ref type="bibr" target="#b32">[33]</ref>. M-RNN also utilizes bi-directional RNN to impute missing values. Differing from our model, it drops the relationships between missing variables. The imputed values in M-RNN are treated as constants and cannot be sufficiently updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>We first present the problem formulation and some necessary preliminaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Multivariate Time Series)</head><p>We denote a multivariate time series X = {x 1 , x 2 , . . . , x T } as a sequence of T observations. The t-th observation x t ∈ R D consists of D features {x 1 t , x 2 t , . . . , x D t }, and was observed at timestamp s t (the time gap between different timestamps may be not same). In reality, due to unexpected accidents, such as equipment damage or communication error, x t may have the missing values (e.g., in <ref type="figure" target="#fig_0">Fig. 1</ref>, x <ref type="bibr" target="#b2">3</ref> 1 in x 1 is missing). To represent the missing values in x t , we introduce a masking vector m t where,</p><formula xml:id="formula_0">m d t = 0 if x d t is not observed 1 otherwise .</formula><p>In many cases, some features can be missing for consecutive timestamps (e.g., blue blocks in <ref type="figure" target="#fig_0">Fig. 1</ref>). We define δ d t to be the time gap from the last observation to the current timestamp s t , i.e.,</p><formula xml:id="formula_1">δ d t =    s t − s t−1 + δ d t−1 if t &gt; 1, m d t−1 = 0 s t − s t−1 if t &gt; 1, m d t−1 = 1 0 if t = 1</formula><p>. See <ref type="figure" target="#fig_0">Fig. 1</ref> for an illustration. In this paper, we study a general setting for time series classification/regression problems with missing values. We use y to denote the label of corresponding classification/regression task. In general, y can be either a scalar or a vector. Our goal is to predict y based on the given time series X. In the meantime, we impute the missing values in X as accurate as possible. In another word, we aim to design an effective multi-task learning algorithm for both classification/regression and imputation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BRITS</head><p>In this section, we describe the BRITS. Differing from the prior work which uses RNN to impute missing values in a smooth fashion <ref type="bibr" target="#b9">[10]</ref>, we learn the missing values directly in a recurrent dynamical system <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> based on the observed data. The missing values are thus imputed according to the recurrent dynamics, which significantly boosts both the imputation accuracy and the final classification/regression accuracy. We start the description with the simplest case: the variables observed at the same time are mutually uncorrelated. For such case, we propose algorithms for imputation with unidirectional recurrent dynamics and bidirectional recurrent dynamics, respectively. We further propose an algorithm for correlated multivariate time series in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unidirectional Uncorrelated Recurrent Imputation</head><p>For the simplest case, we assume that for the t-th step, x i t and x j t are uncorrelated if i = j (but x i t may be correlated with some x j t =t ). We first propose an imputation algorithm by unidirectional recurrent dynamics, denoted as RITS-I.</p><p>In a unidirectional recurrent dynamical system, each value in the time series can be derived by its predecessors with a fixed arbitrary function <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>. Thus, we iteratively impute all the variables in the time series according to the recurrent dynamics. For the t-th step, if x t is actually observed, we use it to validate our imputation and pass x t to the next recurrent steps. Otherwise, since the future observations are correlated with the current value, we replace x t with the obtained imputation, and validate it by the future observations. To be more concrete, let us consider an example.</p><p>Example 1 Suppose we are given a time series X = {x 1 , x 2 , . . . , x 10 }, where x 5 , x 6 and x 7 are missing. According to the recurrent dynamics, at each time step t, we can obtain an estimationx t based on the previous t − 1 steps. In the first 4 steps, the estimation error can be obtained immediately by calculating the estimation loss function L e (x t , x t ) for t = 1, . . . , 4. However, when t = 5, 6, 7, we cannot calculate the error immediately since the true values are missing. Nevertheless, note that at the 8-th step,x 8 depends onx 5 tox 7 . We thus obtain a "delayed" error forx t=5,6,7 at the 8-th step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Algorithm</head><p>We introduce a recurrent component and a regression component for imputation. The recurrent component is achieved by a recurrent neural network and the regression component is achieved by a fully-connected network. A standard recurrent network <ref type="bibr" target="#b16">[17]</ref> can be represented as</p><formula xml:id="formula_2">h t = σ(W h h t−1 + U h x t + b h ),</formula><p>where σ is the sigmoid function, W h , U h and b h are parameters, and h t is the hidden state of previous time steps.</p><p>In our case, since x t may have missing values, we cannot use x t as the input directly as in the above equation. Instead, we use a "complement" input x c t derived by our algorithm when x t is missing. Formally, we initialize the initial hidden state h 0 as an all-zero vector and then update the model by:</p><formula xml:id="formula_3">x t = W x h t−1 + b x , (1) x c t = m t x t + (1 − m t ) x t , (2) γ t = exp{− max(0, W γ δ t + b γ )}, (3) h t = σ(W h [h t−1 γ t ] + U h [x c t • m t ] + b h ),<label>(4)</label></formula><p>Eq. <ref type="formula">(1)</ref> is the regression component which transfers the hidden state h t−1 to the estimated vector x t . In Eq. (2), we replace missing values in x t with corresponding values inx t , and obtain the complement vector x c t . Besides, since the time series may be irregularly sampled, in Eq. <ref type="formula">(3)</ref>, we further introduce a temporal decay factor γ t to decay the hidden vector h t−1 . Intuitively, if δ t is large (i.e., the values are missing for a long time), we expect a small γ t to decay the hidden state. Such factor also represents the missing patterns in the time series which is critical to imputation <ref type="bibr" target="#b9">[10]</ref>. In Eq. (4), based on the decayed hidden state, we predict the next state h t . Here, • indicates the concatenate operation. In the mean time, we calculate the estimation error by the estimation loss function L e in Eq. <ref type="bibr" target="#b4">(5)</ref>. In our experiment, we use the mean absolute error for L e . Finally, we predict the task label y asŷ</p><formula xml:id="formula_4">= f out ( T i=1 α i h i ),</formula><p>where f out can be either a fully-connected layer or a softmax layer depending on the specific task, and α i is the weight for different hidden state which can be derived by the attention mechanism 1 or the mean pooling mechanism, i.e., α i = 1 T . We calculate the output loss by L out (y,ŷ). Our model is then updated by minimizing the accumulated loss 1 T T t=1 t + L out (y,ŷ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Practical Issues</head><p>In practice, we use LSTM as the recurrent component in Eq. (4) to prevent the gradient vanishing/exploding problems in vanilla RNN <ref type="bibr" target="#b16">[17]</ref>. Standard RNN models including LSTM treatx t as a constant. During backpropagation, gradients are cut atx t . This makes the estimation errors backpropagate insufficiently. For example, in Example 1, the estimation errors ofx 5 tox 7 are obtained at the 8-th step as the delayed errors. However, if we treatx 5 tox 7 as constants, such delayed error cannot be fully backpropagated. To overcome such issue, we treatx t as a variable of RNN graph. We let the estimation error passes throughx t during the backpropagation. <ref type="figure" target="#fig_1">Fig. 2</ref> shows how RITS-I method works in Example 1. In this example, the gradients are backpropagated through the opposite direction of solid lines. Thus, the delayed error 8 is passed to steps 5, 6 and 7. In the experiment, we find that our models are unstable during training if we treatx t as a constant. See Appendix C for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bidirectional Uncorrelated Recurrent Imputation</head><p>In the RITS-I, errors of estimated missing values are delayed until the presence of the next observation. For example, in Example 1, the error ofx 5 is delayed until x 8 is observed. Such error delay makes the model converge slowly and in turn leads to inefficiency in training. In the meantime, it also leads to the bias exploding problem <ref type="bibr" target="#b5">[6]</ref>, i.e., the mistakes made early in the sequential prediction are fed as input to the model and may be quickly amplified. In this section, we propose an improved version called BRITS-I. The algorithm alleviates the above-mentioned issues by utilizing the bidirectional recurrent dynamics on the given time series, i.e., besides the forward direction, each value in time series can be also derived from the backward direction by another fixed arbitrary function.</p><p>To illustrate the intuition of BRITS-I algorithm, again, we consider Example 1. Consider the backward direction of the time series. In bidirectional recurrent dynamics, the estimationx 4 reversely depends onx 5 tox 7 . Thus, the error in the 5-th step is back-propagated from not only the 8-th step in the forward direction (which is far from the current position), but also the 4-th step in the backward direction (which is closer). Formally, the BRITS-I algorithm performs the RITS-I as shown in Eq. (1) to Eq. (5) in forward and backward directions, respectively. In the forward direction, we obtain the estimation sequence {x 1 ,x 2 , . . . ,x T } and the loss sequence { 1 , 2 , . . . , T }. Similarly, in the backward direction, we obtain another estimation sequence {x 1 ,x 2 , . . . ,x T } and another loss sequence { 1 , 2 , . . . , T }. We enforce the prediction in each step to be consistent in both directions by introducing the "consistency loss":</p><formula xml:id="formula_5">cons t = Discrepancy(x t ,x t )<label>(6)</label></formula><p>where we use the mean squared error as the discrepancy in our experiment. The final estimation loss is obtained by accumulating the forward loss t , the backward loss t and the consistency loss cons t . The final estimation in the t-th step is the mean ofx t andx t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Correlated Recurrent Imputation</head><p>The previously proposed algorithms RITS-I and BRITS-I assume that features observed at the same time are mutually uncorrelated. This may be not true in some applications. For example, in the air quality data <ref type="bibr" target="#b31">[32]</ref>, each feature represents one measurement in a monitoring station. Obviously, the observed measurements are spatially correlated. In general, the measurement in one monitoring station is close to those values observed in its neighboring stations. In this case, we can estimate a missing measurement according to both its historical data, and the measurements in its neighbors.</p><p>In this section, we propose an algorithm, which utilizes the feature correlations in the unidirectional recurrent dynamical system. We refer to such algorithm as RITS. The feature correlated algorithm for bidirectional case (i.e., BRITS) can be derived in the same way. Note that in Section 4.1, the estimationx t is only correlated with the historical observations, but irrelevant with the the current observation. We refer tox t as a "history-based estimation". In this section, we derive another "feature-based estimation" for each x d t , based on the other features at time s t . Specifically, at the t-th step, we first obtain the complement observation x c t by Eq. (1) and Eq. (2). Then, we define our feature-based estimation asẑ t whereẑ</p><formula xml:id="formula_6">t = W z x c t + b z ,<label>(7)</label></formula><p>W z , b z are corresponding parameters. We restrict the diagonal of parameter matrix W z to be all zeros. Thus, the d-th element inẑ t is exactly the estimation of x d t , based on the other features. We further combine the historical-based estimationx t and the feature-based estimationẑ t . We denote the combined vector asĉ t , and we have that</p><formula xml:id="formula_7">β t = σ(W β [γ t • m t ] + b β ) (8) c t = β t ẑ t + (1 − β t ) x t .</formula><p>(9) Here we use β t ∈ [0, 1] D as the weight of combining the history-based estimationx t and the feature-based estimationẑ t . Note thatẑ t is derived from x c t by Eq. <ref type="bibr" target="#b6">(7)</ref>. The elements of x c t can be history-based estimations or truly observed values, depending on the presence of the observations. Thus, we learn the weight β t by considering both the temporal decay γ t and the masking vector m t as shown in Eq. <ref type="bibr" target="#b7">(8)</ref>. The rest parts are similar as the feature uncorrelated case. We first replace missing values in x t with the corresponding values inĉ t . The obtained vector is then fed to the next recurrent step to predict memory h t :</p><formula xml:id="formula_8">c c t = m t x t + (1 − m t ) ĉ t (10) h t = σ(W h [h t−1 γ t ] + U h [c c t • m t ] + b h ).<label>(11)</label></formula><p>However, the estimation loss is slightly different with the feature uncorrelated case. We find that simply using t = L e (x t ,ĉ t ) leads to a very slow convergence speed. Instead, we accumulate all the estimation errors ofx t ,ẑ t andĉ t :</p><formula xml:id="formula_9">t = L e (x t ,x t ) + L e (x t ,ẑ t ) + L e (x t ,ĉ t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>Our proposed methods are applicable to a wide variety of applications. We evaluate our methods on three different real-world datasets. The download links of the datasets, as well as the implementation codes can be found in the GitHub page 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Air Quality Data</head><p>We evaluate our models on the air quality dataset, which consists of PM2.5 measurements from 36 monitoring stations in Beijing. The measurements are hourly collected from 2014/05/01 to 2015/04/30. Overall, there are 13.3% values are missing. For this dataset, we do pure imputation task. We use the exactly same train/test setting as in prior work <ref type="bibr" target="#b31">[32]</ref>, i.e., we use the 3 rd , 6 th , 9 th and 12 th months as the test data and the other months as the training data. See Appendix D for details. To train our model, we randomly select every 36 consecutive steps as one time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Health-care Data</head><p>We evaluate our models on health-care data in PhysioNet Challenge 2012 <ref type="bibr" target="#b26">[27]</ref>, which consists of 4000 multivariate clinical time series from intensive care unit (ICU). Each time series contains 35 measurements such as Albumin, heart-rate etc., which are irregularly sampled at the first 48 hours after the patient's admission to ICU. We stress that this dataset is extremely sparse. There are up to 78% missing values in total. For this dataset, we do both the imputation task and the classification task. To evaluate the imputation performance, we randomly eliminate 10% of observed measurements from data and use them as the ground-truth. At the same time, we predict the in-hospital death of each patient as a binary classification task. Note that the eliminated measurements are only used for validating the imputation, and they are never visible to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Localization for Human Activity Data</head><p>The UCI localization data for human activity <ref type="bibr" target="#b17">[18]</ref> contains records of five people performing different activities such as walking, falling, sitting down etc (there are 11 activities). Each person wore four sensors on her/his left/right ankle, chest, and belt. Each sensor recorded a 3-dimensional coordinates for about 20 to 40 millisecond. We randomly select 40 consecutive steps as one time series, and there are 30, 917 time series in total. For this dataset, we do both imputation and classification tasks. Similarly, we randomly eliminate 10% observed data as the imputation ground-truth. We further predict the corresponding activity of observed time series (i.e., walking, sitting, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Model Implementations</head><p>We fix the dimension of hidden state h t in RNN to be 64. We train our model by an Adam optimizer with learning rate 0.001 and batch size 64. For all the tasks, we normalize the numerical values to have zero mean and unit variance for stable training.</p><p>We use different early stopping strategies for pure imputation task and classification tasks. For the imputation tasks, we randomly select 10% of non-missing values as the validation data. The early stopping is thus performed based on the validation error. For the classification tasks, we first pre-train the model as an imputation task. Then we use 5-fold cross validation to further optimize both the imputation and classification losses simultaneously.</p><p>We evaluate the imputation performance in terms of mean absolute error (MAE) and mean relative error (MRE). Suppose that label i is the ground-truth of the i-th item, pred i is the output of the i-th item, and there are N items in total. Then, MAE and MRE are defined as</p><formula xml:id="formula_10">MAE = i |pred i − label i | N , MRE = i |pred i − label i | i |label i | .</formula><p>For air quality data, the evaluation is performed on its original data. For heath-care data and activity data, since the numerical values are not in the same scale, we evaluate the performances on their normalized data. To further evaluate the classification performances, we use area under ROC curve (AUC) <ref type="bibr" target="#b7">[8]</ref> for health-care data, since such data is highly unbalanced (there are 10% patients who died in hospital). We then use standard accuracy for the activity data, since different activities are relatively balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Baseline Methods</head><p>We compare our model with both RNN-based methods and non-RNN based methods. The non-RNN based imputation methods include:</p><p>• Mean: We simply replace the missing values with corresponding global mean.</p><p>• KNN: We use k-nearest neighbor <ref type="bibr" target="#b12">[13]</ref> to find the similar samples, and impute the missing values with weighted average of its neighbors. • Matrix Factorization (MF): We factorize the data matrix into two low-rank matrices, and fill the missing values by matrix completion <ref type="bibr" target="#b12">[13]</ref>. • MICE: We use Multiple Imputation by Chained Equations (MICE), a widely used imputation method, to fill the missing values. MICE creates multiple imputations with chained equations <ref type="bibr" target="#b1">[2]</ref>. • ImputeTS: We use ImputeTS package in R to impute the missing values. ImputeTS is a widely used package for missing value imputation, which utilizes the state space model and kalman smoothing <ref type="bibr" target="#b22">[23]</ref>. • STMVL: Specifically, we use STMVL for the air quality data imputation. STMVL is the state-of-the-art method for air quality data imputation. It further utilizes the geo-locations when imputing missing values <ref type="bibr" target="#b31">[32]</ref>.</p><p>We implement KNN, MF and MICE based on the python package fancyimpute <ref type="bibr" target="#b2">3</ref> . In recent studies, RNN-based models in missing value imputations achieve remarkable performances <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>We also compare our model with existing RNN-based imputation methods, including:</p><p>• GRU-D: GRU-D is proposed for handling missing values in health-care data. It imputes each missing value by the weighted combination of its last observation, and the global mean, together with a recurrent component <ref type="bibr" target="#b9">[10]</ref>.  . We implement all the RNN-based models with PyTorch, a widely used package for deep learning. All models are trained with GPU GTX 1080. <ref type="table" target="#tab_0">Table 1</ref> shows the imputation results. As we can see, simply applying naıve mean imputation is very inaccurate. Alternatively, KNN, MF, and MICE perform much better than mean imputation. ImputeTS achieves the best performance among all the non-RNN methods, especially for the heathcare data (which is smooth and contains few sudden waves). STMVL performs well on the air quality data. However, it is specifically designed for geographical data, and cannot be applied in the other datasets. Most of RNN-based methods, except GRU-D, demonstrates significantly better performances in imputation tasks. We stress that GRU-D does not impute missing value explicitly. It actually performs well on classification tasks (See Appendix A for details). M-RNN uses explicitly imputation procedure, and achieves remarkable imputation results. Our model BRITS outperforms all the baseline models significantly. According to the performances of our four models, we also find that both bidirectional recurrent dynamics, and the feature correlations are helpful to enhance the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment Results</head><p>We also compare the accuracies of all RNN-based models in classification tasks. GRU-D achieves an AUC of 0.828 ± 0.004 on health-care data , and accuracy of 0.939 ± 0.010 on human activity; M-RNN is slightly worse. It achieves 0.800 ± 0.003 and 0.938 ± 0.010 on two tasks. Our model BRITS outperforms the baseline methods. The accuracies of BRITS are 0.850 ± 0.002 and 0.969 ± 0.008 respectively. See Appendix A for more classification results.</p><p>Due to the space limitation, we provide more experimental results in Appendix A and B for better understanding our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed BRITS, a novel method to use recurrent dynamics to effectively impute the missing values in multivariate time series. Instead of imposing assumptions over the datagenerating process, our model directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. Our model treats missing values as variables of the bidirectional RNN graph. Thus, we get the delayed gradients for missing values in both forward and backward directions, which makes the imputation of missing values more accurate. We performed the missing value imputation and classification/regression simultaneously within a joint neural network. Experiment results show that our model demonstrates more accurate results for both imputation and classification/regression than state-of-the-art methods. <ref type="table" target="#tab_1">Table 2</ref> shows the performances of different RNN-based models on the classification tasks. As we can see, our model BRITS outperforms other baseline methods for classifications. Comparing with <ref type="table" target="#tab_0">Table  1</ref>, GRU-D performs well for classification tasks. Furthermore, we find that it is very important for GRU-D to carefully apply the dropout techniques in order to prevent the overfitting (we use p = 0.25 dropout layer on the top classification layer). However, our models further utilize the imputation errors as the supervised signals. It seems that dropout is not necessary for our models during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Performance Comparison for Classification Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Performance Comparison for Univariate Synthetic Data</head><p>To better understand our model, we generate a set of univariate synthetic time series. Speficically, we randomly generate a time series with length T = 36, using the state-space representation <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_11">x t = µ t + θ t + t , µ t = µ t−1 + λ t−1 + ξ t , λ t = λ t−1 + ζ t , θ t = s−1 j=1 −θ t−j + ω t ,</formula><p>where x t is the t-th value in time series. The residual terms t , ξ t , ζ t and ω t are randomly drawn from a normal distribution N(0, 0.3). We eliminate about 22% values from the generated series, and compare our model BRITS-I (note the data is univariate) and ImputeTS. We show three examples in <ref type="figure" target="#fig_3">Fig. 3</ref>. The first row corresponds to the imputations of ImputeTS and the second row corresponds to our model BRITS-I. As we can see, our model demonstrates better performance than ImputeTS. Especially, ImputeTS fails when the start part of time series is missing. However, for our model, the imputation errors are backpropagated to previous time steps. Thus, our model can adjust the imputations in the start part with delayed gradient, which leads to much more accurate results.  As we claimed in Section 4.1, we regard the missing values as variables of RNN graph. During the backpropagation, the imputed values can be further updated sufficiently. In the experiment, we find that if we cut the gradient ofx t (i.e., regard it as constant), the models are unstable and easy to overfit during the training. We refer to the model with non-differentiatex t as BRITS-cut. <ref type="figure" target="#fig_5">Fig. 4</ref> shows the validation errors of BRITS and BRITS-cut during training for the health-care data imputation. At the first 20 iterations, the validation error of BRITS-cut decreases fast. However, it soon fails due to the overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Test Data of Air Quality Imputation</head><p>We use the same method as in prior work <ref type="bibr" target="#b31">[32]</ref> to select the test data for air quality imputation. Specifically, we use the 3 rd , 6 th , 9 th and 12 th months as the test set and the rest data as the training set. To evaluate our model, we select the test timeslots by the following rule: if a measurement is observed at a timeslot in one of these four months (e.g., 8 o'clock 2015/03/07), we check the corresponding position in its previous month (e.g., 8 o'clock 2015/02/07). If it is absent in the previous month, we eliminate such value and use it as the imputation ground-truth of test data. Recall that to train our model, we randomly select consecutive 36 time steps as one time series. Thus, a test timeslot may be contained in multiple time series. We use the mean of the imputations in different time series as the final result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of multivariate time series with missing values. x 1 to x 6 are observed at s 1...6 = 0, 2, 7, 9, 14, 15 respectively. Considering the 2nd feature in x 6 , the last observation of the 2nd feature took place at s 2 = 2, and we have that δ 2 6 = s 6 − s 2 = 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Imputation with unidirectional dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>• M-RNN: M-RNN also uses bi-directional RNN. It imputes the missing values according to hidden states in both directions in RNN. M-RNN treats the imputed values as constants. It does not consider the correlations among different missing values<ref type="bibr" target="#b32">[33]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Example for synthetic time series imputation. Each column corresponds to one time series. The figures in the first row are imputations of ImputeTS algorithm, and the figures in the second row are imputations by BRITS-I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Validation errors for BRITS and BRITS-cut during training C Performance for Non-differentiablex t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance Comparison for Imputation Tasks (in MAE(MRE%))</figDesc><table><row><cell cols="2">Method</cell><cell>Air Quality</cell><cell>Health-care</cell><cell>Human Activity</cell></row><row><cell></cell><cell>Mean</cell><cell>55.51 (77.97%)</cell><cell>0.720 (100.00%)</cell><cell>0.767 (96.43%)</cell></row><row><cell></cell><cell>KNN</cell><cell>29.79 (41.85%)</cell><cell>0.732 (101.66%)</cell><cell>0.479 (58.54%)</cell></row><row><cell>Non-RNN</cell><cell>MF MICE</cell><cell>27.94 (39.25%) 27.42 (38.52%)</cell><cell>0.622 (87.68%) 0.634 (89.17%)</cell><cell>0.879 (110.44%) 0.477 (57.94%)</cell></row><row><cell></cell><cell cols="2">ImputeTS 19.58 (27.51%)</cell><cell>0.390 (54.2%)</cell><cell>0.363 (45.65%)</cell></row><row><cell></cell><cell>STMVL</cell><cell>12.12 (17.40%)</cell><cell>/</cell><cell>/</cell></row><row><cell>RNN</cell><cell>GRU-D M-RNN</cell><cell>/ 14.24 (20.43%)</cell><cell>0.559 (77.58%) 0.451 (62.65%)</cell><cell>0.558 (70.05%) 0.248 (31.19%)</cell></row><row><cell></cell><cell>RITS-I</cell><cell>12.73 (18.32%)</cell><cell>0.395 (54.80%)</cell><cell>0.240 (30.10%)</cell></row><row><cell>Ours</cell><cell>BRITS-I RITS</cell><cell>11.58 (16.66%) 12.19 (17.54%)</cell><cell>0.361 (50.01%) 0.300 (41.89%)</cell><cell>0.220 (27.61%) 0.248 (31.21%)</cell></row><row><cell></cell><cell>BRITS</cell><cell cols="3">11.56 (16.65%) 0.281 (39.14%) 0.219 (27.59%)</cell></row><row><cell cols="5">We compare the baseline methods with our four models: RITS-I (Section 4.1), RITS (Section 4.2),</cell></row><row><cell cols="3">BRITS-I (Section 4.3) and BRITS (Section 4.3)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance Comparison for Classification Tasks</figDesc><table><row><cell cols="3">Method Health-care (AUC) Human Activity (Accuracy)</cell></row><row><cell>GRU-D</cell><cell>0.828 ± 0.004</cell><cell>0.939 ± 0.010</cell></row><row><cell>M-RNN</cell><cell>0.800 ± 0.003</cell><cell>0.938 ± 0.010</cell></row><row><cell>RITS-I</cell><cell>0.821 ± 0.007</cell><cell>0.934 ± 0.008</cell></row><row><cell>BRITS-I</cell><cell>0.831 ± 0.003</cell><cell>0.940 ± 0.012</cell></row><row><cell>RITS</cell><cell>0.840 ± 0.004</cell><cell>0.968 ± 0.010</cell></row><row><cell>BRITS</cell><cell>0.850 ± 0.002</cell><cell>0.969 ± 0.008</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = m t , L e (x t ,x t ) .(5)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The design of attention mechanism is out of this paper's scope.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/NIPS-BRITS/BRITS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/iskandr/fancyimpute</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the estimation of arima models with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Ansley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time series analysis of irregularly observed data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984" />
			<biblScope unit="page" from="9" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple imputation by chained equations: what is it and how does it work? International journal of methods in psychiatric research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Azur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frangakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Leaf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Time series prediction by chaotic modeling of nonlinear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1941" to="1948" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning for multivariate financial time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Batres-Estrada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The arrow of time in multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2043" to="2051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kärkkäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The use of the area under the roc curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training energy-based models for time-series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stroobandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2771" to="2797" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Doctor ai: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer series in statistics Springer</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Methods for the estimation of missing values in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Fung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Forecasting, structural time series models and the Kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Harvey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1735</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An agent-based approach to care in independent living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kaluža</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mirchevska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dovgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luštrek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on ambient intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The effects of the irregular sample and missing data in time series analysis. Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using Real Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kreindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lumsden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynammo: Mining and summarization of coevolving sequences with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Directly modeling missing data in sequences with rnns: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="253" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning linear dynamical systems from multivariate time series: A matrix factorization based framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
		<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="810" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bartz-Beielstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">imputeTS: Time Series Missing Value Imputation in R</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2 non-linear time series models and dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="25" to="83" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recent techniques of clustering of time series data: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sikka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in Cardiology (CinC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepsd: supply-demand prediction for online car-hailing services using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 33rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
	<note>In Data Engineering (ICDE</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unifying user-based and item-based collaborative filtering approaches by similarity fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">St-mvl: filling missing values in geo-sensory time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-directional recurrent neural networks: A novel method for estimating missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for highdimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
