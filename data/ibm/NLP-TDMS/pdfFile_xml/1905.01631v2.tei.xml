<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Generative Neural System for Probabilistic Trajectory Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengbo</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
						</author>
						<title level="a" type="main">Conditional Generative Neural System for Probabilistic Trajectory Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are critical for intelligent systems such as autonomous vehicles and wheeled mobile robotics navigating in complex scenarios to achieve safe and high-quality decision making, motion planning and control. Due to the uncertain nature of the future, it is desired to make inference from a probability perspective instead of deterministic prediction. In this paper, we propose a conditional generative neural system (CGNS) for probabilistic trajectory prediction to approximate the data distribution, with which realistic, feasible and diverse future trajectory hypotheses can be sampled. The system combines the strengths of conditional latent space learning and variational divergence minimization, and leverages both static context and interaction information with soft attention mechanisms. We also propose a regularization method for incorporating soft constraints into deep neural networks with differentiable barrier functions, which can regulate and push the generated samples into the feasible regions. The proposed system is evaluated on several public benchmark datasets for pedestrian trajectory prediction and a roundabout naturalistic driving dataset collected by ourselves. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>It is desired for a multi-agent prediction system to satisfy the following requirements to generate diverse, realistic future trajectories. 1) Context-aware: The system should be able to forecast trajectories which are inside the traversable regions and collision-free with static obstacles in the environment. For instance, when the vehicles navigate in a roundabout (see <ref type="figure" target="#fig_0">Fig. 1</ref>(a)) they need to advance along the curves and avoid collisions with road boundaries. 2) Interaction-aware: The system needs to generate reasonable trajectories compliant to traffic or social rules, which takes into account interactions and reactions among multiple entities. For instance, when the vehicles approach an unsignalized intersection (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), they need to anticipate others' possible intentions and motions as well as the influences of their own behaviors on surrounding entities. 3) Feasibility-aware: The system should anticipate naturalistic and physically-feasible trajectories which are compliant to vehicle kinematics or dynamics constraints, although these constraints can be ignored for pedestrians due to the large flexibility of their motions. 4) Probabilistic prediction: Since the future is full of uncertainty, the system should be able to learn an approximated distribution of future trajectories J. <ref type="bibr">Li</ref> In this work, we propose a generative neural system that satisfies all the aforementioned requirements for predicting trajectories in highly interactive scenarios. The system takes advantage of both explicit and implicit density learning in a unified generative system to predict the distributions of trajectories for multiple interactive agents, from which the sampled hypotheses are not only reasonable and feasible but also cover diverse possible motion patterns.</p><p>The main contributions of this paper are as follows:</p><p>• A Conditional Generative Neural System (CGNS) is proposed to jointly predict future trajectories of multiple highly-interactive agents, which takes into account the static context information, interactions among multiple entities and feasibility constraints. • A block attention mechanism and a Gaussian mixture attention mask are proposed and applied to historical trajectories and scene image sequences respectively, which are computationally efficient. • An effective strategy for soft constraint incorporation into deep neural networks is presented. • The latent space learning and variational divergence minimization approaches are integrated into a unified framework in a novel fashion, which combines their strengths on distribution learning. • The proposed CGNS is validated on multiple pedestrian trajectory forecasting benchmarks and is used to solve a task of anticipating motions of on-road vehicles navigating in highly-interactive scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we provide a brief overview on related research and illustrate the distinction and advantages of the proposed generative system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trajectory and Sequence Prediction</head><p>Many research efforts have been devoted to predict behaviors and trajectories of pedestrians and on-road vehicles. Many classical approaches were employed to make time-series prediction, such as variants of Kalman filter based on system process models, time-series analysis and auto-regressive models. However, such methods only suffice for short-term prediction in simple scenarios where interactions among entities can be ignored. More advanced learning-based models have been proposed to cope with more complicated scenarios, such as hidden Markov models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, Gaussian mixture regression <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, Gaussian process, dynamic Bayesian networks, and rapidly-exploring random tree. However, these approaches are nontrivial to handle high-dimensional data and require hand-designed input features, which confines the flexibility of representation learning. Moreover, these methods only predict behaviors for a certain entity. A few works also took advantage of both recurrent neural networks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and generative modeling to learn an explicit or implicit trajectory distribution, which achieved better performance <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. However, they either leveraged only static context images or only trajectories of agents, which is not sufficient to make predictions for the agents that interact with both static and dynamic obstacles. In this paper, we propose a conditional generative neural system which can leverage both historical scene evolution information and trajectories of multiple interactive agents and generate realistic and diverse trajectory hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Attention Mechanisms</head><p>Soft attention mechanisms have been widely used in neural networks to enable the capability of focusing on a subset of input features, which have been extensively studied in the field of image captioning <ref type="bibr" target="#b9">[10]</ref>, visual object tracking <ref type="bibr" target="#b10">[11]</ref> and natural language processing. Several works also brought attention mechanisms into trajectory prediction tasks to figure out the most informative and related obstacles <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>. In this paper, we put forward a block attention mask mechanism for trajectories to extract the most critical features of each entity as well as a Gaussian mixture attention mechanism for context images to extract the most crucial static features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Bayesian Generative Modeling</head><p>The objective of generative models is to approximate the true data distribution, with which one can generate new samples similar to real data points with a proper variance. Generative models have been widely employed in tasks of representation learning and distribution approximation in literature, which basically fall into two categories: explicit density models and implicit density models <ref type="bibr" target="#b15">[16]</ref>. In recent years, since deep neural networks have been leveraged as universal distribution approximators thanks to its high flexibility, two deep generative models have been widely studied:</p><p>Variational Auto-Encoder (VAE) <ref type="bibr" target="#b16">[17]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" target="#b17">[18]</ref>. Since in trajectory forecasting tasks the predicted trajectories are sampled from the posterior distribution conditioned on historical information, the two models were extended to their conditional versions which results in conditional VAE (CVAE) <ref type="bibr" target="#b18">[19]</ref> and conditional GAN (CGAN) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In this paper, we combine the strengths of conditional latent space learning via CVAE and variational divergence minimization via adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>The objective of this paper is to develop a deep generative system that can accurately forecast motions and trajectories for multiple agents simultaneously. The system should take into account the historical state information, static context and interactions among dynamic entities.</p><p>Assume there are in total N entities in the observation area, which may vary in different cases. We denote a set of trajectories covering the history and prediction horizons (T h and T f ) as</p><formula xml:id="formula_0">T k−Th:k+Tf = {t i k−Th:k+Tf |t i k = (x i k , y i k ), i = 1, ..., N } (1) where (x, y)</formula><p>is the 2D coordinate in the pixel space or world space. The latent random variable is denoted as z k , where k is the current time step. The sequence of context images up to time step k is denoted as I k−T h :k . Our goal is to predict the conditional distribution of future trajectories given the historical context images and trajectories p(T k+1:k+T f |T k−T h :k , I k−T h :k ). The long-term prediction is realized by propagating the generative system multiple times to the future. To simplify the notations in the following sections, we denote the condition variable as C = {T k−T h :k , I k−T h :k }, the sequence of predicted variables as Y = {T k+1:k+T f }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>In this section, we first provide an overview of the key components and the architecture of the proposed Conditional Generative Neural System (CGNS). The detailed theories and models of each component are then illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Overview</head><p>The architecture of CGNS is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> where there is a deep feature extractor (DFE) with an environment attention mechanism (EAM) as well as a generative neural sampler (GNS). First, the DFE extracts deep features from a sequence of historical context images and trajectories of multiple interactive agents to obtain both the information of static and dynamic obstacles, where the EAM tells which areas and dynamic entities should be paid more attention to than others when predicting the trajectory of a certain entity. The above information is utilized as the input of GNS which takes advantage of a deep latent variable model and a variational divergence minimization approach to generate a set of feasible, realistic and diverse future trajectories of all the involved entities. All the components are implemented with deep neural networks thus can be trained end-to-end efficiently and consistently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Environment-Aware Deep Feature Extraction</head><p>We take advantage of both context images and historical trajectories of interactive agents to extract deep features of both static and dynamic environments. In order to figure out the most crucial parts to consider when forecasting behaviors of certain agents, we propose a soft block attention mechanism applied to trajectories and a Gaussian mixture attention mechanism applied to context images. The details are illustrated below.</p><p>The historical and future trajectories are constructed as matrices which are treated as 2D images. The former is fed into a convolutional neural network (CNN) and an average pooling layer to obtain a contractable attention mask over the whole trajectory matrix, which is then expanded to the same size as the trajectory matrix by duplicating each column twice corresponding to coordinates x and y. The original trajectory matrix is multiplied by the block attention mask elementwisely. This mechanism is not applied to the future trajectory matrix since it is unreasonable to have particular attention on the future evolution. The context image sequences are also fed into a CNN followed by fully connected layers to obtain a set of parameters of the Gaussian mixture distribution, which is used to calculate the context attention mask. The elementwise multiplication of original images and attention masks is fed to a pre-trained feature extractor, which is the convolution base of VGG-19 <ref type="bibr" target="#b20">[21]</ref> in this paper. The interaction-aware features and contextaware features are concatenated and fed into a recurrent layer followed by fully connected layers to obtain a comprehensive and consistent feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Generative Sampling</head><p>The GNS is composed of an encoder E and a generator G. The goal of encoder is to learn a consistent distribution in a lower-dimensional latent space, from which the latent variable can be sampled efficiently. The generator aims to produce trajectories as real as possible. An auxiliary discriminator D is adopted, which aims to distinguish fake trajectories from groundtruth. The generator G and discriminator D formulates a minimax game. The three components can be optimized jointly via conditional latent space learning and variational divergence minimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Latent Space Learning (CLSL)</head><p>The conditional latent variable model defined in this paper contains three classes of variables: condition variable C, predicted variable Y and latent variable z. We aim to obtain the conditional distribution p(Y |C). Given the training data (C, Y ), the model first samples z from an arbitrary distribution Q. Our goal is to maximize the variational lower bound, which is written as</p><formula xml:id="formula_1">log p(Y |C) − D KL [Q(z|C, Y )||p(z|C, Y )] = E z∼Q [log p(Y |z, C)] − D KL [Q(z|C, Y )||p(z|C)].<label>(2)</label></formula><p>where p(z|C) = N (0, I). This process can be realized with a Conditional Variational Auto-Encoder which consists of an encoder network E to obtain Q(z|C, Y ) and a decoder (generator) network G to model p(Y |z, C). The loss function can be formulated as a weighted sum of the reconstruction error and KL divergence:</p><formula xml:id="formula_2">L G,E RC = Et k+1:k+T f ,z k ∼Q t k+1:k+T f − G(C k , z k ) 2 , (3) L E KL = Et k+1:k+T f [D KL (E(C k )||p(z k ))] ,<label>(4)</label></formula><p>where z k ∼ N (0, I). The optimal encoder and generator can be obtained by</p><formula xml:id="formula_3">G * , E * = arg min G,E λ1L G,E RC + λ2L E KL .<label>(5)</label></formula><p>Variational Divergence Minimization (VDM) Given two conditional distributions P data (Y |C) and P GNS (Y |C) with absolutely continuous density function p data (Y |C) and p GNS (Y |C) which denotes the real data distribution and its approximation with GNS, the f -divergence <ref type="bibr" target="#b21">[22]</ref> is defined as <ref type="bibr" target="#b5">(6)</ref> where f : R + → R is a convex and lower-semicontinuous function with f (1) = 0. A lower bound of f -divergence can be derived with the convex conjugate function f *</p><formula xml:id="formula_4">D f (P data || P GNS ) = Y p GNS (Y |C)f pdata(Y |C) pGNS(Y |C) dY ,</formula><formula xml:id="formula_5">D f (P data || P GNS ) ≥ sup T ∈T Y p data (Y |C)T (Y |C))dY − Y p GNS (Y |C)f * (T (Y |C))dY = sup T ∈T (E Y ∼Pdata [T (Y |C)] − E Y ∼PGNS [f * (T (Y |C))]),<label>(7)</label></formula><p>where T is an arbitrary class of mapping T : Y → R. In order to minimize the variational lower bound in <ref type="formula" target="#formula_5">(7)</ref>, we can formulate a minimax game of p GNS (Y |C) and T (Y |C), which are parameterized by θ and φ, respectively. Then the optimal θ * and φ * can be obtained by</p><formula xml:id="formula_6">θ * , φ * = arg min θ max φ E Y ∼pdata(Y |C) [T φ (Y |C)] − E Y ∼pθ(Y |C) [f * (T φ (Y |C))].<label>(8)</label></formula><p>In this work, we propose to minimize the Pearson-χ 2 divergence between P data +P GNS and 2P GNS</p><formula xml:id="formula_7">D χ 2 Pearson = Y (2p GNS − (p data + p GNS )) 2 p data + p GNS dY.<label>(9)</label></formula><p>Since <ref type="formula" target="#formula_7">(9)</ref> is intractable, we leverage the adversarial learning techniques with a generator G and a discriminator D implemented as deep networks. The adversarial loss functions are derived as</p><formula xml:id="formula_8">L G VDM = 1 2 E z k ∼p(z) [(D(G(C k , z k ))) 2 ],<label>(10)</label></formula><formula xml:id="formula_9">L D VDM = 1 2 Et k+1:k+T f [(D(t k+1:k+T f ) − 1) 2 ] + 1 2 E z k ∼p(z) [(D(G(C k , z k )) + 1) 2 ],<label>(11)</label></formula><p>To discriminate the effect of latent space learning, we also involve two additional terms L G,E VDM and L D,E VDM where the input z k are sampled from the encoded latent distribution. Thus, the optimal encoder, generator and discriminator by variational divergence minimization can be obtained as</p><formula xml:id="formula_10">E * , G * , D * = arg min G,E max D λ3(L G VDM + L D VDM ) + λ4(L G,E VDM + L D,E VDM ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Soft Constraint Incorporation</head><p>In order to make generated samples compliant to feasibility constraints of vehicle kinematics, we propose to incorporate a differentiable barrier (indicator) function I(·) in the loss function, which enables soft constraints in deep neural networks via pushing predicted trajectories to the feasible regions. In this work, we denote the empirical upper bounds on the absolute values of accelerations a k+1:k+T f and path curvatures κ k+1:k+T f as a max and κ max , respectively. Then the feasibility loss can be calculated as</p><formula xml:id="formula_11">L G,E F = α 1 E ak+1:k+T f   k+Tf t=k+1 max (0, sgn(|a t | − a max ))   + α 2 E κk+1:k+T f   k+Tf t=k+1 max (0, sgn(|κ t | − κ max ))   ,<label>(13)</label></formula><p>where sgn(·) refers to the sign function and a t ,κ t can be calculated with the predicted waypoints. This loss term is not applied to human trajectory prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Conditional Generative Neural System (CGNS)</head><p>We leverage both CLSL and VDM in the proposed system, which provides complementary strengths. The objective function of the whole system is formulated as</p><formula xml:id="formula_12">LCGNS = λ1L G,E RC + λ2L E KL + λ3(L G VDM + L D VDM ) +λ4(L G,E VDM + L D,E VDM ) + λ5L G,E F ,<label>(14)</label></formula><p>which can be trained end-to-end. In practice, due to the existence of reconstruction loss, the generator tends to improve faster than the discriminator, which may result in unbalanced training. Therefore, we compensate the unbalance by training the discriminator multiple times in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we validate the proposed CGNS on three benchmark datasets for trajectory prediction which are available online and solve a task of probabilistic behavior prediction for multiple interactive on-road vehicles in a roundabout scenario. The model performance is compared with several state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>ETH <ref type="bibr" target="#b22">[23]</ref> and UCY <ref type="bibr" target="#b23">[24]</ref>: These datasets include bird-eyeview videos and image annotations of pedestrians in various outdoor and indoor scenarios. The trajectories were extracted in the world space. Stanford Drone Dataset (SDD) <ref type="bibr" target="#b24">[25]</ref>: The dataset also contains a set of bird-eye-view videos and the corresponding trajectories of involved entities, which was collected in multiple scenarios within a university campus full of pedestrians, bikers and vehicles. The trajectories were extracted in the pixel space instead of the world space. INTERACTION Dataset (ID) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>: The raw dataset was collected by a drone with camera and our testing vehicle equipped with LiDAR. The trajectories were extracted by visual detection. We visualized the real trajectories in our simulator to obtain the bird-eye-view images, where the static context information came from the Google Earth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics and Baselines</head><p>We evaluate the model performance in terms of average displacement error (ADE) defined as the average distance between the predicted trajectories and the groundtruth over all the involved entities within the prediction horizon, as well as final displacement error (FDE) defined as the distance at the last predicted time step. To allow for fair comparisons with prior works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we predicted the future 12 time steps (4.8s) based on the previous 8 time steps <ref type="bibr">(3.2s</ref>) for ETH and UCY in the Euclidean space. We used the standard training and testing split for SDD and make predictions in the pixel space. For our own dataset ID, we predicted the future 10 time steps (5s) based on the historical 4 time steps (2s) in the Euclidean space. We compared the performance of our proposed system with the following baseline approaches on multiple datasets: Constant Velocity Model (CVM), Linear Regression (LR), Probabilistic LSTM (P-LSTM), Social LSTM (S-LSTM) <ref type="bibr" target="#b4">[5]</ref>, Social GAN (S-GAN and S-GAN-P) <ref type="bibr" target="#b27">[28]</ref>, Clairvoyant attentive recurrent network (CAR-Net) <ref type="bibr" target="#b12">[13]</ref>, SoPhie <ref type="bibr" target="#b11">[12]</ref> and DESIRE <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Since the whole system consists of differentiable functions approximated by deep neural networks, it can be trained endto-end efficiently. The detailed model architecture and hyperparameters are introduced below.</p><p>In the deep feature extractor, the CNN I contains one convlayer with kernel size 5 × 5 and zero padding to keep the same dimension. The CNN II contains three conv-layers with kernel size 3 × 3 and the FC I contains two layers with 64 hidden units. The CNN III is the convolution base of pretrained VGG-19 whose weights are fixed during training. The GRU I , GRU II , FC II and FC III all have 128 hidden units. The Encoder FC IV has three fully-connected layers with 256, 128 and 64 hidden units, respectively. The dimension of encoded latent space is two. The Generator GRU III has 128 hidden units. The Discriminator GRU IV has 128 hidden units and FC V has three layers with 128, 128 and 1 units, respectively. In all the experiments, we set λ 1 = 5.0, λ 2 = λ 3 = λ 4 = λ 5 = 1.0 and α 1 = α 2 = 1000. The Adam optimizer was employed with a learning rate of 0.002. Moreover, we found that the Gaussian mixture in the context image attention mask does not lead to obvious improvement in terms of prediction accuracy and diversity than a single Gaussian in this task. Therefore, we utilized the latter to reduce model complexity and show the corresponding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Analysis</head><p>ETH and UCY Dataset: To allow for fair comparisons with multiple baseline approaches which only leverage historical trajectory information, we deactivated the branch of context feature extraction in our system to illustrate its superiority on prediction accuracy based on the same input as prior works. The ADE and FDE of the proposed CGNS and baseline models in Euclidean space are compared in <ref type="table" target="#tab_0">Table I</ref>. Some of the reported statistics are adapted from the original papers.</p><p>It can be seen that the CVM performs the worst as expected since the constant velocity approximation is insufficient for a crowded scenario with highly interactive agents. The LR performs slightly better in most scenarios than CVM but achieves the smallest error on the HOTEL dataset. A possible reason is that the human trajectories in this dataset tend to be more straight and smooth, which brings an advantage for linear fitting methods. The P-LSTM and S-LSTM provide an improvement with similar accuracy due to the exploitation of recurrent neural networks. The S-GAN and Sophie achieve a bigger progress thanks to the implicit generative modeling of trajectory distribution. Our approach makes a step forward on prediction accuracy, which implies the effectiveness of latent space learning.</p><p>Stanford Drone Dataset: We also compared the ADE and FDE of the CGNS and baseline models in pixel space, which is shown in <ref type="table" target="#tab_0">Table II</ref>. Similarly, the linear method LR performs the worst and the ordinary P-LSTM and S-LSTM give a slightly better accuracy. The CAR-Net makes a step forward by utilizing a physical attention module. The S-GAN and DESIRE provide better results than the above baselines since they solve the task from a probabilistic perspective by learning implicit data distribution and latent space representations, respectively. Our approach achieves the best performance in terms of prediction error, which implies the significance and necessity of leveraging both context and trajectory information. The combination of CLSL and VDM also contributes to the enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTERACTION Dataset:</head><p>We finally compared the model performance on our roundabout driving dataset in Table III. The SoPhie, CAR-Net and DESIRE are not involved since their codes are not publicly available. It is shown that the linear models CVM and LR have similar performance to advanced learning-based models for short-term prediction since the velocity and yaw angle of vehicles cannot vary much in a short period due to kinematics feasibility constraints. However, as the prediction horizon increases, their performance deteriorates much faster. A potential reason is that due to the curving roads within the roundabout area, the vehicles tend to advance along the curving lines to avoid collisions, which is not able to be captured by linear approximations. The P-LSTM and S-LSTM provide similar results, which implies that the social pooling mechanism has little effects on feature extraction in this scenario. Our CGNS is able to achieve the smallest prediction error among baseline models in most cases especially for long-term prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Analysis</head><p>We provide a qualitative analysis of the prediction results on our INTERACTION dataset. To illustrate the effectiveness of the attention module, we visualize the context image masks and trajectory block masks of several typical testing cases in <ref type="figure">Fig. 3</ref>. Detailed analysis can be found in the caption. The distribution of generated future trajectories is approximated by the kernel density estimation, which is visualized in <ref type="figure" target="#fig_2">Fig. 4</ref>. We can see that the system can generate smooth, feasible and realistic vehicle trajectories, which evolve along the road curves. The groundtruth is located at the most dense part of the distribution in most cases. In general, our proposed CGNS can achieve better generation performance in terms of realism and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablative Analysis</head><p>We conduct an ablative analysis on the RD dataset to demonstrate relative significance of each component in the proposed CGNS. The ADE and FDE of each model setting are shown in <ref type="table" target="#tab_0">Table III</ref>. We notice that using the T + CLSL and T + VDM achieves similar performance in terms of prediction error while T + CLSL + VDM provides a notable improvement. Moreover, it is demonstrated that the complete system T + I + CLSL + VDM does not lead to obvious improvement compared with three partial systems for short-term prediction while its superiority becomes more remarkable as the forecasting horizon increases. This is reasonable since the static context has little effect on driver behaviors in a short period. More specifically, since the trajectory segment within a short period can be approximated by a linear segment, learning the road curvature from context images does not provide much assistance for prediction. As the forecasting horizon increases, however, the restriction of road geometry on vehicle motions cannot be ignored any more, which results in larger performance gain of leveraging context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose a conditional generative neural system for long-term trajectory prediction, which takes into account both static context information through images and dynamic evolution of traffic situations through trajectories of interactive agents. We also incorporate attention mechanisms to figure out the most critical portions for predicting motions of a certain entity. The system combines the strengths of both latent space learning and variational divergence minimization to approximate the data distribution, from which realistic and diverse trajectory hypotheses can be sampled. The proposed system is validated on various benchmark datasets as well as a roundabout driving dataset collected by ourselves. The results show that our system can achieve better performance than various baseline models on most datasets in terms of prediction accuracy. <ref type="figure">Fig. 3</ref>. The visualization of the context image masks and trajectory block attention masks. Particularly, in the trajectory masks, there are four rows representing four historical time steps and six columns representing six vehicles in the scene. The first column corresponds to the predicted vehicle and the others corresponds to surrounding ones. Brighter colors indicate larger attention weights. The predicted vehicles are indicated with red bounding boxes. In all the cases, the image masks have a large weight around the predicted vehicle and the area of its heading direction. In the first three cases, only the historical trajectories of the predicted vehicle are assigned large attention weights, which implies that the other vehicles have little effect in these situations. However, in the last three cases, more attention is paid to other vehicles since there exist strong interactions which increases the inter-dependency. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>, H. Ma and M. Tomizuka are with the Department of Mechanical Engineering, University of California, Berkeley, CA 94720, USA (e-mail: jiachen li, hengbo ma, tomizuka@berkeley.edu) Typical urban traffic scenarios with large uncertainty and interactions among multiple entities. The shaded areas represent the reachable sets of possible trajectories. (a) Unsignalized roundabout with four-way yield signs; (b) Unsignalized intersection with four-way stop signs. close to data distribution and generate diverse samples which represent various possible behavior patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e I C Z 5 9 e y O d y 0 V C K F s X M c H w x c 3 w = " &gt; A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 I 6 4 q m L Z Q i y T T a R 2 a Z E I y U U p x 4 w + 4 1 S 8 T / 0 D / w j t j C m o R n Z D k z L n 3 n J l 7 b 5 C E I l O O 8 1 q y 5 u Y X F p f K y 5 W V 1 b X 1 j e r m V i u T e c q 4 x 2 Q o 0 0 7 g Z z w U M f e U U C H v J C n 3 o y D k 7 W B 0 q u P t W 5 5 m Q s a X a p z w X u Q P Y z E Q z F d E e V e y L 9 V 1 t e b U H b P s W e A W o I Z i N W X 1 B V f o Q 4 I h R w S O G I p w C B 8 Z P V 2 4 c J A Q 1 8 O E u J S Q M H G O e 1 R I m 1 M W p w y f 2 B F 9 h 7 T r F m x M e + 2 Z G T W j U 0 J 6 U 1 L a 2 C O N p L y U s D 7 N N v H c O G v 2 N + + J 8 d R 3 G 9 M / K L w i Y h V u i P 1 L N 8 3 8 r 0 7 X o j D A s a l B U E 2 J Y X R 1 r H D J T V f 0 z e 0 v V S l y S I j T u E / x l D A z y m m f b a P J T O 2 6 t 7 6 J v 5 l M z e o 9 K 3 J z v O t b 0 o D d n + O c B a 2 D u k v 4 4 r D W O C l G X c Y O d r F P 8 z x C A 2 d o w i N v g U c 8 4 d k 6 t x L r z h p / p l q l Q r O N b 8 t 6 + A D U G p E / &lt; / l a t e x i t &gt; The overview of proposed conditional generative neural system (CGNS), which consists of four key components: (a) A deep feature extractor with soft attention mechanism, which extracts multi-level features from scene context image sequences and trajectories; (b) An encoder to learn conditional latent space representations; (c) A generator (decoder) to sample future trajectory hypotheses; (d) A discriminator to distinguish predicted trajectories from groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The visualization of sampled future trajectory hypotheses with CLSL+VDM training strategies. The red dashed lines denote groundtruth trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ADE</head><label>I</label><figDesc>/ FDE COMPARISONS OF PEDESTRIAN TRAJECTORY PREDICTION (ETH AND UCY DATASET). FDE COMPARISONS OF VEHICLE TRAJECTORY PREDICTION (RD DATASET). T REPRESENTS ONLY USING INTERACTION-AWARE (TRAJECTORY) FEATURES AND T + I REPRESENTS USING ADDITIONAL CONTEXT-AWARE (IMAGE) FEATURES.</figDesc><table><row><cell></cell><cell>CVM</cell><cell></cell><cell>LR</cell><cell cols="2">P-LSTM</cell><cell>S-LSTM</cell><cell>S-GAN</cell><cell>S-GAN-P</cell><cell>SoPhie</cell><cell>CGNS</cell></row><row><cell>ETH</cell><cell cols="2">1.42 / 2.88</cell><cell>1.33 / 2.94</cell><cell cols="2">1.13 / 2.38</cell><cell>1.09 / 2.35</cell><cell>0.81 / 1.52</cell><cell>0.87 / 1.62</cell><cell>0.70 / 1.43</cell><cell>0.62 / 1.40</cell></row><row><cell>HOTEL</cell><cell cols="2">0.51 / 0.68</cell><cell>0.39 / 0.72</cell><cell cols="2">0.91 / 1.89</cell><cell>0.79 / 1.76</cell><cell>0.72 / 0.61</cell><cell>0.67 / 1.37</cell><cell>0.76 / 1.67</cell><cell>0.70 / 0.93</cell></row><row><cell>UNIV</cell><cell cols="2">0.73 / 1.63</cell><cell>0.82 / 1.59</cell><cell cols="2">0.63 / 1.36</cell><cell>0.67 / 1.40</cell><cell>0.60 / 1.26</cell><cell>0.76 / 1.52</cell><cell>0.54 / 1.24</cell><cell>0.48 / 1.22</cell></row><row><cell>ZARA1</cell><cell cols="2">0.59 / 1.36</cell><cell>0.62 / 1.21</cell><cell cols="2">0.44 / 0.84</cell><cell>0.47 / 1,00</cell><cell>0.34 / 0.69</cell><cell>0.35 / 0.68</cell><cell>0.30 / 0.63</cell><cell>0.32 / 0.59</cell></row><row><cell>ZARA2</cell><cell cols="2">0.84 / 1.55</cell><cell>0.77 / 1.48</cell><cell cols="2">0.51 / 1.16</cell><cell>0.56 / 1.17</cell><cell>0.42 / 0.84</cell><cell>0.42 / 0.84</cell><cell>0.38 / 0.78</cell><cell>0.35 / 0.71</cell></row><row><cell>AVG</cell><cell cols="2">0.82 / 1.62</cell><cell>0.79 / 1.59</cell><cell cols="2">0.72 / 1.53</cell><cell>0.72 / 1.54</cell><cell>0.58 / 1.18</cell><cell>0.61 / 1.21</cell><cell>0.54 / 1.15</cell><cell>0.49 / 0.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">ADE / FDE COMPARISONS OF PEDESTRIAN TRAJECTORY PREDICTION (SDD DATASET).</cell></row><row><cell></cell><cell>LR</cell><cell></cell><cell>P-LSTM</cell><cell cols="2">S-LSTM</cell><cell>S-GAN</cell><cell>SoPhie</cell><cell>CAR-Net</cell><cell>DESIRE</cell><cell>CGNS</cell></row><row><cell>SDD</cell><cell cols="2">37.1 / 63.5</cell><cell>35.8 / 55.4</cell><cell cols="2">31.2 / 57.0</cell><cell>24.8 / 38.6</cell><cell>17.8 / 32.1</cell><cell>25.7 / 51.8</cell><cell>19.3 / 34.1</cell><cell>15.6 / 28.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="5">ADE / Baseline Models</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Proposed CGNS</cell></row><row><cell cols="2">CVM</cell><cell>LR</cell><cell cols="2">P-LSTM</cell><cell>S-LSTM</cell><cell>S-GAN</cell><cell>T + CLSL</cell><cell>T + VDM</cell><cell>T + CLSL + VDM</cell><cell>T + I + CLSL+VDM</cell></row><row><cell cols="9">1.0s 0.16 / 0.29 0.24 / 0.32 0.23 / 0.28 0.24 / 0.30 0.22 / 0.28 0.19 / 0.23 0.22 / 0.27</cell><cell>0.17 / 0.25</cell><cell>0.21 / 0.26</cell></row><row><cell cols="9">2.0s 0.59 / 0.78 0.58 / 0.92 0.47 / 0.60 0.45 / 0.57 0.42 / 0.58 0.34 / 0.42 0.38 / 0.45</cell><cell>0.38 / 0.44</cell><cell>0.35 / 0.40</cell></row><row><cell cols="9">3.0s 1.21 / 1.92 1.43 / 2.28 0.84 / 1.53 0.80 / 1.48 0.81 / 1.54 0.72 / 1.33 0.75 / 1.37</cell><cell>0.69 / 1.24</cell><cell>0.64 / 1.15</cell></row><row><cell cols="9">4.0s 2.94 / 3.98 3.85 / 4.73 1.27 / 1.51 1.21 / 1.69 1.28 / 1.87 1.26 / 1.81 1.35 / 1.76</cell><cell>0.86 / 1.33</cell><cell>0.79 / 1.23</cell></row><row><cell cols="9">5.0s 4.28 / 6.12 5.89 / 6.91 1.78 / 2.21 1.69 / 2.77 1.65 / 2.68 1.85 / 3.20 1.72 / 2.89</cell><cell>1.54 / 2.37</cell><cell>1.47 / 2.12</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generic probabilistic interactive situation recognition and prediction: From virtual to real</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards a fatality-aware benchmark of probabilistic reaction prediction in highly interactive driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3274" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generic vehicle tracking framework capable of handling occlusions based on modified mixture particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>2018 IEEE Intelligent Vehicles Symposium (IV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="936" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generic tracking and probabilistic prediction framework and its application in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07049</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coordination and trajectory prediction for vehicle interactions via bayesian generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interaction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical attentive recurrent tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3053" to="3061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01482</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Car-net: Clairvoyant attentive recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Crowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08835</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="452" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3542" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constructing a Highly Interactive Vehicle Motion Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Scenarios with Semantic Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clausse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kümmerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Königshof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De La Fortelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
