<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Pretrained Language Models for Graph-to-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Pretrained Language Models for Graph-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recently proposed pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-totext generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. In particular, we report new state-of-the-art BLEU scores of 49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets -a relative improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-to-text tasks. We find evidence that their knowledge about true facts helps them perform well even when the input graph representation is reduced to a simple bag of node and edge labels. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are important data structures as they represent complex relations between a set of objects. For example, syntactic and semantic structures of sentences can be represented using different graph representations <ref type="bibr" target="#b4">(Bastings et al., 2017;</ref><ref type="bibr" target="#b3">Banarescu et al., 2013)</ref> and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities <ref type="bibr" target="#b7">(Gardent et al., 2017)</ref>.</p><p>Graph-to-text generation, a subtask of data-totext generation <ref type="bibr" target="#b8">(Gatt and Krahmer, 2018)</ref>, aims to create fluent natural language text to describe an input graph (see <ref type="figure">Figure 1</ref>). This task is important for numerous applications such as question answering <ref type="bibr">(Duan et al., 2017)</ref>, dialogue generation <ref type="bibr" target="#b21">(Moon et al., 2019)</ref>, and summarization <ref type="bibr">(Fan et al., 2019)</ref>.</p><p>Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures have considerably outperformed prior state of the art <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Radford et al., 2019)</ref>. Following this trend, recent works <ref type="bibr" target="#b19">(Mager et al., 2020;</ref><ref type="bibr" target="#b11">Harkous et al., 2020)</ref> apply transfer learning to data-to-text generation, where a language model is first pretrained on large corpora before being fine-tuned on the target task.</p><p>In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART  and T5 <ref type="bibr" target="#b28">(Raffel et al., 2019)</ref>, for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We are also the first to investigate task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin.</p><p>While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation <ref type="bibr" target="#b34">(Song et al., 2018;</ref><ref type="bibr" target="#b29">Ribeiro et al., 2019</ref><ref type="bibr">Ribeiro et al., , 2020</ref><ref type="bibr">Schmitt et al., 2020;</ref><ref type="bibr" target="#b44">Zhao et al., 2020a)</ref>, our approaches based on PLMs consistently outperform these models, even though PLMs -as sequence models -do not exhibit any graphspecific structural bias. 2 This puts into question the importance of encoding the structure of a graph in the presence of a strong language model. In our analysis we investigate to what extent fine-tuned Linearized representation: &lt;H&gt; Apollo 12 &lt;R&gt; backup pilot &lt;T&gt; Alfred Worden &lt;H&gt; Alan Bean &lt;R&gt; was a crew member of &lt;T&gt; Apollo 12 &lt;H&gt; Apollo 12 &lt;R&gt; operator &lt;T&gt; NASA &lt;H&gt; Alan Bean &lt;R&gt; occupation &lt;T&gt; Test pilot &lt;H&gt; Apollo 12 &lt;R&gt; commander &lt;T&gt; David Scott &lt;H&gt; Alan Bean &lt;R&gt; was selected by NASA &lt;T&gt; 1963 &lt;H&gt; Alan Bean &lt;R&gt; alma Mater &lt;T&gt; <ref type="bibr">UT Austin B.S. 1955</ref>  Text: Alan Bean graduated from UT Austin in 1955 with a Bachelor of Science degree. He was hired by NASA in 1963 and served as a test pilot. Apollo 12's backup pilot was Alfred Worden and was commanded by David Scott.</p><p>Text: As his children, we feel very terrible now.</p><p>Linearized representation: ( feel :ARG0 ( we ) :ARG1 ( terrible :degree ( very ) ) :time ( now ) :ARG1-of ( cause :ARG0 ( have-rel-role :ARG0 we :ARG1 ( he ) :ARG2 ( child ) ) ) )  <ref type="figure">Figure 1</ref>: Examples of (a) AMR and (b) WebNLG graphs, the input for the models and the reference texts.</p><p>PLMs make use of the graph structure and whether they need it at all. We notably observe that PLMs can achieve high performance on two popular KGto-text benchmarks even when the KG is reduced to a mere bag of node and edge labels. We also find evidence that factual knowledge from the pretraining phase poses a strong bias on the texts generated by PLMs -to the extent that even unseen corrupted graph facts lead to correct output texts. In summary, our contributions are the following:</p><p>(1) We examine and compare two PLMs, BART and T5, for graph-to-text generation, exploring language model adaptation (LMA) and supervised task adaptation (STA) pretraining strategies, employing additional task-specific data. (2) Our approaches consistently outperform the state of the art by a significant margin on three established graph-totext benchmarks from different domains. (3) We demonstrate that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch. (4) We present evidence that the knowledge about facts acquired during pretraining gives PLMs such an advantage on KG-to-text benchmarks that their performance is almost the same with or without access to the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph-to-text generation can be divided into two main tasks: generating text (i) from meaning representations <ref type="bibr" target="#b15">(Konstas et al., 2017)</ref> and (ii) from KGs <ref type="bibr" target="#b7">(Gardent et al., 2017)</ref>. AMR-to-Text Generation. Abstract meaning representation (AMR) is a semantic formalism that represents the meaning of a sentence as a rooted directed graph expressing "who is doing what to whom" <ref type="bibr" target="#b3">(Banarescu et al., 2013)</ref>. In an AMR graph, nodes represent concepts and edges represent semantic relations. Various neural models have been proposed to generate sentences from AMR graphs. <ref type="bibr" target="#b15">Konstas et al. (2017)</ref> propose the first neural approach for AMR-to-text generation that uses a linearized input graph. Recent approaches <ref type="bibr" target="#b34">(Song et al., 2018;</ref><ref type="bibr" target="#b5">Beck et al., 2018;</ref><ref type="bibr">Damonte and Cohen, 2019;</ref><ref type="bibr" target="#b29">Ribeiro et al., 2019;</ref><ref type="bibr" target="#b44">Zhao et al., 2020a)</ref> propose architectures based on GNNs to directly encode the AMR graph structure. Other methods <ref type="bibr" target="#b47">(Zhu et al., 2019;</ref><ref type="bibr">Cai and Lam, 2020b;</ref><ref type="bibr" target="#b33">Song et al., 2020;</ref><ref type="bibr" target="#b42">Yao et al., 2020)</ref> employ Transformers to learn node representations injecting the graph structure into the self-attention aggregation.</p><p>KG-to-Text Generation. Recent neural approaches for KG-to-text generation linearize the KG triples as input to sequence-to-sequence models <ref type="bibr" target="#b35">(Trisedya et al., 2018;</ref><ref type="bibr" target="#b22">Moryossef et al., 2019;</ref><ref type="bibr">Castro Ferreira et al., 2019)</ref>. <ref type="bibr" target="#b20">Marcheggiani and Perez Beltrachini (2018)</ref> use GNNs to capture node contexts, and demonstrate superior performance compared to LSTMs. <ref type="bibr" target="#b14">Koncel-Kedziorski et al. (2019)</ref> propose a Transformer-based approach which directly encodes the input graph structure. Most recent approaches <ref type="bibr">(Ribeiro et al., 2020;</ref><ref type="bibr">Schmitt et al., 2020)</ref> propose to encode both global and local node contexts in order to better capture the graph topology.</p><p>Pretrained Language Models. Pretrained Transformer-based models, such as <ref type="bibr">BERT (Devlin et al., 2019)</ref>, XLNet , or RoBERTa , have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks.</p><p>Generative pretrained</p><p>Transformer-based methods, such as GPT-2 <ref type="bibr" target="#b27">(Radford et al., 2019)</ref>, BART , and T5 <ref type="bibr" target="#b28">(Raffel et al., 2019)</ref>, are employed on many natural language generation (NLG) tasks. <ref type="bibr" target="#b19">Mager et al. (2020)</ref> were the first to employ a pretrained Transformer-based language model -namely GPT-2 -for AMR-to-text generation. Very recently, <ref type="bibr" target="#b11">Harkous et al. (2020)</ref> and <ref type="bibr" target="#b12">Kale (2020)</ref> demonstrate state-of-the-art results in different data-to-text datasets, employing GPT-2 and T5 models respectively. Different from the above works, we do not only investigate standard fine-tuning approaches but also new task-adaptive pretraining approaches for BART and T5, and we also provide the first analysis aimed at explaining the good performance of PLMs at graph-to-text tasks. Concurrent to our work, <ref type="bibr" target="#b26">Radev et al. (2020)</ref> propose DART, a new data-to-text dataset, and apply BART to the WebNLG dataset, augmenting the training data. Our study not only considers more benchmarks and PLMs, but also differs in that it focuses on transfer learning strategies that separate task-adaptive pretraining from fine-tuning on the actual task training data as opposed to the training data augmentation in <ref type="bibr" target="#b26">Radev et al. (2020)</ref>.</p><p>Recently, Gururangan et al. (2020) explored taskadaptive pretraining strategies for text classification. While our LMA (see ¬ß3) is related to their DAPT as both use a self-supervised objective on a domainspecific corpus, they notably differ in that DAPT operates on the model input while LMA models the output. We are the first to consider additional pretraining for NLG with PLMs.</p><p>3 PLMs for Graph-to-Text Generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models in this Study</head><p>We investigate BART and T5, two PLMs based on the Transformer encoder-decoder architecture <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, for graph-to-text generation. They mainly differ in how they are pretrained and the input corpora used for pretraining.</p><p>BART is pretrained as a text-to-text denoising autoencoder: first, the input text is corrupted with a random noise function; then BART is trained to reconstruct the original text. The training corpus is a combination of books and Wikipedia data. We evaluate BART versions with different capacity: base with 140M and large with 400M parameters.</p><p>T5 generalizes the text-to-text architecture to a variety of NLP tasks. The model is pretrained with randomly corrupted text spans with different mask ratios and span sizes. The training corpus is C4, a large cleaned corpus of web texts. We experiment with the following variants: small with 60M, base with 220M, and large with 770M parameters.</p><p>We fine-tune BART and T5 for a few epochs on the supervised downstream graph-to-text datasets. For T5, in the supervised setup, we add a prefix "translate from Graph to Text:" before the graph input. We add this prefix to imitate the T5 setup, when translating between different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-adaptive Pretraining</head><p>Inspired by previous work <ref type="bibr" target="#b15">(Konstas et al., 2017;</ref><ref type="bibr" target="#b10">Gururangan et al., 2020)</ref>, we investigate whether leveraging additional task-specific data can improve the PLMs' performance on graph-to-text generation. Task-specific data refers to a pretraining corpus that is more task-relevant and usually smaller than the text corpora used for taskindependent pretraining. In order to leverage the task-specific data, we add an intermediate pretraining step between the original pretraining and finetuning phases for graph-to-text generation.</p><p>More precisely, we first continue pretraining BART and T5 using language model adaptation (LMA) or supervised task adaptation (STA) training. In the supervised approach, we use pairs of graphs and corresponding texts collected from the same or similar domain as the target task. In the LMA approach, we follow BART and T5 pretraining strategies for language modeling, using the reference texts that describe the graphs. Note that we do not use the graphs in the LMA pretraining, but only the target text of our task-specific data collections. The goal is to adapt the decoder to the domain of the final task. In particular, we randomly mask text spans, replacing 15% of the tokens. 3 Before evaluation, we finally fine-tune the models using the original training set as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We evaluate the text-to-text PLMs in three graphto-text benchmarks: AMR17 (LDC2017T10), WebNLG <ref type="bibr" target="#b7">(Gardent et al., 2017)</ref>, and AGENDA <ref type="bibr" target="#b14">(Koncel-Kedziorski et al., 2019)</ref>. <ref type="table" target="#tab_15">Table 8</ref> in the Appendix shows statistics for each dataset.</p><p>AMR17. An instance in LDC2017T10 consists of a sentence annotated with its corresponding AMR graph. Following <ref type="bibr" target="#b19">Mager et al. (2020)</ref>, we linearize the AMR graphs using the PENMAN notation (see <ref type="figure">Figure 1a</ref>).</p><p>WebNLG. Each instance of WebNLG contains a KG from DBPedia <ref type="bibr" target="#b2">(Auer et al., 2007)</ref> and a target text with one or multiple sentences that describe the graph. The test set is divided into two partitions: seen, which contains only DBPedia categories present in the training set, and unseen, which covers categories never seen during training. Their union is called all. Following previous work (Harkous et al., 2020), we prepend H , R , and T tokens before the head entity, the relation and tail entity of a triple (see <ref type="figure">Figure 1b</ref>).</p><p>AGENDA. In this dataset, KGs are paired with scientific abstracts extracted from proceedings of AI conferences. Each sample contains the paper title, a KG, and the corresponding abstract. The KG contains entities corresponding to scientific terms and the edges represent relations between these entities. This dataset has loose alignments between the graph and the corresponding text as the graphs were automatically generated. The input for the models is a text containing the title, a sequence of all KG entities, and the triples. The target text is the paper abstract. We add special tokens into the triples in the same way as for WebNLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Additional Task-specific Data</head><p>In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR17) and scientific data (like AGENDA). We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators.</p><p>AMR Silver Data. In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword 4 corpus and use a state-of-the-art AMR parser (Cai and Lam, 2020a) to parse them into AMR graphs. 5 For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences. 6</p><p>Semantic Scholar AI Data. We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar <ref type="bibr" target="#b0">(Ammar et al., 2018)</ref> taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ <ref type="bibr" target="#b37">(Wadden et al., 2019)</ref>, an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system <ref type="bibr" target="#b18">(Luan et al., 2018)</ref>, which also extracts KGs from AI scientific papers. A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset KGAIA (KGs from AI Abstracts). <ref type="table" target="#tab_16">Table 9</ref> in the Appendix shows relevant dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We develop our experiments based on pretrained models released by HuggingFace <ref type="bibr" target="#b40">(Wolf et al., 2019)</ref>. Following <ref type="bibr" target="#b40">Wolf et al. (2019)</ref>, we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 ¬∑ 10 ‚àí5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. We add all edge labels seen in the training set to the vocabulary of the models for AMR17. For the KG datasets, we add the H , R , and T tokens to the models' vocabulary. Dev BLEU is used for model selection.</p><p>Following previous works, we evaluate the results with the automatic metrics BLEU <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref>, METEOR (Denkowski and Lavie, 2014), and chrF++ <ref type="bibr" target="#b25">(Popoviƒá, 2015)</ref>. We also use MoverScore <ref type="bibr" target="#b45">(Zhao et al., 2019)</ref>, BERTScore , and BLEURT <ref type="bibr" target="#b32">(Sellam et al., 2020)</ref> metrics, as they employ contextual embeddings to incorporate semantic knowledge and thus depend less on the surface symbols. Additionally, we also perform a human evaluation (cf. ¬ß5.4) quantifying the fluency, fidelity and meaning similarity of the generated texts. <ref type="table" target="#tab_2">Table 2</ref> shows our results for the setting without additional pretraining, with additional self-supervised task-adaptive pretraining solely using the collected  Gigaword sentences (LMA), and with additional supervised task adaptation (STA), before fine-tuning. We also report several recent results on the AMR17 test set. <ref type="bibr" target="#b19">Mager et al. (2020)</ref> and <ref type="bibr" target="#b11">Harkous et al. (2020)</ref> employ GPT-2 in their approaches. Note that GPT-2 only consists of a Transformer-based decoder. We are the first to employ BART and T5, which have both a Transformer-based encoder and decoder, in AMR-to-text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on AMR-to-Text</head><p>Only considering approaches without task adaptation, BART large already achieves a considerable improvement of 5.77 BLEU and 3.98 METEOR scores over the previous state of the art. With a BLEU score of 45.80, T5 large performs best. The other metrics follow similar trends. See <ref type="table" target="#tab_3">Table 10</ref> in the Appendix for evaluation with more automatic metrics.</p><p>Task-specific Pretraining. LMA already brings some gains with T5 benefitting more than BART in most metrics. It still helps less than STA even though we only have automatically generated annotations. This suggests that the performance increases with STA do not only come from additional exposure to task-specific target texts and that the models learn how to handle graphs and the graphtext correspondence even with automatically generated graphs. Interestingly, gains from STA with 2M over 200K are larger in BART than in T5, suggesting that large amounts of silver data may not be required for a good performance with T5. After task adaptation, T5 achieves 49.72 BLEU points, the new state of the art for AMR-to-text generation. In general, models pretrained on the STA setup converge faster than without task-adaptive pretraining. For example, T5 large without additional pretraining converges after 5 epochs of fine-tuning whereas T5 large with STA already converges after 2 epochs.  hand, fully end-to-end models <ref type="bibr">(Ribeiro et al., 2020;</ref><ref type="bibr">Schmitt et al., 2020)</ref> have strong performance on the seen dataset and usually perform poorly in unseen data. <ref type="bibr" target="#b44">Zhao et al. (2020a)</ref> leverage additional information about the order that the triples are realized and achieve the best performance among approaches that do not employ PLMs. Note that T5 is also used in <ref type="bibr" target="#b12">Kale (2020)</ref>. A particular difference in our T5 setup is that we add a prefix before the input graph. Our T5 approach achieves 59.70, 65.05 and 54.69 BLEU points on all, seen and unseen sets, the new state of the art. We hypothesize that the performance gap between seen and unseen sets stems from the advantage obtained by a model seeing examples of relation-text pairs during training. For example, the relation party (political party) was never seen during training and the model is required to generate a text that verbalizes the tuple: Abdul Taib Mahmud, party, Parti Bumiputera Sarawak . Interestingly, BART performs much worse than T5 on this benchmark, especially in the unseen partition with 9.7 BLEU points lower compared to T5. For lack of a suitable data source (cf. ¬ß4), we did not conduct experiments with LMA or STA for WebNLG. However, we explore cross-domain STA in additional experiments, which we discuss in Appendix A.2.  fluent text helps when generating paper abstracts, even though they were not pretrained in the scientific domain. BART large shows an impressive performance with a BLEU score of 23.65, which is 5.6 points higher than the previous state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on WebNLG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on AGENDA</head><p>Task-specific Pretraining. On AGENDA, BART benefits more from our task-adaptive pretraining, achieving the new state of the art of 25.66 BLEU points, a further gain of 2 BLEU points compared to its performance without task adaptation. The improvements from task-adaptive pretraining are not as large as for AMR17. We hypothesize that this is due to the fact that the graphs do not completely cover the target text, making this dataset more challenging. See <ref type="table" target="#tab_3">Table 11</ref> in the Appendix for more automatic metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Human Evaluation</head><p>To further assess the quality of the generated text, we conduct a human evaluation on AMR17 and WebNLG via crowd sourcing on Amazon Mechanical Turk. 7 Following previous works <ref type="bibr" target="#b7">(Gardent et al., 2017;</ref><ref type="bibr">Castro Ferreira et al., 2019)</ref>, we assess three quality criteria: (i) Fluency (i.e., does the text flow in a natural, easy-to-read manner?), for AMR and WebNLG; (ii) Meaning Similarity (i.e., how close in meaning is the generated text to the reference sentence?) for AMR17; (ii) Adequacy (i.e., does the text clearly express the data?) for WebNLG. We randomly select 100 texts from the Arrabbiata sauce can be found in Italy where Sergio Mattarella is the leader and the capital city is Rome. Italians are the people who live there and the language spoken is Italian.</p><p>Italians live in Italy where the capital is Rome and the language is Italian. Sergio Mattarella is the leader of the country and arrabbiata sauce can be found there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5</head><p>Reference: Arrabbiata sauce is from Italy where the capital is Rome, Italian is the language spoken and Sergio Mattarella is a leader. order shuf <ref type="figure">Figure 2</ref>: Example graph from WebNLG dev linearized with the neutral separator tag, denoted ‚Ä¢, (top left), its shuffled version (top right), texts generated with two fine-tuned versions of T5 small and a gold reference (bottom). Note that T5 can produce a reasonable text even when the input triples are shuffled randomly. generations of each model, which the annotators then rate on a 1-7 Likert scale. For each text, we collect scores from 3 annotators and average them. 8 <ref type="table" target="#tab_6">Table 4</ref> shows the results. There is a similar trend as in the automatic evaluation. Our approaches improve the fluency, meaning similarity, and adequacy on the two datasets compared to other stateof-the-art approaches with statistically significant margins (p &lt; 0.05). Interestingly, the highest fluency improvement (+0.97) is on AMR17, where our approach also has the largest BLEU improvement (+8.10) over <ref type="bibr" target="#b11">Harkous et al. (2020)</ref>. Finally, note that both models score higher than the gold sentences in fluency, highlighting their strong language generation abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Limiting the Training Data</head><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we investigate the PLMs' performance, measured with BLEU score, while varying (from 1% to 100%) the amount of training data used for fine-tuning. We find that, when fine-tuned with only 40% of the data, both BART base and T5 base already achieve similar performance as using the entire training data in all three benchmarks. Also note that in a low-resource scenario T5 base considerably 8 Inter-annotator agreement for the three criteria ranged from 0.40 to 0.79, with an average Krippendorff's Œ± of 0.56.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Influence of the Graph Structure</head><p>We conduct further experiments to examine how much the PLMs consider the graph structure. To this end, we remove parentheses in AMRs and replace H , R , and T tokens with neutral separator tokens, denoted ‚Ä¢, for KGs, such that the graph structure is only defined by the order of node and edge labels. If we shuffle such a sequence, the graph structure is thus completely obscured and the input effectively becomes a bag of node and edge labels. See <ref type="figure">Figure 2</ref> for an example of both a correctly ordered and a shuffled triple sequence. <ref type="table" target="#tab_9">Table 5</ref> shows the effect on T5 small 's performance when its input contains correctly ordered triples (T5 order ) vs. shuffled ones (T5 shuf ) for both training and evaluation. We first observe that T5 order only has marginally lower performance (around 2-4%) with the neutral separators than with the H / R / T tags or parentheses. We see that as evidence that the graph structure is similarly well  <ref type="table">Table 6</ref>: Example generations from corrupted (F) and true (T) WebNLG dev set facts by T5 small fine-tuned on correctly ordered nodes (order) and randomly shuffled nodes (shuf ) from the WebNLG training set, and CGE-LW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitative Analysis</head><p>captured by T5 order . Without the graph structure (T5 shuf ), AMR-to-text performance drops significantly. KG-to-text performance, however, is not much lower, indicating that most of the PLMs' success in this task stems from their language modeling rather than their graph encoding capabilities. It has recently been argued that large PLMs acquire a certain amount of factual knowledge during pretraining <ref type="bibr" target="#b24">(Petroni et al., 2019)</ref>. We hypothesize that this knowledge makes it easier to recover KG facts based on a set of entities and relations than to reconstruct a corrupted AMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative Analysis</head><p>To further test our hypothesis that PLMs make use of their knowledge about true facts during KG-totext generation, we take example facts from the WebNLG dev set, corrupt them, and feed them to both T5 order and T5 shuf . We also feed those triples to CGE-LW (Ribeiro et al., 2020), a state-of-the-art KG-to-text model trained from scratch, i.e., without any pretraining. <ref type="table">Table 6</ref> shows the generated texts. The model trained on correctly ordered input has learned a bit more to rely on the input graph structure. The false fact in example (1) is reliably transferred to the text by T5 order but not by T5 shuf , which silently corrects it. But even T5 order is not completely free from its factual knowledge bias, as illustrated in example (2) where both models refuse to generate an incorrect fact. This indicates that facts seen during pretraining serve as a strong guide during text generation, even for models that were fine-tuned with a clearly marked graph structure. The fact that CGE-LW, a graph encoder model trained from scratch on the WebNLG training set, has no difficulties in textualizing the false triples (except example 5) further supports this argument.</p><p>Interestingly, both T5 models leave the wrong input in (3) uncorrected. The fact that Leinster is a region in Ireland and not, e.g., a neighborhood of the city Dublin is probably unknown to T5. It seems that T5 falls back to the order of words in the input in such a case. Examples (4)-(7) also illustrate this behavior. While the well-known entities "Rome" and "Italy" produce a similar behavior as "Ohio" and "Cleveland", i.e., T5 order complies with generating a false statement and T5 shuf rather follows its factual knowledge, lowercasing the entity names changes that. With the unknown entities "rome" and "italy", both (case-sensitive) models fall back to the order of the input for their generations.</p><p>This experiment is related to testing factuality and trustworthiness of text generation models <ref type="bibr" target="#b39">(Wiseman et al., 2017;</ref><ref type="bibr">Falke et al., 2019)</ref>. It is important for a generation model to stay true to its input as its practical usefulness can be severely limited otherwise. We are the first to detect this issue with the use of PLMs for data-to-text tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We investigated two pretrained language models (PLMs) for graph-to-text generation and show that language model adaptation (LMA) and supervised task adaptation (STA) pretraining strategies are beneficial for this task. Our approaches outperform the state of the art by a substantial margin on three graph-to-text benchmarks. We also examined to what extent the graph structure is taken into account for the text generation process, and we found evidence that factual knowledge is a strong guide for these models. We believe that PLMs will play an important role in future endeavors to solve graphto-text generation tasks and we expect our work to serve as guidance and as a strong baseline for them. A promising direction for future work is to explore ways of injecting a stronger graph-structural bias into large PLMs to thus possibly leveraging their strong language modeling capabilities and keeping the output faithful to the input graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>In this supplementary material, we provide: (i) additional information about the data used in the experiments, and (ii) results that we could not fit into the main body of the paper.</p><p>A.1 Input Graph Size <ref type="figure" target="#fig_2">Figure 4</ref> visualizes the T5 small 's performance with respect to the number of input graph triples in WebNLG dataset. We observe that T5 order and T5 shuf perform similarly for inputs with only one triple but that the gap between the models increases with larger graphs. While it is obviously more difficult to reconstruct a larger graph than a smaller one, this also suggests that the graph structure is more taken into account for graphs with more than 2 triples. For the unseen setting, the performance gap for these graphs is even larger, suggesting that the PLM can make more use of the graph structure when it has to.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Cross-domain Pretraining</head><p>For a given task, it is not always possible to collect closely related data -as we saw, e.g., for WebNLG. We therefore investigate how STA can help in a cross-domain setting for different KG-totext benchmarks. <ref type="table" target="#tab_14">Table 7</ref> shows the results using BART base and T5 base . While the texts in KGAIA and AGENDA share the domain of scientific abstracts, texts in WebNLG are more general. Also note that WebNLG graphs do not share any relations with the other KGs. For BART base , STA increases the performance in the cross-domain setting in most of the cases. For T5 base , STA in KGAIA improves the performance on WebNLG.</p><p>In general, our experiments indicate that exploring additional pretraining for graph-to-text generation can improve the performance even if the data do not come from the same domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained on Fine-tuned &amp; Evaluated on</head><p>WebNLG      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Performance of BART base and T5 base in the dev set when experimenting with different amounts of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>chrF++ scores with respect to the number of triples for WebNLG seen and unseen test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Castro Ferreira et al. (2019) 51.68 56.35 38.92 32.00 41.00 21.00 ---Moryossef et al. (2019) 47.24 53.30 34.41 39.00 44.00 37.00 62.74 41.53 40.18 44.45 35.36 70.02 76.68 62.76 BART large 54.72 63.45 43.97 42.23 45.49 38.61 72.29 77.57 66.53 T5 small 56.34 65.05 45.37 42.78 45.94 39.29 73.31 78.46 67.69 T5 base 59.17 64.64 52.55 43.19 46.02 41.49 74.82 78.40 70.92 T5 large 59.70 64.71 53.67 44.18 45.85 42.26 75.40 78.29 72.25 Table 1: Results on WebNLG. A, S and U stand for all, seen, and unseen partitions of the test set, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">BLEU</cell><cell cols="2">METEOR</cell><cell></cell><cell></cell><cell>chrF++</cell><cell></cell></row><row><cell>Model</cell><cell>A</cell><cell>S</cell><cell>U</cell><cell>A</cell><cell>S</cell><cell>U</cell><cell>A</cell><cell>S</cell><cell>U</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Schmitt et al. (2020)</cell><cell>-</cell><cell>59.39</cell><cell>-</cell><cell>-</cell><cell>42.83</cell><cell>-</cell><cell>-</cell><cell>74.68</cell><cell>-</cell></row><row><cell>Ribeiro et al. (2020)</cell><cell>-</cell><cell>63.69</cell><cell>-</cell><cell>-</cell><cell>44.47</cell><cell>-</cell><cell>-</cell><cell>76.66</cell><cell>-</cell></row><row><cell>Zhao et al. (2020a)</cell><cell cols="6">52.78 64.42 38.23 41.00 46.00 37.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>based on PLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Harkous et al. (2020)</cell><cell>52.90</cell><cell>-</cell><cell>-</cell><cell>42.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Kale (2020)</cell><cell cols="6">57.10 63.90 52.80 44.00 46.00 41.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Radev et al. (2020)</cell><cell cols="6">45.89 52.86 37.85 40.00 42.00 37.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">BART base 53.11 Model BLEU</cell><cell>M</cell><cell>BT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ribeiro et al. (2019)</cell><cell cols="2">27.87 33.21</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zhu et al. (2019)</cell><cell cols="2">31.82 36.38</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Cai and Lam (2020b) 29.80 35.10</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zhao et al. (2020b)</cell><cell cols="2">32.46 36.78</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wang et al. (2020)</cell><cell cols="2">33.90 37.10</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yao et al. (2020)</cell><cell cols="2">34.10 38.10</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>based on PLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mager et al. (2020)</cell><cell cols="2">33.02 37.68</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Harkous et al. (2020)</cell><cell cols="2">37.70 38.90</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BART base BART large</cell><cell cols="3">36.71 38.64 52.47 43.47 42.88 60.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5 small T5 base T5 large</cell><cell cols="3">38.45 40.86 57.95 42.54 42.62 60.59 45.80 43.85 61.93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell cols="3">43.94 42.36 58.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5 large + LMA</cell><cell cols="3">46.06 44.05 62.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">BART large + STA (200K) 44.72 43.65 61.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">BART large + STA (2M) 47.51 44.70 62.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5 large + STA (200K) T5 large + STA (2M)</cell><cell cols="3">48.02 44.85 63.86 49.72 45.43 64.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on AMR-to-text generation for the AMR17 test set. M and BT stand for METEOR and BLEURT, respectively. Bold (Italic) indicates the best score without (with) task-adaptive pretraining.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>shows the results for the WebNLG test set. Neural pipeline models<ref type="bibr" target="#b22">(Moryossef et al., 2019;</ref> Castro Ferreira et al., 2019)  achieve strong performance in the unseen dataset. On the other</figDesc><table><row><cell>Model</cell><cell>BLEU</cell><cell>M</cell><cell>BT</cell></row><row><cell>Koncel et al. 2019</cell><cell cols="2">14.30 18.80</cell><cell>-</cell></row><row><cell>An (2019)</cell><cell cols="2">15.10 19.50</cell><cell>-</cell></row><row><cell cols="3">Schmitt et al. (2020) 17.33 21.43</cell><cell>-</cell></row><row><cell cols="3">Ribeiro et al. (2020) 18.01 22.23</cell><cell>-</cell></row><row><cell>BART base BART large</cell><cell cols="3">22.01 23.54 -13.02 23.65 25.19 -10.93</cell></row><row><cell>T5 small T5 base T5 large</cell><cell cols="3">20.22 21.62 -24.10 20.73 21.88 -21.03 22.15 23.73 -13.96</cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell cols="3">25.30 25.54 -08.79</cell></row><row><cell>T5 large + LMA</cell><cell cols="3">22.92 24.40 -10.39</cell></row><row><cell>BART large + STA</cell><cell cols="3">25.66 25.74 -08.97</cell></row><row><cell>T5 large + STA</cell><cell cols="3">23.69 24.92 -08.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on AGENDA test set.</figDesc><table><row><cell>Bold (Italic)</cell></row><row><cell>indicates best scores without (with) task-adaptive pre-</cell></row><row><cell>training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>lists the results for the AGENDA test set. The models also show strong performance on this dataset. We believe that their capacity to generate</figDesc><table><row><cell>Model</cell><cell cols="2">AMR17</cell></row><row><cell></cell><cell>F</cell><cell>MS</cell></row><row><cell>Mager et al. (2020)</cell><cell>5.69 A</cell><cell>5.08 A</cell></row><row><cell>Harkous et al. (2020)</cell><cell>5.78 A</cell><cell>5.47 AB</cell></row><row><cell>T5 large</cell><cell>6.55 B</cell><cell>6.44 C</cell></row><row><cell>BART large</cell><cell>6.70 B</cell><cell>5.72 BC</cell></row><row><cell>Reference</cell><cell>5.91 A</cell><cell>-</cell></row><row><cell>Model</cell><cell cols="2">WebNLG</cell></row><row><cell></cell><cell>F</cell><cell>A</cell></row><row><cell cols="2">Castro Ferreira et al. (2019) 5.52 A</cell><cell>4.77 A</cell></row><row><cell>Harkous et al. (2020)</cell><cell cols="2">5.74 AB 6.21 B</cell></row><row><cell>T5 large</cell><cell>6.71 C</cell><cell>6.63 B</cell></row><row><cell>BART large</cell><cell>6.53 C</cell><cell>6.50 B</cell></row><row><cell>Reference</cell><cell>5.89 B</cell><cell>6.47 B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Fluency (F), Meaning Similarity (MS) and Ad-</cell></row><row><cell>equacy (A) obtained in the human evaluation. Differ-</cell></row><row><cell>ences between models which have a letter in common</cell></row><row><cell>are not statistically significant and where determined</cell></row><row><cell>by pair-wise Mann-Whitney tests with p &lt; 0.05.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Impact (measured with BLEU) of using a bag</cell></row><row><cell>of entities and relations (shuf ) as input for T5 small .</cell></row><row><cell>outperforms BART base in both datasets. In particu-</cell></row><row><cell>lar, with only 1% of training examples, the differ-</cell></row><row><cell>ence between T5 and BART is 4.98 and 5.64 BLEU</cell></row><row><cell>points for AMR and WebNLG, respectively. This</cell></row><row><cell>suggests that T5 is a good candidate to be employed</cell></row><row><cell>in low-resource graph-to-text tasks. Interestingly,</cell></row><row><cell>the amount of training data has very little influence</cell></row><row><cell>on the models' performance for AGENDA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Ohio ‚Ä¢ is Part Of ‚Ä¢ Cleveland Ohio is part of Cleveland. Cleveland is part of Ohio. ohio is part of cleveland. (2) F ‚Ä¢ United States ‚Ä¢ is Part Of Amarillo, Texas is part of Amarillo, Texas is part of united states is part of ‚Ä¢ Amarillo ‚Ä¢ Texas the United States. the United States. amarillo, texas. (3) F ‚Ä¢ Leinster ‚Ä¢ is Part Of ‚Ä¢ Dublin Leinster is part of Dublin. Leinster is part of Dublin. leinster is part of dublin. (4) T ‚Ä¢ Italy ‚Ä¢ capital ‚Ä¢ Rome italy's capital is rome. Rome is the capital of Italy. rome is the capital of italy. (5) F ‚Ä¢ Rome ‚Ä¢ capital ‚Ä¢ Italy Rome's capital is Italy. Rome is the capital of Italy. rome is the capital of rome. (6) T ‚Ä¢ italy ‚Ä¢ capital ‚Ä¢ rome ‚Ä¢ capital ‚Ä¢ italy The capital of rome is italy. Italy is the capital of rome. -</figDesc><table><row><cell>T/F</cell><cell>Input Fact</cell><cell>T5 order</cell><cell>T5 shuf</cell><cell>CGE-LW</cell></row><row><cell cols="2">(1) F ‚Ä¢ ‚Ä¢ rome</cell><cell>Italy's capital is rome.</cell><cell>Italy's capital is rome.</cell><cell>-</cell></row><row><cell>(7) F</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Effect (measured with BLEU score) of crossdomain STA.</figDesc><table><row><cell></cell><cell cols="3">AMR17 WebNLG AGENDA</cell></row><row><cell>#Train #Dev #Test</cell><cell>36,521 1,368 1,371</cell><cell>18,102 872 1,862</cell><cell>38,720 1,000 1,000</cell></row><row><cell>#Relations Avg #Nodes</cell><cell>155 15.63</cell><cell>373 4.0</cell><cell>7 13.4</cell></row><row><cell>Avg #Tokens</cell><cell>16.1</cell><cell>31.5</cell><cell>157.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Statistics for the graph-to-text benchmarks.</figDesc><table><row><cell></cell><cell cols="3">Title Abstract KG</cell></row><row><cell>Vocab</cell><cell>48K</cell><cell>173K</cell><cell>113K</cell></row><row><cell>Tokens</cell><cell>2.1M</cell><cell>31.7M</cell><cell>9.6M</cell></row><row><cell>Entities</cell><cell>-</cell><cell>-</cell><cell>3.7M</cell></row><row><cell>Avg Length</cell><cell>11.1</cell><cell>167.1</cell><cell>-</cell></row><row><cell>Avg #Nodes</cell><cell>-</cell><cell>-</cell><cell>19.9</cell></row><row><cell>Avg #Edges</cell><cell>-</cell><cell>-</cell><cell>9.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Statistics for the KGAIA dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">chrF++ BS (F1) MS</cell></row><row><cell>Guo et al. (2019)</cell><cell>57.30</cell><cell>-</cell><cell>-</cell></row><row><cell>Ribeiro et al. (2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhu et al. (2019)</cell><cell>64.05</cell><cell>-</cell><cell>-</cell></row><row><cell>Cai and Lam (2020b)</cell><cell>59.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhao et al. (2020b)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. (2020)</cell><cell>65.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Yao et al. (2020)</cell><cell>65.60</cell><cell>-</cell><cell>-</cell></row><row><cell>based on PLMs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mager et al. (2020)</cell><cell>63.89</cell><cell>-</cell><cell>-</cell></row><row><cell>Harkous et al. (2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BART base BART large</cell><cell cols="3">66.65 95.22 60.78 71.06 96.08 65.74</cell></row><row><cell>T5 small T5 base T5 large</cell><cell cols="3">68.78 95.62 63.70 70.81 95.99 65.63 72.57 96.27 67.37</cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell cols="3">71.14 95.94 64.75</cell></row><row><cell>T5 large + LMA</cell><cell cols="3">72.83 96.32 67.44</cell></row><row><cell cols="4">BART large + STA (200K) 72.26 96.21 66.75</cell></row><row><cell>BART large + STA (2M)</cell><cell cols="3">73.58 96.43 68.14</cell></row><row><cell>T5 large + STA (200K) T5 large + STA (2M)</cell><cell cols="3">74.09 96.51 68.86 74.79 96.59 69.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Additional results for the AMR17 test set. BS, MS stand for BertScore, MoverScore, respectively. Bold (Italic) indicates the best score without (with) task-adaptive pretraining.</figDesc><table><row><cell>Model</cell><cell cols="3">chrF++ BS (F1) MS</cell></row><row><cell cols="2">Schmitt et al. (2020) 44.53</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ribeiro et al. (2020) 46.37</cell><cell>-</cell><cell>-</cell></row><row><cell>BART base BART large</cell><cell>48.02 50.44</cell><cell>89.36 88.74</cell><cell>34.33 32.24</cell></row><row><cell>T5 small T5 base T5 large</cell><cell>44.91 48.14 48.14</cell><cell>88.56 88.81 89.60</cell><cell>30.25 31.33 35.23</cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell>51.33</cell><cell>89.12</cell><cell>33.42</cell></row><row><cell>T5 large + LMA</cell><cell>49.37</cell><cell>89.75</cell><cell>36.13</cell></row><row><cell>BART large + STA T5 large + STA</cell><cell>51.63 50.27</cell><cell>89.27 89.93</cell><cell>34.28 36.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Additional results on AGENDA test set. Bold (Italic) indicates best scores without (with) task-adaptive pretraining.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and pretrained model checkpoints are available at https://github.com/UKPLab/plms-graph2text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The model architecture does not explicitly encode the graph structure, i.e., which entities are connected to each other, but has to retrieve it from a sequence that tries to encode this information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Please, refer to and<ref type="bibr" target="#b28">Raffel et al. (2019)</ref> for details about the self-supervised pretraining strategies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://catalog.ldc.upenn.edu/LDC2003T05 5 We filter out sentences that do not yield well-formed AMR graphs.6  Note that Gigaword and AMR17 share similar data sources.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We exclude AGENDA because its texts are scientific in nature and annotators are not necessarily AI experts.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Construction of the literature graph in semantic scholar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu-Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Skjonsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-3011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans -Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Repulsive bayesian sampling for diversified attention modeling</title>
	</analytic>
	<monogr>
		<title level="m">4th workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Bang An</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S√∂ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC&apos;07/ASWC&apos;07</title>
		<meeting>the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC&apos;07/ASWC&apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1209</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
	</analytic>
	<monogr>
		<title level="m">Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>essing and the 9th International Joint Conference on Natural Language essing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="4186" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The WebNLG challenge: Generating text from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00269</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasoviƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Have your text and use it too! end-to-end neural datato-text generation with semantic fidelity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Text-to-text pre-training for data-totext tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text Generation from Knowledge Graphs with Graph Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GPT-too: A language-model-first approach for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram√≥n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arafat</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Sultan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1846" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">Perez</forename><surname>Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>The Netherlands</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Language models as knowledge bases?</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">chrF: character n-gram f-score for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popoviƒá</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinand</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiachun</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadit</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Irwanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faiaz</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murori</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Tarabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<title level="m">Dart: Open-domain structured data record to text generation</title>
		<editor>Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and Richard Socher</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>text transformer. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancing AMR-to-text generation with dual graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1314</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3183" to="3194" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Claire Gardent, and Iryna Gurevych. 2020. Modeling global and local node contexts for text generation from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<title level="m">Philipp Dufter, Iryna Gurevych, and Hinrich Sch√ºtze. 2020. Modeling graph structure via relative position for better text generation from knowledge graphs</title>
		<imprint/>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structural information preserving for graph-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ante</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7987" to="7998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMRto-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GTR-LSTM: A triple encoder for sentence generation from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Amr-to-text generation with graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00297</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©mi</forename><surname>Louf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Morgan Funtowicz, and Jamie Brew</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch√©-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer for graphto-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging the structural gap between encoding and decoding for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2481" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="563" to="578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Line graph enhanced AMR-to-text generation with mix-order graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling graph structure in transformer for better AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1548</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5459" to="5468" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
