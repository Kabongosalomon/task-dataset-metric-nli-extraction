<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Independent Prediction to Reordered Prediction: Integrating Relative Position and Global Label Information to Emotion Cause Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
							<email>dingzixiang@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihui</forename><surname>He</surname></persName>
							<email>hehuihui1994@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Zhang</surname></persName>
							<email>zhangmengran@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
							<email>rxia@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Independent Prediction to Reordered Prediction: Integrating Relative Position and Global Label Information to Emotion Cause Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion cause identification aims at identifying the potential causes that lead to a certain emotion expression in text. Several techniques including rule based methods and traditional machine learning methods have been proposed to address this problem based on manually designed rules and features. More recently, some deep learning methods have also been applied to this task, with the attempt to automatically capture the causal relationship of emotion and its causes embodied in the text. In this work, we find that in addition to the content of the text, there are another two kinds of information, namely relative position and global labels, that are also very important for emotion cause identification. To integrate such information, we propose a model based on the neural network architecture to encode the three elements (i.e., text content, relative position and global label), in an unified and end-to-end fashion. We introduce a relative position augmented embedding learning algorithm, and transform the task from an independent prediction problem to a reordered prediction problem, where the dynamic global label information is incorporated. Experimental results on a benchmark emotion cause dataset show that our model achieves new state-ofthe-art performance and performs significantly better than a number of competitive baselines. Further analysis shows the effectiveness of the relative position augmented embedding learning algorithm and the reordered prediction mechanism with dynamic global labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Emotion cause identification, as a sub-task of emotion analysis, aims at identifying the potential causes that lead to a certain emotion expression in text. It has gained much attention in recent years due to its wide potential applications <ref type="bibr" target="#b10">Russo et al. 2011;</ref><ref type="bibr" target="#b3">Gao, Xu, and Wang 2015a;</ref><ref type="bibr" target="#b2">Cheng et al. 2017</ref>). In the benchmark emotion cause dataset released by <ref type="bibr" target="#b5">(Gui et al. 2016a</ref>), emotion cause identification was formally defined as a binary clause classification problem that detects for each clause in the document whether this clause contains potential emotion causes given the annotation of emotion. In this paper, we refer to a instance of the corpus as a "document", although it is normally a short document.</p><p>For example, in the following document: Yesterday morning, a policeman visited the old man with the lost money, and told him that &lt;cause&gt; the thief was caught &lt;/cause&gt;. The old man was very &lt;emotion word&gt; happy &lt;/emotion word&gt;. Accompanied by the policeman, he deposited the money in the bank. There are six clauses in this document. The emotion "happy" is contained in the fourth clause (we denote this clause as "emotion clause", which refers to a clause that contains emotion expressions), and the corresponding cause "the thief was caught" is in the third clause. The emotion cause identification task was actually defined as a clauselevel binary classification problem in <ref type="bibr" target="#b5">(Gui et al. 2016a;</ref>). The goal is to detect for each clause in a document, whether the clause contains potential emotion causes (i.e., whether the clause is an emotion cause) given the annotation of emotion. As <ref type="bibr" target="#b5">(Gui et al. 2016a)</ref> pointed, this task is more difficult than emotion classification because it requires a deeper understanding of the document.</p><p>In the past, several techniques including rule based methods and traditional machine learning methods have been proposed to address this problem based on manually designed rules and features <ref type="bibr">Lee et al. 2013;</ref><ref type="bibr" target="#b5">Gui et al. 2016a</ref>). More recently, some deep neural networks have also been applied to this task, with the attempt to automatically capture the causal relationship of emotion and its causes. For example, <ref type="bibr" target="#b2">(Cheng et al. 2017)</ref> proposed to use a long short-term memory (LSTM) network to address this task.  introduced a new deep memory network architecture which encodes the context of each word by multiple memory slots.</p><p>While most of these methods emphasized on the usage of content information, we find in this work, in addition to the content information, there are two other kinds of information that are also very important for emotion cause identification:</p><p>• "relative position", which denotes the relative distance to the emotion clause;</p><p>• "global label", which means the predicted labels of the other clauses in the whole document.</p><p>However, most of the previous work including the stateof-the-art predicted clauses independently and ignored the global label information. It was hence possible that none of the clauses in the document was predicted as the emo-tion cause, or too many clauses in the same document were predicted as emotion causes. Although some of them have already used the position information in their neural networks for emotion cause identification, but their methods were relatively simple. For example, <ref type="bibr" target="#b5">(Gui et al. 2016a;</ref>) concatenated position embedding with the representation of a clause. To the best of our knowledge, none of the previous work has explicitly used the global label information for emotion cause identification.</p><p>In this paper, we propose a unified neural architecture for emotion cause identification that takes the relative position and global label information into account. Firstly, we attach a relative position embedding to word embedding, and propose an encode-decode-style module to help learn a relative position augmented embedding. Secondly, we sort the clauses according to the absolute value of relative position in an ascending order, and transform the emotion cause identification task from an independent prediction problem into a reordered prediction problem. The predictions of the previous clauses are used as features for predicting the subsequent clauses in a dynamic manner, so as to integrate the global label information.</p><p>We evaluated our approach on the benchmark emotion cause dataset <ref type="bibr" target="#b5">(Gui et al. 2016a</ref>). The experimental results indicate that our model achieves new state-of-the-art performance in emotion cause identification and performs significantly better than a number of competitive baselines. Further in-depth analysis has proved the effectiveness of the relative position augmented embedding learning algorithm and the reordered prediction mechanism with dynamic global labels.  first presented a task on emotion cause identification. They manually constructed a small-scale corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, <ref type="bibr">(Lee et al. 2013</ref>) developed a rule-based approach to detect emotion causes. (Neviarouskaya and Aono 2013) also explored linguistic relations of emotion cause by applying syntactic and dependency parser. Some studies extended rule-based approaches to informal texts such as microblog texts.  tried to extract the reasons of emotions by introducing knowledge and theories from other fields such as sociology. Based on this idea, they built an automatic rule-based system to detect the cause event of each emotional microblog post. <ref type="bibr" target="#b3">(Gao, Xu, and Wang 2015a)</ref> and <ref type="bibr" target="#b3">(Gao, Xu, and Wang 2015b</ref>) designed a set of complex rules considering a cognitive emotion model and emotions categories to extract emotion cause in Chinese microblogs. More recently, <ref type="bibr" target="#b12">(Yada et al. 2017</ref>) proposed a bootstrapping technique to automatically obtain conjunctive phrases as textual cue patterns for emotion cause identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Most of the above work is based on rules, there were also some machine learning based methods.  and ) used the extended linguistic rules of (Lee, Chen, and Huang 2010) as features, and chose machine learning model, such as SVM, to detect emotion causes. <ref type="bibr" target="#b10">(Russo et al. 2011</ref>) proposed a crowd-sourcing approach to build a common-sense knowledge base which is related to emotion causes. However, automatically expanding the common sense knowledge base is very challenging. <ref type="bibr" target="#b4">(Ghazi, Inkpen, and Szpakowicz 2015)</ref> used conditional random fields (CRFs) to label the semantic roles on the emotion-related text from an English corpus. <ref type="bibr" target="#b11">(Song and Meng 2015)</ref> utilized context-sensitive topical pagerank to detect meaningful multi-word expressions as emotion causes. Recently, some researchers explored structurebased representation of events to improve the performance of emotion cause detection. <ref type="bibr" target="#b5">(Gui et al. 2016a</ref>) and <ref type="bibr" target="#b6">(Gui et al. 2016b</ref>) released an emotion cause dataset and proposed a seven-tuple representation structure to describe emotion cause events and employed SVM-based methods.  proposed an emotion cause detection method by using event extraction framework, in which a tree structure-based representation method is used to represent the events.</p><p>Deep learning techniques have also been applied to emotion cause identification. <ref type="bibr" target="#b2">(Cheng et al. 2017</ref>) used long shortterm memory (LSTM) for emotion cause detection. <ref type="bibr" target="#b6">(Gui et al. 2017</ref>) proposed a new deep memory network architecture to model the context of each word simultaneously by multiple memory slots. In this work, we also address the emotion cause identification task under deep neural networks. We emphasize on the usage of relative position and global label information for emotion cause identification. Although some existing work have also used the position information. However, they simply used the position as a feature or embedding for training their model <ref type="bibr" target="#b5">(Gui et al. 2016a;</ref>. The global label information were also ignored in their methods. In comparison, we use relative position and global label in a novel and more efficient manner: 1) We propose an encode-decodestyle method to incorporate relative position information into clause representation. 2) We predict the clauses in the same document in a special order according to the relative position, and update "dynamic global label" with previous predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>The motivation of this paper is based on the following two observations.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we first summarize the percentage of a clause being emotion cause at different positions, based on the emotion cause benchmark corpus <ref type="bibr" target="#b5">(Gui et al. 2016a</ref>). We label the position of the emotion clause as 0, and use "relative position" to denotes the relative distance to the emotion clause. For example, −1 denotes one clause left to the emotion clause; +2 denotes two clauses right to the emotion clause. It can be observed that the clauses that are closer to the emotion clause are more likely to be an emotion cause. In detail, the clause at position −1 has the highest probability of being an emotion cause. The probabilities of clauses with larger distance reduce gradually. The ascending order of the distance is 0, −1, +1, −2, +2, −3, +3, · · ·. This is natural since people are accustomed to describe the cause near the emotion expression.  In <ref type="table" target="#tab_1">Table 2</ref>, we furthermore summarize the proportion of documents with different number of emotion causes. In each document, only one emotion was annotated. It can be seen that 97.20% of the documents have only one corresponding emotion cause. Only a small number of documents have two or three corresponding emotion causes. The percentages are 2.66% and 0.14% respectively. Documents with more than three emotion causes do not appear in the corpus. However, most of the previous work predicted clauses independently and ignore the global label information. It is hence possible that none of the clauses in the document is predicted as an emotion cause, or too many clauses in the same document are predicted as emotion causes.</p><p>Jointly observing <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>, we infer that in addition to relative position, the global label information is also very important for predicting individual clauses. Let us consider the ascending order of relative positions according to their absolute value (i.e., 0, −1, +1, −2, +2, −3, +3, · · ·). If the clause in the front (which is closer to the emotion clause) was predicted to be a cause with a high probability, the probability of subsequent clause being a cause should be reduced, as most of the documents have only one cause. On the contrary, if none of the previous clause was predicted as an emotion cause, the probability of subsequent clause should be increased.</p><p>In summary, we conclude based on the above analysis that</p><p>• the relative position information (i.e., the relative distance to the emotion clause) plays an important role in emotion cause identification;</p><p>• the global label information (i.e., the predictions of other clauses) must be considered when predicting individual clause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In this section, we first introduce the overall architecture of our model. Then we describe the "relative position augmented embedding learning" (PAE for short) module and the "reordered prediction with dynamic global labels" (DGL for short) module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Description</head><p>The overall architecture of the proposed method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Following the state-of-the-art emotion cause identification works ), we consider the task as a clause-level binary classification problem. For each clause in a document, the model predicts whether this clause is the emotion cause. Formally, a document consisting of N clauses can be rep-</p><formula xml:id="formula_0">resented as D = {c 1 , c 2 , · · · , c N } , where c i is the ith clause.</formula><p>The ith clause consisting of l i words can be represented as</p><formula xml:id="formula_1">c i = {w i 1 , w i 2 , · · · , w i li }. Each word w i j is represented as a word embedding v w i j ∈ R m .</formula><p>In order to capture the text content information, we employ a Bi-directional Long Short-Term Memory (Bi-LSTM) structure to encode each clause separately. Specifically, we feed c i into Bi-LSTM and get hidden states</p><formula xml:id="formula_2">{h i 1 , h i 2 , · · · , h i li }, where h i j ∈ R d</formula><p>is the hidden state of the jth word in c i . Then we adopt the attention mechanism to get the clause representation r i , where r i ∈ R d is a weighted sum of hidden states. Here we omit the details of Bi-LSTM and attention for limited space, readers can refer to (Graves, Mohamed, and Hinton 2013) and <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>.</p><p>The above standard Bi-LSTM and attention architecture only encode the content information. To incorporate the relative position and global label information, we introduce two mechanisms into the network.</p><p>Firstly, we attach a relative position embedding to word embedding, and proposed an encode-decode-style module by using relative position as supervision to help learn a relative position augmented embedding.</p><p>Secondly, we sort the clauses according to the absolute value of relative position in an ascending order, and transform the task from an independent prediction problem into a reordered prediction problem. The predictions of the previous clauses are used as features for predicting the subsequent clauses in a dynamic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Position Augmented Embedding Learning</head><p>We define the relative position of the ith clause as P i , where P i ∈ {· · · , −3, −2, −1, 0, +1, +2, +3, · · ·}. Then we propose to learn an embedding vector for each relative position.</p><p>Firstly, we attach the position embedding to each word embedding to shape a position-augmented word embedding. We use v Pi ∈ R n to represent the embedding of relative position P i , where n is the dimension of the embedding. The position-augmented embedding of a word w i j for a specific relative position P i is:</p><formula xml:id="formula_3">e w i j = v w i j ⊕ v Pi ,<label>(1)</label></formula><p>where operator ⊕ indicates the concatenation operation, thus the dimension of e w i j is (m + n). Then the position- augmented word embedding of each word in c i is feed into Bi-LSTM and attention module to get the positionaugmented clause representation r i . Secondly, we introduce a relative position embedding learning module by using relative position as a kind of supervision, to ensure that the relative position information can be better encoded. Specifically, we take the clause representation r i as feature to predict the relative position of clauses:</p><formula xml:id="formula_4">p i = softmax(W s r i + b s ),<label>(2)</label></formula><p>where W s ∈ R |D|×d and b s ∈ R |D| are weight matrix and bias for softmax classification.p i ∈ R |D| is the predicted distribution of relative position, where |D| is the number of different relative positions. The cross-entropy error of relative position prediction on a document is defined as loss p :</p><formula xml:id="formula_5">loss p = − N i=1 |D|−1 j=0 p i,j · log(p i,j ),<label>(3)</label></formula><p>where N denotes the number of clauses in the document, and p i is the ground truth distribution of relative position. The learning algorithm can be actually regarded as a generalized autoencoder. The autoencoder consists of encoder and decoder, and learn the codings in an unsupervised manner. Our method does not uncompress the middle code into the original signal (relative position embeddings) during decoding, but learns the codings under the supervision of relative position value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reordered Prediction with Dynamic Global Labels</head><p>As we have mentioned, most of the previous work predicted clauses independently and ignored global label information.</p><p>It is hence possible that none of the clauses in the document is predicted as emotion causes, or too many clauses in the same document are predicted as emotion causes. To address this, we propose to sort the clauses according to relative position first and then integrate the global label information in a dynamic manner.</p><p>Reordered Prediction Firstly, we sort the clauses according to the absolute value of relative position in an ascending order (i.e., 0, −1, +1, −2, +2, −3, +3, · · ·). The order of clauses before and after reordering is denoted as P o and P r respectively. Taking <ref type="figure" target="#fig_2">Figure 2</ref> as an example, P o is : [−3, −2, −1, 0, +1, +2], after reordering, P r is :</p><formula xml:id="formula_6">[0, −1, +1, −2, +2, −3].</formula><p>We then transform the emotion cause identification task from an independent prediction problem into a reordered prediction problem. The clauses in a document are predicted sequentially according to the order of P r . In this order, the clauses which are closer to the emotion clause (more likely to be an emotion cause) are predicted first, and the clauses which have a larger distance to the emotion clause (less likely to be an emotion cause) are predicted later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Global Label</head><p>In the order of P r , if a clause in the front was predicted to be emotion cause with a high probability, the probability the subsequent clause being an emotion cause should be reduced, because most of the documents have only one emotion cause (according to <ref type="table" target="#tab_0">Table 1)</ref>. On the contrary, if none of the previous clause was predicted as an emotion cause, the probability the subsequent clause should be increased.</p><p>To meet this end, we propose a dynamic global label feature template to record the predictions of the previous clauses to help the prediction of the subsequent clause.</p><p>Yesterday morning a policeman visited the old man with the lost money and told him that the thief was caught</p><p>The old man was very happy Accompanied by the policeman he deposited the money in the bank</p><p>The old man was very happy   Formally, we use a vector DGL ∈ R q to represent the prediction of all clauses (reordered) in a document, where q serves as the maximum length of documents.</p><p>Firstly, the vector DGL dynamically changes during the reordered prediction of clauses. For a document consisting of N clauses, our model predict for N times. For the first prediction, the vector DGL is set to DGL 0 = [0, 0, 0, · · ·], then the clause at relative position P r 0 is predicted. If the clause is predicted as emotion cause, then DGL is set to DGL 1 = [1, 0, 0, · · ·], otherwise DGL 1 = [−1, 0, 0, · · ·]. Then we predict the clause at relative position P r 1 , and update DGL to DGL 2 according to the prediction result of current clause. We repeat this process until all clauses are predicted.</p><p>Secondly, for the prediction of ith clause c i , we take the concatenation of r i and DGL i−1 as features for classification. r i encodes the text content and relative position information, and DGL i−1 encodes the global label information. The prediction of the clause is calculated as follows:</p><formula xml:id="formula_7">y i = softmax(W c [r i ; DGL i−1 ] + b c ),<label>(4)</label></formula><p>where [r i ; DGL i−1 ] ∈ R d+q is concatenation of the two vectors. W c ∈ R 2×(d+q) and b c ∈ R 2 are weight matrix and bias.ŷ i ∈ R 2 is the predicted distribution of emotion cause. The cross-entropy error of emotion cause prediction on the whole document is defined as loss c :</p><formula xml:id="formula_8">loss c = − N i=1 1 j=0 y i,j · log(ŷ i,j ),<label>(5)</label></formula><p>where y i is the ground truth distribution of emotion cause of the ith clause. The final loss of our model is a weighted sum of loss p and loss c with L2-regularization term as follows:</p><formula xml:id="formula_9">loss = λ p loss p + λ c loss c + λ||θ|| 2 ,<label>(6)</label></formula><p>where λ p , λ c , λ is the weight of loss p , loss c , L2regularization term respectively and θ denotes the parameter set. It should be noted that the dynamic global label is updated by true labels of each clause in the training phase to accelerate the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset and Experimental Settings</head><p>We evaluated our proposed model on a Chinese emotion cause corpus <ref type="bibr" target="#b5">(Gui et al. 2016a</ref>) 1 , which is the mostly used dataset for emotion cause identification. The same as <ref type="bibr" target="#b6">(Gui et al. 2017)</ref>, we stochastically select 90% of the data as training data and the remaining 10% as testing data. Also, in order to obtain statistically credible results, we repeat the experiments 25 times and report the average result. The precision, recall, and F score are used as the metrics for evaluation. These metrics in emotion cause identification are defined by:</p><formula xml:id="formula_10">P = correct causes proposed causes ,<label>(7)</label></formula><p>R = correct causes annotated causes ,</p><formula xml:id="formula_11">F = 2 × P × R P + R .<label>(8)</label></formula><p>proposed causes denotes the number of clauses that are predicted to be emotion cause, annotated causes denotes the number of clauses that are labeled as emotion cause and the correct causes means the number of clause that are both labeled as emotion cause and predicted to be emotion cause.</p><p>We use word vectors that were pre-trained on the 1.1 million Chinese Weibo corpora provided by NLPCC2017 2 with word2vec 3 <ref type="bibr" target="#b9">(Mikolov et al. 2013)</ref> toolkit. Similar performance can be obtained by using the embedding provided in . The dimensions of word embedding and position embedding are set to 200 and 50, respectively. The hidden units of LSTM is set to 100. All weight matrices and bias are randomly initialized by a uniform distribution U (−0.01, 0.01). The code has been made publicly available through Github 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and Comparison</head><p>We compare our model with the following baseline methods:</p><p>• RB (Rule based method): The method based on manually defined linguistic rules proposed in (Lee, Chen, and Huang 2010);</p><p>• CB (Common-sense based method): The approach is based on knowledge and is proposed by <ref type="bibr" target="#b10">(Russo et al. 2011</ref>);</p><p>• RB+CB+ML (Machine learning method trained from rule-based features and common-sense knowledge base): This method uses rules and facts in a knowledge base as features for classification, which is proposed by );</p><p>• SVM: This is a SVM classifier that uses the unigram, bigram and trigram features. It is a baseline previously used in );</p><p>• Word2vec: This is a SVM classifier using word representations learned by Word2vec as features;</p><p>• Multi-kernel: This method uses the multi-kernel method <ref type="bibr" target="#b5">(Gui et al. 2016a</ref>) to identify the emotion cause;</p><p>• CNN: The convolutional neural network for sentence classification (Kim 2014);</p><p>• Memnet: The deep memory network proposes by . Word embeddings are pre-trained by skipgrams and the number of hops is set to 3;</p><p>• ConvMS-Memnet: The convolutional multiple-slot deep memory network proposes by . It is the state-of-the-art method for emotion cause identification. We use the best performance reported in their paper. 4 https://github.com/NUSTM/PAEDGL Results of PAE-DGL and baseline methods are listed in <ref type="table" target="#tab_3">Table 3</ref>. It can be seen that our method achieves significant improvement in precision, which is 5.43% higher than the best result of previous methods. As for the results of recall, CB (common-sense based method) performs the best and reaches 71.3%. However, its precision is quite low, resulting in poor performance in F-measure. Finally, our method outperforms the state-of-the-art method ConvMS-Memnet by 2.87% in F-measure. This shows that our model can better identify emotion causes by using text content, relative position and global label information reasonably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effect of PAE and DGL</head><p>To further explore the factors that bring improvement to the model, we designed three experiments:</p><p>• Bi-LSTM: We only use Bi-LSTM with attention to get clause representation, which is the only feature we use for classification. This method only uses content information</p><p>• PAE: We add PAE module to the Bi-LSTM model, and denote this method as PAE. This method uses text content and relative position information.</p><p>• PAE-DGL: This is the complete model proposed in this paper, which jointly uses the text content, relative position and global label information.</p><p>The results are listed in <ref type="table" target="#tab_4">Table 4</ref>. We can obviously find that Bi-LSTM performs the worst, which indicates the text content information is not enough for emotion cause identification. PAE further incorporates position information based on Bi-LSTM and obtain significant improvement in Fmeasure. Moreover, based on PAE, we further incorporates dynamic label information in PAE-DGL, and the F-measure improves by 4.06%. In addition, we conducted an in-depth analysis on what is gained by using the relative position information and global label.</p><p>By observing the documents that are correctly classified by PAE but misclassified (contain misclassified clauses) by BiLSTM, we found that most of these documents were predicted to be without any cause by BiLSTM. Further quantitative analysis revealed that approximately 65.4% of documents were predicted by BiLSTM as not containing any cause. In contrast, this ratio drops to 12.8% for PAE. To conclude, the relative position plays an indicative role for predicting emotion causes, which can significantly improve the recall.</p><p>By observing the documents that are correctly classified by PAE-DGL but misclassified by PAE, we found that most of these documents were predicted by PAE as containing two causes, however, only one of the two causes was correctly predicted. To conclude, the global label information  can effectively avoid predicting too many causes in the same document, which can significantly improve the precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Evaluation of Different Ways of Modeling Positions</head><p>In this section, we explore different ways of using relative position information. The easiest way is to use the relative position as the last word of the clause, and learn a position embedding with the same dimension as word embedding. We call this way as position serving as the last word (PL). Another way is to take position embedding as feature for classification, that is, to concatenate the position embedding into the representation of the clause. We call this way as position embedding in clause level (PEC). PEC is the way used by <ref type="bibr" target="#b5">(Gui et al. 2016a;</ref>). The last way of using position information is our proposed PAE. <ref type="table" target="#tab_5">Table 5</ref> shows the results of PL, PEC and PAE. Here we use the Bi-LSTM to encode each word in the clauses. Thus the dimension of the clause representation equals to the hidden units of Bi-LSTM. However, PEC concatenates the position embedding into the representation of the clause, thus the feature dimension is much bigger than that of the other methods and may cause over-fitting. In contrast, PAE incorporates position information without increasing feature dimensions. By taking the encode-decode-style, each clause representation is forced to encode more position information. According to <ref type="table" target="#tab_5">Table 5</ref>, PAE performs better than the other methods, which indicates that PAE can make use of position information more sufficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions on Reordered Prediction with DGL Information</head><p>In order to explore the impact of using different ways of clause reordering on the model, we conduct the following experiments:</p><p>• DGL-P o : We do not reorder the clauses in the PAE-DGL model. Instead, we make predictions based on the order that they appear in the document. • DGL-P r : This is the complete model proposed in this paper. We predict the clauses which are closer to the emotion clause first. <ref type="table" target="#tab_6">Table 6</ref> shows the experimental results of PAE-DGL with different orders. PAE-DGL performs much better than PAE- DGL and achieves 4.78% improvement in F-measure. This indicates that the order of clauses plays an important role on the task of emotion cause identification. It also shows that the proposed order is significantly better than the default order.</p><p>Exploring The Upper Bound of DGL In the above experiments, the DGL feature in the training phase is obtained by using true labels, and the DGL feature in the test phase is based on the predicted labels.</p><p>We further conduct an experiment by using the true labels of the test data to construct the DGL feature. Note that this can not be applied to real application. Our goal is only to explore the upper bound of the effect of DGL feature. Specifically, we comprare the following two models:</p><p>• DGL: The DGL feature in the test phase is based on the predicted labels.</p><p>• DGL-Upper-Bound: The DGL feature in the test phase is obtained by using true labels.</p><p>The experimental results are shown in <ref type="table" target="#tab_7">Table 7</ref>. We can find that the F-measure of DGL-Upper-Bound is 5.04% higher than that of DGL. This further shows that 1) the prediction of the clauses in the same document can affect each other, and the DGL module can effectively capture this constraint information, and 2) knowing the overall prediction of the current document can effectively guide the prediction of the current clause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this work, we find that in addition to the content information, the relative position and global label information are also important factors for emotion cause identification. However, the use of the such information in previous work was either too simple or completely ignored. We propose a model based on the neural network architecture to incorporate three factors (i.e., content, relative position and global label) in an unified and end-to-end fashion. The relation position is encoded as an embedding attached to the word embedding, an encode-decode-style module was further proposed to help better learn the embedding by using relative position as supervision. The clauses in a document is then sorted according to their relative positions. Instead of predicting clauses independently, we consider the emotion cause identification task as a reordered clause prediction problem. The predictions of the previous clauses are used as a kind of global label information for predicting the subsequent clauses in a dynamic manner. The experimental results on a benchmark emotion cause dataset showed that our approach achieves new state-of-the-art performance in emotion cause identification and performs significantly better than a number of competitive baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of PAE-DGL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An example of clause reordering with dynamic global labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The percentage of a clause being emotion cause at different relative positions.</figDesc><table><row><cell cols="3">Relative Position Number Percentage</cell></row><row><cell>-3</cell><cell>37</cell><cell>1.71%</cell></row><row><cell>-2</cell><cell>167</cell><cell>7.71%</cell></row><row><cell>-1</cell><cell>1,180</cell><cell>54.45%</cell></row><row><cell>0</cell><cell>511</cell><cell>23.58%</cell></row><row><cell>+1</cell><cell>162</cell><cell>7.47%</cell></row><row><cell>+2</cell><cell>48</cell><cell>2.22%</cell></row><row><cell>+3</cell><cell>11</cell><cell>0.51%</cell></row><row><cell>Others</cell><cell>42</cell><cell>1.94%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: The proportion of documents with different number</cell></row><row><cell>of emotion causes.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Number Percentage</cell></row><row><cell>Document with one cause</cell><cell>2046</cell><cell>97.20%</cell></row><row><cell>Document with two causes</cell><cell>56</cell><cell>2.66%</cell></row><row><cell>Document with three causes</cell><cell>3</cell><cell>0.14%</cell></row><row><cell>All</cell><cell>2105</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with existing methods using precision, recall, and F-measure as metrics. The best result of each column is highlighted in bold.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>RB</cell><cell cols="3">0.6747 0.4287 0.5243</cell></row><row><cell>CB</cell><cell cols="3">0.2672 0.7130 0.3887</cell></row><row><cell>RB+CB</cell><cell cols="3">0.5435 0.5307 0.5370</cell></row><row><cell>RB+CB+ML</cell><cell cols="3">0.5921 0.5307 0.5597</cell></row><row><cell>SVM</cell><cell cols="3">0.4200 0.4375 0.4285</cell></row><row><cell>Word2vec</cell><cell cols="3">0.4301 0.4233 0.4136</cell></row><row><cell>CNN</cell><cell cols="3">0.6215 0.5944 0.6076</cell></row><row><cell>Multi-Kernel</cell><cell cols="3">0.6588 0.6927 0.6752</cell></row><row><cell>Memnet</cell><cell cols="3">0.5922 0.6354 0.6131</cell></row><row><cell cols="4">ConvMS-Memnet 0.7076 0.6838 0.6955</cell></row><row><cell>PAE-DGL</cell><cell cols="3">0.7619 0.6908 0.7242</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: The effect of PAE and DGL.</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="4">Bi-LSTM 0.5445 0.1663 0.2529</cell></row><row><cell>PAE</cell><cell cols="3">0.6897 0.6794 0.6836</cell></row><row><cell cols="4">PAE-DGL 0.7619 0.6908 0.7242</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of different ways of modeling positions.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>PL</cell><cell cols="3">0.7018 0.6496 0.6743</cell></row><row><cell cols="4">PEC 0.7081 0.5867 0.6405</cell></row><row><cell cols="4">PAE 0.6897 0.6794 0.6836</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of different orders for DGL.</figDesc><table><row><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="3">DGL-P o 0.6997 0.6561 0.6764</cell></row><row><cell cols="3">DGL-P r 0.7619 0.6908 0.7242</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Different dynamic global labels in DGL. P R F DGL 0.7412 0.6866 0.7129 DGL-Upper-Bound 0.7402 0.7880 0.7633</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://hlt.hitsz.edu.cn/?page id=694 2 http://www.aihuang.org/p/challenge.html 3 https://code.google.com/archive/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Zixiang Ding and Huihui He contributed equally to this paper. Rui Xia was the main contributor to the idea of this paper. The work was supported by the Natural Science Foundation of China (No. 61672288), and the Natural Science Foundation of Jiangsu Province for Excellent Young Scholars (No. BK20160085).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic constructions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An emotion cause corpus for chinese microblogs with multiple-user structures</title>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A rule-based approach to emotion cause detection for chinese micro-blogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">;</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Gao, Xu, and Wang</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="4517" to="4528" />
		</imprint>
	</monogr>
	<note>Emotion cause detection for chinese micro-blogs based on ecocc model</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic construction in chinese weibo text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkpen</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szpakowicz ; Ghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
	<note>Natural Language Processing and Chinese Computing</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event-driven emotion cause extraction with corpus construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1639" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotion cause extraction, a challenging task with corpus construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05482</idno>
		<idno>arXiv:1408.5882</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<meeting><address><addrLine>Kim; Lee, S. Y. M</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="390" to="416" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Detecting emotion causes with a linguistic rulebased approach 1</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A text-driven rule-based system for emotion cause detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Huang ; Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
		<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text-based emotion classification using emotion cause extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1742" to="1749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and Xu</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="932" to="936" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emocause: an easy-adaptable approach to emotion cause contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis</title>
		<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An ensemble approach for emotion cause detection with event extraction and multi-kernel svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="646" to="659" />
		</imprint>
	</monogr>
	<note>Detecting concept-level emotion cause in microblogging</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A bootstrap method for automatic rule acquisition on emotion cause extraction</title>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining Workshops (ICDMW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="414" to="421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
