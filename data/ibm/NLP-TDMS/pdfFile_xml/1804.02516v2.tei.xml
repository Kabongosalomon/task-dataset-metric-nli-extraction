<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><forename type="middle">Normale</forename><surname>Supérieure</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint understanding of video and language is an active research area with many applications. Prior work in this domain typically relies on learning text-video embeddings. One difficulty with this approach, however, is the lack of large-scale annotated video-caption datasets for training. To address this issue, we aim at learning text-video embeddings from heterogeneous data sources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training. As a result, our framework can learn improved text-video embeddings simultaneously from image and video datasets. We also show the generalization of MEE to other input modalities such as face descriptors. We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video-to-text retrieval tasks. Code: https://github.com/antoine77340/Mixture-of-Embedding-Experts</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic video understanding is an active research topic with a wide range of applications including activity capture and recognition, video search, editing and description, video summarization and surveillance. In particular, the joint understanding of video and natural language holds a promise to provide a convenient interface and to facilitate access to large amounts of video data. Towards this goal recent works study representations of vision and language addressing tasks such as visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, action learning and discovery <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, text-based event localization <ref type="bibr" target="#b5">[6]</ref> as well as video captioning, retrieval and summarization <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Notably, many of these works adopt and learn joint text-video representations where semantically similar video and text samples are mapped to close points in the joint embedding space. Such representations have been proven efficient for joint text-video modeling e.g., in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>Learning video representations is known to require large amounts of training data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. While video data with label annotations is already scarce, obtaining a large number of videos with text descriptions is even more difficult. Currently available video  <ref type="figure">Fig. 1</ref>: We learn a text-video embedding from diverse and partially available data sources. This example illustrates a joint text-video embedding trained from videos and images while combining descriptors for global appearance, motion, audio and faces. The key advantage of our method is the ability to combine samples with different subsets of modalities, e.g., images with no motion and videos with no faces or sound. datasets with ground truth captions include DiDeMo <ref type="bibr" target="#b5">[6]</ref> (27K unique videos), MSR-VTT <ref type="bibr" target="#b12">[13]</ref> (10K unique videos) and the MPII Movie Description dataset <ref type="bibr" target="#b13">[14]</ref> (120K unique videos). To compensate for the lack of video data, one possibility would be to pre-train visual representations on still image datasets <ref type="bibr" target="#b11">[12]</ref> with object labels or image captions such as ImageNet <ref type="bibr" target="#b14">[15]</ref>, COCO <ref type="bibr" target="#b15">[16]</ref>, Visual Genome <ref type="bibr" target="#b16">[17]</ref> and Flickr30k <ref type="bibr" target="#b17">[18]</ref>. Pre-training, however, does not provide a principled way of learning from different data sources and suffers from the "forgetting effect" where the knowledge acquired from still images is removed during fine-tuning on video tasks. More generally, it would be beneficial to have methods that can learn embeddings simultaneously from heterogeneous and partially-available data sources such as appearance, motion and sound but also from other modalities such as facial expressions or human poses.</p><p>In this work we address the challenge of learning from heterogeneous data sources. Our method is designed to learn a joint text-video embedding and is able to handle missing video modalities during training. To enable this property, we propose a Mixture-of-Embedding-Experts (MEE) model that computes similarities between text and a varying number of video modalities. The model is learned end-to-end and generates expert weights determining individual contributions of each modality. During training we combine image-caption and video-caption datasets and treat images as a special case of videos without motion and sound. For example, our method can learn an embedding for "Eating banana" even if "banana" only appears in training images but never in training videos (see <ref type="figure">Fig. 1</ref>). We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms all previously reported methods on both text-to-video and video-to-text retrieval tasks.</p><p>Our MEE model can be easily extended to other data sources beyond global appearance, motion and sound. In particular, faces in video contain valuable information including emotions, gender, age and identities of people. As not all videos contain people, faces constitute a typical case of a potentially missing data source for our model. To demonstrate the generalization of our model and to show the importance of faces for video retrieval, we compute facial descriptors for images and videos with faces. We then treat faces as an additional data source in the MEE model and aggregate facial descriptors within a video (see <ref type="figure" target="#fig_1">Fig. 2</ref>). The resulting MEE combining faces with appearance, motion and sound produces consistent improvements in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>This paper provides the following contributions: (i) First, we propose a new model for learning a joint text-video embedding called Mixture-of-Embedding-Experts (MEE). The model is designed to handle missing video modalities during training and enables simultaneous learning from heterogeneous data sources. (ii) We showcase two applications of our framework. First, we can data augment video-caption datasets with imagecaption datasets during training. We can also leverage face descriptors in videos to improve the joint text-video embedding. In both cases, we show improvements in several video retrieval benchmarks. (iii) By using MEE and leveraging multiple sources of training data we outperform state-of-the-art on the standard text-to-video and video-totext retrieval benchmarks defined by the LSMDC <ref type="bibr" target="#b13">[14]</ref> challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section we review prior work related to vision and language, video representations and learning from sources with missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vision and Language</head><p>There is a large amount of work leveraging language in computer vision. Language is often used as a more powerful and subtle source of supervision than predefined classes. One way to leverage language in vision is to find a joint embedding space for both visual and textual modalities <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. In this common embedding space, visual and textual samples are close if and only if they are semantically similar. This common embedding space enables multiple applications such as text-to-image/video retrieval and image/video-to-text retrieval. The work of Aytar et al. <ref type="bibr" target="#b23">[24]</ref> is going further by learning a cross-modal embedding space for visual, textual and aural samples. In vision, language is also used in captioning where the task is to generate a descriptive caption of an image or a video <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. Another related application is visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. A useful application of learning jointly from video and text is the possibility of performing video summarization with natural language <ref type="bibr" target="#b7">[8]</ref>. Other works also tackle the problem of visual grounding of sentences: it can be applied to spatial grounding in images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref> or temporal grounding (i.e temporal localization) in videos <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. Our method improves text-video embeddings and has potential to improve any method relying on such representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-stream video representation</head><p>Combining different modalities is a straightforward way to improve video representations for many tasks. Most state-of-the-art video representations <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> separate videos into multiple stream of modalities. The appearance, which are features capturing visual cues, the motion, computed from optical flow estimation or dense trajectories <ref type="bibr" target="#b35">[36]</ref>, and the audio signal are the commonly used video modalities. Investigating on which video descriptors to combine and how to efficiently fuse them has been extensively studied. Most prior works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> address the problem of appearance and motion fusion for video representation. Other more recent works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39]</ref> explore appearance-audio two-stream architectures for video representation. This and other work has consistently demonstrated the benefits of combining different video modalities for tasks such as video classification and action recognition. Similar to previous work in video understanding, our model combines multiple modalities but can also handle missing modalities during training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning with missing data</head><p>Our work is also closely related to learning methods designed to handle missing data. Handling missing data in machine learning is far from being a solved problem, yet it is widespread in various fields. Data can be missing due to several reasons: it can be corrupted, it may have not been possible to record the data or, in some cases, the data may be intentionally missing (take an example of forms with answers to some fields being optional). Common practices in machine learning aim at imputing the missing values with a default value such as zero, the mean, the median or the most frequent value in the discrete case 1 . In the matrix completion theory, a low rank approximation of the matrix <ref type="bibr" target="#b39">[40]</ref> can be performed to fill the missing values. In computer vision, one main application of learning with missing data is the inpainting task. Several approaches such as: Low rank matrix factorization <ref type="bibr" target="#b40">[41]</ref>, Generative Adversarial Network <ref type="bibr" target="#b41">[42]</ref> or more recently <ref type="bibr" target="#b42">[43]</ref> have successfully addressed the problem. The UberNet network <ref type="bibr" target="#b43">[44]</ref> is a universal multi-task model aiming at solving multiple problems such as: object detection, object segmentation or surface normal estimation. To do so, the model is trained on a mix of different annotated datasets, each one having its own task-oriented set of annotation. Their work is also related to ours as we also combine diverse types of datasets. However in our case, we have to address the problem of missing video modalities instead of missing task annotation.</p><p>Handling missing modalities can be seen as a specific case of learning from missing data. In image recognition the recent work <ref type="bibr" target="#b44">[45]</ref> has tackled the task of learning with missing modalities to treat the problem of missing sensor information. In this work, we address the problem of missing video modalities. As explained above, videos can be divided into multiple relevant modalities such as appearance, audio and motion. Being able to train and infer models without all modalities makes it possible to mix different type of data such as illustrated in <ref type="figure">Figure 1</ref>. input sentence X and video Y as a weighted combination of expert embeddings, one for each input descriptor type including appearance, motion, facial descriptors or audio. The appropriate weight of each expert is estimated from the input text. Our model can deal with missing video input such as face descriptors missing for videos without people depicted above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mixture of embedding experts for video and text</head><p>In this section we introduce the proposed mixture of embedding experts (MEE) model and explain how this model handles heterogeneous input sources with incomplete sets of data streams during both training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model overview and notation</head><p>Our goal is to learn a common embedding space for video and text. More formally, if X is a sentence and Y a video, we would like to learn embedding functions f and g such that similarity s = f (X), g(Y ) is high if and only if X and Y are semantically similar. We assume that each input video is composed of N different streams of descriptors, {I i } i∈1...N that represent, for example, motion, appearance, audio, or facial appearance of people. Note that as we assume the videos come from diverse data sources a particular video may contain only a subset of these descriptor types. For example, some videos may not have audio, or will not have face descriptors when they don't depict people. As we will show later, the same model will be able to represent still images as (very) simple videos composed of a single frame without motion. To address the issue that not all videos will have all descriptors, we design a model inspired by the mixture of experts <ref type="bibr" target="#b45">[46]</ref>, where we learn a separate "expert" embedding model for each descriptor type. The expert embeddings are combined in an end-to-end trainable fashion using weights that depend on the input caption. As a result, the model can learn to increase the relative weight of motion descriptors for input captions concerning human actions, or increase the relative weight of face descriptors for input captions that require detailed face understanding.</p><p>The overview of the model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Descriptors of each input stream I i are first aggregated over time using the temporal aggregation module h i and the resulting aggregated descriptor is embedded using a gated embedding module g i (see <ref type="bibr">3.4)</ref>. Similarly, the individual word embeddings from the input caption are first aggregated using a text aggregation module into a single descriptor, which is then embedded using gated embedding modules f i , one for each input source i. The resulting expert embeddings for each input source are then weighted using normalized weights w i (X) estimated by the weight estimation module from caption X to obtain the final similarity score s. Details of the individual components are given next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text representation</head><p>The textual input is a sequence of word embeddings for each input sentence. These individual word embedding vectors are then aggregated into a single vector representing the entire sentence using a NetVLAD <ref type="bibr" target="#b46">[47]</ref> aggregation module, denoted h(X). This is motivated by the recent results <ref type="bibr" target="#b33">[34]</ref> demonstrating superior performance of NetVLAD aggregation over other common aggregation architectures such as long short-term memory (LSTM) <ref type="bibr" target="#b47">[48]</ref> or gated recurrent units (GRU) <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal aggregation module</head><p>Similar to input text, each input stream I i of video descriptors is first aggregated into a single vector using temporal aggregation module h i . For this, we use NetVLAD <ref type="bibr" target="#b46">[47]</ref> or max pooling, depending on the input descriptors. Details are given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gated embedding module</head><p>The gated embedding module Z = f (Z 0 ) takes a d 1 -dimensional feature Z 0 as input and embeds (transforms) it into a new feature Z in d 2 -dimensional output space. This is achieved using the following sequence of operations:</p><formula xml:id="formula_0">Z 1 = W 1 Z 0 + b 1 ,<label>(1)</label></formula><formula xml:id="formula_1">Z 2 = Z 1 • σ(W 2 Z 1 + b 2 ),<label>(2)</label></formula><formula xml:id="formula_2">Z = Z 2 Z 2 2 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">W 1 ∈ R d2×d1 , W 2 ∈ R d2×d2 , b 1 ∈ R d2 , b 2 ∈ R d2</formula><p>are learnable parameters, σ is an element-wise sigmoid activation and • is the element-wise multiplication (Hadamard product). Note that the first layer, given by (1), describes a projection of the input feature Z 0 to the embedding space Z 1 . The second layer, given by <ref type="formula" target="#formula_1">(2)</ref>, performs context gating <ref type="bibr" target="#b33">[34]</ref>, where individual dimensions of Z 1 are reweighted using learnt gating weights σ(W 2 Z 1 + b 2 ) with values between 0 and 1, where W 2 and b 2 are learnt parameters. The motivation for such gating is two-fold: (i) we wish to introduce nonlinear interactions among dimensions of Z 1 and (ii) we wish to recalibrate the strengths of different activations of Z 1 through a self-gating mechanism. Finally, the last layer, given by <ref type="formula" target="#formula_2">(3)</ref>, performs L2 normalization to obtain the final output Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Estimating text-video similarity with a mixture of embedding experts</head><p>In this section we explain how to compute the final similarity score between the input text sentence X and video Y . Recall, that each video is represented by several input streams I i of descriptors. Our proposed model learns separate (expert) embedding between the input text and each of the input video streams. These expert embeddings are then combined together to obtain the final similarity score. More formally, we first compute a similarity score s i between the input sentence X and input video stream I i</p><formula xml:id="formula_4">s i (X, I i ) = f i (h(X)), g i (h i (I i )) ,<label>(4)</label></formula><p>where f i (h(X)) is the text embedding composed of aggregation module h() and gated embedding module f i (); g i (h i (I i )) is the embedding of the input video stream I i composed of descriptor aggregation module h i and gated embedding module g i ; and a, b denotes a scalar product. Please note that we learn a separate text embedding f i for each input video stream i. In other words, we learn different embedding parameters to match the same input sentence X to different video descriptors. For example, such embedding can learn to emphasize words related to facial expressions when computing similarity score between the input sentence and the input face descriptors, or to emphasize action words when computing the similarity between the input text and input motion descriptors.</p><p>Estimating the final similarity score with a mixture of experts. The goal is to combine the similarity scores s i (X, I i ) between the input sentence X and different streams of input descriptors I i into the final similarity score. To achieve that we employ the mixture of experts approach <ref type="bibr" target="#b45">[46]</ref>. In detail, the final similarity score s(X, Y ) between the input sentence X and video Y is computed as</p><formula xml:id="formula_5">s(X, Y ) = N i=1 w i (X)s i (X, I i ), with w i (X) = e h(X) ai N j=1 e h(X) aj ,<label>(5)</label></formula><p>where w i (X) is the weight of similarity score s i predicted from the input sentence X, h(X) is the aggregated sentence representation and a i , i = 1 . . . N the learnt parameters. Please note again that the weights w i of experts s i are predicted from sentence X. In other words, the input sentence provides a prior on which of the embedding experts to put more weight to compute the final global similarity score. The estimation of the weight of the different input streams can be seen as an attention mechanism that uses the input text sentence. For instance, we may expect to have high weight on the motion stream for input captions such as: "The man is practicing karate", facial descriptors for captions such as "Barack Obama is giving a talk", or on audio descriptors for input captions such as "The woman is laughing out loud".</p><p>Single text-video embedding. Please note that equation <ref type="formula" target="#formula_5">(5)</ref> can be viewed as a single text-video embedding s(X, Y ) = f (X), g(Y )</p><p>, where: f (X) = [w 1 (X)f 1 (h(X)), . . . , w N (X)f N (h(X))] is the vector concatenating individual text embedding vectors f i (h(X)) weighted by estimated expert weights w i , and g(Y ) = [g 1 (h 1 (I 1 )), . . . , g N (h(I N ))] is the concatenation of the individual video embedding vectors g i (h i (I i )). This is important for retrieval applications in large-scale datasets, where individual embedding vectors for text and video can be pre-computed offline and indexed for efficient search using techniques such as product quantization <ref type="bibr" target="#b49">[50]</ref>.</p><p>Handling videos with incomplete input streams. The formulation of the similarity score s(X, Y ) as a mixture of experts provides a proper way to handle situations where the input set of video streams is incomplete. For instance, when audio descriptors are missing for silent videos or when face descriptors are missing in shots without people. In detail, in such situations we estimate the similarity score s using the remaining available experts by renormalizing the remaining mixture weights to sum to one as</p><formula xml:id="formula_6">s(X, Y ) = i∈D w i (X) j∈D w j (X) s i (X, I i ),<label>(6)</label></formula><p>where D ⊂ {1 . . . N } indexes the subset of available input streams I i for the particular input video Y . When training the model, the gradient thus only backpropagates to the available branches of both text and video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Bi-directional ranking loss</head><p>To train the model, we use the bi-directional max-margin ranking loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> as we would like to learn an embedding that works for both text-to-video and video-totext retrieval tasks. More formally, at training time, we sample a batch of sentence-video pairs (X i , Y i ) i∈ <ref type="bibr">[1,B]</ref> where B is the batch size. We wish to enforce that, for any given <ref type="figure">i ∈ [1, B]</ref>, the similarity score s i,i = s(X i , Y i ) between video Y i and its ground truth caption X i is greater than every possible pair of scores s i,j and s j,i , where j = i of non-matching videos and captions. This is implemented by using the following loss for each batch of B sentence-video pairs</p><formula xml:id="formula_7">(X i , Y i ) i∈[1,B] l = 1 B B i=1 j =i max(0, m + s i,j − s i,i ) + max(0, m + s j,i − s i,i ) ,<label>(7)</label></formula><p>where s i,j = s(X i , Y j ) is the similarity score of sentence X i and video Y j , and m is the margin. We set m = 0.2 in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report experiments with our mixture of embedding experts (MEE) model on different text-video retrieval tasks. We perform an ablation study to highlight the benefits of our approach and compare the proposed model with current state-of-theart methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>In the following, we describe the used datasets and details of data pre-processing and training procedures.</p><p>Datasets. We perform experiments on the following three datasets: 1 -MPII movie description/LSMDC dataset. We report results on the MPII movie description dataset <ref type="bibr" target="#b13">[14]</ref>. This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed audio description. The dataset is used in the Large Scale Movie Description Challenge (LSMDC). We report experiments on two LSMDC challenge tasks: movie retrieval and movie annotation. The first task evaluates text-to-video retrieval: given a sentence query, retrieve the corresponding video from 1,000 test videos. The performance is measured using recall@k (higher is better) for different values of k, or median rank (lower is better). The second, movie annotation task evaluates video-to-text retrieval: we are provided with 10,053 short clips, where each clip comes with five captions, with only one being correct. The goal is to find the correct one. The performance is measured using the accuracy. For both tasks we follow the same evaluation protocol as described on the LSMDC website 2 .</p><p>2 -MSR-VTT dataset. We also report several experiments on the MSR-VTT dataset <ref type="bibr" target="#b12">[13]</ref>. This dataset contains 10,000 unique Youtube video clips. Each of them is annotated with 20 different text captions, which results in a total of 200,000 unique video-caption pairs. Because we are only provided with URLs for each video, some of the video are, unfortunately, not available for download anymore. In total, we have successfully downloaded 7,656 videos (out of the original 10k videos). Similar to the LSMDC challenge and <ref type="bibr" target="#b13">[14]</ref>, we evaluate on the MSR-VTT dataset the text-to-video retrieval task on randomly sampled 1,000 video-caption pairs from the test set.</p><p>3 -COCO 2014 Image-Caption dataset. We also report results on the text to still image retrieval task on the 2014 version of the COCO image-caption dataset <ref type="bibr" target="#b15">[16]</ref>. Again, we emulate the LSMDC challenge and evaluate text-to-image retrieval on randomly sampled 1000 image-caption pairs from the COCO 2014 validation set.</p><p>Data pre-processing. For text pre-processing, we use the Google News 3 trained word2vec word embeddings <ref type="bibr" target="#b52">[53]</ref>. For sentence representation, we use NetVLAD <ref type="bibr" target="#b46">[47]</ref> with 32 clusters. For videos, we extract frames at 25 frames per seconds and resize each frame to have a consistent height of 300 pixels. We consider up to four different descriptors representing the visual appearance, motion, audio and facial appearance. We pre-extract the descriptors for each input video resulting in up to four input streams of descriptors. The appearance features are extracted using the Imagenet pre-trained ResNet-152 <ref type="bibr" target="#b53">[54]</ref> CNN. We extract 2048-dimensional features from the last global average pooling layer. The motion features are computed using a Kinetics pre-trained I3D flow network <ref type="bibr" target="#b11">[12]</ref>. We extract the 1024-dimensional features from the last global average pooling layer. The audio features are extracted using the audio CNN <ref type="bibr" target="#b54">[55]</ref>. Finally, for the face descriptors, we use the dlib framework 4 to detect and align faces. Facial features are then computed on the aligned faces using the same framework, which implements a ResNet CNN trained for face recognition. For each detected face, we extract 128-dimensional representation. We use max-pooling operation to aggregate appearance, motion and face descriptors over the entire video. To aggregate the audio features, we follow <ref type="bibr" target="#b33">[34]</ref> and use a NetVLAD module with 16 clusters.</p><p>Training details. Our work was implemented using the PyTorch 5 framework. We train our models using the ADAM optimizer <ref type="bibr" target="#b55">[56]</ref>. On the MPII dataset, we use a learning rate of 0.0001 with a batch size of 512. On the MSR-VTT dataset, we use a learning rate of 0.0004 with a batch size of 64. Each training is performed using a single GPU and takes only several minutes to finish.   Ablation study on the MPII movie dataset. <ref type="table" target="#tab_0">Table 1</ref> shows a detailed ablation study on the LSMDC Text-to-Video and Video-to-Text retrieval tasks on the MPII movie dataset. Unfortunately here, we notice that incorporating heterogeneous data does not seem to significantly help the retrieval performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benefits of learning from heterogeneous data</head><p>Augmenting videos with images. Next, we evaluate in detail the benefits of augmenting captioned video datasets (MSR-VTT and MPII movie) with captioned still images from the Microsoft COCO dataset. <ref type="table" target="#tab_2">Table 2</ref> shows the effect of adding the still image data during training. For all models, we report results on both the COCO image dataset and the MPII videos. Adding COCO images to the video training set improves performance on both COCO images but also MPII videos, showing that a single model trained from the two different data sources can improve performance on both datasets. This is an interesting result as the two datasets are quite different in terms of depicted scenes and textual captions. MS COCO dataset contains mostly Internet images of scenes containing multiple objects. MPII dataset contains video clips from movies often depicting people interacting with each other or objects. We also evaluate the impact of augmenting MSR-VTT video caption dataset with the captioned still images from the MS COCO dataset. As the MSR-VTT is much smaller than the COCO dataset, it becomes crucial to carefully sample COCO imagecaption samples when augmenting MSR-VTT during training. In detail, for each epoch, we randomly inject image-caption samples such that the ratio of image-caption samples to video-caption samples is set to a fixed sampling rate: α ∈ R ≥0 . Note that α = 0 means that no data augmentation is performed and α = 1.0 means that exactly the same amount of COCO image-caption and MSR-VTT video-caption samples are used at each training epoch. <ref type="table" target="#tab_3">Table 3</ref> shows the effect of still image augmentation on the MSR-VTT dataset for the text-to-video retrieval task when the proportion of image-caption samples is half of the MSR-VTT video caption samples, i.e. α = 0.5. Our proposed MEE model fully leverages the additional still images. Indeed, we observe gains in video retrieval performances for all metrics. <ref type="figure" target="#fig_2">Figure 3</ref> shows qualitative results of our model highlighting some of the best relative improvement in retrieval ranking using the still image data augmentation. Note that many of the improved queries involve objects frequently appearing in the COCO dataset including elephant, umbrella, baseball or train. <ref type="table" target="#tab_4">Table 4</ref> compares our best approach to the state-of-the-art results on the LSMDC challenge test sets. Note that our approach significantly outperforms all other available results including JSFusion <ref type="bibr" target="#b57">[58]</ref>, which is the winning method of the LSMDC 2017 Text-to-Video and Video-to-Text retrieval challenge. We also reimplemented the normalized CCA approach from Klein et al. <ref type="bibr" target="#b19">[20]</ref>. To make the comparison fair, we used our video features and word embeddings. Finally, we also significantly outperform the C+LSTM+SA+FC7 <ref type="bibr" target="#b56">[57]</ref> baseline that augments the MPII movie dataset with COCO image caption data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have described a new model, called mixture of embedding experts (MEE), that learns text-video embeddings from heterogeneous data sources and is able to deal with missing video input modalities during training. We have shown that our model can be trained from image-caption and video-caption datasets treating images as a special case of videos without motion and sound. In addition, we have demonstrated that our model can optionally incorporate at training, input stream of facial descriptors, where faces are present in videos containing people but missing in videos without people. We have evaluated our model on the task of video retrieval. Our approach outperforms all reported results on the MPII Movie Description. Our work opens-up the possibility of learning text-video embedding models from large-scale weakly-supervised image and video datasets such as the Flickr 100M <ref type="bibr" target="#b58">[59]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"</head><label></label><figDesc>Obama is eating a sandwich" "A cat eating a banana" "A yellow banana"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Mixture of embedding experts (MEE) model that computes similarity score s between</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Example videos with large relative improvement in text-to-video retrieval ranking (out of 1000 test videos) on the MSR-VTT dataset when incorporating still images from the COCO dataset at training using our proposed MEE model. Notice that the improved videos involve querying objects frequently present in the COCO dataset including: elephant, umbrella, baseball or train.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on the MPII movie dataset. R@k denotes recall@k (higher is better), MR denotes mean rank (lower is better). Multiple choice is measured in accuracy (higher is better).</figDesc><table><row><cell>Evaluation task</cell><cell cols="2">Text-to-Video retrieval Video-to-Text retrieval</cell></row><row><cell>Method</cell><cell>R@1 R@5 R@10 MR</cell><cell>Multiple Choice</cell></row><row><cell>MEE</cell><cell>10.2 25.0 33.1 29</cell><cell>74.0</cell></row><row><cell>MEE + Face</cell><cell>9.3 25.1 33.4 27</cell><cell>74.8</cell></row><row><cell>MEE + COCO</cell><cell>9.8 25.6 34.7 27</cell><cell>73.4</cell></row><row><cell cols="2">MEE + COCO + Face 10.1 25.6 34.6 27</cell><cell>73.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The proposed embedding model is designed for learning from diverse and incomplete inputs. We demonstrate this ability on two examples. First, we show how a text-video embedding model can be learnt by augmenting captioned video data with captioned still images. For this we use the Microsoft COCO dataset<ref type="bibr" target="#b15">[16]</ref> that contains captions provided by humans. Methods augmenting training data with still images from the COCO dataset are denoted (+COCO). Second, we show how our embedding model can incorporate an incomplete input stream of facial descriptors, where face descriptors are present in videos containing people but are absent in videos without people. Methods that incorporate face descriptors are denoted (+Face).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The effect of augmenting the MPII movie caption dataset with captioned still images from the MS COCO dataset. R@k denotes recall@k (higher is better), MR denotes Median Rank (lower is better) and MC denotes Multiple Choice (higher is better).</figDesc><table><row><cell>Evaluation set</cell><cell>COCO images</cell><cell>MPII videos</cell></row><row><cell>Model</cell><cell cols="2">R@1 R@5 R@10 MR R@1 R@5 R@10 MR MC</cell></row><row><cell>MEE + Face</cell><cell cols="2">10.4 29.0 42.6 15 9.3 25.1 33.4 27 74.8</cell></row><row><cell cols="2">MEE + Face + COCO 31.4 64.5 79.3</cell><cell>3 10.1 25.6 34.6 27 73.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The effect of augmenting the MSR-VTT video caption dataset with captioned still images from the MS COCO dataset when relative image to video sampling rate α = 0.5. R@k stands for recall@k, MR stands for Median Rank.</figDesc><table><row><cell>Evaluation set</cell><cell>COCO images</cell><cell>MSR-VTT videos</cell></row><row><cell>Model</cell><cell cols="3">R@1 R@5 R@10 MR R@1 R@5 R@10 MR</cell></row><row><cell>MEE + Face</cell><cell cols="3">8.4 24.9 38.9 18 13.6 37.9 51.0 10</cell></row><row><cell cols="2">MEE + Face + COCO 20.7 54.5 72.0</cell><cell>5 14.2 39.2 53.8</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Text-to-video and Video-to-Text retrieval results from the LSMDC test sets. MR stands for Median Rank, MC for Multiple Choice.</figDesc><table><row><cell>Evaluation task</cell><cell cols="3">Text-to-Video retrieval Video-to-Text retrieval</cell></row><row><cell>Method</cell><cell cols="2">R@1 R@5 R@10 MR</cell><cell>MC Accuracy</cell></row><row><cell>Random baseline</cell><cell>0.1 0.5</cell><cell>1.0 500</cell><cell>20.0</cell></row><row><cell>C+LSTM+SA+FC7 [57]</cell><cell cols="2">4.2 13.0 19.5 90</cell><cell>58.1</cell></row><row><cell>SNUVL [52] (LSMDC16 Winner)</cell><cell cols="2">3.6 14.7 23.9 50</cell><cell>65.7</cell></row><row><cell>CT-SAN [2]</cell><cell cols="2">5.1 16.3 25.2 46</cell><cell>67.0</cell></row><row><cell>Miech et al. [3]</cell><cell cols="2">7.3 19.2 27.1 52</cell><cell>69.7</cell></row><row><cell cols="3">CCA (FV HGLMM) [20] (same features) 7.5 21.7 31.0 33</cell><cell>72.8</cell></row><row><cell>JSFusion [58] (LSMDC17 Winner)</cell><cell cols="2">9.1 21.2 34.1 36</cell><cell>73.5</cell></row><row><cell>MEE + COCO + Face (Ours)</cell><cell cols="2">10.1 25.6 34.6 27</cell><cell>73.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://scikit-learn.org/stable/modules/generated/sklearn. preprocessing.Imputer.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sites.google.com/site/describingmovies/lsmdc-2017 3 GoogleNews-vectors-negative300</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://dlib.net/ 5 http://pytorch.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors would like to thank Valentin Gabeur for spotting a bug in our codebase that affected multiple results from the initial paper. The bug was fixed in the following commit: https://tinyurl.com/s6hvn9s. This work has been partly supported by ERC grants ACTIVIA (no. 307574) and LEAP (no. 336845), CI-FAR Learning in Machines &amp; Brains program, European Regional Development Fund under the project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468) and a Google Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning from Video and Text via Large-Scale Discriminative Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weaklysupervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Enhancing video summarization via visionlanguage embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="4584" to="4593" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning two-branch neural networks for imagetext matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00932</idno>
		<title level="m">See, hear, and read: Deep aligned representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="457" to="468" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<title level="m">Learnable pooling with context gating for video classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Actionvlad: Learning spatiotemporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Long-term Temporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Low-rank matrix completion using alternating minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-fifth annual ACM symposium on Theory of computing</title>
		<meeting>the forty-fifth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sairo</surname></persName>
		</author>
		<editor>JMLR.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<title level="m">Semantic image inpainting with deep generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10925</idno>
		<title level="m">Deep image prior</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Missing modalities imputation via cascaded residual autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Neural Computing</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video captioning and retrieval models with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV LSMDC2016 Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yfcc100m: the new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
