<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correspondence Networks with Adaptive Neighbourhood Consensus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Active Vision Lab &amp; 2 Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<email>khan@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Active Vision Lab &amp; 2 Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
							<email>costain@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Active Vision Lab &amp; 2 Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
							<email>henryhj@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Active Vision Lab &amp; 2 Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
							<email>victor@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Active Vision Lab &amp; 2 Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Correspondence Networks with Adaptive Neighbourhood Consensus</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we tackle the task of establishing dense visual correspondences between images containing objects of the same category. This is a challenging task due to large intra-class variations and a lack of dense pixel level annotations. We propose a convolutional neural network architecture, called adaptive neighbourhood consensus network (ANC-Net), that can be trained end-to-end with sparse keypoint annotations, to handle this challenge. At the core of ANC-Net is our proposed non-isotropic 4D convolution kernel, which forms the building block for the adaptive neighbourhood consensus module for robust matching. We also introduce a simple and efficient multi-scale self-similarity module in ANC-Net to make the learned feature robust to intra-class variations. Furthermore, we propose a novel orthogonal loss that can enforce the one-to-one matching constraint. We thoroughly evaluate the effectiveness of our method on various benchmarks, where it substantially outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Establishing visual correspondences has long been a fundamental problem in computer vision. It has seen variety of applications in areas such as 3D reconstruction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>, image editing <ref type="bibr" target="#b5">[6]</ref>, scene understanding <ref type="bibr" target="#b23">[24]</ref>, and object detection <ref type="bibr" target="#b3">[4]</ref>.</p><p>Earlier works mainly focused on estimating correspondences for images of the same scene or object (i.e. instancelevel correspondences) using hand-crafted features such as SIFT <ref type="bibr" target="#b25">[26]</ref> or HOG <ref type="bibr" target="#b2">[3]</ref>. Recently, finding correspondences for different instances from the same category (i.e. * indicates equal contribution c 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. semantic correspondences) has attracted more and more attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>. In this paper, we focus on the problem of establishing dense correspondences for a pair of images depicting different instances from the same category. This task is extremely challenging due to large intraclass variation in properties such as colour, scale, pose, and illumination. Further, it is unreasonably expensive, if not impossible, to provide dense annotations for such image pairs.</p><p>To deal with the challenges mentioned above, we introduce a convolutional neural network (CNN), called Adaptive Neighbourhood Consensus Network (ANC-Net), which can produce reliable semantic correspondences without requiring dense human annotations. ANC-Net takes a pair of images as input and predicts a 4D correlation map, containing the matching scores for all possible matches between the two images. The most likely matches can then be retrieved by finding the matches giving the maximum matching scores.</p><p>ANC-Net consists of a CNN feature extractor, a multiscale self-similarity module, and an adaptive neighbourhood consensus module. At the core of ANC-Net is our proposed non-isotropic 4D convolution, which incorporates an adaptive neighbourhood consensus constraint for robust matching, and our proposed multi-scale selfsimilarity module, which aggregates multiple self-similarity features, which are insensitive to intra-class appearance variation <ref type="bibr" target="#b16">[17]</ref>.</p><p>CNN features have been very popular for the task of correspondence estimation due to their promising performance, and most state-of-the-art methods are based on CNN features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref>. Like other methods, ANC-Net also extracts features with a pre-trained CNN. However, instead of directly using the CNN features to calculate matching scores, we introduce the multi-scale selfsimilarity. Self-similarity has been introduced in existing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>. Unlike other methods that either use self-similarity as an extra feature alongside raw CNN features <ref type="bibr" target="#b9">[10]</ref>, or use computationally expensive irregular self-  <ref type="figure">Figure 1</ref>: An overview of ANC-Net. Given a pair of images (I s , I t ), ANC-Net can predict their pixel-wise semantic correspondences. A CNN backbone F first extracts features F s and F t . Our multi-scale self-similarity module then captures the self-similarity features S s and S t based on F s and F t . We can then obtain C s from S s and S t , and C f from F s and F t .</p><formula xml:id="formula_0">S F F F s F t S s S t C f C s I s I t C</formula><p>Taking C f and C s as input, our ANC module N will predict a refinedC, from which the pixel-wise correspondences can be retrieved with interpolation. similarity patterns <ref type="bibr" target="#b16">[17]</ref>, our self-similarity features are both computationally cheap to obtain, and do not need combining with raw CNN features, whist still capturing the complex self-similarity patterns.</p><p>With reliable feature representation, the matching scores for each individual feature pair can then be calculated. However, as the individual feature pairs do not contain any matching validity information, matching by direct feature comparison can be rather noisy. To mitigate this, correspondence validity constraints should be applied to obtain reliable matching scores. Neighbourhood consensus, which measures how many pairs are matched in the neighbourhoods of the two points under consideration, turns to be one of the most effective correspondence validity constraints, and has been successfully introduced in recent work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10]</ref>. However, <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b9">[10]</ref> assume neighbourhoods of the same size for the two points in consideration. Unfortunately, this assumption does not hold in practice, as objects in real images typically have different scales and shapes. Therefore, adopting neighbourhoods of the same size is very likely to be affected by unrelated neighbours (e.g. background parts). To address this issue, we propose an adaptive neighbourhood consensus module, which can select the correct neighbourhoods.</p><p>As mentioned earlier, the cost of labelling ground truth means fully supervised learning with dense annotations is not feasible. Instead, our model can effectively make use of sparse key-point annotations. To enforce the one-to-one mapping constraint, which is crucial for plausible correspondences, we further propose a novel one-to-one mapping loss, called orthogonal loss, to regularise the training.</p><p>To summarise, our contributions are four fold:</p><p>• We introduce ANC-Net for the task of dense semantic correspondence estimation, which can be trained with sparse key-point annotations. • We propose a non-isotropic 4D convolutional kernel, which forms the building block for the adaptive neighbourhood consensus module for robust matching. • We propose a simple and efficient multi-scale selfsimilarity to make the feature matching robust to intraclass variation. • We propose a novel orthogonal loss that can enforce the one-to-one matching constraint, encouraging plausible matching results. We thoroughly evaluate the effectiveness of our method on various benchmarks, where it substantially outperforms state-of-the-art methods. Our code can be found at https: //ancnet.avlcode.org/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The semantic correspondence estimation problem is often considered as either a pixel-wise matching problem, an image alignment problem, or a flow estimation problem. Earlier works used hand-crafted features, such as SIFT <ref type="bibr" target="#b25">[26]</ref> or HOG <ref type="bibr" target="#b2">[3]</ref>, to establish semantic correspondences <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. Here, we briefly review recent CNN based methods. Pixel-wise matching. Long et al. <ref type="bibr" target="#b24">[25]</ref> transferred the features pre-trained on an image classification task to pixelwise correspondence estimation. Choy et al. <ref type="bibr" target="#b1">[2]</ref> introduced a method to learn a feature embedding for the correspondence problem, by pulling positive features pairs close and pushing negative feature pairs away. Han et al. <ref type="bibr" target="#b8">[9]</ref> proposed a CNN model that tries to match image patches considering both appearance and geometry information, and obtains the pixel-wise correspondences by interpolation. Novotny et al. <ref type="bibr" target="#b27">[28]</ref> introduced a method to learn geometrically stable features with self-supervised learning by applying a syn-thetic warp to the images. More recently, Rocco et al. <ref type="bibr" target="#b31">[32]</ref> proposed to construct a CNN model that incorporates neighbourhood consensus information to refine the 4D tensor storing all the matching scores, which are obtained from pre-trained CNN features. Huang et al. <ref type="bibr" target="#b9">[10]</ref> introduced a method to incorporate self-similarity based on <ref type="bibr" target="#b31">[32]</ref> and fuse different features with an attention mechanism. Min et al. <ref type="bibr" target="#b26">[27]</ref> showed that effectively combining features extracted from different layers can provide significant benefits for the dense semantic correspondence estimation task. Image alignment. Rocco et al. <ref type="bibr" target="#b29">[30]</ref> developed a CNN architecture that can predict the global geometric transformation between two images by training on synthetically warped data. Seo et al. <ref type="bibr" target="#b33">[34]</ref> improved <ref type="bibr" target="#b29">[30]</ref> by introducing attention based offset-aware correlation kernels. Rocco et al. <ref type="bibr" target="#b30">[31]</ref> presented an end-to-end trainable CNN architecture that uses weak image-level supervision, which is trained by a soft inlier counting loss in a similar spirit to RANSAC. Jeon et al. <ref type="bibr" target="#b12">[13]</ref> introduced a hierarchical learning procedure to progressively learn affine transformations to align the images in a chaos-to-fine manner. Kim et al. <ref type="bibr" target="#b15">[16]</ref> introduced to a recurrent transformer network, which is trained with an iterative process and can predict the transformations between a pair of images. Flow estimation. Fischer et al. <ref type="bibr" target="#b4">[5]</ref> introduced an endto-end trainable model called FlowNet, which is trained on synthetic data to predict optical flow. FlowNet is further improved by Ilg et al. <ref type="bibr" target="#b11">[12]</ref> in several aspects. Kim et al. <ref type="bibr" target="#b16">[17]</ref> proposed a learnable self-similarity feature, which is then used to estimate an dense affine transformation flow for each feature location. The semantic correspondences can then be obtained by applying such transformations. Lee et al. <ref type="bibr" target="#b21">[22]</ref> introduced a method to use images annotated with binary foreground masks, and subjected to synthetic geometric deformations, to train a CNN model with a mask consistency loss and a flow consistency loss. Besides these, there are also some methods that learn the flow using videos <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20]</ref> by considering temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a pair of images (I s , I t ), our objective is to find pixel-wise correspondences between the two images. We propose a CNN, ANC-Net, which takes (I s , I t ) as input and produces a 4D correlation map containing the matching scores for all possible pairs in the feature space of the two images. Pixel-wise correspondence then can be extracted by interpolation among the most likely matches in the feature space. The model can be trained with a supervised loss on sparse key-point annotations in an end-to-end manner. To encourage one-to-one matching, we propose using a novel loss, called the orthogonal loss, together with the supervised loss on sparse key-point annotations, for training our model. <ref type="figure">Figure 1</ref> illustrates the main architecture of our net-work. It consists of a feature extractor F, a multi-scale selfsimilarity module, and an adaptive neighbourhood consensus (ANC) module N . The feature extractor F is composed of a sequence of standard convolutional layers. We first feed the two images into F, and get a pair of feature maps F s and F t . The multi-scale self-similarity module S consists of two convolutional layers followed by a concatenation operation to fuse them into the multi-scale features. With F s and F t , S will produce the multi-scale self-similarity feature maps S s and S t which capture the complex self-similarity patterns. We can then obtain the 4D correlation map C s from S s and S t , and the 4D correlation map C f from F s and F t . However, C s and C f are often noisy as they lack the constraints to enforce the correspondence validity, and thus are unreliable for directly extracting correspondences. Our proposed ANC module N , which is realised with a stack of non-isotropic 4D convolutions, takes C s and C f as input, refining them by considering neighbourhoods with varying sizes. Finally, the ANC module combines the refined correlation maps by simply summing up the two, producing a single 4D correlation mapC, from which reliable correspondences can be retrieved. C s is introduced to capture the second order (and higher) cues derived from the raw features. C s shares a similar structure to C f , allowing both to be refined using a neighbourhood consensus module without introducing extra learnable parameters. Experiments show that the proposed self-similarity module outperforms similar methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In this section, we will first introduce the multi-scale self-similarity module in Section 3.1. We then, in Section 3.2, describe the adaptive neighbourhood consensus matching validity module. Section 3.3 will discuss the approach to enforcing global constraints over the output of the neighbourhood consensus by maximising an a posteriori estimation. Finally, we describe the learning objectives for training our network in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale self-similarity</head><p>Self-similarity has been shown to be effective for the task of semantic correspondence estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>. Given a feature map F ∈ R h f ×w f ×d established by the backbone feature extractor, a self-similarity map measures the local similarity pattern at each feature location. One way to extract the self-similarity feature for the feature vector f ij at (i, j) in F is to calculate the cosine distance between itself and its neighbours. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the self-similarity module when considering the 3 × 3 neighbours of a given feature vector. This approach results in 9 self-similarity scores for each f ij . We further vectorise each of the 3 × 3 self-similarity scores into a 9-vector, which make up the self-similarity feature map S 0 ∈ R h f ×w f ×9 .</p><p>To further capture the correlations among different selfsimilarity features, we apply two 2D convolutional layers In the bottom, we first calculate the S 0 from the feature map F, and then perform two levels of 2D convolutions, each followed by an activation function (ReLU) to produce S 1 and S 2 . Finally, the initial similarity score S 0 , its first scale filtered features S 1 , and second filtered features S 2 are concatenated together to form final feature map S.</p><p>with zero padding on S 0 . Given the output feature maps for the two layers are S 1 and S 2 , we then concatenate the 3-scales S 0 , S 1 , and S 2 together to form an enhanced feature map S, which will serve as the input to the later layers.</p><p>With the feature maps S s and S t of source and target images respectively, we can obtain the 4D correlation map C s . Unlike DCCNet <ref type="bibr" target="#b9">[10]</ref>, where the self-similarity of a single scale is considered, and the self-similarity scores are then concatenated with F and convolved using a point-wise convolution which is intended to use the self-similarity to re-weight the raw features, our method avoids fusing with F to reduce redundancy, as the features in F have already been implicitly included in S 0 . Further, we extract more complex self-similarities than DCCNet and make use of multi-scale self-similarities to bootstrap the features. Thus, we capture more complex features from a much larger local window as well as second order (and higher) information.</p><p>As will be shown in the experiments, our multi-scale self-similarity module performs better than that of DCCNet. It is also worth noting that FCSS <ref type="bibr" target="#b16">[17]</ref> proposes a similar design, however their self-similarity score is defined using a set of irregular point pairs within the local window which is more complex to implement. In contrast, we adopt the design of correlating the centre feature with neighbours for simplicity and computation efficiency, and as a result, our simplified self-similarity module outperforms FCSS in all benchmarks.</p><p>Both C f and C s are complementary to each other as we hypothesise they are dominated by first order and higher order cues respectively. They will be refined by the following ANC module independently and then combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive neighbourhood consensus</head><p>Neighbourhood consensus has been shown to be effective for filtering the noisy 4D correlation map <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10]</ref>. Multiple layers of the isotropic 4D convolutional kernels, i.e. kernels with identical size in each dimension, are applied on the 4D correlation map to refine it. The isotropic 4D convolution with size 5 × 5 × 5 × 5 is illustrated in top left of <ref type="figure">Figure 3</ref>. It can be seen that the kernel establishes two neighbourhoods with the same size for both images. However, objects in real images often have varying scales and shapes, therefore, two neighbourhoods depicting the same semantic meaning are very likely to have different sizes. Thus, using neighbourhoods of the same size for both images may introduce noise (e.g. unrelated background) when determining a match.</p><p>To deal with the problem, we introduce the adaptive neighbourhood consensus (ANC) module which contains a set of non-isotropic 4D convolutional layers. As illustrated in the top right of <ref type="figure">Figure 3</ref>, the non-isotropic 4D convolution has dimensions of 3 × 3 × 5 × 5, defining the neighbourhood of 3 × 3 and 5 × 5.</p><p>To handle objects in real images with varying scales and shapes, we can combine our non-isotropic 4D kernels with isotropic 4D kernels so that the model can dynamically determine which set of convolutions should be activated to handle objects of various sizes. We consider 3 candidate architectures (shown in <ref type="figure">Figure 3</ref>) in our experiments with each non-isotropic 4D convolution using zero padding. Unless stated otherwise, we use (d) in our experiments, as it gives the best performance in our evaluation. This is possibly because (d) allows for more scale variation than the others. This choice might ignore better designs than (d), but the main point in this work is to demonstrate the effectiveness of the ANC module.</p><p>It is also worth noting that it is unnecessary to have both p × p × q × q and q × q × p × p kernels in the model where p and q are the sizes of some kernel dimensions, as the bidirectional neighbourhood consensus filter in Eq. 1 (which will be explained next) effectively tries both the configurations of small vs large neighbourhood and large vs small by reversing the matching direction, and the effect of both filters are equivalent due to the bidirectional matching.</p><p>Let N be the module of our adaptive neighbourhood consensus. It takes a 4D correlation map C s or C f as input and refining them. Their refined counterparts can then be combined to formC. We apply N to both matching directions (i.e. matching I s to I t and matching I t to I s ), so that our model is invariant to the order of the images. More importantly, this allows N to only include one p × p × q × q non-isotropic kernel to handle the small to large as well as the large to small neighbourhood. In particular, the refined 4D correlation map can be obtained bȳ</p><formula xml:id="formula_1">C = N (C s )+ N C s +N (C f )+ N C f ,<label>(1)</label></formula><p>where denotes the swapping of the matching direction given an image pair, i.e., (C ) ijkl = C klij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Most likely matches</head><p>After obtaining the refined 4D correlation mapC, we follow <ref type="bibr" target="#b31">[32]</ref> to apply soft mutual nearest neighbour filtering, i.e., for eachc ijcd inC, we replace it byĉ ijcd = r s ijkl r t ijklc ijkl where r s ijkl =c ijkl max abcabkl and r t ijkl = c ijkl max cdcijcd , which downweights the scores of matches that are not mutual nearest neighbours. Next, we perform softmax normalisation to the scoresĉ ijkl . The normalised scores can be interpreted as the matching probabilities. In particular, the probability of a given point at (i, j) in I s being matched with an arbitrary point</p><formula xml:id="formula_2">(k, l) in I t is v t ijkl = exp (ĉ ijkl ) cd exp (ĉ ijcd ) .<label>(2)</label></formula><p>Similarly, the probability of a given point at (k, l) in I t being matched with an arbitrary point</p><formula xml:id="formula_3">(i, j) in I s is v s ijkl = exp (ĉ ijkl ) ab exp (ĉ abkl ) .<label>(3)</label></formula><p>For a given position (i, j) in I s , the most likely match (k, l) in I t can be found by</p><formula xml:id="formula_4">(k, l) = arg max cd v t ijcd .<label>(4)</label></formula><p>Similarly, for a given position (k, l) in I t , the most likely match (i, j) in I s can be found by</p><formula xml:id="formula_5">(i, j) = arg max ab v s abkl .<label>(5)</label></formula><p>After retrieving the correspondences in the feature space with Eq. 4 and Eq. 5, the pixel-wise correspondences can be obtained by interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning objective</head><p>For the tasks of establishing dense semantic correspondences, it is impossible to obtain dense ground-truth labelling for all training image pairs due to the huge amount of human labour required. In practice, one can easily label only a few key-points of the objects in an image. These key-points often indicate the objects parts with concrete semantic meaning (e.g. eyes, mouths, body joints, etc.). Sparse key-point annotations are included in many existing datasets including PF-PASCAL <ref type="bibr" target="#b7">[8]</ref>, Spair-71k <ref type="bibr" target="#b26">[27]</ref>, CUB <ref type="bibr" target="#b36">[37]</ref> and others. There are also other forms of alternative annotations, such as image level pairwise annotations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10]</ref>, or object masks <ref type="bibr" target="#b21">[22]</ref>. In this paper, we are interested in the sparse key-point annotations, as they are more directly linked to our objective to learn semantic correspondences.</p><p>The sparse key-point annotations provide a straightforward way to train a CNN model for semantic matching, in which we minimise the distances between features of matched key-points (e.g. <ref type="bibr" target="#b1">[2]</ref>). However, this is not applicable to ANC, because the feature space ANC operates is a 4D correlation map, rather than a 3D feature map consisting of per pixel feature vectors. Therefore, we introduce a simple but effective supervised loss on 4D correlation maps to train our model.</p><p>For each key-point (x, y) in the image (e.g. <ref type="figure" target="#fig_2">Figure 4(a)</ref>), we first re-scale (x, y) to the same resolution as the feature map, giving the re-scaled coordinates (x c , y c ). Since (x c , y c ) is a sub-pixel coordinate, it can not be used as the target in the feature map directly. Instead, we can simply pick the nearest neighbour (x n , y n ) of (x c , y c ) in the feature map to be the target (see <ref type="figure" target="#fig_2">Figure 4 (b)</ref>). However, this will introduce errors caused by ignoring the offset between the (x n , y n ) and (x c , y c ). As the resolution of the feature map is much smaller than that of the image, small offsets in the feature map will cause large errors in the image. To compensate for the offset, we take the four nearest neighbours into consideration (see <ref type="figure" target="#fig_2">Figure 4</ref> (c)), rather than the single nearest neighbour. In particular, we pick the four nearest neighbours (x 1 n , y 1 n ), (x 2 n , y 2 n ), (x 3 n , y 3 n ), and (x 4 n , y 4 n ), and set scalar values t 1 , t 2 , t 3 , and t 4 to them representing the probability of being the considered as target. t 1 , t 2 , t 3 , and t 4 are proportional to their distances to (x c , y c ), and 4 j=1 t j = 1. We then apply 2D Gaussian smoothing on the four nearest neighbour probability map obtained above. We found that such smoothing can effectively enhance the performance. In this way, each keypoint location annotation is converted into a 2D probability map. Next, we reshape the smoothed 2D probability map into a (h c × w c )-vector for the key-point (x, y), followed by L 2 normalisation. For the source image I s containing n key-points, we can therefore construct its target as a matrix M gt ∈ R n×(hc×wc) with each row being a probability vector of a ground truth matching key-point in the target image I t . Let M gt and M be the ground truth and prediction. Note that M can be obtained by flattening the first two and last two dimentions ofC (after mutual nearest neighbour filtering), and taking the same n rows corresponding to M gt . The loss function is then the Frobenius Norm between them for both matching directions:</p><formula xml:id="formula_6">L k = M s − M s gt F + M t − M t gt F ,<label>(6)</label></formula><p>where M s denotes target probability map from I s to I t and M t denotes inverse direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Enforcing one-to-one matching</head><p>The one-to-one mapping (i.e. one point can be only matched to one other point) turns out to be a useful clue for improve the matching accuracy in classic graph matching (GM) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14]</ref>, which aims to match two given point sets (graphs) in two images. Ideally, for our semantic correspondence estimation task, the result should also agree with the one-to-one mapping constraint. This is especially helpful when there exist some repetitive patterns in the image (e.g. a building with multiple identical windows). GM methods always assume that the number of key-points in two images are exactly the same. However, this is often not the case in real applications. For example, due to pose variation, some key-points may be visible in one image, but not in the other. In this case, there exist one-to-none mappings in both images. A plausible one-to-one matching constraint should be able to ignore the one-to-none matches in the data automatically. To handle this problem, we introduce a novel loss, named the orthogonal loss, as it was inspired by the nonnegative orthogonal GM algorithm <ref type="bibr" target="#b13">[14]</ref>. The idea is that when MM is an identity matrix I, each row of M contains only one element, and the rest are zero, so we include a difference between MM and I in the loss. However, in reality, M may contain zero rows for one-to-none case. Therefore, our orthogonal loss term can be defined as</p><formula xml:id="formula_7">L o = MM − M gt M gt F ,<label>(7)</label></formula><p>where . F is a the Frobenius norm. It is worth noting that M gt M gt has zeros on its diagonal that allows both oneto-one and one-to-none matches to be accurately penalised.</p><p>The orthogonal loss has to be combined with Eq. 6 as it has no impact over the prediction accuracy. It simply regularises the model by encouraging one-to-one predictions.</p><p>The overall loss of our model can be written as</p><formula xml:id="formula_8">L = L k + αL m o ,<label>(8)</label></formula><p>where α is a weight balancing term, which is set to 0.001 in all our experiments, and</p><formula xml:id="formula_9">L m o = M s M s − M s gt M s gt F + M t M t − M t gt M t gt</formula><p>F by considering both matching directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and implementation details</head><p>Datasets. We evaluate our method on four public datasets, namely, PF-PASCAL <ref type="bibr" target="#b7">[8]</ref>, Spair-71k <ref type="bibr" target="#b26">[27]</ref>, and CUB <ref type="bibr" target="#b36">[37]</ref>. PF-PASCAL contains 1351 image pairs, which is approximately divided into 700 pairs for training 300 pairs for validation and 300 pairs for testing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>. Spair-71k dataset is much more challenging than the others as it contains both large view point differences and scale differences. We use the 12,234 pairs of testing pairs. Spair-71k is only used to evaluate the transferrability of the models trained on the PF-PASCAL training split. The CUB dataset contains 11,788 images of various species of birds with large variation of appearance, shape and pose. We randomly sample about 10,000 pairs from the CUB training data and test using the 5,000 pairs selected by <ref type="bibr" target="#b18">[19]</ref>. Implementation details. Our ANC-Net is implemented in the PyTorch <ref type="bibr" target="#b28">[29]</ref> framework. We experiment with three convolutional networks as feature backbones, namely, ResNet-50, ResNet-101 and ResNeXt-101. All of them are pre-trained on ImageNet <ref type="bibr" target="#b22">[23]</ref>, and the parameters are fixed during the training of our ANC-Net. The size of the selfsimilarity window is set to 5×5, and channels of ANC module are set to {1, 16, 16, 1}. The model is initially trained for 10 epochs using an Adam optimiser <ref type="bibr" target="#b17">[18]</ref> with a learning rate of 0.001 and applying Gaussian smoothing with a kernel size of 5 for ground truth probability map generation. The model is then fine-tuned for 5 epochs applying Gaussian smoothing with a kernel size of 3 followed by another 5 epochs with a kernel size of 0. To compare with DCC-Net <ref type="bibr" target="#b9">[10]</ref>, we implemented it based on the publicly available official implementation of NC-Net <ref type="bibr" target="#b31">[32]</ref>. Our implementation slightly surpassed the reported accuracy in <ref type="bibr" target="#b9">[10]</ref>. We also implemented UCN ResNet-101 based on the publicly available official code <ref type="bibr" target="#b1">[2]</ref>. Evaluation metric. Following common practice, we use the percentage of correct key-points (PCK@α) as our evaluation metric. We report the results under PCK threshold α = 0.1. α is set w.r.t. max(w r , h r ) where w r and h r are the width and height of either the image or the object bounding box. Following existing works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>, we use α w.r.t. the image size on PF-PASCAL, and w.r.t. the object bounding box on CUB and Spair-71k.  <ref type="bibr" target="#b8">[9]</ref> 72.2 --Weakalign ResNet-101 <ref type="bibr" target="#b30">[31]</ref> 74.8 -21.1 RTNet ResNet-101 <ref type="bibr" target="#b15">[16]</ref> 75.9 --NC-Net ResNet-101 <ref type="bibr" target="#b31">[32]</ref> 78.9 64.7 26.4 DCCNet ResNet-101 <ref type="bibr" target="#b9">[10]</ref> 82.6 66.1 26.7 SFNet ResNet-101 <ref type="bibr" target="#b20">[21]</ref> 81.9 -26.0 HPF ResNet-101 <ref type="bibr" target="#b26">[27]</ref> 84.8 -28.2 HPF ResNet-101-FCN <ref type="bibr" target="#b26">[27]</ref> 88. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark comparisons</head><p>We compare our method with recent state-of-the-art methods, and present our results in <ref type="table" target="#tab_1">Table 1</ref>. For results on PF-PASCAL and Spair-71k, all methods are trained on PF-PASCAL. For results on CUB, the methods are trained and tested on CUB. We used three different feature backbones, i.e. ResNet-50, ResNet-101, and ResNext-101 for our method. When using an identical feature backbone (ResNet-101) with other methods, our ANC-Net achieves the best performance on all the datasets. For example, we achieve 86.1% and 28.7% on PF-PASCAL and Spair-71k respectively. Note that even with the ResNet-50 feature backbone, our model outperforms NC-Net and DCC-Net with the more powerful ResNet-101 feature backbone on all datasets. Further, when we replace our feature backbone with ResNext-101, the performance of our method can be further boosted on all datasets (86.1% to 88.7% on PF-PASCAL, 72.4% to 74.1% on CUB, and 28.7% to 30.1% on Spair-71k). Our results are also better than the previous best results achieved HPF with ResNet-101-FCN. The results clearly demonstrate the effectiveness of our approach. Unbiased evaluation on FP-PASCAL. As discussed in <ref type="bibr" target="#b20">[21]</ref>, there are 302 images in the training split overlapping with either target or source images in the testing split. In terms of images pairs, there are 95 target-to-source pairs in the training split overlapping with the source-to-target pairs in the testing split. Hence, we further conduct an unbiased evaluation by excluding the 302 images and the 95 image pairs respectively. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. Our method consistently outperforms NC-Net and DCCNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>In the ablation experiments, we analyse the effectiveness of all the proposed modules of ANC-Net on PF-PASCAL, with ResNet-101 as the feature backbone. We experiment on four variants of our ANC-Net, namely, ANC-Net (our model with all components), ANC-Net w/o ANC (our model without ANC, i.e. replacing our non-isotropic 4D kernels with the isotropic counterparts), ANC-Net w/o MS (our model with out the multi-scale self-similarity), and ANC-Net w/o Orth (our model trained without orthogonal loss). We also evaluate the three ANC module candidates, denoted as, ANC b , ANC c and ANC d in <ref type="figure">Figure 3</ref>. We also compare with NC-Net and DCCNet. For a fair comparison with them, we also retrain them with the same sparse annotations. The retrained NC-Net is the plain baseline of our method, and the retrained DCCNet can be compared with ANC-Net w/o ANC module for evaluating our multi-scale self-similarity module against the self-similarity module of DCCNet. The results are reported in <ref type="table" target="#tab_4">Table 3</ref>. As can be seen, when we remove each of our proposed modules, the performance drops, showing that all our proposed modules are effective. However, ANC-Net and all its variants perform consistently better than the retrained NC-Net and DC-CNet as well as the original NC-Net and DCCNet. Among the three ANC architectures in <ref type="figure">Figure 3</ref>, ANC d performs better than the other two by a noticeable margin. This might be explained by the fact that ANC d contains more flexible feature combination paths to deal with objects having more severe scale variations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative evaluations</head><p>Source DCCNet NC-Net ANC-Net <ref type="figure">Figure 5</ref>: Predicted correspondence and correlation map for a query key-point. The first column shows the source images with a query key-point marked with cyan cross. The remaining columns show the correlation maps superimposed with the target image. The red and cyan crosses represent the prediction and the ground truth respectively. ANC-Net predicts single-peak correlation maps, avoiding catastrophic failure between distant, but ambiguous keypoints, such as the legs of the dog in the first row. Best viewed in electronic form.</p><p>We show two sets of qualitative experiments. The first set of qualitative experiments is shown in <ref type="figure">Figure 5</ref>. It includes examples of key-points with some degree of ambiguity, such as the limbs of an animal or a table. With both NC-Net and DCCNet, it can be seen that there are often multiple peaks in the correlation maps. In some cases, this can lead to failures where, although the key-points look</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>DCCNet NC-Net ANC-Net Target <ref type="figure">Figure 6</ref>: Dense correspondence prediction. Given the correlation map predicted by the model, we compute a dense flow field to warp the source image to the target image. ANC-Net can capture the scale of the objects better than other methods. Best viewed in electronic form. alike, they are far from the true correspondence. In contrast, ANC-Net tends to produce correlation maps with a single dominant peak. This drastically reduces the occurrence of these failures due to the ambiguous nature of a key-point. We qualitatively evaluate the dense correspondence prediction of ANC-Net in <ref type="figure">Figure 6</ref>. From a correlation map predicted by the network, we compute a dense flow field, which maps pixel locations from the source to the target image. In general, ANC-Net and NC-Net preserve more details in the warping than DCCNet, and ANC-Net is able to capture the scale of the target more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a convolutional neural network, called ANC-Net, for dense semantic matching. ANC-Net takes a pair of images depicting different objects from the same category as input, and produces a dense 4D correlation map containing all the pair-wise matches in the feature space. Pixel-wise semantic correspondences can then be extracted from the 4D correlation map. ANC-Net can be trained end-to-end with sparse key-point annotations. At the core of ANC-Net is our proposed 4D nonisotropic convolution kernels, which incorporates an adaptive neighbourhood consensus constraint for robust matching, and our proposed multi-scale self-similarity module, which aggregates multiple self-similarity features that are insensitive to intra-class appearance variation. We also proposed a novel loss, called orthogonal loss, that can enforce a one-to-one matching constraint, encouraging plausible matching results. We have thoroughly evaluated the effectiveness of our method on various benchmarks, and it substantially outperforms state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Self-similarity module. The top left figure illustrates the calculation of a self-similarity score over the 3 × 3 window. Specifically, the cosine distances between each of the 9 features and the centre feature are calculated and then vectorised into the S 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 Figure 3 :</head><label>53</label><figDesc>Adaptive neighbourhood consensus. The top row illustrates an isotropic and a non-isotropic 4D convolutional kernel. The bottom row illustrates the architecture of (a) the non-isotropic in NC-Net<ref type="bibr" target="#b31">[32]</ref> and (b-d) three ANC candidates. ⊕ denotes concatenation of feature maps. The numbers {1, 16, 16, 1} denote the input and output channels for the 4D kernels. The non-isotropic 4D convolutions are always zero padded so that the size of the 4D correlation remains the same size after each convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Generating the ground-truth probability map for each key point. (a) The key point (x, y) is a key point in the image coordinates. (b) (x n , y n ) is the nearest neighbour of (x c , y c ) which is re-scaled coordinate (x, y) to the feature map resolution. (c) (x 1 n , y 1 n ), (x 2 n , y 2 n ), (x 3 n , y 3 n ), and (x 4 n , y 4 n ) are the four nearest neighbours to (x c , y c ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art methods.</figDesc><table><row><cell>Methods</cell><cell cols="3">PF-PASCAL CUB Spair-71k</cell></row><row><cell>Identity mapping</cell><cell>37.0</cell><cell>14.6</cell><cell>3.7</cell></row><row><cell>UCN GoogLeNet [2]</cell><cell>55.6</cell><cell>48.3</cell><cell>15.1</cell></row><row><cell>UCN ResNet-101 [2]</cell><cell>75.1</cell><cell>52.1</cell><cell>17.7</cell></row><row><cell>SCNet VGG-16</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Unbiased evaluation on PF-PASCAL.</figDesc><table><row><cell>Methods</cell><cell cols="3">Original w/o 95 w/o 302</cell></row><row><cell>NC-Net ResNet-101 [32]</cell><cell>78.9</cell><cell>78.8</cell><cell>80.3</cell></row><row><cell>DCCNet ResNet-101 [10]</cell><cell>82.6</cell><cell>78.7</cell><cell>75.7</cell></row><row><cell>ANC-Net ResNet-101</cell><cell>86.1</cell><cell>84.2</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study experimental results.</figDesc><table><row><cell>Method</cell><cell>PCK@0.1</cell></row><row><cell>NC-Net [32] (original/retrain)</cell><cell>78.9/81.9</cell></row><row><cell cols="2">DCCNet [10] (original/retrain) 82.6/83.7</cell></row><row><cell>ANC-Net w/o ANC</cell><cell>84.1</cell></row><row><cell>ANC-Net w/o MS</cell><cell>84.3</cell></row><row><cell>ANC-Net w/o Orth</cell><cell>85.9</cell></row><row><cell>ANC-Net w/ ANC b</cell><cell>82.7</cell></row><row><cell>ANC-Net w/ ANC c</cell><cell>83.8</cell></row><row><cell>ANC-Net w/ ANC d</cell><cell>86.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We gratefully acknowledge the support of the European Commission Project Multiple-actOrs Virtual EmpathicCARegiver for the Elder (MoveCare) and the EPSRC Programme Grant Seebibyte EP/M013774/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building Rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Conf. on Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal Correspondence Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A graphmatching kernel for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Conf. on Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-rigid dense correspondence with applications for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of ACM Special Interest Group on GRAPHICS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Trans. Pattern Anal. Machine Intell. (PAMI)</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Conf. on Computer Vision (ICCV</title>
		<meeting>Intl. Conf. on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic Context Correspondence Network for Semantic Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Conf. on Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized deformable spatial pyramid: Geometrypreserving dense correspondence estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwasup</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Chul Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parn: Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonnegative Orthogonal Graph Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligience(AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligience(AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4089" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Conf. on Learning Representations (ICLR)</title>
		<meeting>Intl. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised Learning for Video Correspondence Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning semantic correspondence exploiting an object-level prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonkyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12914</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet: a Large-Scale Hierarchical Image Database Shrimp Project View project hybrid intrusion detction systems View project ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell. (PAMI)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conf. on Neural Information Processing Systems-Natural and Synthetic (NIPS)</title>
		<meeting>IEEE Conf. on Neural Information Processing Systems-Natural and Synthetic (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Conf. on Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotný</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-End Weakly-Supervised Semantic Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neighbourhood Consensus Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conf. on Neural Information Processing Systems-Natural and Synthetic (NIPS)</title>
		<meeting>IEEE Conf. on Neural Information Processing Systems-Natural and Synthetic (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Benchmarking 6DOF Urban Visual Localization in Changing Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deunsol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Correspondence from the Cycle-consistency of Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010- 001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning of graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2684" to="2693" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
