<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Positive Bounding Boxes for Balanced Training of Object Detectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
							<email>kemal.oksuz@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Can</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cam</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<email>eakbas@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
							<email>skalkan@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Positive Bounding Boxes for Balanced Training of Object Detectors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two-stage deep object detectors generate a set of regions-of-interest (RoI) in the first stage, then, in the second stage, identify objects among the proposed RoIs that sufficiently overlap with a ground truth (GT) box. The second stage is known to suffer from a bias towards RoIs that have low intersection-over-union (IoU) with the associated GT boxes. To address this issue, we first propose a sampling method to generate bounding boxes (BB) that overlap with a given reference box more than a given IoU threshold. Then, we use this BB generation method to develop a positive RoI (pRoI) generator that produces RoIs following any desired spatial or IoU distribution, for the secondstage. We show that our pRoI generator is able to simulate other sampling methods for positive examples such as hard example mining and prime sampling. Using our generator as an analysis tool, we show that (i) IoU imbalance has an adverse effect on performance, (ii) hard positive example mining improves the performance only for certain input IoU distributions, and (iii) the imbalance among the foreground classes has an adverse effect on performance and that it can be alleviated at the batch level. Finally, we train Faster R-CNN using our pRoI generator and, compared to conventional training, obtain better or on-par performance for low IoUs and significant improvements when trained for higher IoUs for Pascal VOC and MS COCO datasets. The code is available at: https://github.com/kemaloksuz/ BoundingBoxGenerator. * Equal contribution for senior authorship.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An important challenge in object detection is class imbalance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>: even from a single image, an infinite number of negative examples can be sampled, in contrast to only a limited set of positive RoIs. Naturally, this leads to significant imbalance between negative and positive RoIs. Class imbalance also exists within foreground <ref type="figure" target="#fig_4">Figure 1</ref>: (a) An illustration of Bounding Box (BB) Generation. Given a reference box (in blue) and an IoU threshold T , a BB having at least T IoU is generated (drawn in green). (b) An illustration of training an object detector with positive region-of-interests. Given distribution requirements on foreground classes and BBs, we generate positive RoIs using the BB generator <ref type="figure" target="#fig_4">(Fig 1(a)</ref>). Negative RoIs are still generated by the region proposal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>classes.</head><p>A prominent solution to the foreground-background class imbalance is to have two stages <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>: The first stage estimates regions (i.e., region-of-interests -RoIs) that are likely to contain objects, significantly discarding background samples, and the second-stage classifies these regions into objects, and also fine-tunes the coordinates of the bounding boxes. Other solutions generally employ sampling with hard constraints (e.g., online hard example mining <ref type="bibr" target="#b30">[31]</ref>, Libra RCNN <ref type="bibr" target="#b26">[27]</ref>) or soft constraints (e.g., focal loss <ref type="bibr" target="#b19">[20]</ref>, harmonizing gradients <ref type="bibr" target="#b16">[17]</ref>).</p><p>The foreground-foreground class imbalance problem, i.e., the imbalance in the number of examples pertaining to different positive classes at the image, dataset or mini-batch levels, has not attracted as much attention. In addition, the intersection-over-union (IoU) distribution of the RoIs generated by the region proposal network (RPN) <ref type="bibr" target="#b29">[30]</ref> is imbalanced <ref type="bibr" target="#b0">[1]</ref>, which biases the BB regressor in favor of the IoU that the distribution is skewed towards. We call this imbalance problem as BB IoU imbalance. Addressing these problems requires a careful analysis of the positive RoIs.</p><p>In this paper, we analyze and address foregroundforeground class imbalance and BB IoU imbalance by actively generating BBs. We first propose the "BB generator", a method that can generate an arbitrary BB overlapping with a reference box with an IoU larger than a given threshold <ref type="figure" target="#fig_4">(Figure 1(a)</ref>). Using the BB generator, we develop a positive RoI (pRoI) generator that can produce RoIs conforming to desired BB IoU and spatial distributions <ref type="figure" target="#fig_4">(Figure 1(b)</ref>).</p><p>Considering that there is a correlation between the hardness of an example and its IoU <ref type="bibr" target="#b26">[27]</ref>, the pRoI generator is able to generate (rather than sample) not only positive or negative samples, but also samples with any desired property such as hard examples <ref type="bibr" target="#b30">[31]</ref> or prime samples <ref type="bibr" target="#b1">[2]</ref>.</p><p>We use our pRoI generator to perform several analyses and improvements. Specifically, we (i) show that BB IoU and foreground class distributions affect performance, (ii) make a comparative analysis for RPN RoIs and (iii) improve the performance of Faster RCNN for IoU intervals where RPN is not able to generate enough samples.</p><p>Finally, we devise an online, foreground-balanced (OFB) sampling method which considers the imbalance among the foreground classes dynamically within a training batch based on multinomial sampling.</p><p>Overall, our contributions can be summarized as follows: 1. Generators: (i) A BB generator to generate BBs for a given IoU threshold and (ii) a positive RoI generator to generate RoIs with desired foreground class, IoU and spatial distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Imbalance Problems and Analysis:</head><p>We introduce the BB imbalance problem. Using our pRoI generator, we show that these imbalance problems and the foregroundforeground class imbalance within a training batch affect the performance of the object detectors. We also provide an analysis of RPN RoIs and show that the effect of the hard examples depends on the IoU distribution of the BBs. 3. Practical Improvements: We train a detection network using our pRoI generator, which increases the amount and the diversity of the positive examples especially for the larger IoUs, and show that the performance improves compared to the standard training (e.g. for IoU = 0.8, mAP@0.8 improves by 10.9% for Pascal VOC). We also train the conventional detection pipeline by using the proposed OFB sampling, and improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Object Detectors: We can group deep object detectors into two: One-stage methods and two-stage methods. While one-stage methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> predict the object categories and their bounding boxes directly from anchors, two-stage methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> first estimate a set of RoIs from anchors and then predict objects from these RoIs in the second stage. Both approaches use a deep feature extractor <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>, optionally followed by steps like feature pyramid networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Our BB sampling approach is more suitable for the second stage of the two-stage methods since one-stage detectors have structural constraints owing to the fact that each output of a one-stage detector corresponds to a predefined anchor having fixed location, shape and scale. For this reason, an additional module is required to employ our generator. However, BB imbalance and OFB sampling are relevant for any object detection pipeline since any object detector needs to deal with bounding boxes even if they are estimated or fixed (in the case of anchors). Class Imbalance in Object Detection: Following the work by Oksuz et al. <ref type="bibr" target="#b24">[25]</ref>, we categorize the class imbalance problem for the deep object detectors into two: foreground-background class imbalance and foregroundforeground class imbalance.</p><p>Foreground-background class imbalance has attracted more attention from the community with hard sampling, soft sampling and generative approaches. In hard sampling, certain samples are shown more to the network to address imbalance. This can be performed e.g. via random sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>, or by relying on "sample usefulness" heuristics as in hard-example mining <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> and prime sampling <ref type="bibr" target="#b1">[2]</ref>. Hard-example mining methods usually assume that examples with higher loss are more difficult to learn, and therefore, they train a network more with such examples. This approach is adopted for negative samples in SSD <ref type="bibr" target="#b22">[23]</ref>, while a more systematic approach considering both the positive and negative samples is proposed in online hard example mining (OHEM - <ref type="bibr" target="#b30">[31]</ref>). An alternative hardness definition was proposed in Libra R-CNN <ref type="bibr" target="#b26">[27]</ref> based on a sample's IoU, and a solution was proposed using hard example mining using BB IoUs without computing the loss for the entire set. A recent interesting method, "prime sampling" <ref type="bibr" target="#b1">[2]</ref>, asserts that positive samples with higher IoUs are more representative and proposed ranking the positive samples based on its IoU with the ground truth, while still showing that hard example mining for the negative class works well. BB IoU imbalance is addressed by Cascade R-CNN <ref type="bibr" target="#b0">[1]</ref> by employing cascaded detectors in such a way that a later-stage detector is trained by a distribution skewed towards higher IoU. However, this, requiring multiple detectors being trained, is computationally prohibitive.</p><p>In soft sampling, a weight is assigned to each sample rather than performing a discrete (hard) selection of samples. Prominent examples include focal loss <ref type="bibr" target="#b19">[20]</ref>, which promotes hard examples; prime sampling <ref type="bibr" target="#b1">[2]</ref>, which assigns more weight to examples with higher IoUs; and finally gradient harmonizing mechanism <ref type="bibr" target="#b16">[17]</ref>, which assigns lower weights to easy negatives and suppresses the effect of the outliers. The generative methods address imbalance with a different perspective by introducing generated samples. Example approaches include generating hard examples with various deformations and occlusion <ref type="bibr" target="#b31">[32]</ref> and generating synthetic examples <ref type="bibr" target="#b11">[12]</ref>.</p><p>Foreground-foreground class imbalance is critical as well. Kuznetsova et al. <ref type="bibr" target="#b15">[16]</ref> showed that object detection datasets are highly imbalanced also for foreground classes. The only method to consider the problem at the dataset level handcrafts a similarity measure, and based on the measure clusters the classes to have a more balanced training <ref type="bibr" target="#b25">[26]</ref>. In the classification domain where there is no background class, this imbalance is studied more <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> by, e.g., performing class-aware sampling <ref type="bibr" target="#b17">[18]</ref>. However, these methods are not directly applicable for two-stage object detectors because the second stage's input is very dynamic since it depends on RoIs estimated by the first stage. Despite this difference, class-aware sampling is said to be adopted by <ref type="bibr" target="#b21">[22]</ref>, however no comparison is presented for balanced and imbalanced training from the object detection perspective.</p><p>The ideas in this paper are relevant for both foregroundbackground and foreground-foreground class imbalance. One can generate any number of positive RoIs to address the foreground-background imbalance, and the generated set can also be chosen equally from each class to address the foreground-foreground imbalance. Among the three types of methods mentioned above, we classify our approach as a generative method. Since the end-to-end training pipeline is not disrupted (see <ref type="figure" target="#fig_4">Figure 1</ref>(b)), any hard sampling method <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b26">27]</ref> can also be simulated. In addition, we directly address foreground-foreground class imbalance by online foreground balanced (OFB) sampling. Its main difference from the previously proposed class-aware sampling <ref type="bibr" target="#b17">[18]</ref> is that while they use a static dataset, our OFB sampling is able to handle the dynamic nature of the RoIs (i.e. the batch depends on the sampled RoIs at each iteration) owing to the proposal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Generators</head><p>In this section, we describe the methods for generating bounding boxes and balanced positive RoIs. area of B is simply defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Definitions and Notation</head><formula xml:id="formula_0">A(B) = (x 2 − x 1 ) × (y 2 − y 1 ),<label>(1)</label></formula><p>and the area of the intersection between B andB is:</p><formula xml:id="formula_1">I(B,B) = (min (x 2 , x 2 ) − max (x 1 , x 1 )) × (2) (min (ȳ 2 , y 2 ) − max (ȳ 1 , y 1 ).</formula><p>Based on this notation, IoU(B,B) can be easily defined as:</p><formula xml:id="formula_2">IoU(B,B) = I(B,B) A(B) + A(B) − I(B,B) .<label>(3)</label></formula><p>Finally, we note two useful properties of the IoU function: (Theorem 1) IoU(B,B) is scale-invariant, and (Theorem 2) IoU(B,B) is translation-invariant (see Appendix for proofs). These theorems allow us to shift and scale the input BBs to a reference box during BB generation and then shift and scale them back to their original aspect ratio and location..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bounding Box Generator</head><p>Algorithm 1 Bounding Box Generator. See Section 3.2 and the Appendix for the definitions of the functions. This order leads to a non-isotropic distribution with respect to the reference box. To make it isotropic, we can also sample in the reverse order: i.e. sample BR first then TL. We then randomly choose the order, before sampling. <ref type="figure">Fig.  3</ref> superimposes 1000 generated boxes with T = 0.6.</p><p>The following two sections discuss how the feasible space is computed (i.e. findTLFeasibleSpace(B, T )) and how a point can be sampled within a polygon (i.e. samplePolygon(T LP oly)). We refer the interested reader to check the Appendix for BR(B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Determining Feasible Space for the Desired IoU</head><p>findTLFeasibleSpace(B, T ) is the function determining the feasible set of points that can be the top left point of a box ensuring the desired IoU. In order to find the set of these feasible points (i.e. TL(B)) that satisfy Eq. 3, we assume that BR(B) = BR(B) and manipulate Eq. 3, otherwise, some feasible points are excluded in the feasible top left space. Even though BR(B) is fixed, there are still two unknown variablesx 1 andȳ 1 . That's why, we first bound one of these two variables and then find the value Denoting the minimum and maximum bounds ofx 1 in Region I by x I min and x I max respectively, we bound the values in x axis. It is obvious that x I min = x 1 due to the boundary of Region I. To find x I max , we manipulate Eq. 3 by exploiting thatȳ 1 = y 1 for x I max , which yields:</p><formula xml:id="formula_3">x I max = x 2 − (x 2 − x 1 ) × T.<label>(4)</label></formula><p>Having determined the boundaries forx 1 , now we derive a function that determinesȳ 1 givenx 1 . Finally, moving within the bounds yieldsx 1 ,ȳ 1 pairs satisfying IoU(B,B) = T when BR(B) = BR(B). In region I, note that I(B,B) does not rely onȳ 1 (i.e. I(B,B) = (x 2 −x 1 )(y 2 − y 1 )). Bringing these together,ȳ 1 can be defined as (see Appendix for the entire derivation of x I max andȳ 1 ):ȳ</p><formula xml:id="formula_4">1 = y 2 − I(B,B) T + I(B,B) − A(B) (x 2 −x 1 ) .<label>(5)</label></formula><p>Here, we only show the derivation steps for Region I and present the equations for all regions in Appendix. Combining the points in all these regions yields the polygon limiting feasible region with IoU ≥ T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Controlling the Spatial Distribution of the Boxes</head><p>samplePolygon(T LP oly) function determines the BB spatial distribution. We follow rejection sampling <ref type="bibr" target="#b2">[3]</ref> in such a way that a point is proposed by the proposal distribution until it hits the inside of the polygon. Accordingly, the proposal distribution determines the BB spatial distribution. <ref type="figure" target="#fig_2">Fig. 4</ref>(b) presents an example for spatial uniform distribution for the top-left space polygon with T = 0.75. We sample a point in the rectangle uniformly, which corresponds basically to generating two uniform numbers within a range. If the point is in the polygon, then it is accepted, else a new point is proposed until it is inside the polygon. Note that different proposal distributions lead to different BB spatial distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">pRoI Generator: Training by Generated BBs</head><p>This section provides an application of our BB generator for generating positive RoIs for training a two-stage object detector. By applying our BB generator to the ground-truth boxes, we can generate positive RoIs with desired characteristics. This enables us to (i) analyze how the performance of Faster R-CNN is affected by the properties of the positive RoIs and (ii) improve the performance for IoU intervals where RPN is not able to generate enough samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Positive RoI Generator.</head><p>See Section 3.3 and the Appendix for the definitions of functions fgBalancedRoIAlloc and genRoIs.</p><formula xml:id="formula_5">1: procedure GENERATEPROI(GT s, ψ IoU , W IoU , RoIN um) 2: perGtRoI = fgBalancedRoIAlloc(GT s, RoIN um) 3: RoIs = genRoIs(GT s, perGtRoI, ψ IoU , W IoU , RoIN um) 4:</formula><p>return RoIs 5: end procedure The method, "Positive RoI Generator" (pRoI Generator), described in Algorithm 2, can control several different characteristics of the set of positive RoIs. fgBalancedRoIAlloc() first divides RoIN um by the number of different classes in the given ground truth set, GT s, to determine the allocated box number per class, and then shares this value among each example of the same class equally. As a result, fgBalancedRoIAlloc() determines the number of boxes to be generated for each ground truth box in GT s. Secondly, given the allocated number of boxes for each ground truth, genRoIs() iteratively uses BB generator as a subroutine to provide a set of RoIN um RoIs. In this step the BB IoU distribution requirement is determined by the inputs ψ IoU , the base of the IoU bins and the weight of the each bin denoted by W IoU . W IoU is basically a multinomial distribution over the bins determined by ψ IoU . An important benefit of pRoI generator is that training with the generated RoIs has no impact on the gradient flow for the training process (see Appendix). At each training iteration, RPN generates a set of RoIs among which we discard the positive ones and use the positive RoIs generated by the proposed method ( <ref type="figure" target="#fig_4">Fig. 1(b)</ref>). Using our pRoI generator, we can address the imbalance problems regarding RoIs at three different levels:</p><p>(1) Foreground-foreground class imbalance, which occurs when a dataset or mini-batch (or batch) contains different numbers of positive examples from different classes. To illustrate on a batch, an image (used as a batch) from PASCAL dataset <ref type="bibr" target="#b5">[6]</ref> includes 4 bottles, 2 persons, 2 dining tables and 1 chair. In such a case, having equal number of RoIs per instance may lead the model to be biased in favor of the bottle class while ignoring the chair class. In our pRoI Generator, fgBalancedRoIAlloc() function allocates the same number of RoIs for each class within the batch.</p><p>(2) BB IoU imbalance, which occurs when the positive RoIs have a skewed IoU distribution ( <ref type="figure" target="#fig_3">Fig. 5</ref>). It has been shown that the hardness of a RoI is related to its IoU <ref type="bibr" target="#b26">[27]</ref> and also the regressor overfits to RoIs which has IoU around 0.5 when the distribution of the RPN proposals is concentrated towards 0.5 <ref type="bibr" target="#b0">[1]</ref>. Thus, these recent findings imply that the IoU distribution has an important effect on training. As aforementioned, genRoIs() is able to control the IoU distribution of the BBs.</p><p>(3) BB spatial imbalance, which occurs when the BBs intersect significantly and a diverse set of examples can not be provided to the detection network. This level of imbalance is controlled in our pRoI generator in the subroutine BB generator as discussed in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Dataset and Implementation Details: We evaluate our generative methods on Faster R-CNN in two different settings: (i) on Pascal VOC 2007 <ref type="bibr" target="#b5">[6]</ref> with backbone ResNet-101 following the implementation and training in <ref type="bibr" target="#b33">[34]</ref> with batch size 1 image on 1 GPU, and (ii) on MS COCO <ref type="bibr" target="#b20">[21]</ref> with backbone ResNet-50 following the implementation and training in <ref type="bibr" target="#b3">[4]</ref> with batch size 2 images/GPU on 2 GPUs. Performance Measures: We exhaustively search for the best mean-average-precision (mAP) and mean-optimallocalization-precision-recall (moLRP) error <ref type="bibr" target="#b23">[24]</ref> values over epochs and report them. moLRP is a recently introduced metric for object detection, which represents recall, precision and average tightness of the BBs. Note that mAP is a higher-is-better measure, while moLRP is an error metric and thus, it is a lower-is-better measure. RoI Sources: In addition to RoIs output by RPN, we use the RoIs generated by our pRoI generator, with a given distribution, during the analysis and training. The different distributions are obtained by controlling W IoU (see Appendix for the exact configurations of W IoU ) in Algorithm 2. Unless otherwise stated, we set ψ IoU = [0.5, 0.6, 0.7, 0.8, 0.9] and RoIN um = 32. We train these RoI sources with and without foreground balanced sampling in order to see the effects of different imbalance problems on different RoI sources. The results are presented in <ref type="table" target="#tab_0">Table 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Imbalance Problems and Analysis of RPN RoIs</head><p>In this section, we first point out some imbalance on the distribution of BBs. Then, we investigate how several characteristics of RoIs affect detection performance by generating RoIs with our pRoI generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">BB IoU Imbalance</head><p>Our BB generator method (Algorithm 1) samples boxes for a given IoU threshold, spatially uniformly. It does not impose an upper bound for the IoUs of the sampled boxes. Therefore, in order to analyze the density of the different IoUs for the positive samples, we uniformly generate 100K boxes for each IoU distribution type and plot the distribution of the generated boxes in <ref type="figure" target="#fig_3">Fig. 5</ref>. Note that training a detector with different IoU distributions of positive examples affects the resulting test performance (see <ref type="table" target="#tab_0">Table 1</ref>), which implies the effect of BB IoU imbalance.</p><p>From <ref type="figure" target="#fig_3">Fig. 5</ref>, we observe the following:</p><p>(1) The distribution of the boxes with baseIoU = 0.5 is highly biased towards 0.5 and includes very low samples with higher IoUs. This implies that the proportion of the boxes with IoU &gt; 0.9 is far too low than that of the boxes with 0.6 &gt; IoU &gt; 0.5 when T = 0.5.</p><p>(2) RPN RoIs follow a similar tendency to the sampled boxes with baseIoU = 0.5 since the RoIs are based on anchors, which are uniformly distributed with a fixed set of boxes on the image. Thanks to the RPN regressor, the IoU distribution improves compared to the distribution of the sampled boxes with baseIoU = 0.5. On the other hand, this bias towards 0.5 is previously argued to make the regressor overfit for smaller IoUs <ref type="bibr" target="#b0">[1]</ref>.</p><p>(3) RPN is able to provide hard positive examples inherently; however, the number of prime samples (i.e. examples with larger IoUs) is quite low. This is critical since it is shown that prime sampling performs better than hard positive mining <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Foreground-Foreground Class Imbalance</head><p>We observe that, for each RoI source, addressing foreground-foreground imbalance (f g balance = 1) improves performance in terms of both mAP and moLRP, especially for the right skew and uniform cases (See <ref type="table" target="#tab_0">Table 1</ref>). Moreover, addressing foreground-foreground class imbalance does not seem to affect the localization error (moLRP IoU ) but improves the classification performance since mAP@0.5, moLRP FP and moLRP FN get better (except for the left-skew case). Therefore, we conclude that foreground-foreground class imbalance can also be alleviated by employing methods in the batch level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of Online Hard Positive Mining</head><p>Here we demonstrate another useful use-case of our pRoI generator by simulating OHEM <ref type="bibr" target="#b30">[31]</ref> on positive examples. OHEM chooses the positive and negative examples with the highest loss values after applying NMS to the examples to preserve example diversity. A recent study <ref type="bibr" target="#b26">[27]</ref> showed that the IoU and the hardness of an example are correlated. On the other hand, another study <ref type="bibr" target="#b1">[2]</ref> proposed an opposite perspective to the OHEM based on prioritizing "prime samples", i.e. samples with high IoUs. To be more clear, OHEM <ref type="bibr" target="#b30">[31]</ref> implies preferring positive examples with IoUs just above 0.5, while prime sampling asserts that the higher the IoU, the better the example. To make an analysis on the positive examples, we simulate OHEM by (i) initially generating 128 BBs by pRoI generator, (ii) applying NMS using loss value of an example, (iii) finally selecting the ones with the larger loss values. We coin this as online hard positive mining (OHPM). OHPM also presents an example where pRoI generator can simulate sampling schemes.</p><p>In our experiments, we also observe that the effect of the c <ref type="figure">Figure 6</ref>: Spatial Distribution of the top left points of 2, 500 RPN RoIs and maximum IoU Limits from IoU = 0.9 to 0.5 (in-out direction) hard examples depends on the IoU distribution of the RoIs and high-quality samples are required during training: In <ref type="table" target="#tab_0">Table 1</ref>, when OHPM is applied, uniform and right-skew distributions, which have more difficult examples due to their distribution ( <ref type="figure" target="#fig_3">Fig. 5)</ref>, have better performance compared to the left-skew and "Base IoU=0.5" cases. Moreover, while OHPM does not improve the performance of left-skew and "Base IoU=0.5" cases, it is crucial for the right-skew and uniform distributions <ref type="table" target="#tab_0">(Table 1)</ref>. Therefore, similar to prime sampling <ref type="bibr" target="#b1">[2]</ref>, we show that examples with higher IoUs are crucial during training, however, we also show that these examples should be supported by hard examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">BB Spatial Imbalance</head><p>We now analyze the spatial distribution of the RPN RoIs and how they fit within the theoretical IoU boundaries in <ref type="figure">Fig. 6</ref>. To be able to make such an analysis, we selected a reference box with [x 1 , y 1 , x 2 , y 2 ] = [0.3, 0.3, 0.6, 0.6]. At the final epoch of the RPN training, we track positive RPN proposals and their associated ground truths. As discussed in Section 3.2, we scaled and shifted the ground truths to the reference box and applied the same transformations to their associated positive RPN proposals (i.e., RoIs). Among the positive RPN proposals, top-left (TL) points of the 2, 500 RoIs are plotted with green dots in <ref type="figure">Fig. 6</ref>. Then, using findTLFeasibleSpace() function in Algorithm 1, we plot the theoretical limits for the top left points for RoIs with IoUs larger than 0.5, 0.6, 0.7, 0.8 and 0.9. <ref type="figure">Fig. 6</ref> leads to several key findings: (1) As expected, as the IoU decreases, the boundaries occupy a larger space around the TL point of the reference box. A result of this is that the sample space for 0.9 is very small, which makes it more difficult to have distinct RoIs with IoU &gt; 0.9. (2)  We observe that no TL point is outside of the 0.5 boundary, which is a sanity check for the boundaries since a RoI is labeled as positive if it has at least 0.5 IoU with a ground truth.</p><p>(3) The TL points of the RPN RoIs are accumulated around the TL point of the reference box and they are not uniformly distributed within the 0.5 boundary. (4) The TL points of the RPN RoIs tend to be inside the reference box more than to be outside. Specifically, RPN RoIs between x &gt; 0.3, y &gt; 0.3 and x &lt; 0.3, y &lt; 0.3 are 28.2% and 21.0% of the all, respectively. Especially the last two observations may be critical for an object detector since they may result in a positive bias towards specific RoIs and may make the generalization difficult over the entire spatial space. However, the effects of all these observations require experimental or theoretical validation that is not provided in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Practical Improvements</head><p>In this section, we use OFB sampling and BB generator to improve an object detector by addressing foregroundclass imbalance and by controlling the number and distribution of RoIs for training the second-stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Online Foreground Balanced Sampling</head><p>In end-to-end training, the set of positive RoIs are limited and they are not generated as in pRoI generator. Motivated from the analysis using pRoI generator on the effect of foreground-foreground class imbalance (see Section 5), we propose an online sampling method to be used in the conventional training pipeline. Denoting the total number of classes in a batch by C and the number of positive RoIs for class c by k c , each RoI is assigned a probability 1/(Ck c ) and the subset of RoIs to train Faster R-CNN is sampled from this multinomial distribution. We call this sampling scheme as Online Foreground Balanced (OFB) Sampling.</p><p>In order to see the effect, we train Faster R-CNN with and without OFB sampling and present results in <ref type="table" target="#tab_1">Tables 2  and 3</ref>. For the Pascal VOC <ref type="bibr" target="#b5">[6]</ref>, we observe 0.5% improvement in mAP@0.5 and moLRP, with better performance in precision and recall components of moLRP and no impact on the regression branch. In our experiments with MS COCO <ref type="table" target="#tab_2">(Table 3)</ref>, we compared our results with hard example mining <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. Similar to the findings of Cao et al. <ref type="bibr" target="#b1">[2]</ref> and our analysis in Section 5, while hard positive mining does not improve performance, our OFB sampling is beneficial for foreground examples. Moreover, the table shows that OFB sampler can be combined with sampling approaches for negative BBs. In any case, similar to our experiments for Pascal VOC, the best performance gain is in mAP@0.5. This suggests that controlling RoIs to balance foreground classes has also a role during training of the object detectors and OFB, an efficient sampling algorithm, can be considered a basic solution for the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Generating More Samples in Higher IoUs</head><p>Our approach can be integrated into an object detector without any hindrance on the gradient paths (see Appendix). In this section, we compare a detector trained with our pRoI Generator with a detector trained with the conventional method (i.e. using RPN RoIs) -see <ref type="table">Table 4</ref>. We use Uniform RoI source with foreground balance and OHPM since it performed the best in <ref type="table" target="#tab_0">Table 1</ref>. For IoU = Θ, we randomly sample negative samples from the output of the RPN in the range [0.1, Θ] and the positive samples are provided by the pRoI generator also using OHPM. To apply OHPM, we first generate RoIN um boxes, then select f g many from them. In IoUs 0.6 − 0.8, for which fewer RoIs are possible than 0.5, we initially train the models for 1 epoch by setting f g = 32 and bg = 96 and track "Mean RoI #" to see an upper bound for the models to generate RoIs and prevent class imbalance modelwise. In this run, Mean RoI # for IoUs 0.6, 0.7, 0.8 are 17.26, 7.60, 1.72 for RPN and 20.0, 11.41, 4.67 for pRoI-Uniform respectively. Then using IoU = 0.5 as an example, we multiply the resulting "Mean RoI #" by 1.5 and set f g approximately to it with bg = 3 × f g as in the conventional training. This approach makes training more stable and fair especially for the RPN (see <ref type="table">Table 4</ref>) by balancing foreground and background consistently.</p><p>Looking at <ref type="table">Table 4</ref> and comparing the methods in the IoUs that they are trained for, we observe the following: (1) For IoU = 0.5, 0.6 and IoU = 0.7 we get comparable results with the conventional training.</p><p>(2) For IoU = 0.8, where RPN is not able to generate sufficient samples, the performance increases significantly in terms of both metrics since, at each iteration, generated positive boxes are provided consistently to the second stage. <ref type="figure">(3)</ref> Overall, the mean RoI # is approximately four times higher at IoU = 0.8; and, mAP@0.8 and moLRP improve by 10.9% and 4.8% respectively. A similar trend is also achieved for IoU = 0.9.</p><p>In short, these results demonstrate that it is possible to train an object detector using BB generator with comparable results for lower IoUs and significantly better performance for higher IoUs. On par performance for low IoUs can be owing to the fact that there are sufficient amount of samples for these cases to see any imbalance effect. Effect of RoIN um: Apart from the input parameters to determine the nature of the RoI source, RoIN um is the only new hyperparameter in Algorithm 2. In <ref type="table" target="#tab_4">Table 5</ref>, we observe that training improves (mAP increases) when RoIN um is increased because we have more positive samples at each iteration. However, more samples mean slower (yet still acceptable) training speed compared to conventional training having 0.23s training speed. Preliminary Results on MS COCO: In order to back up our claims, we also conducted an experiment on MS COCO dataset using IoU = 0.8 with Faster R-CNN. Compared to the baseline achieving moLRP = 95.1 and mAP@0.8 = 13.2, using pRoI generator the model has moLRP = 93.7 and mAP@0.8 = 15.3. These results suggest that our model is able to generate more diverse examples than the baseline in larger IoUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we proposed a BB generator and a positive RoI generator. We showed that generated RoIs can be used both as an analysis tool (owing to its controllable nature) and a training method for the two-stage object detectors.</p><p>We showed that there is a bias in the RPN RoIs' IoU and spatial distribution with respect to the IoU boundaries that are physically possible and analyzed the IoU distributions of RPN and other RoI sources.</p><p>Using our BB generator, we developed a pRoI generator that can generate RoIs overlapping with a GT box with a desired IoU or spatial distribution. Then, we trained Faster R-CNN's second-stage with the RoIs generated according to different distributions. We showed that, by producing more samples than RPN, we can achieve better or comparable performance to Faster R-CNN. Moreover, our results reconciliated two conflicting recent studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref> that both using high-IoU RoIs and hard examples can have positive effect on the training if the IoU distribution is appropriate.</p><p>Our ideas can be used for analyzing the anchors of a one- <ref type="table">Table 4</ref>: Performance Comparison with RPN on PASCAL VOC. RoIN um is the input of pRoI generator, fg/bg is the desired fg and bg RoI numbers during training, and Mean RoI # is the actual mean of number of positive RoIs. Note that fg/bg RoI numbers are set differently for pRoI and RPN so that the best performance is achieved for both of these RoI sources in order to provide a fair comparison especially in favor of RPN. We trained the models (except the one with the * mark) for 16 epochs with a learning rate decay at epochs 9 and 14 since our model provides more diverse data than RPN (see in <ref type="figure">Fig. 6</ref>   stage detector (as well as those of a two-stage detector) in order to design a better anchor set. Furthermore, other applications, e.g. tracking, that require spatially distributed BBs with certain properties can also exploit our approach.</p><p>Following upon the notation in Section 3 of the paper, we introduce the following properties. For clarity we assume that intersection of two boxes is greater than 0 and the last pixel is not taken into account (i.e. instead of A(B) = (x 2 − x 1 + 1), we adopted A(B) = (x 2 − x 1 )). </p><formula xml:id="formula_6">IoU(B s ,B s ) = I(B s ,B s ) A(B s ) + A(B s ) − I(B s ,B s )<label>(6)</label></formula><formula xml:id="formula_7">= (min (k xx2 , k x x 2 ) − max (k xx1 , k x x 1 )) × (min (k yȳ2 , k y y 2 ) − max (k yȳ1 , k y y 1 )) (k x x 2 − k x x 1 ) × (k y y 2 − k y y 1 ) + (k xx2 − k xx1 ) × (k yȳ2 − k yȳ1 ) − I(B s ,B s ) (7) = k x (min (x 2 , x 2 ) − max (x 1 , x 1 )) × k y (min (ȳ 2 , y 2 ) − max (ȳ 1 , y 1 )) k x (x 2 − x 1 ) × k y (y 2 − y 1 ) + k x (x 2 −x 1 ) × k y (ȳ 2 −ȳ 1 ) − I(kB,kB) (8) = k x k y I(B,B) k x k y (x 2 − x 1 ) × (y 2 − y 1 ) + k x k y (x 2 −x 1 ) × (ȳ 2 −ȳ 1 ) − k x k y I(B,B) (9) = k x k y I(B,B) k x k y (x 2 − x 1 ) × (y 2 − y 1 ) + (x 2 −x 1 ) × (ȳ 2 −ȳ 1 ) − I(B,B)<label>(10)</label></formula><p>= IoU(B,B)</p><p>Eq. 6 defines the IoU and Eq. 7 replaces area and intersection definitions. In Eq. 8, we use the property that multiplying by a positive scalar does not change minimum and maximum of two numbers. Eq. 9 incorporates the intersection definition. Eq. 10 gets the denominator in the kk parenthesis, which simplifies the term to the definition of IoU <ref type="figure">(B,B)</ref>.</p><formula xml:id="formula_9">Theorem 2. IoU(B,B) is translation-invariant.</formula><p>Proof. Assuming that k x ∈ R and k y ∈ R are the perturbation in the x and y axis respectively and B t ,B t are the perturbed boxes. We show that IoU(B t ,B t ) = IoU(B,B) as follows:</p><formula xml:id="formula_10">IoU(Bt,Bt) = I(Bt,Bt) A(Bt) + A(Bt) − I(Bt,Bt)<label>(12)</label></formula><p>= (min (x 2 + kx, x 2 + kx) − max (x 1 + kx, x 1 + kx)) × (min (ȳ 2 + ky, y 2 + ky) − max (ȳ 1 + ky, y 1 + ky))</p><formula xml:id="formula_11">((x 2 + kx) − (x 1 + kx)) × ((y 2 + ky) − (y 1 + ky)) + ((x 2 + kx) − (x 1 + kx)) * ((ȳ 2 + ky) − (ȳ 1 + ky)) − I(Bt,Bt)<label>(13)</label></formula><formula xml:id="formula_12">= (min (x 2 , x 2 ) + kx − max (x 1 , x 1 ) − kx) × (min (ȳ 2 , y 2 ) + ky − max (ȳ 1 , y 1 ) − ky) (x 2 + kx − x 1 − kx) × (y 2 + ky − y 1 − ky) + (x 2 + kx −x 1 − kx) × (ȳ 2 + ky −ȳ 1 − ky) − I(Bt,Bt)<label>(14)</label></formula><formula xml:id="formula_13">= (min (x 2 , x 2 ) − max (x 1 , x 1 )) × (min (ȳ 2 , y 2 ) − max (ȳ 1 , y 1 )) (x 2 − x 1 ) × (y 2 − y 1 ) + (x 2 −x 1 ) × (ȳ 2 −ȳ 1 ) − I(Bt,Bt) (15) = I(B,B) A(B) + A(B) − I(B,B) (16) = IoU(B,B)<label>(17)</label></formula><p>Again, Eq. 13 replaces area and intersection definitions in the IoU definition. In Eq. 14, we use the property that adding a scalar to numbers adds a scalar to the minimum and maximum of two numbers. In Eq. 15, constants cancel each other and Eq. 16 replaces area and intersection for the IoU <ref type="figure">(B,B)</ref>, which simplifies to the definition of IoU(B,B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Details of the Bounding Box Generator</head><p>In this section we present the derivation of the Equation 4 and 5, and explain the findBRFeasibleSpace(B, T, TL(B)) function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">findTLFeasibleSpace(B, T ) function</head><p>Here, we derive Equation 4 and 5 in the paper, and present the equations for the top-left space..</p><p>In order to derive Equation 4 depicting x I max , we bound the x coordinate first. It is obvious that x I min = x 1 due to the boundary of Region I. For x I max , we know that y 1 = y 1 again thanks to the region boundary. Therefore, since we have only one unknown, x I max , we use Eq. the definition of the IoU to determine its value in Eq. 18-23. Eq. 19 defines IoU based on Eq. 18. In Eq. 20, we set min (x 2 , x 2 ) = x 2 , max (x 1 , x 1 ) = x I max , min (ȳ 2 , y 2 ) = y 2 and max (ȳ 1 , y 1 ) = y 1 by taking into the intersection definition in Region I. Also note thatx 1 = x I max ,ȳ 1 = y 1 ,x 2 = x 2 and y 2 = y 2 in this case. In Eq. 21-23, we just rearrange the terms to have x I max as a left hand side term.  <ref type="figure" target="#fig_2">Fig. 4</ref> in the paper.</p><formula xml:id="formula_14">IoU(B,B) = I(B,B) A(B) + A(B) − I(B,B)<label>(18)</label></formula><formula xml:id="formula_15">= (min (x 2 , x 2 ) − max (x 1 , x 1 )) × (min (ȳ 2 , y 2 ) − max (ȳ 1 , y 1 )) (x 2 − x 1 ) × (y 2 − y 1 ) + (x 2 −x 1 ) × (ȳ 2 −ȳ 1 ) − I(B,B)<label>(19)</label></formula><formula xml:id="formula_16">⇒ T = (x 2 − x I max ) × (y 2 − y 1 ) (x 2 − x 1 ) × (y 2 − y 1 ) + (x 2 − x I max ) × (y 2 − y 1 ) − (x 2 − x I max ) × (y 2 − y 1 )<label>(20)</label></formula><formula xml:id="formula_17">⇒ (x 2 − x 1 ) × (y 2 − y 1 ) × T = (x 2 − x I max ) × (y 2 − y 1 )<label>(21)</label></formula><formula xml:id="formula_18">⇒ x I max = x 2 − (x 2 − x 1 ) × (y 2 − y 1 ) × T (y 2 − y 1 )<label>(22)</label></formula><formula xml:id="formula_19">⇒ x I max = x 2 − (x 2 − x 1 ) × T<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region</head><p>Min Bound Max Bound Equation </p><formula xml:id="formula_20">Ix 1 = x 1x1 = x 2 − (x 2 − x 1 ) × Tȳ 1 = y 2 − I(B,B) T +I(B,B)−A(B) (x 2 −x 1 ) IIȳ 1 = y 1ȳ1 = y 2 − A(B)×T x 2 −x 1x 1 = x 2 − I(B,B)×A(B) (y 2 −ȳ 1 ) IIIȳ 1 = y 1ȳ1 = y 2 − A(B)×T x 2 −x 1x 1 = x 2 − I(B,B) T −A(B)+I(B,B) (y 2 −ȳ 1 ) IVȳ 1 = (y 2 ×(T −1))+y 1 Tȳ 1 = y 1x1 = x 2 − A(B) T ×(y 2 −ȳ 1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region</head><p>Min Bound Max Bound</p><formula xml:id="formula_21">Iȳ 2 = T ×A(B)+T ×(x 2 −α)×β+β×(x 2 −α)−T ×ȳ 1 ×(x 2 −x 1 ) ((T +1)×(x 2 −α)−T ×(x 2 −x 1 ))ȳ 2 = y 2 IIx 2 = x 2x2 =x 1 + I(B,B) T −A(B)+I(B,B) (y 2 −ȳ 1 ) IIIȳ 2 = y 2ȳ2 =ȳ 1 + I(B,B) T −A(B)+I(B,B) (x 2 −x 1 ) IVx 2 = T ×A(B)+T ×(y 2 −β)×α+α×(y 2 −β)−T ×x 1 ×(y 2 −ȳ 1 ) ((T +1)×(y 2 −β)−T ×(y 2 −ȳ 1 ))x 2 = x 2</formula><p>Now since we know the values ofx 1 based on the bounds, we can derive the Equation 5 (in the paper) for anyȳ 1 value in equations by moving within bounds. <ref type="figure">Since I(B,B)</ref> = (x 2 −x 1 ) × (y 2 − y 1 ), it does not rely onȳ 1 and we directly use I(B,B) in the following equations: <ref type="table" target="#tab_5">Table 6</ref> presents all of the equations derived using the same methodology.</p><formula xml:id="formula_22">IoU(B,B) = I(B,B) A(B) + (x 2 −x 1 ) × (y 2 −ȳ 1 ) − I(B,B)<label>(24)</label></formula><formula xml:id="formula_23">⇒ T × (x 2 −x 1 ) × (y 2 −ȳ 1 ) = I(B,B) + T × I(B,B) − T × A(B)<label>(25)</label></formula><formula xml:id="formula_24">⇒ȳ 1 = y 2 − I(B,B) T + I(B,B) − A(B) (x 2 −x 1 )<label>(26)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">findBRFeasibleSpace(B, T, TL(B)) Function</head><p>We follow the same approach for the bottom right corner with the top left corner. However, different from topleft space this step is required also consider the point gen- We add two additional parameters for the sake of clarity: α = max(x 1 , x 1 ), β = max(ȳ 1 , y 1 ),α = min(x 2 , x 2 ) andβ = min(ȳ 2 , y 2 ). The bounds and the equations are derived by the same methodology that is illustrated in the first step presented in <ref type="table" target="#tab_6">Tables 7 and 8</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Integrating pRoI Generator into the Training</head><p>The training of the two-stage object detectors involves 3 different networks as shown in <ref type="figure">Fig. 7</ref>. The first network is the feature extractor (i.e. ResNet <ref type="bibr" target="#b12">[13]</ref>) which presents the base features to the second network, the proposal generator (i.e. RPN <ref type="bibr" target="#b29">[30]</ref>), and the third network, which is the object detector (i.e. R-CNN <ref type="bibr" target="#b9">[10]</ref>, R-FCN <ref type="bibr" target="#b4">[5]</ref>). The feature extractor is trained with the gradients back-propagated from the proposal generator and the object detector. The proposal generator is trained by a subset of the anchor-ground truth combinations (chosen by Sample Anchors to Train RPN in <ref type="figure">Figure 7</ref>) and a subset of these RPN proposals (i.e. RoIs) (chosen by Sample RoIs to Train R-CNN in <ref type="figure">Figure 7</ref>) are fed into the R-CNN after a series of operations including NMS and RoI Pooling that do not include learnable parameters. Finally, the loss is back-propagated through the entire network to update the parameters. However, the RoIs from the RPN is limited in number and diversity, which can impact the analysis and training. To address this, pRoI generator aims to generate RoIs with any desired property and in any number. Note that the gradients can also be back-propagated to the feature extractor as in the conventional training (i.e. RPN) since positive RoI Generator uses ground truths to generate an RoI in a similar manner to the conventional training, but differently it can generate boxes  with the desired properties. During training, only for positive RoIs, pRoI Generator does not use the modules that are under the transparent red rectangle in <ref type="figure">Figure 7</ref>. However, during test time, our method follows the conventional approach, namely the RoIs from RPN are used due to the fact that no ground truth information is available during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Connection Between</head><p>genRoIs() and generateBB() As described in the text, generateBB() is a low-level function and any approach uses generated BBs approach is to rely on this function. That's why it is a subrou-tine of genRoIs(). The main idea in our implementation is to generate bounding boxes by iteratively calling the generateBB() for RoIN um times.</p><p>Apart from RoIN um, the number of RoIs to be generated, there are two main input sets to the genRoIs function. Firstly, GT s and perGT RoI together have the information about the box coordinates and the number of RoIs to be generated from each ground truth box. Therefore, for i th ground truth box (GT s i ), we call generateBB() function for perGT RoI i times. And the second set of input comprises ψ IoU and W IoU , which together have information about the weights of each IoU interval. Therefore, for deter- <ref type="table">Table 9</ref>: The configurations of W IoU for the different tables in the paper. mining an IoU for each box, we first generate perGT RoI number of samples of IoU intervals using the multinomial distribution defined in W IoU and then, for each resulting interval, we again sample uniformly an IoU within its limits. These IoUs are clipped from 0.95 in order to prevent the problems arising from the precision problem in the samplePolygon() acceptance process. This sampling strategy distributes the input IoUs over an interval evenly. Finally, we randomly shuffle this set of IoUs and associate them to the ground truths, which completes the generation of the ground truth and desired IoU pairs as the input of the generateBB() function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Configurations of W IoU</head><p>The configurations of the W IoU (i.e. the distribution over ψ IoU = [0.5, 0.6, 0.7, 0.8, 0.9]) used for the experiments is shown in <ref type="table">Table 9</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Let B = [x 1 , y 1 , x 2 , y 2 ] denote a ground-truth box with top-left corner TL(B) = (x 1 , y 1 ) and bottom-right corner BR(B) = (x 2 , y 2 ) satisfying x 2 &gt; x 1 and y 2 &gt; y 1 . The (a,b) Applying Algorithm 1 on the blue BB (B) with T = 0.5. Red polygons denote boundaries for top-left and bottom-right points that can be sampled with an IoU larger than T = 0.5. Red dots are sampled points, and green box is the generated box (B) with IoU = 0.5071.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :Figure 3 :</head><label>13</label><figDesc>procedure GENERATEBB(B, T ) 2: # Step-1: Find top-left corner 3: T LP oly ← findTLFeasibleSpace(B, T ) 4: TL(B) ← samplePolygon(T LP oly) 5: # Step-2: Find bottom-right corner 6: BRP oly ← findBRFeasibleSpace(B, T, TL(B)) 7: BR(B) ← samplePolygon(BRP oly) 8: return [TL(B), BR(B)] 9: end procedure Given a reference box B and a threshold T , the goal of the bounding box (BB) generator is to determine a new box c 1K generated boxes (shown with red) by Algorithm 1 for reference box drawn in blue (B) and IoU threshold T = 0.6. B = [x 1 ,ȳ 1 ,x 2 ,ȳ 2 ] such that IoU(B,B) ≥ T . To generate such a box, we propose a 2-step algorithm presented in Algorithm 1 and illustrated in Fig. 2. The first step (lines 3-4) finds the polygon 1 that computes the feasible space for TL(B) = (x 1 ,ȳ 1 ), which satisfies the desired IoU, and samples a point in this polygon. The second step (lines 6-7) takes into account the sampled TL(B) and, similar to Step 1, determines a feasible space for bottom-right corner, then, samples BR(B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) The regions around TL(B) and BR(B) are splitted into four each. Red and green dashed lines split the top left and bottom right regions respectively. The numbers label the splitted regions.), (b) In the execution of the sample polygon function for T = 0.75, green dashed box is the enclosing box for the TL space polygon.of the unbounded variable by moving within the limits of the bounded variable with some precision (we use 0.0001 as precision). Since the definition of the IoU(B,B)is different in each of the four regions depicted inFig. 4(a)due to the max and min operations, an equation is to be derived for each region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>cFigure 5 :</head><label>5</label><figDesc>IoU distribution of different RoI Sources. See Appendix for the configurations of the RoI sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 1 .</head><label>1</label><figDesc>IoU(B,B) is scale-invariant. Proof. Assume that k x &gt; 0 and k y &gt; 0 are the scaling factors in the x and y axes respectively and B s ,B s are the scaled boxes. We show that IoU(B s ,B s ) = IoU(B,B) as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>erated top-left point. Note that the size of the polygon in the bottom-right space is affected by the distance between TL(B) and TL(B). Maximum bottom-right polygon size, with exactly the same size of the top-left polygon, is achieved when TL(B) = TL(B). Conversely, bottomright polygon degenerates to a point at BR(B) if the sampled TL(B) hits the border of the top-left polygon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>x 2 −x 1 IIIx 2 =Figure 7 :</head><label>127</label><figDesc>T ×A(B)+α×T ×(β−β)+α×(β−β)−T ×x 1 ×(ȳ 2 −ȳ 1 ) (T +1)×(β−β)−T ×(ȳ 2 −ȳ 1 )) IVȳ 2 = T ×A(B)+β×T ×(α−α)+β×(α−α)−T ×ȳ 1 ×(x 2 −x 1 ) (T +1)×(α−α)−T ×(x 2 −x 1 )) ConventionalFaster R-CNN Training and our modification. During training, Positive RPN RoIs are not utilized and thus the modules presented under the large red rectangle are not used for positive RoIs. These RoIs are generated by the Positive RoI Generator shown in yellow box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of the batch properties for generated positive samples (seeFig. 5for different RoI sources) on Pascal VOC 2007. We trained each RoI source with balanced foreground-foreground distribution and simulating OHPM. RS, Unif, LS and Base respectively denote pRoI-Right Skew, pRoI-Uniform, pRoI-Left Skew and pRoI-Base IoU=0.5 distributions. FGB refers to foreground balanced generation of RoIs.</figDesc><table><row><cell>Distrib.</cell><cell></cell><cell></cell><cell>↓</cell><cell cols="2">(IoU) ↓ (FP) ↓</cell><cell>(FN) ↓</cell><cell>↑</cell></row><row><cell></cell><cell>No</cell><cell>No</cell><cell>64.6</cell><cell>21.4</cell><cell>18.7</cell><cell>29.8</cell><cell>74.9</cell></row><row><cell>RS</cell><cell>Yes</cell><cell>No</cell><cell>64.5</cell><cell>21.5</cell><cell>18.7</cell><cell>29.5</cell><cell>75.3</cell></row><row><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>60.4</cell><cell>19.5</cell><cell>16.8</cell><cell>27.2</cell><cell>77.4</cell></row><row><cell></cell><cell>No</cell><cell>No</cell><cell>61.3</cell><cell>19.5</cell><cell>17.9</cell><cell>28.5</cell><cell>76.3</cell></row><row><cell>Unif.</cell><cell>Yes</cell><cell>No</cell><cell>61.1</cell><cell>19.5</cell><cell>17.0</cell><cell>28.8</cell><cell>76.9</cell></row><row><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>59.9</cell><cell>19.2</cell><cell>16.0</cell><cell>27.6</cell><cell>77.8</cell></row><row><cell></cell><cell>No</cell><cell>No</cell><cell>60.4</cell><cell>19.1</cell><cell>16.9</cell><cell>28.3</cell><cell>77.0</cell></row><row><cell>LS</cell><cell>Yes</cell><cell>No</cell><cell>60.3</cell><cell>19.0</cell><cell>17.3</cell><cell>28.2</cell><cell>77.2</cell></row><row><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>60.7</cell><cell>19.3</cell><cell>17.7</cell><cell>27.8</cell><cell>76.9</cell></row><row><cell></cell><cell>No</cell><cell>No</cell><cell>61.5</cell><cell>19.7</cell><cell>17.2</cell><cell>28.8</cell><cell>76.6</cell></row><row><cell>Base</cell><cell>Yes</cell><cell>No</cell><cell>61.4</cell><cell>19.3</cell><cell>16.3</cell><cell>29.4</cell><cell>76.7</cell></row><row><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>61.2</cell><cell>19.7</cell><cell>16.6</cell><cell>28.6</cell><cell>76.7</cell></row></table><note>RoI F GB? OHPM moLRP moLRP moLRP moLRP mAP@0.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average performance of 3 runs for Faster R-CNN with our OFB sampling on Pascal VOC. Lower is better for moLRP and its components, whereas higher is better for mAP.</figDesc><table><row><cell>Sampling</cell><cell></cell><cell></cell><cell>moLRP</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>moLRP ↓</cell><cell>IoU ↓</cell><cell>FP ↓</cell><cell>FN ↓</cell><cell>mAP@0.5 ↑</cell></row><row><cell>Random</cell><cell>59.4</cell><cell>18.7</cell><cell>16.2</cell><cell>27.7</cell><cell>78.0</cell></row><row><cell>OFB</cell><cell>58.9</cell><cell>18.7</cell><cell>15.6</cell><cell>27.2</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Comparison of different sampling mechanisms</cell></row><row><cell cols="5">on MS COCO using Faster R-CNN. Lower is better for</cell></row><row><cell cols="5">moLRP and its components, whereas higher is better for</cell></row><row><cell cols="5">mAP. mAP stands for COCO-style mAP. R and H denote</cell></row><row><cell cols="5">random and hard sampling respectively, and OFB is our</cell></row><row><cell cols="5">sampling method for positive RoIs. The first block com-</cell></row><row><cell cols="5">pares among different positive sampling schemes combined</cell></row><row><cell cols="5">with random sampling, while the second block compares</cell></row><row><cell cols="5">their combinations with hard example mining.</cell></row><row><cell cols="2">Sampling Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Positive</cell><cell>Negative</cell><cell>moLRP ↓</cell><cell>mAP ↑</cell><cell>mAP@0.5 ↑</cell></row><row><cell>R</cell><cell>R</cell><cell>72.4</cell><cell>34.1</cell><cell>55.2</cell></row><row><cell>H</cell><cell>R</cell><cell>75.3</cell><cell>31.0</cell><cell>51.7</cell></row><row><cell>OFB</cell><cell>R</cell><cell>72.1</cell><cell>34.7</cell><cell>55.8</cell></row><row><cell>R</cell><cell>H</cell><cell>71.9</cell><cell>35.3</cell><cell>54.6</cell></row><row><cell>H</cell><cell>H</cell><cell>74.6</cell><cell>31.1</cell><cell>50.0</cell></row><row><cell>OFB</cell><cell>H</cell><cell>70.9</cell><cell>35.6</cell><cell>55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>that the TL points of the RPN RoIs clusters around TL point of B) and there are fewer samples for training in higher IoUs (see Mean RoI # inTable 4)</figDesc><table><row><cell>RoI Source</cell><cell>IoU</cell><cell>RoIN um</cell><cell>f g/bg</cell><cell>Mean RoI # ↑</cell><cell>moLRP ↓</cell><cell>moLRPIoU ↓</cell><cell>moLRPFP ↓</cell><cell>moLRPFN ↓</cell><cell>mAP@IoU ↑</cell></row><row><cell>RPN*</cell><cell>0.5</cell><cell>N/A</cell><cell>32/96</cell><cell>27.12</cell><cell>59.3</cell><cell>18.7</cell><cell>16.0</cell><cell>27.7</cell><cell>78.0</cell></row><row><cell>pRoI-Uniform</cell><cell>0.5</cell><cell>128</cell><cell>32/96</cell><cell>25.49</cell><cell>59.2</cell><cell>18.4</cell><cell>15.5</cell><cell>28.2</cell><cell>77.1</cell></row><row><cell>RPN</cell><cell>0.6</cell><cell>N/A</cell><cell>27/81</cell><cell>16.92</cell><cell>65.4</cell><cell>17.0</cell><cell>19.4</cell><cell>31.9</cell><cell>71.2</cell></row><row><cell>pRoI-Uniform</cell><cell>0.6</cell><cell>128</cell><cell>27/81</cell><cell>18.28</cell><cell>65.4</cell><cell>16.9</cell><cell>20.8</cell><cell>31.0</cell><cell>70.6</cell></row><row><cell>RPN</cell><cell>0.7</cell><cell>N/A</cell><cell>9/27</cell><cell>5.39</cell><cell>74.9</cell><cell>14.7</cell><cell>27.2</cell><cell>42.1</cell><cell>57.3</cell></row><row><cell>pRoI-Uniform</cell><cell>0.7</cell><cell>128</cell><cell>18/54</cell><cell>9.93</cell><cell>74.5</cell><cell>14.9</cell><cell>28.0</cell><cell>39.8</cell><cell>57.5</cell></row><row><cell>RPN</cell><cell>0.8</cell><cell>N/A</cell><cell>2/6</cell><cell>1.08</cell><cell>92.5</cell><cell>13.2</cell><cell>58.8</cell><cell>69.8</cell><cell>21.3</cell></row><row><cell>pRoI-Uniform</cell><cell>0.8</cell><cell>64</cell><cell>8/24</cell><cell>3.92</cell><cell>87.7</cell><cell>12.1</cell><cell>47.8</cell><cell>59.3</cell><cell>32.2</cell></row><row><cell>RPN</cell><cell>0.9</cell><cell>N/A</cell><cell>2/6</cell><cell>0.17</cell><cell>99.5</cell><cell>7.4</cell><cell>94.2</cell><cell>97.1</cell><cell>0.5</cell></row><row><cell>pRoI-Uniform</cell><cell>0.9</cell><cell>32</cell><cell>2/6</cell><cell>1.62</cell><cell>99.3</cell><cell>7.3</cell><cell>92.4</cell><cell>96.0</cell><cell>0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of RoIN um on PASCAL VOC. Speeds are reported on a single Geforce GTX 1080 Ti.</figDesc><table><row><cell>RoI Source</cell><cell>RoIN um</cell><cell>moLRP ↓</cell><cell>moLRPIoU ↓</cell><cell>moLRPFP ↓</cell><cell>moLRPFN ↓</cell><cell>mAP@0.5 ↑</cell><cell>Train Speed ↓</cell><cell>Mean RoI # ↑</cell></row><row><cell>pRoI-Uniform</cell><cell>32</cell><cell>60.3</cell><cell>19.3</cell><cell>16.4</cell><cell>27.8</cell><cell>77.5</cell><cell>0.41s</cell><cell>14.81</cell></row><row><cell>pRoI-Uniform</cell><cell>64</cell><cell>59.7</cell><cell>19.0</cell><cell>16.1</cell><cell>27.4</cell><cell>77.6</cell><cell>0.58s</cell><cell>21.32</cell></row><row><cell>pRoI-Uniform</cell><cell>128</cell><cell>59.9</cell><cell>19.2</cell><cell>16.0</cell><cell>27.6</cell><cell>77.8</cell><cell>0.97s</cell><cell>25.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Top-Left space bounds and equations. See</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Bottom-Right space bounds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Bottom-right space equations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table RoI Source</head><label>RoI</label><figDesc>IoU = 0.5 IoU = 0.6 IoU = 0.7 IoU = 0.8 IoU = 0.9</figDesc><table><row><cell>1</cell><cell>Right Skew</cell><cell>0.02</cell><cell>0.10</cell><cell>0.20</cell><cell>0.30</cell><cell>0.38</cell></row><row><cell>1</cell><cell>Balanced</cell><cell>0.33</cell><cell>0.17</cell><cell>0.18</cell><cell>0.17</cell><cell>0.15</cell></row><row><cell>1</cell><cell>Left Skew</cell><cell>0.73</cell><cell>0.12</cell><cell>0.15</cell><cell>0.05</cell><cell>0</cell></row><row><cell>4</cell><cell>Balanced, IoU=0.5</cell><cell>0.33</cell><cell>0.17</cell><cell>0.18</cell><cell>0.17</cell><cell>0.15</cell></row><row><cell>4</cell><cell>Balanced, IoU=0.6</cell><cell>0</cell><cell>0.38</cell><cell>0.20</cell><cell>0.22</cell><cell>0.20</cell></row><row><cell>4</cell><cell>Balanced, IoU=0.7</cell><cell>0</cell><cell>0</cell><cell>0.48</cell><cell>0.25</cell><cell>0.27</cell></row><row><cell>4</cell><cell>Balanced, IoU=0.8</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.64</cell><cell>0.36</cell></row><row><cell>4</cell><cell>Balanced, IoU=0.9</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the shape is not strictly a polygon; however, we approximate it as one at regular small intervals, and therefore, we call it a polygon for the sake of simplicity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the Scientific and Technological Research Council of Turkey (TÜBİTAK) through the project titled "Object Detection in Videos with Deep Neural Networks" (grant number 117E054). Ke-malÖksüz is supported by the TÜBİTAK 2211-A National Scholarship Programme for Ph.D. students. The experiments were partially performed at TÜBİTAK ULAKBIM, High Performance and Grid Computing Center (TRUBA) resources.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Prime Sample Attention in Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Tutorial Introduction to Monte Carlo methods, Markov Chain Monte Carlo and Particle Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Academic Press Library in Signal Processing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>1701.06659</idno>
		<title level="m">DSSD: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NAS-FPN: learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning from Imbalanced Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The open images dataset V4: unified image classification, object detection, and visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>at scale. arXiv, 1811.00982</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient harmonized singlestage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qingming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Localization recall precision (LRP): A new performance metric for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00169</idno>
		<title level="m">Imbalance Problems in Object Detection: A Review. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Libra R-CNN: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>arXiv, 1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A faster pytorch implementation of faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
