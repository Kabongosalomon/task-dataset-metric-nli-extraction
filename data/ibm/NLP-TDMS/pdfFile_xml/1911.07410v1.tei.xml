<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Temporal Recurrent Neural Networks For Progressive Non-Uniform Single Image Deblurring With Incremental Temporal Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Un</forename><surname>Dong</surname></persName>
							<email>dong1@unist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Se</roleName><forename type="first">Jisoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Chun</surname></persName>
							<email>sychun@unist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Temporal Recurrent Neural Networks For Progressive Non-Uniform Single Image Deblurring With Incremental Temporal Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-scale (MS) approaches have been widely investigated for blind single image / video deblurring that sequentially recovers deblurred images in low spatial scale first and then in high spatial scale later with the output of lower scales. MS approaches have been effective especially for severe blurs induced by large motions in high spatial scale since those can be seen as small blurs in low spatial scale. In this work, we investigate alternative approach to MS, called multi-temporal (MT) approach, for non-uniform single image deblurring. We propose incremental temporal training with constructed MT level dataset from timeresolved dataset, develop novel MT-RNNs with recurrent feature maps, and investigate progressive single image deblurring over iterations. Our proposed MT methods outperform state-of-the-art MS methods on the GoPro dataset in PSNR with the smallest number of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Blind single image deblurring is a challenging ill-posed inverse problem to recover the original sharp image from a given blurred image with or without estimating unknown non-uniform blur kernels and there has been much effort to tackle this problem. One is to simplify the given problem by assuming uniform blur and to recover both the latent ground truth image and the blur kernel <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref>. However, uniform blur is often not accurate enough to approximate the actual blur, and thus there has been much research on nonuniform blur by extending the degree of freedom of the blur model from uniform to non-uniform in a limited way compared to the dense matrix <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24]</ref>. Other nonuniform blur models have been investigated such as additional segmentations within which simple blur models were used <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> or motion estimation based deblurs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, deep-learning-based approaches for single image deblurring have been proposed with excellent quan- * equal contribution, * * corresponding <ref type="figure">Figure 1</ref>: Number of parameters (in Million) vs. PSNR (in dB) for different deblurring methods evaluated on the Go-Pro dataset. Our proposed method (Ours) yielded the best PSNR (31.15dB) with the smallest number of parameters (2.6M) among all methods including Nah <ref type="bibr" target="#b21">[22]</ref>, Tao <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr">Kupyn [19]</ref>, Aljadaany <ref type="bibr" target="#b0">[1]</ref>, Gao <ref type="bibr" target="#b6">[7]</ref> and Zhang <ref type="bibr" target="#b37">[38]</ref>. 'Ours-Z' is our MT approach with the network of Zhang <ref type="bibr" target="#b37">[38]</ref>.</p><p>titative results and with fast computation time. There are largely two different ways of using deep neural networks (DNNs) for deblurring. One is to use DNNs to explicitly estimate non-uniform blurs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> and the other is to use DNNs to directly estimate the original sharp image without estimating blurs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. Most state-of-the-art methods such as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref> are estimating the original sharp image directly from the given blurred image (see <ref type="figure">Figure 1</ref>). Single-scale (SS) or one-stage approaches <ref type="figure" target="#fig_0">(Figure 2 (a)</ref>) are frequently used, but many state-of-the-art methods are using multi-scale (MS) approaches (or coarseto-fine) with down-scaled image(s) in spatial domain.</p><p>The MS approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref> utilize down-scaled images to restore the latent sharp image progressively over scales as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (b). This approach makes use of the fact that blurs become relatively smaller as scale of image decreases <ref type="bibr" target="#b6">[7]</ref>. Thus, a DNN with MS approach is able to perform deblurring from large blur to small blur pro- gressively. Recently, there have been some works on sharing network parameters of MS structures over scales <ref type="bibr" target="#b31">[32]</ref> or efficiently sharing parameters except for feature extraction layers that are independent over scales assuming that blurs are varied with scales <ref type="bibr" target="#b6">[7]</ref>. One drawback of typical MS approaches seems to lose much high-frequency information during down-sampling in a sub-optimal way for image deblurring considering the fact that strong edge information is important for reliable deblurring <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>In this paper, we investigate an alternative approach, called multi-temporal (MT) approach, to MS approach for single image deblurring. Instead of using down-scaled images, we exploit a typical dataset generation pipeline using high-speed camera to construct a blurred image by averaging multiple frames of images such as the GoPro dataset <ref type="bibr" target="#b21">[22]</ref>. We conjecture that recovering sharp latent images from mild blurs is easier than recovering them from severe blurs and propose DNNs to deblur little by little as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (c). Thus, our MT approach allows to use full image information in the original scale for reliable deblurring <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref> and to deblur for mild blurs at each iteration progressively (see <ref type="figure" target="#fig_1">Figure 3</ref>) for potentially better performance than SS approach. Without any special parameter sharing schemes like <ref type="bibr" target="#b6">[7]</ref>, our proposed methods achieved state-of-the-art performance with the smallest number of parameters as shown in <ref type="figure">Figure 1</ref>.</p><p>Here is the summary of our contributions: 1) proposing MT approach with incremental temporal training for highspeed camera dataset to divide challenging severe blur into a series of mild blurs and then deblur each mild blur progressively, 2) developing MT-recurrent neural network (RNN) with recurrent feature maps for blind single image deblurring, and 3) achieving state-of-the-art performance on Go-Pro dataset with the smallest number of parameters among recently proposed blind single image deblurring methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Conventional approaches to blind single image / video deblurring usually require to explicitly estimate blur kernels. There have been works on estimating uniform blurs using optimization with MS approach <ref type="bibr" target="#b5">[6]</ref>, using a model of the spatial randomness of noise and a local smoothness prior <ref type="bibr" target="#b28">[29]</ref>, exploiting blurred strong edges to reliably estimate blur kernel <ref type="bibr" target="#b3">[4]</ref>, and developing a metric to measure the usefullness of image edges for blur kernel estimation <ref type="bibr" target="#b34">[35]</ref>.</p><p>There have also been many works on predicting nonuniform blurs assuming spatially linear blur <ref type="bibr" target="#b9">[10]</ref>, simplified camera motion <ref type="bibr" target="#b8">[9]</ref>, parametrized geometric model in terms of camera rotation velocity during exposure <ref type="bibr" target="#b32">[33]</ref>, filter flow framework based blur model <ref type="bibr" target="#b10">[11]</ref>, l 0 sparsity for blurs <ref type="bibr" target="#b36">[37]</ref>, and dark channel prior <ref type="bibr" target="#b23">[24]</ref>. There was also an attempt to exploit multiple images from videos assuming spatially varying blur <ref type="bibr" target="#b19">[20]</ref>. There have also been some works to utilize segmentation information by assuming uniform blur on each segmentation area <ref type="bibr" target="#b4">[5]</ref> and to segment motion blur using optimization <ref type="bibr" target="#b12">[13]</ref>, to simplify motion model as local linear without segmentation using MS approach <ref type="bibr" target="#b13">[14]</ref>, and to use bidirectional optical flows for video deblurring <ref type="bibr" target="#b14">[15]</ref>.</p><p>Recently, many blind single image / video deblurring works employed DNNs for estimating blur kernels and/or original sharp images from given blurred input images. There are several works to predict non-uniform blur kernels explicitly: predicting the probabilistic distribution of motion blur at the patch level <ref type="bibr" target="#b30">[31]</ref>, estimating the complex Fourier coefficients of a deconvolution filter <ref type="bibr" target="#b2">[3]</ref>, performing blur kernel estimation by division in Fourier space from extracted deep features <ref type="bibr" target="#b27">[28]</ref>, and analyzing the spectral con- tent of blurry image patches by reblurring them <ref type="bibr" target="#b1">[2]</ref>.</p><p>There are also many works to directly estimate the original sharp image from the given blurred input image without explicitly estimating non-uniform blur kernels. For video blind deblurring, there have been some works to exploit temporal information: blending temporal information in spatio-temporal recurrent network for online video deblurring <ref type="bibr" target="#b15">[16]</ref>, taking temporal information into account with recurrent deblur network consisting of several deblur blocks <ref type="bibr" target="#b33">[34]</ref>, and developing an encoder-decoder network with the input of multiple video frames to accumulate information across frames <ref type="bibr" target="#b29">[30]</ref>.</p><p>There are a few works for blind single image deblurring without temporal information. Xu et al. proposed a direct estimation of the original sharp image based on optimization to approximate deconvolution by a series of convolution steps using DNNs <ref type="bibr" target="#b35">[36]</ref>. Later, Nah et al.  <ref type="bibr" target="#b18">[19]</ref> proposes generative adversarial network (GAN) framework based on feature pyramid network (FPN) and relativistic discriminator <ref type="bibr" target="#b20">[21]</ref> with a least-square loss <ref type="bibr" target="#b11">[12]</ref>.</p><p>Lastly, RNN plays an important role in using sequential data or iterative approach. Zhou <ref type="bibr" target="#b39">[40]</ref> proposed spatiotemporal variant RNN for video deblurring. RNN is often introduced to utilize previous frames effectively such as previous features using convolutional LSTM <ref type="bibr" target="#b31">[32]</ref>. Similarly, we propose an approach that recurrently makes use of previous feature information for each iteration. However, unlike other RNN based approaches, our proposed methods use incremental temporal training procedure that does not train from the most severe blur to the ground truth, but trains from more blured to less blurred image incrementally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Temporal (MT) Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GoPro Dataset</head><p>The GoPro dataset consists of 15,000 sharp images (frames), captured by GoPro4 Hero Black camera (240 frame per sec), including 22 videos for training and 11 videos for testing <ref type="bibr" target="#b21">[22]</ref>. 7-13 frames were averaged to yield blur-sharp image pairs where a middle image among mul- <ref type="figure">Figure 4</ref>: Blurred images are generated by averaging multiple frames. More frames result in severe blurs. tiple frames was selected as a ground truth as in <ref type="figure">Figure 4</ref>. Temporal level (TL) N is defined to be a blurred image from N frames. The GoPro dataset contains TL 7-13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset For Incremental Temporal Training</head><p>For the GoPro dataset with TL 1 (ground truth) and TL 7-13 pairs, we further generated data for MT approach and incremental temporal training. For example, for a blurred image with TL 7, we generated intermediate blurred images with TL 1-13 as shown in <ref type="figure">Figure 4</ref>. Thus, our MT approach does not try to estimate TL 1 from TL 7 directly, but tries to estimate from TL 7 to TL 5, TL 5 to TL 3, and finally TL 3 to TL 1, progressively.</p><p>We quickly validated our conjecture for MT approach: will it be easier to estimate TL 1 from TL 7 than to estimate TL 1 from TL 5 or TL 3? <ref type="table" target="#tab_0">Table 1</ref> shows the performance of U-Net <ref type="bibr" target="#b26">[27]</ref> that was trained only with one TL images for TL 3-13. As TL increases, PSNR clearly decreases. Thus, our conjecture for MT approach seems reasonable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Incremental Temporal Training</head><p>Our training method is based on the dataset with more intermediate TL images as explained in Section 3.2. During training, our proposed network is recurrently iterated by 5 or 7. At iteration 1, we train the network with randomly selected temporal blurred images (TL 13 or 11 or 9 or 7) as inputs and desired temporal blurred images (TL 11 or 9 or 7 or 5) as ground truth, respectively. Note that the TL difference between input and ground truth is 2. At the next iteration, the estimated image from iteration 1 is taken as input and desired temporal blurred image (TL 9 or 7 or 5 or 3) as ground truth. Similarly, other iterations are processed sequentially and take the estimated image from previous iteration as input and corresponding desired blurred or sharp image as ground truth. Finally, 1-3 more iterations of training to TL 1 as ground truth are repeated. If the number of iterations is over 7, return to iteration 1 for training the model. Note that model parameters are shared and training is performed independently for each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Progressive Deblurring With MT Approach</head><p>The methods of Tao <ref type="bibr" target="#b31">[32]</ref> and Nah <ref type="bibr" target="#b21">[22]</ref> are based on MS approach for deblurring. The DNN of Tao <ref type="bibr" target="#b31">[32]</ref> shares parameters over scales that can be modeled as follows:</p><formula xml:id="formula_0">I j , h j = DNN Tao (U (I 0 ), U (Î j−1 ), U (h j−1 ); θ Tao ) (1)</formula><p>Where j refers to a scale where j = 1 represents the finest scale. I 0 andÎ j are blurred and estimated latent images at We propose MT-RNN with recurrent feature maps using temporal iterations that can be modeled as follows:</p><formula xml:id="formula_1">I i , F i = DNN Ours (Î i−1 , I 0 , F i−1 1 , F i−1 2 ); θ Ours ) (2)</formula><p>where i refers to an iteration number where i = 1 represents the first iteration. I 0 is an input blurred image. I i−1 and I i are blurred and estimated latent images at ith iteration, respectively. F i−1 1 and F i−1 2 are recurrent feature maps from (i−1)th decoder. Since the network utilizes previous feature maps, the recurrent feature maps F i−1 1 and F i−1 2 move to the feature extraction layer in the next iteration. DNN Ours is our MT-RNN and θ Ours is a set of parameters in the network to be trained. This model is illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Proposed MT-RNN With Feature Maps</head><p>Our proposed network is based on the network of Tao <ref type="bibr" target="#b31">[32]</ref>. Base model is U-Net architecture <ref type="bibr" target="#b26">[27]</ref> and consists of encoders and decoders as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. Each stage has 1 feature extraction layer and residual blocks (Resblocks) that is identical to the Resblock in <ref type="bibr" target="#b21">[22]</ref> that consists of 32 channels, 64 channels and 128 channels at the top, middle and bottom encoder-decoders, respectively.</p><p>Residual learning Kupyn <ref type="bibr" target="#b18">[19]</ref> and Zhou <ref type="bibr" target="#b39">[40]</ref> utilize residual learning for deblurring. Both of them produce enhanced image and the network learns a residual image I R to correct the blurred image I B where I deblur = I B + I R . Residual learning is efficient to train the network faster and resulting model generalizes better. In the deblurring problem, input and output are highly correlated. Therefore, the residual learning helps training the network.</p><p>We conducted an ablation study for residual learning. In <ref type="figure" target="#fig_3">Figure 5</ref>, our proposed network takes I 0 andÎ i−1 as input and residual skip connection is linked with I 0 . Two cases for I B was considered: I 0 and non residual skip. PSNR of connection with I 0 is higher than non residual learning by 0.15dB on the GoPro dataset with intermediate TL images.</p><p>Recurrent feature maps As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, recurrent features F i−1 are from the last ResBlock of each de-coder and are concatenated with the feature maps of previous encoder at feature extraction layer:</p><formula xml:id="formula_2">F i enc = Cat(F i−1 , f i )<label>(3)</label></formula><p>where f i is the feature map of previous encoder at the ith iteration. Estimated imageÎ i−1 is concatenated with I 0 :</p><formula xml:id="formula_3">I i cat = Cat(Î i−1 , I 0 )<label>(4)</label></formula><p>and then the encoder takes the I i cat and F i encoder as input. Tao <ref type="bibr" target="#b31">[32]</ref> utilized convolutional LSTM for passing intermediate feature maps to the next spatial scale stage. Nah <ref type="bibr" target="#b22">[23]</ref> also makes use of hidden state h t−1 in RNN cell. Similarly, our network uses intermediate feature maps F i−1 from decoder that may include information about blur patterns and intermediate results for I i . Thus, F i−1 is utilized to encode I i , having more details of blur patterns and other information for deburring. Using recurrent feature maps F i−1 improves performance by 0.31dB.</p><p>Loss Function We use L1 loss function that measures the difference between a restored image and its corresponding latent ground truth normalized by channel, height and width of image. Ground truth images consists of TL 1-11 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Convergence of MT-RNN over Iterations</head><p>Determining the number of iterations for MT-RNN is important for performance. We studied iteration vs. PSNR for the network that was trained only with one type of TL images (e.g., TL 13) for all <ref type="bibr">TL 7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13</ref>. Training was performed until the 7th iteration for all cases. As illustrated in <ref type="figure">Figure 6</ref>, all networks yielded increased PSNR over iterations until 5th or 6th iterations, and then decreased PSNR beyond training iterations. From training procedure, iteration 6 was chosen and it was applied to all experiments for our methods. Note that in all cases with different TL images, our proposed MT-RNN methods outperform state-of-the-art MS methods (Tao <ref type="bibr" target="#b31">[32]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The GoPro dataset <ref type="bibr" target="#b21">[22]</ref> consists of 3214 blurred images with the size of 1280×720 that are divided into 2103 training images and 1111 test images. In both validation and test sets, TL 7, 9, 11, 13 images were evenly distributed. We generated more intermediate TL images along with the Go-Pro dataset so that this new dataset consists of 5500 training, 110 validation and 1200 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement the proposed network on pytorch <ref type="bibr" target="#b24">[25]</ref>. For fair comparisons, we evaluate our proposed method and <ref type="figure">Figure 6</ref>: Iteration vs. PSNR for our proposed MT-RNN trained using images with one of <ref type="bibr">TL 7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13</ref>. Corresponding models are trained only with each TL. state-of-the-art methods on the same machine with NVIDIA Titan V GPU. During training, Adam optimizer <ref type="bibr" target="#b16">[17]</ref> was used with learning rate 2 × 10 −4 , β 1 = 0.9, β 2 = 0.999, and = 10 −8 . For Tables 2, 3, 4 and 6, total iteration was 92 × 10 3 with reducing learning rate by half every 46 × 10 3 iterations and the GoPro dataset with additional intermediate TL images was used. PSNR/SSIM were evaluated on python. Unlike others, for <ref type="table">Table 5</ref>, total iteration is 46 × 10 4 with reducing learning rate by half every 46 × 10 3 iterations and the GoPro dataset was used. PSNR/SSIM were evaluated on MATLAB. Patch size was 256×256. Random crop, horizontal flip, and 90 • rotation were used for data augmentation. Note that since the number of channel is changed by concatenation to replace add operation of skip connection in <ref type="bibr" target="#b31">[32]</ref>, 1×1 convolution was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies for Model Architecture</head><p>We performed ablation studies from the base model <ref type="bibr">(Tao [32]</ref>) by adding our proposed components such as residual leaning and recurrent feature map. Note that while Tao <ref type="bibr" target="#b31">[32]</ref> is spatially iterative due to MS approach, our proposed MT-RNN is temporally iterative due to our proposed MT structure. <ref type="table" target="#tab_1">Table 2</ref> shows PSNR (dB), SSIM and num- ber of parameter(Million) (denoted by Parm) along different components such as Approaches including MS approach or our proposed MT approach, size of kernel (denoted by K), residual learning (denoted by R), recurrent feature map (denoted by F). As a baseline MS network, Tao <ref type="bibr" target="#b31">[32]</ref> was used as shown in <ref type="table" target="#tab_1">Table 2</ref> (a). Changing kernel size from 5 to 3 resulted in improved performance by 0.13dB, and substantially decreased parameter size as in <ref type="table" target="#tab_1">Table 2</ref> (b). Using residual learning instead of direct learning also improved performance as in <ref type="table" target="#tab_1">Table 2</ref> (c) due to the effect of specified blurry region on back-propagation. Our proposed MT approach improves performance over conventional MS approach using the same DNN as shown in <ref type="table" target="#tab_1">Table 2</ref> (d). Further improvement was observed when using recurrent feature map as in <ref type="table" target="#tab_1">Table 2</ref> (e). Lastly, it turns out that using MT alone is better than using both MS and MT in performance. Thus, temporal iterative approach helps the network to achieve high performance as in <ref type="table" target="#tab_1">Table 2</ref> (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Studies on Temporal Steps and Parameters</head><p>We studied the effect of temporal steps on performance as shown in <ref type="table" target="#tab_3">Table 3</ref> (g), (h) and (e) with one-stage SS approach (0 temporal step), 2 and 4 temporal steps, respectively. Using MT approach yielded better performance than SS approach and using small temporal step was more advantageous than using larger temporal step in performance even though computation time was increased. Thus, we chose temporal step 2 as the best step size.</p><p>Then, we also investigated the effect of parameter size on performance. <ref type="table" target="#tab_3">Table 3</ref> (e), (j), (k) show that the number of parameters is proportional to performance with the cost of increased computation. While twice larger parameters in (k) did not improve performance much over (e), its computation time and memory were substantially increased. Using half the parameter size in (j) did degrade performance substantially while computation speed of (j) is similar to (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Our MT Approach to Other Deblur DNNs</head><p>We investigate the feasibility of applying our proposed MT approach and incremental temporal training to other state-of-the-art deblurring methods such as Kupyn <ref type="bibr" target="#b17">[18]</ref> and Zhang <ref type="bibr" target="#b37">[38]</ref> where the performances for them are reported in <ref type="table" target="#tab_2">Table 4</ref> (l) and (n), respectively.   with incremental temporal training in Kupyn and Zhang, respectively. In both cases, our proposed approach successfully increased performance over the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Benchmark Results</head><p>We performed studies on the GoPro dataset <ref type="bibr" target="#b21">[22]</ref> for benchmarking. Tables 5 presents quantitative results of our proposed methods and other state-of-the-art methods. Our proposed model, MT-RNN (e) were trained with the GoPro dataset along with intermediate TL images and achieve the best result (31.15 dB in PSNR) over other previous stateof-the-art methods on the GoPro test dataset (1111 images). Our MT approach for the network of Zhang <ref type="bibr" target="#b37">[38]</ref> (o) also improved performance over the original network of Zhang. <ref type="figure">Figure 7</ref> shows qualitative evaluation in the case of four different models which are our proposed method (last row), the work of Nah <ref type="bibr" target="#b21">[22]</ref> (2nd row), the work of Tao <ref type="bibr" target="#b31">[32]</ref> (3rd row) and the work of Zhang <ref type="bibr" target="#b37">[38]</ref> (4th row) for given blurred images (1st row). Qualitative results show that our proposed method outperforms other state-of-the-art methods visually. <ref type="table">Table 5</ref>: Benchmarks on the GoPro dataset <ref type="bibr" target="#b21">[22]</ref> in terms of PSNR(dB), SSIM, parameter size (M) and run time (sec). 'Ours-Z' indicates Zhang <ref type="bibr" target="#b37">[38]</ref> with our MT approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PSNR SSIM Parm(M) Time Xu <ref type="bibr" target="#b36">[37]</ref> 25.10 0.890 -13.41s Kim <ref type="bibr" target="#b13">[14]</ref> 23.64 0.824 -1h Sun <ref type="bibr" target="#b30">[31]</ref> 24.64 0.843 -20m Gong <ref type="bibr" target="#b7">[8]</ref> 27.19 0.908 --Ram. <ref type="bibr" target="#b25">[26]</ref> 28.94 0.922 --Nah <ref type="bibr" target="#b21">[22]</ref> 29.08 0.914 21 3.09s Kupyn <ref type="bibr" target="#b17">[18]</ref> 28.70 0.958 -0.85s Tao <ref type="bibr" target="#b31">[32]</ref> 30.26 0.934 6.4 1.87s Kupyn <ref type="bibr" target="#b18">[19]</ref> 28.17 0.925 3.3 0.04s Zhang <ref type="bibr" target="#b37">[38]</ref> 30  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>On Imperfect Ground Truth Videos and images from high speed cameras often have mild blur assuming your subjects or objects move quickly. Thus, obtaining perfect ground truth for single image deblurring problems is quite challenging. Considering imperfect ground truth scenarios for single image deblurring, we perform experiments to observe the behaviors of our proposed MT approaches and conventional MS approaches as illustrated in <ref type="figure" target="#fig_4">Figure 8</ref>. During training, we used TL 1, 3, or 5 as ground truth and TL 7 -13 as input images. Two approaches, MS and MT methods, were applied to these simulations: using TL 3 as ground truth and using TL 5 as ground truth for training. <ref type="table">Table 6</ref> shows that both MT and MS approaches yielded excellent performance assuming known perfect ground truth. However, our proposed MT method yielded about 0.5dB better PSNR than the MS method. When TL 3 images are given as ground truth, both MT and MS methods still yielded good performance. In this case, our MT approach yielded better performance than conventional MS approach, that is also consistent with other performance comparison results in this paper. One of the possible explanations on these results is that TL 3 images are already good enough as ground truth. When TL 5 images are used as ground truth, the performance difference between MS and MT approaches became larger than other cases. Thus, these preliminary results suggest that our proposed MT approaches may be more robust to imperfect ground truth dataset for deblurring than MS approaches.</p><p>Decreasing PSNR Beyond Trained Iterations In <ref type="figure">Figure 6</ref>, MT-RNN yielded increasing PSNR during early iterations (usually, before 6 or 7 iterations) and then yielded decreasing PSNR later iterations. To study the reasons for <ref type="table">Table 6</ref>: Performance results for different approaches (MT or MS) with different ground truth that contains certain TL images only (e.g., TL 3). Our proposed MT approaches outperform MS approaches for all imperfect ground truth cases. MT-n = MT approach with TL n data as ground truth. decreasing PSNR after stopping point, we visually investigated deblurred images from our proposed methods. In images, we observed that there are often tiny artifacts appearing near the center of images. Then, as iteration increases, artifacts grows rapidly and they significantly Computation Time for "Ours-Z" In the <ref type="table">Table.</ref> 5, Ours-Z takes 2.08 seconds and iterates 6 times, while Zhang <ref type="bibr" target="#b37">[38]</ref> takes 0.02 seconds without any iteration. Generally, running time of Ours-Z is expected around 0.02 seconds, but it takes 2.08 seconds in reality. For analyzing this issue, we measure the iteration time with one blurred image. The running time on a example image is 0.015 sec, 0.092 sec, 0.581 sec, 1.073 sec, 1.568 sec and 2.065 sec in the order of iterations. Actually, the first iteration is similar with 0.02 seconds. However, after that, the running time increases exponentially. Further investigation on this issue is necessary such as looking into GPU related issues.</p><p>Weight Sharing There are a few works on DNN based MS single image deblurring that share network weights across different scales in MS architecture <ref type="bibr" target="#b31">[32]</ref> or that partially share network weights (except for feature extraction layers) <ref type="bibr" target="#b6">[7]</ref> so that the number of parameters is reduced significantly while performance is not degraded. Note that our MT approach is similar to weight sharing across temporal iterations. However, partial shared parameters that may be much more efficient <ref type="bibr" target="#b6">[7]</ref> were not be investigated in MT structure. Thus, it will be interesting to further investigate partial weights schemes for MT approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we investigate alternative approach to MS, called multi-temporal (MT) approach, for non-uniform single image deblurring. We propose incremental temporal training with constructed MT level dataset from timeresolved dataset, develop novel MT-RNNs with recurrent feature maps, and investigate progressive single image deblurring over iterations. Our proposed MT methods outperform state-of-the-art MS methods on the GoPro dataset in PSNR with the smallest number of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Pipelines of three approaches for single image deblurring including single-scale (SS), multi-scale (MS) and our proposed multi-temporal (MT). MS and MT are progressively recovering images spatially and temporally, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Progressively deblurred images over iterations using our proposed MT-RNN with incremental temporal training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>proposed a MS network architecture with Gaussian pyramid and MS loss functions [22] and Tao et al. proposed convolution long short-term memory (LSTM)-based MS DNN for single image deblurring [32]. Gao et al. proposed MS parameter sharing and nested skip connections [7]. Zhang et al. proposed a deep multi-patch hierarchical network for different feature levels on the same resolution [38]. Aljadaany et al. proposed a learning both the image prior and data fidelity terms for single image deblurring [1]. Kupyn et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Proposed architecture of MT-RNN. the jth scale, respectively. DNN Tao is their MS network and θ T ao is a set of parameters in their network. h and U are an intermediate feature map convolutional LSTM and a upsampling operation by bilinear interpolation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Simulation setups for training with imperfect ground truth for deblurring and for testing with better ground truth than that during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>MT- 5</head><label>5</label><figDesc>MT-3 MT-1 MS-5 MS-3 MS-1 PSNR 29.47 30.28 30.74 28.61 29.76 30.25 SSIM 0.895 0.901 0.917 0.876 0.897 0.908</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PSNR (dB) for single image deblurring using U-Net<ref type="bibr" target="#b26">[27]</ref> with input images with TL 3-13.</figDesc><table><row><cell>TL</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell></row><row><cell cols="7">PSNR (dB) 37.8 34.4 32.3 30.5 29.1 27.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The ablation study with multi-scale approach (MS) and our proposed approach (MT). The components of ablation study are kernel size (K), residual learning (R), recurrent feature amp (F) and approaches (MS and MT).</figDesc><table><row><cell>Approach</cell><cell>K R F PSNR SSIM Parm</cell></row><row><cell cols="2">(a) MS [32] 5 X X 29.97 0.905 6.881</cell></row><row><cell>(b) MS</cell><cell>3 X X 30.10 0.906 2.584</cell></row><row><cell>(c) MS</cell><cell>3 O X 30.25 0.908 2.594</cell></row><row><cell>(d) MT</cell><cell>3 O X 30.43 0.911 2.594</cell></row><row><cell>(e) MT</cell><cell>3 O O 30.74 0.917 2.635</cell></row><row><cell cols="2">(f) MS+MT 3 O O 30.58 0.915 2.637</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>(m) and (o) are</cell></row><row><cell>performance results when using our proposed MT approach</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Temporal steps (TS) and parameter sizes vs. performance. Parm in M, PSNR in dB, Time in sec.</figDesc><table><row><cell>TS</cell><cell cols="2">Iter Parm PSNR SSIM Time</cell></row><row><cell>(g) 0</cell><cell>1</cell><cell>2.63 29.93 0.904 0.005</cell></row><row><cell>(h) 4</cell><cell>4</cell><cell>2.63 30.44 0.913 0.020</cell></row><row><cell>(e) 2</cell><cell>6</cell><cell>2.63 30.74 0.917 0.073</cell></row><row><cell>(j) 2</cell><cell>6</cell><cell>1.46 30.21 0.908 0.060</cell></row><row><cell>(k) 2</cell><cell>6</cell><cell>5.35 30.84 0.918 0.290</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Applying our MT approach to other state-of-theart methods, Kupyn<ref type="bibr" target="#b18">[19]</ref> and Zhang<ref type="bibr" target="#b37">[38]</ref>. Recurrent feature maps was not applied. PSNR in dB, Parm in M.</figDesc><table><row><cell>Method</cell><cell cols="2">MT PSNR SSIM Parm</cell></row><row><cell>(l) Kupyn [19]</cell><cell>X</cell><cell>28.27 0.870 3.28</cell></row><row><cell>(m) Kupyn [19]</cell><cell>O</cell><cell>28.36 0.872 3.28</cell></row><row><cell>(n) Zhang [38]</cell><cell>X</cell><cell>30.25 0.908 5.42</cell></row><row><cell>(o) Zhang [38]</cell><cell>O</cell><cell>30.91 0.918 5.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Qualitative evaluations of various state-of-the-art methods as well as our proposed method on the GoPro dataset<ref type="bibr" target="#b21">[22]</ref>. Four input blurred images are on the 1st row, deblurred images of Nah<ref type="bibr" target="#b21">[22]</ref> on the 2nd row, deblurring results of Tao<ref type="bibr" target="#b31">[32]</ref> on the 3rd row, results of Zhang<ref type="bibr" target="#b38">[39]</ref> on the 4th row. Our results using MT-RNN are on the 5th row (bottom row). Our proposed method yielded deblurred images that are visually better than the results of other state-of-the-art methods for all 4 image cases, especially for fine details of images.</figDesc><table><row><cell>Figure 7:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>.21 0.934</cell><cell>5.4</cell><cell>0.02s</cell></row><row><cell cols="2">Aljadaany [1] 30.35 0.961</cell><cell>6.7</cell><cell>1.2s</cell></row><row><cell>Gao [7]</cell><cell>30.92 0.942</cell><cell>2.8</cell><cell>1.6s</cell></row><row><cell>(o) Ours-Z</cell><cell>30.78 0.940</cell><cell>5.4</cell><cell>2.08s</cell></row><row><cell>(e) Ours</cell><cell>31.15 0.945</cell><cell>2.6</cell><cell>0.07s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Douglas-rachford networks: Learning both the image prior and data fidelity terms for blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raied</forename><surname>Aljadaany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10235" to="10244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-uniform Blind Deblurring by Reblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netalee</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Neural Approach to Blind Motion Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast Motion Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="145" to="146" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to estimate and remove non-uniform image blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Couzinie-Devy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1075" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="787" to="794" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Qinfeng Shi</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image deblurring using motion density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Space-variant single-image blind deconvolution for removing camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="829" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongjoo</forename><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3160" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5426" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online Video Deblurring via Dynamic Temporal Blending Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ADAM: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8878" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating sharp panoramas from motion-blurred videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2424" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep generative filter for motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aalok</forename><surname>Pachori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Gangopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to Deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="73" to="74" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Blind Motion Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik P A</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Network for Image Deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12257</idno>
		<title level="m">Spatio-temporal filter adaptive network for video deblurring</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
