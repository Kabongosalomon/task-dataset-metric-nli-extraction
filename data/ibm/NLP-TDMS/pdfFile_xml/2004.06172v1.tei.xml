<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event detection in coarsely annotated sports videos via parallel multi receptive field 1D convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
							<email>k2vats@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnaz</forename><surname>Fani</surname></persName>
							<email>mfani@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Walters</surname></persName>
							<email>pbwalter@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
							<email>dclausi@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zelek</surname></persName>
							<email>jzelek@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Event detection in coarsely annotated sports videos via parallel multi receptive field 1D convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In problems such as sports video analytics, it is difficult to obtain accurate frame level annotations and exact event duration because of the lengthy videos and sheer volume of video data. This issue is even more pronounced in fast-paced sports such as ice hockey. Obtaining annotations on a coarse scale can be much more practical and time efficient. We propose the task of event detection in coarsely annotated videos. We introduce a multi-tower temporal convolutional network architecture for the proposed task. The network, with the help of multiple receptive fields, processes information at various temporal scales to account for the uncertainty with regard to the exact event location and duration. We demonstrate the effectiveness of the multi-receptive field architecture through appropriate ablation studies. The method is evaluated on two tasks -event detection in coarsely annotated hockey videos in the NHL dataset and event spotting in soccer on the SoccerNet dataset. The two datasets lack frame-level annotations and have very distinct event frequencies. Experimental results demonstrate the effectiveness of the network by obtaining a 55% average F1 score on the NHL dataset and by achieving competitive performance compared to the state of the art on the SoccerNet dataset. We believe our approach will help develop more practical pipelines for event detection in sports video.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sports analytics has recently emerged as one of the major applications of computer vision. Various problems such as player tracking <ref type="bibr" target="#b20">[20]</ref>, sports broadcast video registration <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">27]</ref> and sports camera selection <ref type="bibr" target="#b5">[6]</ref> are being solved with the aid of computer vision. Event detection is a challenging problem when it comes to applications of computer vision in sports. This is because of the fast paced events in cer-tain sports such as ice hockey and lack of publicly available datasets dedicated to sports.</p><p>Most of the current papers in sports event detection take advantage of frame-level annotations. Despite the availability of a large number of sports videos on online platforms such as YouTube, frame-by-frame annotations are quite difficult to obtain. As such, it can be much easier to obtain coarser, second or minute-wise annotations. The downside of this is that the annotations will be coarse and approximate, which can cause problems in sports where events last for short time spans.</p><p>In this paper, we introduce a practical paradigm for event detection in coarsely annotated untrimmed sports videos. To accomplish this, we introduce a multi-towered temporal 1D convolutional architecture for event detection. Video frames are input into a pretrained 2D CNN to obtain input feature vectors. The feature vectors are fed to the 1D convolutional towers. Each tower processes input feature vectors on different temporal scales with the help of varying temporal receptive fields. The activations from the parallel towers are finally added up to obtain class probabilities. The overall network architecture is shown in <ref type="figure">Fig. 1</ref>.</p><p>We evaluate our methodology on the NHL dataset and SoccerNet dataset <ref type="bibr" target="#b12">[13]</ref>. The NHL dataset is a densely annotated dataset with a high event frequency where each second is annotated with an event. The frame level location and duration of the event is not defined. The SoccerNet dataset also presents a more practical scenario where soccer events are anchored to particular seconds (called spots) in the video. Hence, the two datasets represent two coarsely annotated datasets with exactly opposite event frequencies.</p><p>For the high event frequency hockey dataset, the output node of each tower observes a different receptive field taking into account the uncertainty in the location and duration of the event in the coarsely annotated video. We experimentally demonstrate the effectiveness of our network architecture when compared to a fixed receptive field network with an appropriate ablation study. We obtain an F1 score of ∼ 55% on the dataset. We address the task of event spotting in Input frames sampled from untrimmed video <ref type="figure">Figure 1</ref>: The overall network architecture. The network takes t frames of dimension w × h × 3 sampled from untrimmed sports video as an input. A CNN extract l dimensional features from the video frames. The t × l features are input into a three tower temporal convolutional network. Any two towers with nodes at a particular layer have different receptive fields. The output is t 0 × c dimensional where t 0 is the number of contiguous events predicted for the t frames and c is the number of output classes. Here, b denotes the batch size. Finally softmax function is applied to obtain probabilities.</p><p>the sparsely annotated SoccerNet dataset using our network and obtain competitive performance compared to the state of the art <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Video understanding. Video understanding is one of the most important avenues for computer vision research. Action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b31">31]</ref> and temporal event localization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> are two major problems addressed in video understanding literature. Action recognition consists of recognizing actions from trimmed video clips. Various techniques such as two stream networks <ref type="bibr" target="#b29">[29]</ref>, 3D convolutions <ref type="bibr" target="#b31">[31]</ref> and recurrent neural networks <ref type="bibr" target="#b8">[9]</ref> have been utilized for action recognition. Other popular works <ref type="bibr" target="#b2">[3]</ref> in action recognition use two-stream inflated 3D convolutions obtained by pretraining 2D CNN filters on Imagenet <ref type="bibr" target="#b7">[8]</ref> and then inflating 2D filters to 3D by repeating weights depth wise.</p><p>Temporal event localization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> consists of locating the start and end frame for actions in untrimmed videos. Although <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> make use of temporal 1D convolutions, they however, require frame level annotations. Our work is related to weakly supervised approaches for temporal event localization <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28]</ref>, since we estimate event locations in untrimmed videos without frame-level annotations. Our work is also in line with TAL-Net <ref type="bibr" target="#b3">[4]</ref> and Timeception <ref type="bibr" target="#b15">[16]</ref>. TAL-Net <ref type="bibr" target="#b3">[4]</ref> ,based on the structure of Faster R-CNN, TAL-Net performs action recognition using multi-scale anchor proposals by suggesting segments from an untrimmed video with a small 1D CNN. Hussein et al. <ref type="bibr" target="#b15">[16]</ref> introduce Timeception layers for long-range complex action recognition. Timeception layers perform multi-scale temporal only convolutions with reduced complexity and can be used with either a 2D or 3D CNN backbone. The focus in this paper is on event localization in sports videos without frame level annotations, using ice hockey and soccer datasets having different event frequencies.</p><p>Sports video analytics. Event recognition <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b30">30]</ref>, player level action recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> and event detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b33">33]</ref> in sport videos are some of the active research efforts computer vision. Tora et al. <ref type="bibr" target="#b30">[30]</ref> predict hockey events using a single layered LSTM <ref type="bibr" target="#b14">[15]</ref> architecture on top of a pre-trained AlexNet <ref type="bibr" target="#b17">[18]</ref>. Mehrasa et al. <ref type="bibr" target="#b22">[22]</ref> perform group activity recognition in hockey with the help of player trajectories using 1D convolutions. Fani et al. <ref type="bibr" target="#b9">[10]</ref> recognize individual hockey player's action type by estimating the pose of the player in each video frame, using a stacked hourglass network <ref type="bibr" target="#b23">[23]</ref>, without incorporating temporal information. Cai et al. <ref type="bibr" target="#b1">[2]</ref> use the coordinates of the player's hockey stick as part of the pose of the hockey players in each frame, in conjunction with optical flow in a two stream architecture. In another work, Fani et al. <ref type="bibr" target="#b10">[11]</ref> performs action recognition of individual soccer players from video by extracting the pose of the player, normalizing it and applying LSTM layers for capturing the temporal variation of the player's pose during the action performance.</p><p>(a) Towers T 1 , T 2 and T 3 <ref type="figure">Figure 2</ref>: Illustration of temporal convolutional towers denoted by T 1 , T 2 and T 3 . The first block of each tower has a receptive field of 3, 5 and 2 respectively. The coloured nodes represent the input feature vectors. The two output nodes of each network corresponds to two contiguous events. The green colored nodes represent the input feature vectors in the receptive field of the first output node. The receptive field of a particular node covers the event itself and the context around it. Note that the receptive field of the output nodes in each tower is different.</p><p>Due to the lack of standard datasets for event detection in sports videos, many researchers generate their own datasets which usually have limited size, and are often not generalizable. To address this issue, Giancola et al. <ref type="bibr" target="#b12">[13]</ref> introduce SoccerNet, a benchmark for event spotting in soccer videos. This benchmark, which is generated for the purpose of localizing very sparse events within long videos, spots three main event types in 500 soccer games. McNally et al. <ref type="bibr" target="#b21">[21]</ref> introduce a benchmark database for detecting eight events in the golf swing, named GolfDB consisting of 1400 golf swing videos.</p><p>The above research, with the exception of Giancola et al. <ref type="bibr" target="#b12">[13]</ref>, either utilize frame level annotations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b33">33]</ref> or classify trimmed video clips <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>Here, we focus more on the practical case where frameby-frame video annotations are not available in untrimmed video datasets of different event frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The proposed approach for event detection is explained in Subsection 3.1. The designed network is shown in <ref type="figure">Figure  1</ref> and explained in Subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed Approach</head><p>Events in sport videos occur at varying temporal scales. For instance, in hockey, events such as 'shots', usually occur in a shorter time span than an event like a 'faceoff'. To take this factor into account, we employ 1D CNNs of varying kernel sizes and receptive fields. The information from these parallel 1D CNNs is fused to obtain event class probabilities. The network architecture is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>To detect events in untrimmed sport videos, we make use of a multi-tower architecture. The towers represent temporal 1D CNNs with different receptive fields. The output node gives the probability of an event occurring in the video. The network architecture is illustrated in <ref type="figure">Figure 1</ref>.</p><p>The input to the network is a sequence of t frames {I k ∈ R w×h×3 : k ∈ {1, 2, ..., t}} sampled uniformly at a frame rate of f frames per second from an untrimmed sports video. The images are passed through a 2D CNN in order to obtain features F k ∈ R l from an intermediate layer.</p><p>Separate 1D convolution towers of varying kernel sizes (or varying receptive fields) are applied on top of the features F k . The kernel size and stride of CNN filters in the towers is chosen such that for each tower, a node in a particular layer has a different receptive field than the corresponding node in other towers. We incorporate contextual features such that the network sees what happens immediately before and after an event.</p><p>Each 1D convolutional network is composed of a number of 1D convolution blocks named TConv block, of structure illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. The blocks are composed of a 1D convolutional layer followed by a batch normalization layer <ref type="bibr" target="#b16">[17]</ref> and ReLU non-linearity. In addition, skip connections <ref type="bibr" target="#b13">[14]</ref> are also added by slicing the input to match the output. While adding skip connections, downsampling is done in case dimensionality of input is different than that of the output. The output O t ∈ R t0×c where t 0 is the number of contiguous events predicted and c represents the number of output classes. The output of 1D convolutional towers are added together and then softmaxed to obtain event class probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets Used</head><p>We have used two different datasets for our experiments. The two datasets have a large variation in event frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">NHL dataset</head><p>The NHL dataset consists of 10 NHL games of three periods each, with separate 60 fps videos for all game periods. The videos come from broadcast footage and include advertisements, replays, shots of varying range, and overlayed graphics. Each video has a spatial resolution of 1280 × 720 pixels. The videos are annotated with one second resolution, whereby the event is expected to occur within the one Since t &lt; t, the residuals are sliced symmetrically for making the skip connection. b and l denote the batch size and feature dimension respectively. second interval. The events are annotated with at least one of the following labels: Faceoff, Shot, Advance, or Play. <ref type="table" target="#tab_1">Table 1</ref> gives descriptions of the event types and <ref type="figure" target="#fig_1">Figure 4</ref> shows example frames. The dataset is heavily imbalanced with Play consisting of ∼ 80% of all events. In some cases, an annotated Play event may overlap with another event of another type. In this case, the time frame is simply assigned to the non-Play event without affecting the data distribution. The dataset contains 589 Faceoffs, 1,062 Shots , 1,306 Advance and 11,116 Play events. The dataset has a high event rate of one event every 4.5 seconds. The actual event rate is higher when excluding advertisements.</p><p>The annotations are approximate and coarse. The frames that represent an event usually span a fraction of a second and may actually be present outside the annotated one second window. This also means that the exact frame-wise location of the event is not defined. The annotations were collected manually and this annotation scheme is more practical than frame-level annotations, which are very difficult and time consuming to obtain. The dataset is split such that nine games are used for training, and one period and two periods from the remaining game are used for validation and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SoccerNet Dataset</head><p>The SoccerNet dataset <ref type="bibr" target="#b12">[13]</ref> is composed of 500 soccer games from the main European Championships from three seasons with a total duration of 764 hours. The events are categorized into three categories: Yellow/Red Card, Goal, or Substitution. The dataset is very sparse such that it contains an average of one event every 6.9 minutes, making the task of event localization difficult. For each event, temporal anchors of one second resolution are obtained according to well-defined soccer rules. The 500 games are randomly split into 300 games for training, 100 for validation and 100 for testing. We use the same split as Giancola et al. <ref type="bibr" target="#b12">[13]</ref> for our experiments. PCA reduced 512 dimensional backbone features are provided corresponding to ResNet <ref type="bibr" target="#b13">[14]</ref>, C3D <ref type="bibr" target="#b31">[31]</ref> and I3D <ref type="bibr" target="#b2">[3]</ref> networks. The features were extracted every 0.5 seconds from the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We perform experiments on the NHL and the Soccer-Net datasets mentioned above. We introduce the task of event detection in coarsely annotated videos using the NHL dataset and address the task of event spotting on the Soccer-Net dataset <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Event detection in coarsely annotated NHL videos 5.1.1 Objective</head><p>The objective of this task is to detect events from coarsely annotated untrimmed hockey videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experiment Settings</head><p>The videos are first downsampled to a resolution of 284 × 160 (w = 284, h = 160) pixels such that the initial aspect ratio is maintained. Images are sampled uniformly at a rate of 10 frames per second. We sample a total of t = 30 frames for a period of 3 seconds. The network architecture is designed such that the number of output nodes are two i.e., we predict the output for t 0 = 2 contiguous seconds (detail in <ref type="figure">Figure 2</ref>). This is because, the NHL dataset is quite dense and two different events for instance, shot and play can occur consecutively. We subtract ImageNet <ref type="bibr" target="#b7">[8]</ref> mean and divide by ImageNet standard deviation for normalization. A MobileNetV2 <ref type="bibr" target="#b26">[26]</ref> pretrained on ImageNet is used to extract features from the video. Global average pooling is performed on the final layer of MobileNetV2 to obtain l = 1280 dimensional features. The network architecture used to process the 1280 dimensional features is shown in <ref type="table" target="#tab_4">Table 4</ref>. We use a three towered architecture with the first block of the towers having an effective receptive field of 2,3 and 5 respectively. Random horizontal flipping is used for data augmentation.</p><p>Since the background covers a major proportion of the video, in order to handle the heavy class imbalance in the dataset, we explicitly control the event sampling such that the background events are sampled with a probability of p 0 and the events are sampled with probability 1 − p 0 . The value of p 0 is empirically chosen as 0.2. This is done to ensure that the training batches contain an even distribution </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Description Faceoff</head><p>The puck is dropped between the sticks of two opposing players Shot A player attempts to shoot the puck on goal Advance A player moves the puck into or out of the defensive or offensive zone without an intended recipient (e.g., dump in, clearing attempt) Play A player moves the puck with an intended recipient (e.g., pass, stickhandle) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Post processing</head><p>During evaluation/testing phase, we apply the network in a sliding window fashion with a stride of one second on the untrimmed video. Since the testing is done with a stride of one second, each event is predicted twice. We take the maximum confidence of these two predictions. Furthermore, we take advantage of the fact that events such as Faceoff, Advance and Shot are extremely less likely to occur consecutively. If the network predicts one of these event n times in a row where n &gt; 1, we only consider the prediction with maximum confidence. The rest of the predictions are assigned the prediction with second largest confidence. This leads to an average improvement of 2-3 % in F1 score values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Results and Analysis</head><p>A predicted event for a one second interval is considered correct if it is within one second of any ground truth. It can be understood as accuracy within a tolerance of δ = 1 second. We calculate the precision, recall and F1 score for each class according to the above definition. <ref type="table" target="#tab_2">Table 2</ref> shows the F1 scores for each class. Faceoff and Play events have the highest F1 score whereas advance event has the lowest F1 score values (65.62 and 65.04 respectively). From the low precision value (32.23) of Advance events, it can be concluded that other events are often mistaken for Advance events. A common observation is that long passes (Play events) are often mistaken as Advance events. Another issue arises when a faceoff is called off before actually happening, usually due to a faceoff violation on the play. The model, in this scenario, gets enough spatiotemporal information to classify it as a Faceoff event because of which false alarms are generated. These kind of failure cases are illustrated in <ref type="figure">Fig. 5</ref>. Also, many times, the hockey players are occluded by the near boards of the rink. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of the individual towers in the first three rows. For comparing inherent network performance, comparison values are based on network outputs excluding post processing. Repeating the same tower three times and jointly training the three towers performs at-least as good or better than a single tower as demonstrated by the higher F1 score of T 2 + T 2 + T 2 and T 3 + T 3 + T 3 configuration than their single tower counterparts. This is due to  <ref type="figure">Figure 5</ref>: Two of the common kinds of failure cases. In (a), the network predicts a long pass as an Advance event. The red boxes denote the two players between whom the pass is being made. In (b),the faceoff is called off before actually happening, usually due to a faceoff violation on the play. the increase in representational power from the increase in network capacity. This is further seen in the training loss of the respective models <ref type="figure" target="#fig_3">(Fig. 6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Ablation studies</head><p>We perform ablation experiments on the number of temporal convolutional towers. The purpose of the study is to demonstrate that the increase in performance is not merely because of the increase in network capacity. <ref type="table" target="#tab_3">Table 3</ref> demonstrates that using towers with different receptive fields is important. Repeating the same tower three times, although increases parameters, but does not improve performance when compared to a multi receptive field network. This is evident by the highest F1 scores obtained by the three different receptive field setting (51.56). This is also demonstrated by the lowest training loss value of T 1 + T 2 + T 3 setting in <ref type="figure" target="#fig_3">Fig 6.</ref> Repeating a fixed receptive field tower three times has one of the two following effects: either redundant information is provided to the network, (which means the  receptive field is too large for an event), or less information is provided if the receptive field is too small, (which results in lower accuracy).  . The vertical dashed lines denote the ground truth spot timings. The network generates clean proposal segments for each event type. The high substitution probabilities between 900 -1050 second occur during replay-highlights of card and goal. The replay-highlights are a card and substitution are often similar, where camera is focused on a single player leading to false positives and lower precision for these events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Event spotting in soccer 5.2.1 Objective</head><p>The objective of this task is to find the anchors of soccer events in soccer game videos. We demonstrate the effectiveness of our approach by achieving competitive performance compared to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experiment Settings</head><p>Instead of a two step approach used by Giancola et al. <ref type="bibr" target="#b12">[13]</ref>, consisting of classification and then spotting, we train our model directly on the spotting task. The model is trained on 15 second windows consisting of t = 30 features (features are extracted at 2fps from the video). We again use the three tower architecture used in the NHL dataset. However, the model now outputs a single node (t 0 = 1) representing the probability of the event. This is done by simply averaging the output of of final two nodes of the model used in the NHL dataset. This is done because, unlike the NHL dataset, the SoccerNet dataset is quite sparse and it can be safely assumed that a single event occurs in the 15 second interval. The ground truth anchor is kept at the center of the sampled window of 15 second.</p><p>On the testing data, we slide the network with a stride of one second in order to obtain event probabilities for one second resolution. As per Giancola et al. <ref type="bibr" target="#b12">[13]</ref> we use a watershed method to generate segment proposals and use the center time in the segment to define the spotting candidate.</p><p>To handle the dataset imbalance resulting from the addition of the background class, we control event sampling with the value of parameter p 0 = 0.6. A weighted cross entropy loss is used where the background class is given a weight of 0.33 and rest of the classes given a weight of 1 each. The training is done with a batch size of b = 120. For data augmentation, we tried shifting the windows containing events by a random offset s ∈ [−7.5, 7.5] seconds from the event anchor, which, however did not bring any accuracy improvement. Adam optimizer is used with an initial leaning rate of 0.001. The training is done on an Nvidia GTX 1080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results and Analysis</head><p>Giancola et al. <ref type="bibr" target="#b12">[13]</ref> define the task of event spotting as finding the anchor time, called spot candidate that identifies the location of an event. A candidate spot is defined as positive if it lands within a tolerance δ around the ground truth anchor. Intuitively, the closer the candidate to a target, the better is the spotting performance. Mean average precision (mAP) is calculated for a given tolerance δ. The accuracy  metric is the average mAP between δ = 5 to δ = 60 seconds. Giancola et. al. <ref type="bibr" target="#b12">[13]</ref> showed that the I3D and C3D features already include temporal information, further incorporating these features in a temporal architecture leaves them redundant. Therefore, we use ResNet features in our experiments. <ref type="table" target="#tab_5">Table 5</ref> shows the highest per-class mAP of the network (corresponding to δ = 60 seconds) compared with Giancola et. al. Goal events are the easiest to spot obtaining an mAP of 79%. Card events are the most difficult to spot with an mAP of 63.1%. <ref type="figure" target="#fig_6">Figure 9</ref> shows the corresponding precision-recall curves for the three classes. <ref type="figure" target="#fig_4">Figure 7</ref> shows the event probability vs game time plot for one of the soccer games in the test set. The network generates clean proposal segments for each event type. A reason why the mAP for substitution and cards is low is because the replay-highlights of a card and substitution are often similar, where camera is focused on a single player leading to false positives and lower precision for these events. An example of this can be seen in <ref type="figure" target="#fig_4">Figure 7</ref> with significant value for substitution probability after the first card event. <ref type="figure" target="#fig_5">Figure 8</ref> shows the mAP vs tolerance(δ) curve for tolerance between δ = 5 to δ = 60. From the shape of the curve, the mAP decreases almost linearly for tolerance below which the model was trained on i.e. 15 seconds. Around 60 second tolerance, the mAP saturates to ∼ 70 %. We obtain an average-mAP of 60.1 % averaged over the tolerances which exceeds Giancola et. al (49.7%) by 10.4% <ref type="table" target="#tab_6">(Table 6</ref>). We argue that this is because our approach is able to understand the temporal aspect of the game better when compared to the two step NetVLAD <ref type="bibr" target="#b0">[1]</ref> pooling (64 clusters) and Resnet152 <ref type="bibr" target="#b13">[14]</ref> based classification-detection approach used in Giancola et. al. <ref type="bibr" target="#b12">[13]</ref>. Cioppa et. al.  <ref type="bibr" target="#b12">[13]</ref> 34.5 Giancola et. al (20s) <ref type="bibr" target="#b12">[13]</ref> 49.7 Giancola et. al (60s) <ref type="bibr" target="#b12">[13]</ref> 40.6 Cioppa et. al <ref type="bibr" target="#b6">[7]</ref> 62.5 Ours 60.1 <ref type="bibr" target="#b6">[7]</ref> recently introduced a context aware loss function for soccer action spotting using a combination of segmentation loss followed by an iterative matching procedure and a separate spotting loss. Our work achieves a competitive performance, with a difference of 2.4% mAP ( <ref type="table" target="#tab_6">Table 6</ref>) and outperforms the ablation study baselines in Cioppa et. al. <ref type="bibr" target="#b6">[7]</ref>, by using a much simpler network/approach using cross entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we address the difficulty of obtaining frame-level annotations in sport event detection. We introduce a multi-scale temporal 1D convolutional network for detecting events in two coarsely annotated datasets of completely different event frequencies. The results obtained on the SoccerNet dataset are more impressive than the hockey results. A reason for this is that the events in hockey are much more fast paced and frequent as compared to soccer, making the hockey dataset more challenging. Future work will be focused on taking advantage of player level contextual features, hockey puck position and game audio for the task of event detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>TConv block consists of a 1D Convolutional layer, Batch Normalization layer and ReLU activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Examples of frames from each of the annotated events of all c = 5 event classes (including background), without which the model finds it difficult to converge. Further, in all experiments, a weighted cross entropy loss is used with play event and background assigned a weight of .05 and .033 respectively and the rest of the classes are assigned a weight of 1 each. Adam optimizer is used with an initial leaning rate of 0.001. The training is done on an Nvidia GTX 1080 Ti GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Ground Truth: Play, Predicted: Advance (b) Ground Truth: No event, Predicted: Faceoff</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Training loss vs Number of Iterations for multi tower settings. The configuration T 1 + T 2 + T 3 (red curve) attains the lowest loss values as compared to the other configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Event detection probability vs game time plot for a 5 minute interval in the second half of 2016-2017 season UEFA Champions league Barcelona vs PSG game</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>mAP as a function of tolerance δ. The model obtains an average mAP of 60.1%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Precision recall curves corresponding to the best performing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This work was supported by Stathletes through the Mitacs Accelerate Program and Natural Sciences and Engineering Research Council of Canada (NSERC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Event descriptions in the NHL dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Precision, Recall and F1 score values for the network for the NHL dataset</figDesc><table><row><cell></cell><cell>Faceoff</cell><cell>Shot</cell><cell>Advance</cell><cell>Play</cell><cell>Average</cell></row><row><cell>Precision</cell><cell>77.78</cell><cell>52.74</cell><cell>32.23</cell><cell>51.42</cell><cell>53.54</cell></row><row><cell>Recall</cell><cell>56.76</cell><cell>44.86</cell><cell>44.88</cell><cell>88.48</cell><cell>58.74</cell></row><row><cell>F1 score</cell><cell>65.62</cell><cell>48.49</cell><cell>40.70</cell><cell>65.04</cell><cell>54.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of various 3-tower configurations for the NHL dataset. T 1 , T 2 and T 3 represent temporal convolutional towers having different receptive fields. Highest F1 score values are obtained on using towers with different receptive fields.</figDesc><table><row><cell cols="4">Network type Faceoff Shot Advance Play Avg F1</cell></row><row><cell>T 1</cell><cell>54.32 44.44</cell><cell>35.13</cell><cell>64.36 49.56</cell></row><row><cell>T 2</cell><cell>45.98 41.32</cell><cell>23.91</cell><cell>63.79 43.75</cell></row><row><cell>T 3</cell><cell>50.70 40.81</cell><cell>30.34</cell><cell>64.55 46.60</cell></row><row><cell>T 1 + T 1 + T 1</cell><cell>49.11 44.77</cell><cell>34.41</cell><cell>65.72 48.50</cell></row><row><cell>T 2 + T 2 + T 2</cell><cell>55.42 48.02</cell><cell>33.12</cell><cell>65.69 50.56</cell></row><row><cell>T 3 + T 3 + T 3</cell><cell>49.35 43.83</cell><cell>30.63</cell><cell>64.48 47.07</cell></row><row><cell>T 1 + T 2 + T 3</cell><cell>56.76 45.59</cell><cell>38.86</cell><cell>65.18 51.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Network architecture for the NHL dataset. Each column denotes a temporal convolution tower T k . k,s,d and p denote kernel size, stride, dilation and padding respectively</figDesc><table><row><cell>T 1</cell><cell>T 2</cell><cell>T 3</cell></row><row><cell>k=3, s=3,d=1,p=0</cell><cell cols="2">k=3, s=5,d=2,p=0 k=2, s=2,d=1,p=0</cell></row><row><cell>Batch Norm 1D</cell><cell>Batch Norm 1D</cell><cell>Batch Norm 1D</cell></row><row><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell>k=3, s=3, d=1,p=1</cell><cell cols="2">k=3, s=1,d=1,p=0 k=3, s=3,d=1,p=0</cell></row><row><cell>Batch Norm 1D</cell><cell>Batch Norm 1D</cell><cell>Batch Norm 1D</cell></row><row><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell cols="3">k=3, s=1, d=1, p=0 k=2, s=2,d=1,p=0 k=3, s=2,d=1,p=0</cell></row><row><cell>Batch Norm 1D</cell><cell>Batch Norm 1D</cell><cell>Batch Norm 1D</cell></row><row><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell></cell><cell>Sum</cell><cell></cell></row><row><cell></cell><cell>Softmax</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Class wise mAP scores for the SoccerNet dataset. Our method outperforms<ref type="bibr" target="#b12">[13]</ref> on all the classes</figDesc><table><row><cell></cell><cell cols="4">Cards Subs Goals Average</cell></row><row><cell>Giancola et. al [13]</cell><cell>52.1</cell><cell>59.3</cell><cell>73.0</cell><cell>61.5</cell></row><row><cell>Ours</cell><cell>63.1</cell><cell>69.1</cell><cell>79.0</cell><cell>70.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>mAP scores for the soccer action spotting task. Our work achieves competitive results compared to the state of the art. Method mAP Giancola et. al (5s)</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal hockey action recognition via pose and optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sports camera calibration via synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning sports camera selection from internet videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="1682" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A context-aware loss function for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deliège</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hockey action recognition via integrated stacked hourglass network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnaz</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pose-projected action recognition hourglass network (parhn) in soccer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnaz</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3570" to="3579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soccernet: A scalable dataset for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohieddine</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Dghaily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tracking sports players with context-conditioned motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1830" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Golfdb: A video database for golf swing sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning of player trajectory representations for team activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bornn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th MIT Sloan Sports Analytics Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Early detection of injuries in mlb pitchers from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-tocoarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated top view registration of broadcast football videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors</editor>
		<imprint>
			<biblScope unit="page" from="162" to="179" />
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos. ArXiv, abs/1406.2199</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classification of puck possession events in ice hockey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Tora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV 15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV 15<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">44894497</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream action recognition in ice hockey using player pose sequences and optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Continuous video to simple signals for swimming stroke detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Miniutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
