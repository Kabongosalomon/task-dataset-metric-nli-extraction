<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">HERE Technologies</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Wormpex AI Research</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Temporal Action Localization; Weakly-Supervised Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Weakly-supervised Temporal Action Localization (W-TAL) aims at simultaneously localizing and classifying all action instances in a long untrimmed video given only video-level categorical labels in the learning phase. Compared to its fully-supervised counterpart, which requires frame-level annotations of all action instances during training, W-TAL greatly simplifies the procedure of data collection and avoids annotation bias of human annotators, therefore has been widely studied <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20]</ref> in recent years.</p><p>Several W-TAL methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref> adopt a Multiple Instance Learning (MIL) framework, where a video is treated as a bag of frames/snippets  <ref type="figure">Fig. 1</ref>: Visualization of two-stream outputs and their late fusion result. The first two rows are an input video and the ground truth action instances, respectively. The last three rows are attention sequences (scaled from 0 to 1) predicted by the RGB stream, the flow stream and their weighted sum (i.e., the fusion result), respectively, and the horizontal and vertical axes denote the time and the intensity of attention values, respectively. The green boxes denote the localization results generated by thresholding the attention at the value of 0.5. By properly combining the two different attention distributions predicted by the RGB and flow streams, the late fusion result achieves a higher true positive rate and a lower false positive rate, and thus has better localization performance to perform the video-level action classification. During testing, the trained model slides over time and generates a Temporal-Class Activation Map (T-CAM) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b26">27]</ref> (i.e., a sequence of probability distributions over action classes at each time step) and an attention sequence that measures the relative importance of each snippet. The action proposals are generated by thresholding the attention value and/or the T-CAM. This MIL framework is usually built on two feature modalities, i.e., RGB frames and optical flow, which are fused in two possible ways. Early fusion methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20]</ref> concatenate the RGB and optical flow features before they are fed into the network, and late fusion methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26]</ref> compute a weighted sum of their respective outputs before generating action proposals. An example of late fusion is shown in <ref type="figure">Fig. 1</ref>.</p><p>Despite these recent development, two major challenges still persist. One of the most critical problems that prior W-TAL methods suffer from is the lack of ability to rule out false positive action proposals. Without frame-level annotations, they localize action instances that do not necessarily correspond to the video-level labels. For example, a model may falsely localize the action "swimming" by only checking the existence of water in the scene. Therefore, it is necessary to exploit more fine-grained supervision to guide the learning process. Another problem lies in the generation of action proposals. In previous methods, action proposals are generated by thresholding the activation sequence with a fixed threshold, which is preset empirically. It has a significant impact on the quality of action proposals: a high threshold may result in incomplete action proposals while a low threshold can bring more false positives. But how to get out of this dilemma was rarely studied.</p><p>In this paper, we introduce a Two-Stream Consensus Network (TSCN) to address the two aforementioned problems. To eliminate false positive action proposals, we design an iterative refinement training scheme, where a frame-level pseudo ground truth is generated from late fusion attention sequence, and serves as a more precise frame-level supervision to iteratively update two-stream models. Our intuition is simple: late fusion is essentially a voting ensemble of the RGB and flow streams, and if a proper fusion parameter (i.e., the hyperparameter to control the relative importance of two streams) is selected, late fusion can provide more accurate result compared with each individual stream. The advantage of combining these two streams has been demonstrated by the Two-Stream Convolutional Networks <ref type="bibr" target="#b36">[37]</ref> for action recognition. As shown in <ref type="figure">Fig. 1</ref>, the two streams produce different activation distributions, which lead to different false positives and false negatives. However, when they are combined, the false positive action proposals that only exist in one stream can be largely eliminated, and a high activation value occurs only when both streams are confident that an action instance exists. Since the late fusion result is of higher quality than single stream result, it can in turn serve as a frame-level pseudo ground truth to supervise and refine both streams. To generate high-quality action proposals, we introduce a new attention normalization loss. It pushes the predicted attention to approach extreme values, i.e., 0 and 1, so as to avoid ambiguity. As a result, simply setting the threshold to 0.5 yields high-quality action proposals.</p><p>Formally, given an input video, RGB and optical flow features are first extracted from pre-trained deep networks. Then two-stream base models are trained with video-level labels on RGB and optical flow features, respectively, where the attention normalization loss is used to learn the attention distribution. After obtaining two-stream attention sequences, a frame-level pseudo ground truth is generated based on their weighted sum (i.e., the late fusion attention sequence), and in turn provides frame-level supervision to improve the two-stream models. We iteratively update the pseudo ground truth and refine the two-stream base models, and the normalization term at the same time forces the predicted attention to approach a binary selection. The final localization result is obtained by thresholding the late fusion attention sequence.</p><p>To summarize, our contribution is threefold:</p><p>-We introduce a Two-Stream Consensus Network (TSCN) for W-TAL. The proposed TSCN uses an iterative refinement training method, where a pseudo ground truth generated from late fusion attention sequence at previous iteration can provide more precise frame-level supervision to current iteration. -We propose an attention normalization loss function, which forces the attention to act like a binary selection, and thus improves the quality of action proposals generated by the thresholding method.</p><p>-Extensive experiments are conducted on two standard benchmarks (i.e., THUMOS14 and ActivityNet) to demonstrate the effectiveness of the proposed method. Our TSCN significantly outperforms previous state-of-the-art W-TAL methods, and even achieves comparable results to some recent fullysupervised TAL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Action Recognition. Traditional methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref> aim to model spatiotemporal information via hand-crafted features. Two-Stream Convolutional Networks <ref type="bibr" target="#b36">[37]</ref> use two separate Convolutional Neural Networks (CNNs) to exploit appearance and motion clues from RGB frames and optical flow, respectively, and use a late fusion method to reconcile the two-stream outputs. <ref type="bibr" target="#b9">[10]</ref> focuses on studying different ways to fuse the two streams. The Inflated 3D ConvNet (I3D) <ref type="bibr" target="#b2">[3]</ref> expands the 2D CNNs in two-stream networks to 3D CNNs. Several recent methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31]</ref> focus on directly learning motion clues from RGB frames instead of calculating optical flow. Fully-supervised Temporal Action Localization. Fully-supervised TAL requires frame-level annotations of all action instances during training. Several large-scale datasets have been created for this task, such as THUMOS <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>, ActivityNet <ref type="bibr" target="#b1">[2]</ref>, and Charades <ref type="bibr" target="#b35">[36]</ref>. Many methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref> adopt a two-stage pipeline, i.e., action proposal generation followed by action classification. Several methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref> adopt the Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> framework to TAL.</p><p>Most recently, some methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21]</ref> try to generate action proposals with more flexible durations. Zeng et al. <ref type="bibr" target="#b44">[45]</ref> apply the Graph Convolutional Networks (GCN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref> to TAL to exploit proposal-proposal relations.</p><p>Weakly-supervised Temporal Action Localization. W-TAL, which only requires video-level supervision during training, greatly relieves the data annotation efforts, and draws more and more attention from the community recently. Hide-and-Seek <ref type="bibr" target="#b17">[18]</ref> randomly hides part of the input video to guide the network to discover other relevant parts. UntrimmedNet <ref type="bibr" target="#b40">[41]</ref> consists of a selection module to select the important snippets and a classification module to perform per snippet classification. Sparse Temporal Pooling Network (STPN) <ref type="bibr" target="#b26">[27]</ref> improves UntrimmedNet by adding a sparse loss to enforce the sparsity of selected segments. W-TALC <ref type="bibr" target="#b29">[30]</ref> jointly optimizes a co-activity similarity loss and a multiple instance learning loss to train the network. AutoLoc <ref type="bibr" target="#b33">[34]</ref> is one of the first two-stage methods in W-TAL, and it first generates initial action proposals and then regresses the boundaries of the action proposals with an Outer-Inner-Contrastive loss. CleanNet <ref type="bibr" target="#b23">[24]</ref> improves AutoLoc by leveraging the temporal contrast in snippet-level action classification predictions. Liu et al. <ref type="bibr" target="#b22">[23]</ref> propose a multi-branch network to model different stages of action. Besides, several methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20]</ref> focus on modeling the background and achieve state-of-the-art performances.</p><p>Recently, RefineLoc <ref type="bibr" target="#b0">[1]</ref> uses an iterative refinement method to help the model capture a complete action instance. And our method is distinct from RefineLoc in three main aspects. (1) We adopt a late fusion framework, while RefineLoc adopts an early fusion framework. (2) Our pseudo ground truth is generated from two-stream late fusion attention sequences, which provides better localization performance than each single stream, while RefineLoc generates the pseudo ground truth by expanding previous localization results, which might result in coarser and over-complete action proposals. (2) two-stream base models are separately trained using these RGB and optical flow features; (3) frame-level pseudo ground truth is generated from the two-stream late fusion attention sequence, and in turn provides frame-level supervision to two-stream base models normalization loss to explicitly avoid the ambiguity of attention, while RefineLoc has no explicit constraints on attention values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two-Stream Consensus Network</head><p>In this section, we first formulate the task of Weakly-supervised Temporal Action Localization (W-TAL), and then describe the proposed Two-Stream Consensus Network (TSCN) in detail. The overall architecture is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Assume we are given a set of training videos. For each video v, we only have its video-level categorical label y, where y ∈ R C is a normalized multi-hot vector, and C is the number of action categories. The goal of temporal action localization is to generate a set of action proposals {(t s , t e , c, ψ)} for each testing video, where t s , t e , c, ψ denote the start time, the end time, the predicted action category and the confidence score of the action proposal, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>Following recent W-TAL methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20]</ref>, we construct TSCN upon snippet-level feature sequences extracted from the raw video vol-ume. The RGB and optical flow features are extracted with pre-trained deep networks (e.g., I3D <ref type="bibr" target="#b2">[3]</ref>) from non-overlapping fixed-length RGB frame snippets and optical flow snippets, respectively. They provide high-level appearance and motion information of the corresponding snippets. Formally, given a video with T non-overlapping snippets, we denote the RGB features and optical flow features</p><formula xml:id="formula_0">as {f RGB,i } T i=1 and {f flow,i } T i=1 , respectively, where f RGB,i , f flow,i ∈ R D</formula><p>are the feature representations of the i-th RGB frame and optical flow snippet, respectively, and D denotes the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two-Stream Base Models</head><p>After obtaining the RGB and optical flow features, we first use two-stream base models to perform the video-level action classification, and then iteratively refine the base models with a frame-level pseudo ground truth. The features of two modalities are fed into two separate base models, respectively, and the two base models use the same architecture but do not share parameters. Therefore, in this subsection, we omit the subscript RGB and flow for conciseness.</p><p>Since the features are not originally trained for the W-TAL task, we concatenate the T input features {f i } T i=1 , and use a set of temporal convolutional layers to generate a set of new features</p><formula xml:id="formula_1">{x i } T i=1 , where x i ∈ R D , and D denotes the output feature dimension.</formula><p>As a video may contain background snippets, to perform video-level classification, we need to select snippets that are likely to contain action instances and meanwhile filter out snippets that are likely to contain background. To this end, an attention value A i ∈ (0, 1) to measure the likelihood of the i-th snippet containing an action is given by a fully-connected (FC) layer:</p><formula xml:id="formula_2">A i = σ (w A · x i + b A ) ,<label>(1)</label></formula><p>where σ(·), w A , and b A are the sigmoid function, weight vector and bias of the attention layer. We then perform attention-weighted pooling over the feature sequence to generate a single foreground feature x fg , and feed it to an FC softmax layer to get the video-level prediction:</p><formula xml:id="formula_3">x fg = 1 T i=1 A i T i=1 A i x i ,<label>(2)</label></formula><formula xml:id="formula_4">y c = e wc·x fg +bc C i=1 e wi·x fg +bi ,<label>(3)</label></formula><p>whereŷ c is the probability that the video contains the c-th action, and w c and b c are the weight and bias of the FC layer for category c. The classification loss function L cls is defined as the standard cross entropy loss:</p><formula xml:id="formula_5">L cls = − C c=1 y c log(ŷ c ),<label>(4)</label></formula><p>where y c denotes the value of label vector y at index c. Ideally, an attention value is expected to be binary, where 1 indicates the presence of action while 0 indicates background. Recently, several methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20]</ref> introduce a background category, and use the background classification to guide the learning of attention. In this work, instead of using background classification, we introduce an attention normalization term to force the attention to approach extreme values:</p><formula xml:id="formula_6">L att = 1 l min A⊂{Ai} |A|=l a∈A a − 1 l max A⊂{Ai} |A|=l a∈A a,<label>(5)</label></formula><p>where l = max 1, T s and s is a hyperparameter to control the selected snippets. This normalization loss aims to maximize the difference between the average top-l attention values and the average bottom-l attention values, and force the foreground attention to be 1 and background attention to be 0.</p><p>Therefore, the overall loss for the base model training is the weighted sum of the classification loss and the attention normalization term:</p><formula xml:id="formula_7">L base = L cls + αL att ,<label>(6)</label></formula><p>where α is a hyperparameter to control the weight of the normalization loss.</p><p>In addition, the temporal-class activation map (T-CAM) {s i } T i=1 , s i ∈ R C is also generated by sliding the classification FC softmax layer over all snippets:</p><formula xml:id="formula_8">s i,c = e wc·xi+bc C j=1 e wj ·xi+bj ,<label>(7)</label></formula><p>where s i,c is the T-CAM value of i-th snippet for category c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pseudo Ground Truth Generation</head><p>We iteratively refine the two-stream base models with a frame-level pseudo ground truth. Specifically, we divide the whole training process into several refinement iterations. At refinement iteration 0, only video-level labels are used for training. And at refinement iteration n + 1, a frame-level pseudo ground truth is generated at refinement iteration n, and provides frame-level supervision for the current refinement iteration. However, without true ground truth annotations, we can neither measure the quality of the pseudo ground truth, nor guarantee the pseudo ground truth can help the base models achieve higher performance. Inspired by two-stream late fusion, we introduce a simple yet effective method to generate the pseudo ground truth. Intuitively, locations at which both streams have high activations are likely to contain ground truth action instances; locations at which only one stream has high activations are likely to be either false positive action proposals or true action instances that only one stream can detect; locations at which both streams both have low activations are likely to be the background.</p><p>Following this intuition, we use the fusion attention sequence {A</p><formula xml:id="formula_9">(n) fuse,i } T i=1 at refinement iteration n to generate pseudo ground truth {G (n+1) i } T i=1 for refinement iteration n + 1, where A (n) fuse,i = βA (n) RGB,i + (1 − β)A (n)</formula><p>flow,i , and β ∈ [0, 1] is a hyperparameter to control the relative importance of RGB and flow attentions. We introduce two pseudo ground truth generation methods. Soft pseudo ground truth means to directly use the fusion attention values as pseudo labels: G</p><formula xml:id="formula_10">(n+1) i = A (n)</formula><p>fuse,i . The soft pseudo labels contain the probability of a snippet being the foreground action, but also add uncertainty to the model. Hard pseudo ground truth thresholds the attention sequence to generate a binary sequence:</p><formula xml:id="formula_11">G (n+1) i = 1, A (n) fuse,i &gt; θ; 0, A (n) fuse,i ≤ θ,<label>(8)</label></formula><p>where θ is the threshold value. Setting a large value of θ will eliminate the action proposals that only one stream has high activations, and therefore reduces the false positive rate. In contrast, setting a small value of θ will help models to generate more action proposals and achieve a higher recall. Hard pseudo labels remove the uncertainty and provide stronger supervision, but introduce a hyperparameter.</p><p>After generating the frame-level pseudo ground truth, we force the attention sequence generated by each stream to be similar to the pseudo ground truth with a mean square error (MSE) loss <ref type="bibr" target="#b5">6</ref> :</p><formula xml:id="formula_12">L (n+1) G = 1 T T i=1 A (n+1) i − G (n+1) i 2 .<label>(9)</label></formula><p>At refinement iteration n + 1, the total loss for each stream is</p><formula xml:id="formula_13">L (n+1) total = L base + γL (n+1) G ,<label>(10)</label></formula><p>where γ is a hyperparameter to control the relative importance of two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Action Localization</head><p>During testing, following BaS-Net <ref type="bibr" target="#b19">[20]</ref>, we first temporally upsample the attention sequence and T-CAM by a factor of 8 via linear interpolation. Then, we select top-k action categories from the fusion video-level predictionŷ fuse to perform action localization, whereŷ fuse = βŷ RGB + (1 − β)ŷ flow . For each of these categories, following our intention that the attention performs a binary selection, we generate action proposals by directly thresholding the attention value at 0.5 and concatenating consecutive snippets. The action proposals are scored via a variant of the Outer-Inner-Constrastive score <ref type="bibr" target="#b33">[34]</ref>: instead of using average T-CAM, we use attention weighted T-CAM to measure the outer and inner temporal contrast. Formally, given action proposal (t s , t e , c), fusion attention</p><formula xml:id="formula_14">{A fuse,i } T i=1 and T-CAM {s fuse,i } T i=1 , where s fuse,i = βs RGB,i + (1 − β)s flow,i , the score ψ is computed as ψ = te i=ts A fuse,i s fuse,i,c t e − t s − Te i=Ts A fuse,i s fuse,i,c − te i=ts A fuse,i s fuse,i,c T e − T s − (t e − t s ) ,<label>(11)</label></formula><p>where T s = t s − L 4 , T e = t e + L 4 , and L = t e − t s . We discard action proposals with confidence scores lower than 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation</head><p>THUMOS14 dataset <ref type="bibr" target="#b14">[15]</ref> contains 200 validation videos and 213 testing videos within 20 categories for the TAL task. We use the 200 validation videos for training, and use the 213 testing videos for evaluation. ActivityNet dataset <ref type="bibr" target="#b1">[2]</ref> has two release versions, i.e., ActivityNet v1.3 and ActivityNet v1.2. ActivityNet v1.3 covers 200 action categories, with a training set of 10, 024 videos and a validation set of 4, 926 videos. ActivityNet v1.2 is a subset of ActivityNet v1.3, and covers 100 action categories, with 4, 819 and 2, 383 videos in the training and validation set, respectively. <ref type="bibr" target="#b6">7</ref> We use the training set and the validation set for training and testing, respectively. Evaluation Metrics. Following the standard protocol on temporal action localization, we evaluate our method with mean Average Precision (mAP) under different Intersection-over-Union (IoU) thresholds. We use the evaluation code provided by ActivityNet 8 to perform the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Two off-the-shelf feature extraction backbones are used in our experiments, i.e., UntrimmedNet <ref type="bibr" target="#b40">[41]</ref> and I3D <ref type="bibr" target="#b2">[3]</ref>, with snippet lengths of 15 frames and 16 frames, respectively. The two backbones are pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> and Kinetics <ref type="bibr" target="#b2">[3]</ref>, respectively, and are not fine-tuned for fair comparison. The RGB and flow snippet-level features are extracted at the global pool layer as 1024-D vectors.</p><p>The networks are implemented in PyTorch <ref type="bibr" target="#b28">[29]</ref>. We use the Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with a fixed learning rate 0.0001. We train the base models 200 and 80 epochs at refinement iteration 0, and 100 and 40 epochs for later refinement iterations for ActivityNet and THUMOS14, respectively. We set the maximal number of refinement iterations to 4 for the THUMOS14 dataset, and 24 for the --37.0 30.9 23.9 13.9 7.1 --BaS-Net (UNT) <ref type="bibr" target="#b19">[20]</ref>  ActivityNet datasets, and choose base models that achieve the lowest loss at the previous refinement iteration to generate the pseudo ground truth. To eliminate fragmentary action proposals, temporal max pooling of kernel size 5 and stride 1 is used on the fusion attention sequence before pseudo ground truth generation on ActivityNet dataset. We use a whole video as a batch. All hyperparameters are determined via grid search: s = 8, α = 0.1, β = 0.4, γ = 2. We set θ to 0.55 and 0.5 for THUMOS14 and ActivityNet, respectively. We choose top-2 action categories and also reject categories whose fusion classification prediction scores are lower than 0.1 to perform action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-art</head><p>Experiments on THUMOS14. <ref type="table" target="#tab_0">Table 1</ref> summarizes the performance comparison between the proposed TSCN and state-of-the-art fully-supervised and weaklysupervised TAL methods on the THUMOS14 testing set. With UntrimmedNet features, TSCN outperforms other W-TAL methods by a large margin, and even  achieves comparable results to some recent W-TAL methods with I3D features (e.g., Nguyen et al. <ref type="bibr" target="#b27">[28]</ref> and BaS-Net <ref type="bibr" target="#b19">[20]</ref>) at high IoU thresholds. With I3D features, our performance boosts significantly, and outperforms previous W-TAL methods at most IoU thresholds. We note the proposed TSCN can achieve a comparable performance to some recent fully-supervised methods (e.g., R-C3D <ref type="bibr" target="#b41">[42]</ref>). TSCN even outperforms TAL-net <ref type="bibr" target="#b3">[4]</ref> at IoU thresholds 0.1 and 0.2. However, as the IoU threshold increases, the performance of TSCN drops significantly, because generating more precise action boundaries need true frame-level ground truth supervision. Experiments on ActivityNet. The performance comparisons on ActivityNet v1.2 and v1.3 are shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>, respectively, where our models are trained with I3D features. The proposed TSCN outperforms previous W-TAL methods at the average mAP at IoU threshold 0.5 : 0.05 : 0.95 on both release versions of ActivityNet, verifying the efficacy of our design intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this subsection, a set of ablation studies is conducted on the THUMOS14 testing set with UntrimmedNet feature to analyze the efficacy of each component in the proposed TSCN. Ablation study on L att . The goal of L att in Equation <ref type="formula" target="#formula_6">(5)</ref> is to force the attention values to approach extreme values, and therefore generate a clean foreground feature x fg and improve action proposal quality. Some recent methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20]</ref> introduce background classification to W-TAL. Particularly, background classification loss L bg <ref type="bibr" target="#b27">[28]</ref> is introduced to classify the background, where a background attention is defined as 1 − A i , and a background feature is generated via background attention-weighted pooling over all snippets to perform the background classification. Therefore, L bg is in essence an implicit attention normalization loss. However, one drawback of such background loss is that assigning background labels to all videos will make the value of the background category in the T-CAM increase. We reproduce L bg in our model, compare it with our proposed L att , and <ref type="table">Table 4</ref>: Comparison of our method with different attention normalization functions on the THUMOS14 testing set. L bg is the background classification loss introduced in <ref type="bibr" target="#b27">[28]</ref>, and L att is defined in Equation <ref type="formula" target="#formula_6">(5)</ref>   <ref type="table">Table 4</ref>. The results reveal that both L bg and L att help improve the performance. And the proposed L att achieves higher attention variance and better localization performance than L bg , demonstrating that the our attention normalization term L att can better avoid the ambiguity of attention. Surprisingly, with both L bg and L att , the localization performance is still lower than that with only L att , and we think this is because the noise of background classification reduces the accuracy of action proposal scores.</p><p>Ablation study on Pseudo Ground Truth. <ref type="figure" target="#fig_3">Fig. 3</ref> plots performance comparison between different pseudo ground truth methods at different refinement iterations. Both soft and hard pseudo ground truth help improve the localization performance. The hard pseudo ground truth removes uncertainty to the model, and thus achieves higher performance improvement. However, with the same frame-level supervision, the flow stream outperforms the RGB stream by a large margin. We think this is because of the nature of two modalities: the RGB modality is less sensitive to actions than the optical flow modality. To demonstrate this, we generate a true frame-level ground truth actionness sequence (action categories are not used), train our model in the same way as the pseudo ground truth. The results are plotted in <ref type="figure" target="#fig_3">Fig. 3</ref> as an upper bound. The results verify our hypothesis and demonstrate that the optical flow modality is more suitable for the action localization task than the RGB modality. <ref type="table" target="#tab_4">Table 5</ref> lists the detailed performance comparison between the model trained with only video-level labels and that trained with the hard pseudo ground truth. The results show that pseudo ground truth improves the localization performance for both modalities at all IoU thresholds, and thus improves the performance of the fusion result. Also, the pseudo ground truth greatly improves the precision and recall for the RGB stream and the fusion result, and improves the precision for the flow stream with a minor loss of recall (the overall F-measure improves significantly), which demonstrates that the pseudo ground truth can help eliminate false positive action proposals.  <ref type="figure">Fig. 4</ref> to illustrate the efficacy of the proposed pseudo supervision. In the first example of diving and cliff diving, with only video-level labels, the RGB stream provides worse localization result than the flow stream, and thus leads to a noisy fusion attention sequence. The pseudo ground truth guides the RGB stream to identify false positive action proposals and discover true action instances, and further leads to a cleaner fusion attention sequence, where high activations correspond better to the ground truth. In the second example of cricket shot, with only video-level supervision, the RGB stream can only distinguish certain scenes, and fails to separate proximate action instances. In contrast, the flow stream can precisely detect action instances. Therefore, the pseudo ground truth helps the RGB stream to separate consecutive action instances. In the last example of soccer penalty, both streams have high activations on certain false positive temporal locations. Under this circumstance, the false positive action proposals will have higher activations under frame-level pseudo supervision. To eliminate such false positive action proposals, however, need true ground truth supervision. To summarize, the two modalities have their own strengths and limitations: the RGB stream is sensitive to appearance, thus it fails in scenes shot from unusual angles or separating proximate action instances in the same scene; the flow stream is sensitive to motion, and provides more accurate results, but it fails in slow or occluded motion. Qualitative results reveal that the pseudo ground truth helps two streams reach a consensus at most temporal locations. Therefore, the fusion attention sequence becomes cleaner and helps generate more precise action proposals and more reliable confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a Two-Stream Consensus Network (TSCN) for W-TAL, which benefits from an iterative refinement training method and a new attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Video</head><p>Ground Truth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video-level Supervision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Flow Fusion</head><p>Frame-level Supervision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Flow Fusion</head><p>Input Video</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Video-level Supervision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Flow Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame-level Supervision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Flow Fusion</head><p>Input Video</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Video-level Supervision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Flow Fusion</head><p>Frame-level Supervision RGB Flow Fusion <ref type="figure">Fig. 4</ref>: Qualitative results on the THUMOS14 testing set. The eight rows in each example are input video, ground truth action instance, RGB stream, flow stream, and fusion attention sequences from the model trained with only videolevel labels and frame-level pseudo ground truth, respectively. Action proposals are represented by green boxes. The horizontal and vertical axes are time and intensity of attention, respectively normalization loss. The iterative refinement training uses a novel frame-level pseudo ground truth as fine-grained supervision, and iteratively improves the two-stream base models. The attention normalization loss function reduces the ambiguity of attention values, and thus leads to more precise action proposals. Experiments on two benchmarks demonstrate the proposed TSCN outperforms current state-of-the-art methods, and verify our design intuition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )Fig. 2 :</head><label>32</label><figDesc>We introduce a new attention An overview of the proposed Two-Stream Consensus Network, which consists of three parts: (1) RGB and optical flow snippet-level features are extracted with pre-trained models;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison between models trained with different pseudo ground truth on the THUMOS14 testing set. The upper bounds denote models trained with ground truth actionness sequence list the results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our method with state-of-the-art TAL methods on the THUMOS14 testing set. UNT and I3D are abbreviations for UntrimmedNet feature and I3D feature, respectively</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">mAP@IoU (%) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell></row><row><cell></cell><cell>Yuan et al. [44]</cell><cell cols="2">51.0 45.2 36.5 27.8 17.8 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fully-supervised</cell><cell>CDC [33] R-C3D [42] SSN [48] BSN [22] TAL-Net [4] GTAN [25]</cell><cell cols="4">-54.5 51.5 44.8 35.6 28.9 --40.1 29.4 23.3 13.1 7.9 -66.0 59.4 51.9 41.0 29.8 ----53.5 45.0 36.9 28.4 20.0 ----59.8 57.1 53.2 48.5 42.8 33.8 20.8 -69.1 63.7 57.8 47.2 38.8 ---</cell><cell>------</cell></row><row><cell></cell><cell>BMN [21]</cell><cell>-</cell><cell cols="3">-56.0 47.4 38.8 29.7 20.5 -</cell><cell>-</cell></row><row><cell></cell><cell>UntrimmedNet [41]</cell><cell cols="2">44.4 37.7 28.2 21.1 13.7 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STPN (UNT) [27]</cell><cell cols="4">45.3 38.8 31.1 23.5 16.2 9.8 5.1 2.0 0.3</cell></row><row><cell></cell><cell>AutoLoc (UNT) [34]</cell><cell>-</cell><cell cols="2">-35.8 29.0 21.2 13.4 5.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">W-TALC (UNT) [30] 49.0 42.8 32.0 26.0 18.8 -</cell><cell>6.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Weakly-supervised</cell><cell cols="4">Liu et al. (UNT) [23] 53.5 46.8 37.5 29.1 19.9 12.3 6.0 RefineLoc (UNT) [1] --36.1 -22.6 -5.8 CleanNet (UNT) [24]</cell><cell>--</cell><cell>--</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Comparison of our method</cell></row><row><cell cols="3">with state-of-the-art W-TAL methods</cell></row><row><cell cols="3">on the ActivityNet v1.2 validation set.</cell></row><row><cell cols="3">The Avg column indicates the average</cell></row><row><cell cols="3">mAP at IoU thresholds 0.5:0.05:0.95</cell></row><row><cell>Method</cell><cell cols="2">mAP@IoU (%) Avg 0.5 0.75 0.95</cell></row><row><cell cols="3">UntrimmedNet [41] 7.4 3.2 0.7 3.6</cell></row><row><cell>AutoLoc [34]</cell><cell cols="2">27.3 15.1 3.3 16.0</cell></row><row><cell>W-TALC [30]</cell><cell>37.0 -</cell><cell>-18.0</cell></row><row><cell>Liu et al. [23]</cell><cell cols="2">36.8 22.0 5.6 22.4</cell></row><row><cell>Ours</cell><cell cols="2">37.6 23.7 5.7 23.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Comparison of our method</cell></row><row><cell cols="3">with state-of-the-art W-TAL methods</cell></row><row><cell cols="3">on the ActivityNet v1.3 validation set.</cell></row><row><cell cols="3">The Avg column indicates the average</cell></row><row><cell cols="3">mAP at IoU thresholds 0.5:0.05:0.95</cell></row><row><cell>Method</cell><cell cols="2">mAP@IoU (%) Avg 0.5 0.75 0.95</cell></row><row><cell>STPN [27]</cell><cell>29.3 16.9 2.7</cell><cell>-</cell></row><row><cell cols="3">Liu et al. [23] 34.0 20.9 5.7 21.2</cell></row><row><cell cols="2">Nguyen et al. [28] 36.4 19.2 2.9</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="2">35.3 21.4 5.3 21.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between the model trained with only video-level labels and the model trained with hard pseudo ground truth on the THUMOS14 testing set. The label column denotes the supervision used in training, where "video" indicates only video-level labels are leveraged, and "frame" indicates the hard pseudo ground truth is also leveraged during training. Precision, recall and F-measure are calculated under IoU threshold 0.5</figDesc><table><row><cell cols="2">Modality Label</cell><cell>mAP@IoU (%) 0.3 0.4 0.5 0.6 0.7</cell><cell cols="3">Precision (%) Recall (%) F-measure</cell></row><row><cell>RGB</cell><cell cols="2">video 19.8 13.2 8.2 4.5 1.9</cell><cell>10.2</cell><cell>20.9</cell><cell>0.1371</cell></row><row><cell>RGB</cell><cell cols="2">frame 31.4 22.1 14.4 8.9 5.2</cell><cell>20.9</cell><cell>30.8</cell><cell>0.2489</cell></row><row><cell>Flow</cell><cell cols="2">video 40.2 32.0 23.2 15.4 7.2</cell><cell>25.5</cell><cell>43.3</cell><cell>0.3207</cell></row><row><cell>Flow</cell><cell cols="2">frame 40.8 32.7 24.1 16.8 8.7</cell><cell>30.9</cell><cell>42.4</cell><cell>0.3573</cell></row><row><cell cols="3">Fusion video 40.9 32.4 24.0 15.9 8.2</cell><cell>23.6</cell><cell>44.4</cell><cell>0.3078</cell></row><row><cell cols="3">Fusion frame 45.0 36.5 27.6 18.8 10.2</cell><cell>31.3</cell><cell>44.6</cell><cell>0.3680</cell></row></table><note>Qualitative Analysis. Three representative examples of TAL results are plotted in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Although it is straightforward to use a cross entropy loss for hard pseudo ground truth, we found in practice that the cross entropy loss and the MSE loss achieve similar performance. To simplify training, we use the MSE loss for both kinds of pseudo ground truth.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">In our experiments, there are 9, 937 and 4, 575 videos in training and validation set of ActivityNet v1.3, respectively, and 4, 471 and 2, 211 videos in training and validation set of ActivityNet v1.2, respectively, because the rest of the videos are unaccessible from YouTube. 8 https://github.com/activitynet/ActivityNet/tree/master/Evaluation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00227</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mars: Motion-augmented rgb stream for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7882" to="7891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5793" to="5802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01180</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<title level="m">Thumos challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3175" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<title level="m">Thumos challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3524" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8679" to="8687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cdc: Convolutionalde-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Autoloc: Weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dmc-net: Generating discriminative motion cues for fast compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning graph structure for multi-label image classification via clique generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4100" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hallucinating idt descriptors and i3d optical flow features for action recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8698" to="8708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5522" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action coherence network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3696" to="3700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recognize actions by disentangling components of dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
