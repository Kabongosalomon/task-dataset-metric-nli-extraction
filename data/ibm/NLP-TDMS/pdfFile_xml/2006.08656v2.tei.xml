<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiscale Deep Equilibrium Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>Bosch Center for AI</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiscale Deep Equilibrium Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only O(1) memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach. The code and pre-trained models are at https://github.com/locuslab/mdeq.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State-of-the-art pattern recognition systems in domains such as computer vision and audio processing are almost universally based on multi-layer hierarchical feature extractors <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. These models are structured in stages: the input is processed via a number of consecutive blocks, each operating at a different resolution <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b24">25]</ref>. The architectures explicitly express hierarchical structure, with up-and downsampling layers that transition between consecutive blocks operating at different scales. An important motivation for such designs is the prominent multiscale structure and extremely high signal dimensionalities in these domains. A typical image, for instance, contains millions of pixels, which must be processed coherently by the model.</p><p>An alternative approach to differentiable modeling is exemplified by recent progress on implicit deep networks, such as Neural ODEs (NODEs) <ref type="bibr" target="#b11">[12]</ref> and deep equilibrium models (DEQs) <ref type="bibr" target="#b4">[5]</ref>. These constructions replace explicit, deeply stacked layers with analytical conditions that the model must satisfy, and are able to simulate models with "infinite" depth within a constant memory footprint. A notable achievement for implicit modeling is its successful application to large-scale sequences in natural language processing <ref type="bibr" target="#b4">[5]</ref>.</p><p>Is implicit deep learning relevant for general pattern recognition tasks? One clear challenge here is that implicit networks do away with flexible "layers" and "stages". It is therefore not clear whether they can appropriately model multiscale structure, which appears essential to high discriminative power in some domains. This is the challenge that motivates our work. Can implicit models that forego deep sequences of layers and stages attain competitive accuracy in domains characterized by rich multiscale structure, such as computer vision?</p><p>To address this challenge, we introduce a new class of implicit networks: the multiscale deep equilibrium model (MDEQ). It is inspired by DEQs, which attained high accuracy in sequence modeling <ref type="bibr" target="#b4">[5]</ref>. We expand upon the DEQ construction substantially to introduce simultaneous equilibrium modeling of multiple signal resolutions. MDEQ solves for equilibria of multiple resolution streams simultaneously by directly optimizing for stable representations on all feature scales at the same time. Unlike standard explicit deep networks, MDEQ does not process different resolutions in succession, with higher resolutions flowing into lower ones or vice versa. Rather, the different feature scales are maintained side by side in a single "shallow" model that is driven to equilibrium. This design brings two major advantages. First, like the basic DEQ, our model does not require backpropagation through an explicit stack of layers and has an O(1) memory footprint during training. This is especially important as pattern recognition systems are memory-intensive. Second, MDEQ rectifies one of the drawbacks of DEQ by exposing multiple feature scales at equilibrium, thereby providing natural interfaces for auxiliary losses and for compound training procedures such as pretraining (e.g., on ImageNet) and fine-tuning (e.g., on segmentation or detection tasks). Multiscale modeling enables a single MDEQ to simultaneously train for multiple losses defined on potentially very different scales, whose equilibrium features can serve as "heads" for a variety of tasks.</p><p>We demonstrate the effectiveness of MDEQ via extensive experiments on large-scale image classification and semantic segmentation datasets. Remarkably, this shallow implicit model attains comparable accuracy levels to state-of-the-art deeply-stacked explicit ones. On ImageNet classification, MDEQs outperform baseline ResNets (e.g., ResNet-101) with similar parameter counts, reaching 77.5% top-1 accuracy. On Cityscapes semantic segmentation (dense labeling of 2-megapixel images), identical MDEQs to the ones used for ImageNet experiments match the performance of recent explicit models while consuming much less memory. Our largest MDEQ surpasses 80% mIoU on the Cityscapes validation set, outperforming strong convolutional networks and coming tantalizingly close to the state of the art. This is by far the largest-scale application of implicit deep learning to date and a remarkable result for a class of models that until recently were applied largely to "toy" domains.</p><p>2 Background Implicit Deep Learning. Virtually all modern deep learning approaches use explicit models, which provide explicit computation graphs for forward propagation. Backward passes proceed in reverse order through the same graph. This approach is the core of popular deep learning frameworks <ref type="bibr" target="#b0">[1]</ref> and is associated with the very concept of "architecture". In contrast, implicit models do not have prescribed computation graphs. They instead posit a specific criterion that the model must satisfy (e.g., the endpoint of an ODE flow, or the root of an equation). Importantly, the algorithm that drives the model to fulfill this criterion is not prescribed. Therefore, implicit models can leverage black-box solvers in their forward passes and enjoy analytical backward passes that are independent of the forward pass trajectories. Implicit modeling of hidden states has been explored by the deep learning community for decades. Pineda <ref type="bibr" target="#b41">[42]</ref> and Almeida <ref type="bibr" target="#b1">[2]</ref> studied implicit differentiation techniques for training recurrent dynamics, also known as recurrent back-propagation (RBP) <ref type="bibr" target="#b35">[36]</ref>. Implicit approaches to network design have recently attracted renewed interest <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. For example, Neural ODEs (NODEs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref> model a recursive residual block using implicit ODE solvers, equivalent to a continuous ResNet taking infintesimal steps. Deep equilibrium models (DEQs) <ref type="bibr" target="#b4">[5]</ref> solve for the fixed point of a sequence model with black-box root-finding methods, equivalent to finding the limit state of an infinite-layer network. Other instantiations of implicit modeling include optimization layers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3]</ref>, differentiable physics engines <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43]</ref>, logical structure learning <ref type="bibr" target="#b55">[56]</ref>, and continuous generative models <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our work takes the deep equilibrium approach <ref type="bibr" target="#b4">[5]</ref> into signal domains characterized by rich multiscale structure. We develop the first one-layer implicit deep model that is able to scale to realistic visual tasks (e.g., megapixel-level images), and achieve competitive results in these regimes. In comparison, ODE-based models have so far only been applied to relatively low-dimensional signals due to numerical instability. For example, Chen et al. <ref type="bibr" target="#b11">[12]</ref> downsampled 28 × 28 MNIST images to 7 × 7 before feeding them to Neural ODEs. More broadly, our work can be seen as a new perspective on implicit models, wherein the models define and optimize simultaneous criteria over multiple data streams that can have different dimensionalities. While DEQs and NODEs have so far been defined on a single stream of features, a single MDEQ can jointly optimize features for different tasks, such as image segmentation and classification.</p><p>Multiscale Modeling in Computer Vision. Computer vision is a canonical application domain for hierarchical multiscale modeling. The field has come to be dominated by deep convolutional networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>. Computer vision problems can be viewed in terms of the granularity of the desired output: from low-resolution, such as a label for a whole image <ref type="bibr" target="#b15">[16]</ref>, to high-resolution output that assigns a label to each pixel, as in semantic segmentation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62]</ref>. State-of-the-art models for these problems are explicitly structured into sequential stages of processing that operate at different resolutions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b24">25]</ref>. For example, a ResNet <ref type="bibr" target="#b24">[25]</ref> typically consists of 4-6 sequential stages, each operating at half the resolution of the preceding one. A dilated ResNet <ref type="bibr" target="#b59">[60]</ref> uses a different schedule for the progression of resolutions. A DenseNet <ref type="bibr" target="#b25">[26]</ref> uses different connectivity patterns to carry information between layers, but shares the overarching structure: a sequence of stages. Other designs progressively decrease feature resolution and then increase it step by step <ref type="bibr" target="#b43">[44]</ref>. Downsampling and upsampling can also be repeated, again in an explicitly choreographed sequence <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Multiscale modeling has been a central motif in computer vision. The Laplacian pyramid is an influential early example of multiscale modeling <ref type="bibr" target="#b6">[7]</ref>. Multiscale processing has been integrated with convolutional networks for scene parsing by Farabet et al. <ref type="bibr" target="#b19">[20]</ref> and has been explicitly addressed in many subsequent architectures <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Our work brings multiscale modeling to implicit deep networks. MDEQ has in essence only one stage, in which the different resolutions coexist side by side. The input is injected at the highest resolution and then propagated implicitly to the other scales, which are optimized simultaneously by a (black-box) solver that drives them to satisfy a joint equilibrium condition. Just like DEQs, an MDEQ is able to represent an "infinitely" deep network with only a constant memory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multiscale Deep Equilibrium Models</head><p>We begin by briefly summarizing the basic DEQ construction and some major challenges that arise when extending it to computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Equilibrium (DEQ): Generic Formulation</head><p>One of the core ideas that motivated the DEQ approach was weight-tying: the same set of parameters can be shared across the layers of a deep network. Formally, Bai et al. <ref type="bibr" target="#b4">[5]</ref> formulated an L-layer weight-tied transformation with parameter θ on hidden state z as</p><formula xml:id="formula_0">z [i+1] = f θ (z [i] ; x), i = 0, . . . , L − 1<label>(1)</label></formula><p>where the input representation x was injected into each layer. When sufficient stability conditions were ensured, stacking such layers infinitely (i.e., L → ∞) was shown to essentially perform fixed-point iterations and thus tend to an equilibrium z = f θ (z ; x). Intuitively, as we iterate the transformation f θ , the hidden representation tends to converge to a stable state, z . Such construction has a number of appealing properties. First, we can directly solve for the fixed point, which can be done faster than explicitly iterating through the layers. We formulate this as a root-finding problem:</p><formula xml:id="formula_1">g θ (z; x) := f θ (z; x) − z =⇒ z = Rootfind(g θ ; x)<label>(2)</label></formula><p>For example, one can leverage Newton or quasi-Newton methods to achieve quadratic or superlinear convergence to the root. Second, one can directly backpropagate through the equilibrium state using the Jacobian of g θ at z , without tracing through the forward root-finding process. Formally, given a loss = L(z , y) (where y is the target), the gradients can be written as</p><formula xml:id="formula_2">∂ ∂θ = ∂ ∂z −J −1 g θ | z ∂f θ (z ; x) ∂θ ∂ ∂x = ∂ ∂z −J −1 g θ | z ∂f θ (z ; x) ∂x .<label>(3)</label></formula><p>See Bai et al. <ref type="bibr" target="#b4">[5]</ref> for the proof, which is based on the implicit function theorem <ref type="bibr" target="#b28">[29]</ref>. This means that the forward pass of a DEQ can rely on any black-box root solver, while the backward pass is based independently on differentiating through only one layer (or block) at the equilibrium (i.e., ∂f θ (z ;x)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂(·)</head><p>). The memory consumption of the entire training process is equivalent to that of just one block rather than L → ∞ blocks. Since the Jacobian of g θ can be expensive to compute, DEQs solve for a linear equation involving a vector-Jacobian product, which is a lot cheaper:</p><formula xml:id="formula_3">x(J g θ | z ) + ∂ ∂z = 0.<label>(4)</label></formula><p>The DEQ model therefore solves for the network output at its infinite depth, with each step of the model now implicitly defined to reach an analytical objective (the equilibrium).</p><formula xml:id="formula_4">z1 : H1 ⇥ W1 ⇥ C1 z2 : H2 ⇥ W2 ⇥ C2 zn : Hn ⇥ Wn ⇥ Cn z3 : H3 ⇥ W3 ⇥ C3</formula><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-resolution Fusion</head><p>n resolutions:</p><formula xml:id="formula_5">z [i] = [z [i] 1 , . . . , z [i] n ] x = [x 1 , . . . , x n ] f ✓ (z; x) z ? 1 z ? 2 z ? 3 z ? 4 . . .</formula><p>. . .  Challenges. The construction of Bai et al. <ref type="bibr" target="#b4">[5]</ref>, which we have just summarized, was primarily aimed at processing sequences. As we transition from sequences to high-resolution images, we note important differences between these domains. First, unlike typical autoregressive sequence learning problems (e.g., language modeling), where input and output have identical length and dimensionality, general pattern recognition systems (such as those in vision) entail multi-stage modeling via a combination of up-and downsampling in the architecture. The basic DEQ construction does not exhibit such structure. Second, the output of a computer vision task such as image classification (a label) or object localization (a region) may have very different dimensionality from the input (a full image): again a feature that the basic DEQ does not support. Third, state-of-the-art models for tasks such as semantic segmentation are commonly based on "backbones" that are pretrained for image classification, even though the tasks are structurally different and their outputs have very different dimensionalities (e.g., one label for the whole image versus a label for each pixel). It's not clear how a DEQ construction can support such transfer. Fourth, whereas past work on DEQs could leverage state-of-the-art weight-tied architectures for sequence modeling as the basis for the transformation f θ <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, no such counterparts exist in state-of-the-art computer vision modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The MDEQ Model</head><p>Notation. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the entire structure of MDEQ. As before, f θ denotes the transformation that is (implicitly) iterated to a fixed point, x is the (precomputed) input representation provided to f θ , and z is the model's internal state. We omit the batch dimension for clarity.</p><p>Transformation f θ . The central part of MDEQ is the transformation f θ that is driven to equilibrium. We use a simple design in which features at each resolution are first taken through a residual block. The blocks are shallow and are identical in structure. At resolution i, the residual block receives the internal state z i and outputs a transformed feature tensor z + i at the same resolution. Notably, the highest resolution stream (i.e., i = 1) also receives an input injection x that is precomputed directly from the source image and injected to the highest-resolution residual block. (See Eq. (5) and the discussion below.)</p><p>The internal structure of the residual block is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We largely adopt the design of He et al. <ref type="bibr" target="#b24">[25]</ref>, but use group normalization <ref type="bibr" target="#b56">[57]</ref> rather than batch normalization <ref type="bibr" target="#b27">[28]</ref>, for stability reasons that are discussed in Section 3.3. The residual block at resolution i can be formally expressed as</p><formula xml:id="formula_6">z i = GroupNorm Conv2d(z i ) ẑ i = GroupNorm Conv2d(ReLU(z i )) + 1 {i=1} · x z + i = GroupNorm ReLU(ẑ i + z i )<label>(5)</label></formula><p>Following these blocks, the second part of f θ is a multi-resolution fusion step that mixes the feature maps across different scales (see <ref type="figure" target="#fig_0">Figure 1</ref>). The transformed features z + i undergo either upsampling or downsampling from the current scale i to each other scale j = i. In our construction, downsampling is performed by j − i consecutive 2-strided 3 × 3 Conv2d, whereas upsampling is performed by direct bilinear interpolation. The final output at scale j is formed by summing over the transformed feature maps provided from all incoming scales i (along with z + j ); i.e., the output feature tensor at each scale is a mixture of transformed features from all scales. This forces the features at all scales to be consistent and drives the whole system to a coordinated equilibrium that harmonizes the representations across scales.</p><p>Input Representation. The raw input first goes through a transformation (e.g., a linear layer that aligns the feature channels) to form x, which will be provided to f θ . The existence of such input injection is vital to implicit models as it (along with θ) correlates the flow of the dynamical system with the input. However, unlike multiscale input representations used by some explicit vision architectures <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>, we only inject x to the highest-resolution feature stream (see Eq. <ref type="formula" target="#formula_6">(5)</ref>). The input is provided to MDEQ at a single (full) resolution. The lower resolutions hence start with no knowledge at all about the input; this information will only implicitly propagate through them as all scales are gradually driven to coordinated equilibria z by the (black-box) solver.</p><p>(Limited-memory) Multiscale Equilibrium Solver. In the DEQ, the internal state is a single tensor z <ref type="bibr" target="#b4">[5]</ref>. The MDEQ state, however, is a collection of tensors at n resolutions: z = [z 1 , . . . , z n ]. Note that this is not a concatenation, as the different z i have different dimensionalities, feature resolutions, and semantics.</p><p>With this in mind, our equilibrium solver leverages Broyden's method. We initialize the internal states by setting z [0] i = 0 for all scales i. z = [z 1 , . . . , z n ] is maintained as a collection of n tensors whose respective equilibrium states (i.e., roots) are solved for and backpropagated through simultaneously (with each resolution inducing its own loss).</p><p>The original Broyden solver was not efficient enough when applied to computer vision datasets, which have very high dimensionality. For example, in the Cityscapes segmentation task (see Section 4), the Jacobian of a 4-resolution MDEQ at z is well over 2,000 times larger than its single-scale counterpart in word-level language modeling <ref type="bibr" target="#b4">[5]</ref>. Note that even with low-rank approximations of the Jacobian in quasi-Newton methods, the high dimensionality of images can make storing these updates extremely expensive. To address this, we improve the memory efficiency of the forward and backward passes by optimizing Broyden's method. We implemented a new solver that is inspired by Limited-memory BFGS (L-BFGS) <ref type="bibr" target="#b37">[38]</ref>, where we only keep the latest m low-rank updates at any step and discard the earlier ones (see Appendix B.1).</p><p>Pretraining and Auxiliary Losses. <ref type="figure">Figure 3</ref> provides a comparison of MDEQ with single-stream implicit models such as the DEQ, and with explicit deep networks in computer vision. These different models expose different "interfaces" that can be used to define losses for different tasks. Prior implicit models such as neural ODEs and DEQs typically assume that a loss is defined on a single stream of implicit hidden states, which has a uniform input and output shape ( <ref type="figure">Figure 3b</ref>). It is therefore not clear how such a model can be flexibly transferred across structurally different tasks (e.g., pretraining on image classification and fine-tuning on semantic segmentation). Furthermore, there is no natural way to define auxiliary losses <ref type="bibr" target="#b32">[33]</ref>, because there are no "layers" and the forward and backward computation trajectories are decoupled.</p><p>In comparison, MDEQ exposes convenient "interfaces" to its states at multiple resolutions. One resolution (the highest) can be the same as the resolution of the input, and can be used to define losses for dense prediction tasks such as semantic segmentation. Another resolution (the lowest) can be a vector in which the spatial dimensions are collapsed, and can be used to define losses for image-level labeling tasks such as image classification. This suggests clean protocols for training the same model for different tasks, either jointly (e.g., multi-task learning in which structurally different supervision flows through multiple heads) or in sequence (e.g., pretraining for image classification through one head and fine-tuning for semantic segmentation through another). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 Ln</head><p>High-resolution loss(es) (e.g., segmentation) </p><formula xml:id="formula_7">z [0]</formula><p>Equilibrium Solver for f✓(z ? ; x) = z ? (a) MDEQ exposes multiple interfaces at equilibrium z ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2 L3</head><p>Laux. <ref type="table">(auxiliary losses)</ref> . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 Ln</head><p>High-resolution loss(es) (e.g., segmentation) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 Ln</head><p>High-resolution loss(es) (e.g., segmentation) </p><formula xml:id="formula_8">z [0]</formula><p>Equilibrium Solver for f✓(z ? ; x) = z ? (c) Explicit deep models in vision <ref type="figure">Figure 3</ref>: A visual comparison of MDEQ with prior implicit models and with standard explicit models in computer vision. Equilibrium states at multiple resolutions enable MDEQ to incorporate supervision in different forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integrating Common DL Techniques with MDEQs</head><p>MDEQ simulates an "infinitely" deep network by implicitly modeling one layer. Such implicitness calls for care when adapting common deep learning practices. We provide an exploration of such adaptations and their impact on the training dynamics of MDEQ. We believe these observations will also be valuable for future research on implicit models.</p><p>Normalization. Layer normalization of hidden activations in f θ played an important role in constraining the output and stabilizing DEQs on sequences <ref type="bibr" target="#b4">[5]</ref>. A natural counterpart in vision is batch normalization (BN) <ref type="bibr" target="#b27">[28]</ref>. However, BN is not directly suitable for implicit models, since it estimates population statistics based on layers, which are implicit in our setting, and the Jacobian matrix of the transformation f θ will scale badly to make the fixed point significantly harder to solve for. We therefore use group normalization (GN) <ref type="bibr" target="#b56">[57]</ref>, which groups the input channels and performs normalization within each group. GN is independent of batch size and offers more natural support for transfer learning (e.g., pretraining and fine-tuning on structurally different tasks). Unlike in DEQs, we keep the learnable affine parameters of GN.</p><p>Dropout. The conventional spatial dropout used by explicit vision models applies a random mask to given layers in the network <ref type="bibr" target="#b49">[50]</ref>. A new mask is generated whenever dropout is invoked. Such layer-based stochasticity can significantly hurt the stability of convergence to the equilibrium. In fact, as two adjacent calls to f θ most probably will have different Bernoulli dropout masks, it is almost impossible to reach a fixed point where f θ (z ; x) = z . We therefore adopt variational dropout <ref type="bibr" target="#b20">[21]</ref> and apply the exact same mask at all invocations of f θ in a given training iteration. The mask is reset at each training iteration.</p><p>Nonlinearities. The multiscale features are initialized to z [0] i = 0 for all resolutions i. However, we found that this could induce certain instabilities when training MDEQ (especially in the starting phase of it), most likely due to the drastic change of slope of the ReLU non-linearity at the origin, where the derivative is undefined <ref type="bibr" target="#b21">[22]</ref>. To combat this, we replace the last ReLU in both the residual block and the multiscale fusion by a softplus <ref type="bibr" target="#b21">[22]</ref> in the initial phase of training. These are later switched back to ReLU. The softplus provides a smooth approximation to the ReLU, but has slope 1 − 1 1+exp(βz) → 1 2 around z = 0 (where β controls the curvature).</p><p>Convolution and Convergence to Equilibrium. Whereas the original DEQ model focused primarily on self-attention transformations <ref type="bibr" target="#b53">[54]</ref>, where all hidden units communicate globally, MDEQ models face additional challenges due to the nature of typical vision models. Specifically, our MDEQ models employ convolutions with small receptive fields (e.g., the two 3 × 3 convolutional filters in f θ 's residual block) on potentially very large images: for instance, we eventually evaluate our semantic segmentation model on megapixel-scale images. In consequence, we typically need a higher number of root-finding iterations to converge to an exact equilibrium. While this does pose a challenge, we find that using the aforementioned strategies of 1) multiscale simultaneous up-and downsampling and 2) quasi-Newton root-finding, drives the model close to equilibrium within a reasonable number of iterations. We further analyze convergence behavior in Appendix B.</p><p>(a) Training dynamics of implicit models CIFAR-10 (w/ data augmentation) CIFAR-10 (w/o data augmentation) <ref type="bibr">(</ref>   <ref type="table" target="#tab_5">Table 1</ref>. For all metrics, lower is better.  <ref type="bibr" target="#b15">[16]</ref> and semantic segmentation on the Cityscapes dataset <ref type="bibr" target="#b12">[13]</ref>. These tasks have extremely high-dimensional inputs (e.g., 2048 × 1024 images for Cityscapes) and are dominated by explicit models. We provide more detailed descriptions of the tasks, hyperparameters, and training settings in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our focus is on the behavior of MDEQs and their competitiveness with prior implicit or explicit models. We are not aiming to set a new state of the art on ImageNet classification or Cityscapes segmentation, as this typically involves substantial additional investment <ref type="bibr" target="#b57">[58]</ref>. However, we do note that even with the implicit modeling of layer f θ , the mini explicit structure within the design of f θ (e.g., the residual block) is still very helpful empirically in improving the equilibrium representations.</p><p>All experiments with MDEQs use the limited-memory version of Broyden's method in both forward and backward passes, and the root solvers are stopped whenever 1) the objective value reaches some predetermined threshold ε or 2) the solver's iteration count reaches a limit T . On large-scale vision benchmarks (ImageNet and Cityscapes), we downsample the input twice with 2-strided convolutions before feeding it into MDEQs, following the common practice in explicit models <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b54">55]</ref>. We use the cosine learning rate schedule for all tasks <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparing with Prior Implicit Models on CIFAR-10</head><p>Following the setting of Dupont et al. <ref type="bibr" target="#b17">[18]</ref>, we run the experiments on CIFAR-10 classification (without data augmentation) for 50 epochs and compare models with approximately the same number of parameters. However, unlike the ODE-based approaches, we do not perform downsamplings on the raw images before passing the inputs to the MDEQ solver (so the highest-resolution stream stays at 32 × 32). When training the MDEQ model, all resolutions are used for the final prediction: higherresolution streams go through additional downsampling layers and are added to the lowest-resolution output to make a prediction (i.e., a form of auxiliary loss).</p><p>The results of MDEQ models on CIFAR-10 image classification are shown in <ref type="table" target="#tab_5">Table 1</ref>. Compared to NODEs <ref type="bibr" target="#b11">[12]</ref> and Augmented NODEs <ref type="bibr" target="#b17">[18]</ref>, a small MDEQ with a similar parameter count improves accuracy by more than 20 percentage points: an error reduction by more than a factor of 2. MDEQ also improves over the single-stream DEQ (applied at the highest resolution). The training dynamics of the different models are visualized in <ref type="figure">Figure 4a</ref>. Finally, a larger MDEQ matches and even  exceeds the accuracy of a ResNet-18 with the same capacity: the first time such performance has been demonstrated by an implicit model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageNet Classification</head><p>We now test the ability of MDEQ to scale to a much larger dataset with higher-resolution images: ImageNet <ref type="bibr" target="#b15">[16]</ref>. As with CIFAR-10 classification, we add a shallow classification layer after the MDEQ module to fuse the equilibrium outputs from different scales, and train on a combined loss.</p><p>We benchmark both a small MDEQ model and a large MDEQ to provide appropriate comparisons with a number of reference models, such as ResNet-18, -34, -50, and -101 <ref type="bibr" target="#b24">[25]</ref>. Note that MDEQ has only one layer of residual blocks followed by multi-resolution fusion. Therefore, to match the capacity of standard explicit models, we need to increase the feature dimensionality within MDEQ. This is accomplished mainly by adjusting the width of the convolutional filter within the residual block (see <ref type="figure" target="#fig_1">Figure 2</ref>). <ref type="table" target="#tab_6">Table 2</ref> shows the accuracy of two MDEQs (of different sizes) in comparison to well-known reference models in computer vision. MDEQs are remarkably competitive with strong explicit models. For example, a small MDEQ with 18M parameters outperforms ResNet-18 (13M parameters), ResNet-34 (21M parameters), and even ResNet-50 (26M parameters). A larger MDEQ (64M parameters) reaches the same level of performance as ResNet-101 (52M parameters). This is far beyond the scale and accuracy levels of prior applications of implicit modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cityscapes Semantic Segmentation</head><p>After training on ImageNet, we train the same MDEQs for semantic segmentation on the Cityscapes dataset <ref type="bibr" target="#b12">[13]</ref>. When transferring the models from ImageNet to Cityscapes, we directly use the highest-resolution equilibrium output z 1 to train on the highest-resolution loss. Thus MDEQ is its own "backbone". We train on the Cityscapes train set and evaluate on the val set. Following the evaluation protocol of Zhao et al. <ref type="bibr" target="#b62">[63]</ref> and Wang et al. <ref type="bibr" target="#b54">[55]</ref>, we test on a single scale with no flipping.</p><p>MDEQs attain remarkably high levels of accuracy. They come close to the current state of the art, and match or outperform well-known and carefully architected explicit models that were released in the past two years. A small MDEQ (7.8M parameters) achieves a mean IoU of 75.1. This improves upon a MobileNetV2Plus <ref type="bibr" target="#b45">[46]</ref> of the same size and is close to the SOTA for models on this scale. A large MDEQ (53.5M parameters) reaches 77.8 mIoU, which is within 1 percentage point of highly regarded recent semantic segmentation models such as DeepLabv3 <ref type="bibr" target="#b8">[9]</ref> and PSPNet <ref type="bibr" target="#b61">[62]</ref>, whereas a larger version (70.9M parameters) surpasses them. It is surprising that such levels of accuracy can be achieved by a "shallow" implicit model, based on principles that have not been applied to this domain before. Examples of semantic segmentation results are shown in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Runtime and Memory Consumption</head><p>We provide a runtime and memory analysis of MDEQs using CIFAR-10 data, with input batch size 32. Since prior implicit models such as ANODEs <ref type="bibr" target="#b17">[18]</ref>  </p><formula xml:id="formula_9">z [i]</formula><p>) as a function of the number of times we evaluate f θ . As input image resolution grows (from CIFAR-10 to Cityscapes), MDEQ takes more steps to converge with (L-)Broyden's method. Standard deviation is calculated on 5 randomly selected batches from each dataset.</p><p>to the ResNet-101 model (about 150ms per batch) on a single RTX 2080 Ti GPU. The results are summarized in <ref type="figure">Figure 4b</ref>.</p><p>MDEQ saves more than 60% of the GPU memory at training time compared to explicit models such as ResNets and DenseNets, while maintaining competitive accuracy. Training a large MDEQ on ImageNet consumes about 6GB of memory, which is mostly used by Broyden's method. This low memory footprint is a direct result of the analytical backward pass. Meanwhile, MDEQs are generally slower than explicit networks. We observe a 2.7× slowdown for MDEQ compared to ResNet-101, a tendency similar to that observed in the sequence domain <ref type="bibr" target="#b4">[5]</ref>. A major factor contributing to the slowdown is that MDEQs maintain features at all resolutions throughout, whereas explicit models such as ResNets gradually downsample their activations and thus reduce computation (e.g., 70% of ResNet-101 layers operate on features that are downsampled by 8 × 8 or more). However, when compared to ANODEs with 172K parameters, an MDEQ of similar size is 3× faster while achieving a 3× error reduction. Additional discussion of runtime and convergence is provided in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Equilibrium Convergence on High-resolution Inputs</head><p>As we scale MDEQ to higher-resolution inputs, the equilibrium solving process becomes more challenging. This is illustrated in <ref type="figure">Figure 5</ref>, where we show the equilibrium convergence of MDEQ on CIFAR-10 (low-resolution), ImageNet (medium-resolution) and Cityscapes (high-resolution) images by measuring the change of residual with respect to the number of function evaluations. We empirically find that (limited-memory) Broyden's method and multiscale fusion both help stabilize the convergence on high-resolution data. For example, in all three cases, Broyden's method (blue lines in <ref type="figure">Figure 5</ref>) converges to the fixed point in a more stable and efficient manner than simply iterating f θ (yellow lines). Further analysis of the multiscale convergence behavior is provided in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced multiscale deep equilibrium models (MDEQs): a new class of implicit architectures for domains characterized by high dimensionality and multiscale structure. Unlike prior implicit models, such as DEQs and Neural ODEs, an MDEQ solves for and backpropagates through synchronized equilibria of multiple feature representations at different resolutions. We show that a single MDEQ can be used for different tasks, such as image classification and semantic segmentation. Our experiments demonstrate for the first time that "shallow" implicit models can scale to practical computer vision tasks and achieve competitive performance that matches explicit architectures characterized by sequential processing through deeply stacked layers.</p><p>The remarkable performance of implicit models in this work brings up core questions in machine learning. Are complex stage-wise hierarchical architectures, which have dominated deep learning to date, necessary? MDEQ exemplifies a different approach to differentiable modeling. The most significant message of our work is that this approach may be much more relevant in practice than previously appeared. We hope that this will contribute to the development of implicit deep learning and will further broaden the agenda in differentiable modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Task Descriptions and Training Settings</head><p>We provide a detailed description of all tasks and some additional details on the training of MDEQ.</p><p>Image Classification on CIFAR-10. CIFAR-10 is a well-known computer vision dataset that consists of 60,000 color images, each of size 32 × 32 <ref type="bibr" target="#b29">[30]</ref>. There are 10 object classes and 6,000 images per class. The entire dataset is divided into training (50K images) and testing (10K) sets.</p><p>We use two different training settings for evaluating the MDEQ model on CIFAR-10. Following Dupont et al. <ref type="bibr" target="#b17">[18]</ref>, we compare MDEQ-small with other implicit models on CIFAR-10 images without data augmentation (i.e., the original, raw images), using approximately 170K learnable parameters in the model. In the second setting, we apply data augmentation to the input images (i.e., random cropping, horizontal flipping, etc.), a setting that most competitive vision baselines (e.g., ResNets) use by default.</p><p>Image Classification on ImageNet. The dataset we use contains 1.2 million labeled training images from ImageNet <ref type="bibr" target="#b30">[31]</ref> distributed over 1,000 classes, and a test set of 150,000 images. The original ImageNet consists of variable-resolution images, and we follow the standard setting <ref type="bibr" target="#b24">[25]</ref> to use the 224 × 224 crops as inputs to the model.</p><p>ImageNet is frequently used for pretraining general-purpose image feature extractors that are used on downstream tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b54">55]</ref>. We train a small and large MDEQ model, which will act as their own "backbone" when later fine-tuned on the Cityscapes segmentation task. We train MDEQ on ImageNet for 100 epochs. Following the practice of Bai et al. <ref type="bibr" target="#b4">[5]</ref> with DEQ models for sequences, we start the training (the first few epochs) of MDEQ with a shallow (5-layer) weight-tied stacking of f θ to warm up the weights, and then switch to the implicit equilibrium (root) solver for the rest of the training epochs.</p><p>Semantic Segmentation on Cityscapes. Cityscapes is a large-scale urban scene understanding dataset containing high-quality, pixel-level annotated street scene images from 50 cities <ref type="bibr" target="#b12">[13]</ref>. The dataset consists of 5,000 images, which are divided into 2,975 (train), 500 (val) and <ref type="bibr">1,525 (test)</ref> sets. Each pixel is classified in a 19-way fashion for evaluation.</p><p>We follow the training protocol of prior works <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b54">55]</ref> to train the MDEQ models on the Cityscapes train, and perform random cropping (to 1024 × 512) and random horizontal flipping on the training inputs. The models are evaluated on the Cityscapes val (single scale and no random flipping) with the original resolution 2048 × 1024. We use the identical MDEQ model(s) as used in ImageNet training, but now predict with the high-resolution head.</p><p>Hyperparameters. We provide the hyperparameters of the models we used in each of these tasks in <ref type="table">Table 4</ref>. Note that we use a single model for both ImageNet classification and Cityscapes segmentation, so the models share the same configuration (highlighted in red in <ref type="table">Table 4</ref> for clarity). For all tasks, the MDEQ features in resolution i = 1, . . . , n take the shape</p><formula xml:id="formula_10">H 2 i−1 , W 2 i−1 i=1,.</formula><p>..,n , where H, W are the dimensions of the original input. In other words, each resolution uses half the feature size of its next higher resolution stream. We apply weight normalization <ref type="bibr" target="#b44">[45]</ref> to all of the learnable weights in f θ .</p><p>Hardware. For both ImageNet and Cityscapes experiments, MDEQ-Large models were trained on 4 RTX-2080 Ti GPUs, while MDEQ-XL models were trained on 8 Quadro RTX 8000 GPUs. The CIFAR-10 classification models were trained on 1 GPU (including the baselines).</p><p>Initialization of MDEQ Models. For CIFAR-10 and ImageNet, we initialize the parameters of f θ randomly from N (0, 0.01) (Cityscapes MDEQs use pretrained ImageNet MDEQs). Generally, we observe that the final performance of MDEQ is not sensitive to the choice of initialization distribution. However, such random initialization could occasionally induce instabilities in the starting phase of the training (see red lines in <ref type="figure">Figure 6</ref>). We solve this problem by either 1) temporarily replacing ReLU with softplus in the first few epochs of training; or 2) warming up the weights by training a shallow (e.g., 5-layer) weight-tied stacking of f θ , then switching to MDEQ's equilibrium solver for the rest of the training. <ref type="table">Table 4</ref>: Settings &amp; hyperparameters of each task. "cls." means classification task, and "seg." means segmentation task. These models coorespond to the ones reported in <ref type="table" target="#tab_5">Tables 1, 2, and 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Equilibrium Solving and Convergence Analysis</head><p>We extend our discussion on the convergence to equilibrium in Section 3.3 here. First, we briefly introduce the (limited-memory) Broyden's method that we use to perform the root-solving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 (Limited-memory) Broyden's Method</head><p>As our goal is to solve the equation g θ (z ; x) = f θ (z ; x) − z = 0 for the (root) equilibrium point z as efficiently as possible, an ideal choice would be Newton's method:</p><formula xml:id="formula_11">z [i+1] = z [i] − (J −1 g θ z [i] )g θ (z [i] ; x); z [0] = 0<label>(6)</label></formula><p>However, in practice this involves two major difficulties. First, for a deep network with realistic size, the Jacobians are typically prohibitively large to compute and store. For instance, for a layer converting an input tensor of dimension 32 × 32 × 80 (e.g., height × width × channels) to an output of the same shape, the resulting Jacobian will have dimension 81920 × 81920, which needs 25GB of memory to store. Second, even if we can store this Jacobian, inverting it would be an extremely expensive (cubic complexity) operation.</p><p>We therefore use a variant of Broyden's method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>:</p><formula xml:id="formula_12">z [i+1] = z [i] − α · B [i] g θ (z [i] ; x); z [0] = 0<label>(7)</label></formula><p>where α is an adjustable step size and B [i] is a low-rank approximation to J −1 g θ z <ref type="bibr">[i]</ref> . Notably, we do not need to form the Broyden matrix B [i] explicitly, as we can write it as a sum of low-rank updates:</p><formula xml:id="formula_13">B [i+1] = B [0] + i k=1 u [k] v [k] = B [0] + U V<label>(8)</label></formula><p>where u, v comes from the Sherman-Morrison formula <ref type="bibr" target="#b47">[48]</ref>. We initialize the Broyden matrix to B [0] = −I. As described in Section 3.2, we further extended Broyden's method with a limitedmemory version that stores no more than m low-rank updates u, v each. Specifically, when the maximum storage memory m is used, we free up memory by discarding the oldest update in U and V (other schemes are also possible).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Discussions</head><p>Runtime. The rate of convergence of MDEQ is directly related to the runtime of MDEQ. Because an MDEQ does not have "layers", a good indicator of computational complexity of MDEQ is the number of root-finding iterations (e.g., each Broyden iteration evalute f θ exactly once). In practice, we stop the Broyden iterations at some threshold limit (e.g., 22 iterations), which usually does not yield the exact equilibrium (see <ref type="figure">Figure 6</ref> and the discussion below). However, we find these estimates of the equilibria are usually good enough and sufficient for very competitive training of the MDEQ models. Similar observations have also been made in sequence-level DEQs <ref type="bibr" target="#b4">[5]</ref>. </p><formula xml:id="formula_14">z [i]</formula><p>) as a function of the number of times we evaluate f θ . As input image resolution grows (from CIFAR-10 to Cityscapes), MDEQ takes more steps to converge with (L-)Broyden's method. Standard deviation is calculated on 5 randomly selected batches from each dataset.</p><p>Convergence on High-resolution Inputs. As we scale MDEQ to higher-resolution inputs, the equilibrium solving process also becomes increasingly challenging. We identify at least two major reasons behind this phenomenon.</p><p>1. As the input resolution gets higher, so does the size of the Jacobian of f θ which we try to approximate via Broyden's method. Therefore, more low-rank updates are expected for the Broyden matrix approximate the Jacobian and solve for the high-dimensional root.</p><p>2. Due to the nature of typical vision models, MDEQ employs convolutions with small receptive fields (e.g., the two 3 × 3 convolutions in f θ 's residual block) on very large inputs. To see how this complicates the equilibrium solving, consider a case where we simply iterate f θ (·; x) on z to reach the equilibrium point (i.e., not using Broyden's method; assuming f θ is stable). Then we need at least as many iterations as required for the stacked f θ to have a receptive field large enough to cover the entire image. Otherwise, new pixels covered by the larger receptive field will be available for each additional stack of f θ (which disrupts the equilibrium).   This phenomenon is visualized in <ref type="figure">Figure 6</ref>, where we show equilibrium convergence of MDEQ models on CIFAR-10 (low resolution), ImageNet (medium resolution), and Cityscapes (high resolution) images by measuring the change of residual z [i+1] −z <ref type="bibr">[i]</ref> z <ref type="bibr">[i]</ref> with respect to calls to f θ . As with our experimental setting in Section 4, we initialize the Cityscapes MDEQ with the weights pretrained on ImageNet classification (pink line in <ref type="figure">Figure 6c</ref>). In particular, we observe that more Broyden iterations were required to reach the fixed point as the images get larger. For example, whereas MDEQ typically finds the equilibria with a good level of accuracy within 30 steps on CIFAR-10 images (cf. <ref type="figure">Figure 6a)</ref>, over 100 steps are used on Cityscapes images (cf. <ref type="figure">Figure 6c)</ref>. Moreover, in all three cases, Broyden's method (blue lines in <ref type="figure">Figure 6</ref>) converges to the fixed point in a more stable and efficient manner than simply iterating f θ (yellow lines), which often converges poorly or does not converge at all.</p><p>We find that the simultaneous multiscale fusion also effectively stabilizes the equilibrium convergence of an MDEQ. <ref type="figure" target="#fig_5">Figure 7</ref> visualizes the convergence of all equilibrium streams (i.e.,</p><formula xml:id="formula_15">z [i+1] k −z [i] k z [i] k</formula><p>for resolution k) in an MDEQ that is applied on CIFAR-10. For comparison, we also visualize the convergence of a single-stream DEQ <ref type="bibr" target="#b4">[5]</ref> that maintains only the highest-resolution stream (i.e., 32 × 32). Specifically, from <ref type="figure" target="#fig_5">Figure 7</ref> one can observe that: 1) all MDEQ resolution streams indeed converge to their equilibria in parallel; 2) lower-resolution streams converge faster than higher- resolution streams; and 3) high-resolution convergence is much faster in multiscale setting (pink line) than in the single-stream setting (orange line).</p><p>We hypothesize that Broyden's method and the multiscale fusion help with the equilibrium convergence because both techniques provide a faster way to expand the receptive field of f θ (than simply stacking it). For Broyden's method (see Eq. <ref type="formula" target="#formula_12">(7)</ref>), the Broyden matrix B [i] is a full matrix that mixes all locations of the feature map (which is represented by g θ (z [i] ; x)); whereas typical convolutional filters only mix the signals locally. On the other hand, multiscale up-and downsamplings broaden the effective receptive field on the high-resolution stream by direct interpolation from lower-resolution feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Segmentation Results on Cityscapes</head><p>We demonstrate in <ref type="figure" target="#fig_6">Figure 8</ref> some examples of the segmentation results of the MDEQ-large model (see <ref type="table" target="#tab_7">Table 3</ref>) on Cityscapes (val) images (of resolution 2048 × 1024).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The structure of a multiscale deep equilibrium model (MDEQ). All components of the model are shown in this figure. MDEQ consists of a transformation f θ that is driven to equilibrium. Features at different scales coexist side by side and are driven to equilibrium simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The residual block used in MDEQ. An MDEQ contains only one such layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(b) Runtime and memory consumption on CIFAR- 10 Figure 4 :</head><label>104</label><figDesc>Left: test accuracy as a function of training epochs. Right: MDEQ-Small and ANODEs correspond to the settings and results reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Comparing MDEQ with single-stream DEQ on CIFAR-10. All resolutions of MDEQ converge simultaneously and in a much stabler way than the single-scale DEQ model. Larger scale index means higher resolution (e.g., "scale 1" is the highest scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Examples of MDEQ-large segmentation results on the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Low-resolution loss(es) (e.g., classification) . . .. . . DEQ Solver for f✓(z; x)or ODE Solver for f✓(z(t), t)</figDesc><table><row><cell></cell><cell></cell><cell>L1</cell><cell>L2</cell><cell>L3</cell></row><row><cell>Explicit</cell><cell></cell><cell></cell></row><row><cell>Module</cell><cell>Decoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Decoder(s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pre-trained</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Backbone</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(e.g., ResNet-101)</cell></row><row><cell></cell><cell></cell><cell cols="2">Laux. (auxiliary losses)</cell></row><row><cell>Explicit Module</cell><cell>Encoder</cell><cell>Encoder</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Equilibrium Solver for f ✓ (z ? ; x) = z ?</figDesc><table><row><cell cols="2">Low-resolution loss(es)</cell><cell></cell><cell></cell><cell>L1</cell><cell>L2</cell><cell>L3</cell></row><row><cell cols="2">(e.g., classification)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L2</cell><cell>L3 Laux. (auxiliary losses)</cell><cell>Explicit Module</cell><cell>Decoder</cell><cell>Decoder(s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pre-trained</cell></row><row><cell></cell><cell cols="3">or ODE Solver for f✓(z(t), t) DEQ Solver for f✓(z; x) z ?</cell><cell cols="2">Laux. (auxiliary losses) . . .</cell><cell>Backbone (e.g., ResNet-101)</cell></row><row><cell>. . .</cell><cell>z [0]</cell><cell>Explicit Module</cell><cell>Encoder</cell><cell>Encoder</cell></row><row><cell></cell><cell cols="4">(b) Single-stream implicit mod-</cell></row><row><cell></cell><cell cols="3">els (e.g., DEQs and NODEs)</cell><cell></cell></row></table><note>. . .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Low-resolution loss(es) (e.g., classification) . . .. . . DEQ Solver for f✓(z; x)or ODE Solver for f✓(z(t), t)</figDesc><table><row><cell></cell><cell></cell><cell>L1</cell><cell>L2</cell><cell>L3</cell></row><row><cell>Explicit</cell><cell></cell><cell></cell></row><row><cell>Module</cell><cell>Decoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Decoder(s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pre-trained</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Backbone</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(e.g., ResNet-101)</cell></row><row><cell></cell><cell></cell><cell cols="2">Laux. (auxiliary losses)</cell></row><row><cell>Explicit Module</cell><cell>Encoder</cell><cell>Encoder</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on CIFAR-10. Standard deviations are calculated on 5 runs.</figDesc><table><row><cell></cell><cell>Model Size</cell><cell>Accuracy</cell></row><row><cell cols="3">CIFAR-10 (without data augmentation)</cell></row><row><cell>Neural ODEs [18] Aug. Neural ODEs [18] Single-stream DEQ [5] ResNet-18 [25] [Explicit] MDEQ-small (ours)</cell><cell>172K 172K 170K 170K 170K</cell><cell>53.7% ± 0.2% 60.6% ± 0.4% 82.2% ± 0.3% 81.6% ± 0.3% 87.1% ± 0.4%</cell></row><row><cell cols="3">CIFAR-10 (with data augmentation)</cell></row><row><cell>ResNet-18 [25] [Explicit] MDEQ (ours)</cell><cell>10M 10M</cell><cell>92.9% ± 0.2% 93.8% ± 0.3%</cell></row></table><note>formance of MDEQs from two aspects. First, as prior implicit approaches such as NODEs have mostly evaluated on smaller-scale benchmarks such as MNIST [32] and CIFAR-10 (32 × 32 images) [30], we compare MDEQs with these baselines on the same benchmarks. We evaluate both training-time stability and inference-time performance. Second, we evaluate MDEQs on large-scale computer vision tasks: ImageNet classification</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on ImageNet classification with top-1 and top-5 accuracies reported. MDEQs were trained for 100 epochs.</figDesc><table><row><cell></cell><cell cols="3">Model Size top1 Acc. top5 Acc.</cell></row><row><cell>AlexNet [31]</cell><cell>238M</cell><cell>57.0%</cell><cell>80.3%</cell></row><row><cell>ResNet-18 [25]</cell><cell>13M</cell><cell>70.2%</cell><cell>89.9%</cell></row><row><cell>ResNet-34 [25]</cell><cell>21M</cell><cell>74.8%</cell><cell>91.1%</cell></row><row><cell>Inception-V2 [28]</cell><cell>12M</cell><cell>74.8%</cell><cell>92.2%</cell></row><row><cell>ResNet-50 [25]</cell><cell>26M</cell><cell>75.1%</cell><cell>92.5%</cell></row><row><cell>HRNet-W18-C [55]</cell><cell>21M</cell><cell>76.8%</cell><cell>93.4%</cell></row><row><cell>Single-stream DEQ + global pool [5]</cell><cell>18M</cell><cell>72.9%</cell><cell>91.0%</cell></row><row><cell>MDEQ-small (ours) [Implicit]</cell><cell>18M</cell><cell>75.5%</cell><cell>92.7%</cell></row><row><cell>ResNet-101 [25]</cell><cell>52M</cell><cell>77.1%</cell><cell>93.5%</cell></row><row><cell>W-ResNet-50 [61]</cell><cell>69M</cell><cell>78.1%</cell><cell>93.9%</cell></row><row><cell>DenseNet-264 [26]</cell><cell>74M</cell><cell>79.7%</cell><cell>94.8%</cell></row><row><cell>MDEQ-large (ours) [Implicit]</cell><cell>63M</cell><cell>77.5%</cell><cell>93.6%</cell></row><row><cell>Unrolled 5-layer MDEQ-large</cell><cell>63M</cell><cell>75.9%</cell><cell>93.0%</cell></row><row><cell>MDEQ-XL (ours) [Implicit]</cell><cell>81M</cell><cell>79.2%</cell><cell>94.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on Cityscapes val semantic segmentation. "*" marks the current SOTA. Higher mIoU (mean Intersection over Union) is better.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell cols="2">Model Size mIoU</cell></row><row><cell>ResNet-18-A [39]</cell><cell>ResNet-18</cell><cell>3.8M</cell><cell>55.4</cell></row><row><cell>ResNet-18-B [39]</cell><cell>ResNet-18</cell><cell>15.24M</cell><cell>69.1</cell></row><row><cell>MobileNetV2Plus [46]</cell><cell>MobileNetV2</cell><cell>8.3M</cell><cell>74.5</cell></row><row><cell>GSCNN [53]</cell><cell>ResNet-50</cell><cell>-</cell><cell>73.0</cell></row><row><cell>HRNetV2-W18-Small-v2* [55]</cell><cell>HRNet</cell><cell>4.0M</cell><cell>76.0</cell></row><row><cell>MDEQ-small (ours) [Implicit]</cell><cell>MDEQ</cell><cell>7.8M</cell><cell>75.1</cell></row><row><cell>U-Net++ [64]</cell><cell>ResNet-101</cell><cell>59.5M</cell><cell>75.5</cell></row><row><cell>Dilated-ResNet [60]</cell><cell>D-ResNet-101</cell><cell>52.1M</cell><cell>75.7</cell></row><row><cell>PSPNet [62]</cell><cell>D-ResNet-101</cell><cell>65.9M</cell><cell>78.4</cell></row><row><cell>DeepLabv3 [9]</cell><cell>D-ResNet-101</cell><cell>58.0M</cell><cell>78.5</cell></row><row><cell>PSANet [63]</cell><cell>ResNet-101</cell><cell>-</cell><cell>78.6</cell></row><row><cell>HRNetV2-W48* [55]</cell><cell>HRNet</cell><cell>65.9M</cell><cell>81.1</cell></row><row><cell>MDEQ-large (ours) [Implicit]</cell><cell>MDEQ</cell><cell>53.0M</cell><cell>77.8</cell></row><row><cell>MDEQ-XL (ours) [Implicit]</cell><cell>MDEQ</cell><cell>70.9M</cell><cell>80.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell cols="2">CIFAR-10 (cls.)</cell><cell cols="2">ImageNet (cls.)</cell><cell cols="2">Cityscapes (seg.)</cell></row><row><cell></cell><cell>MDEQ-Small</cell><cell>MDEQ</cell><cell>MDEQ-Small</cell><cell>MDEQ-Large</cell><cell>MDEQ-Small</cell><cell>MDEQ-Large</cell></row><row><cell>Input Image Size</cell><cell cols="2">32 × 32</cell><cell cols="2">224 × 224</cell><cell cols="2">1024 × 512 (train) 2048 × 1024 (test)</cell></row><row><cell>Number of Epochs</cell><cell>50</cell><cell>200</cell><cell>100</cell><cell>100</cell><cell>480</cell><cell>480</cell></row><row><cell>Batch Size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>12</cell><cell>12</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>SGD</cell><cell>SGD</cell><cell>SGD</cell><cell>SGD</cell></row><row><cell>(Start) Learning Rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.05</cell><cell>0.05</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Nesterov Momentum</cell><cell>-</cell><cell>-</cell><cell>0.9</cell><cell>0.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Weight Decay</cell><cell>0</cell><cell>0</cell><cell>5e-5</cell><cell>1e-4</cell><cell>2e-4</cell><cell>3e-4</cell></row><row><cell>Use Pre-trained Weights</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">Yes, from ImageNet Yes, from ImageNet</cell></row><row><cell>Number of Scales</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell># of Channels for Each Scale</cell><cell>[8,16,32]</cell><cell cols="3">[28,56,112,224] [32,64,128,256] [80,160,320,640]</cell><cell></cell><cell></cell></row><row><cell>Width Expansion (in the residual block) Normalization (# of groups)</cell><cell cols="2">5× GroupNorm(4) GroupNorm(4) 5×</cell><cell>5× GroupNorm(4)</cell><cell>5× GroupNorm(4)</cell><cell cols="2">(Exact same model as in ImageNet)</cell></row><row><cell>Weight Normalization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># of Downsamplings Before Equilirbium Solver</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Forward Quasi-Newton Threshold Tf</cell><cell>15</cell><cell>15</cell><cell>22</cell><cell>22</cell><cell>27</cell><cell>27</cell></row><row><cell>Backward Quasi-Newton Threshold Tb</cell><cell>18</cell><cell>18</cell><cell>25</cell><cell>25</cell><cell>30</cell><cell>30</cell></row><row><cell>Limited-Mem. Broyden's Method Storage Size m</cell><cell>12</cell><cell>12</cell><cell>18</cell><cell>18</cell><cell>18</cell><cell>18</cell></row><row><cell>Variational Dropout Rate</cell><cell>0.2</cell><cell>0.25</cell><cell>0.0</cell><cell>0.0</cell><cell>0.03</cell><cell>0.05</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A learning rule for asynchronous perceptrons with feedback in a combinatorial environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A class of methods for solving nonlinear simultaneous equations. Mathematics of Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Broyden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end differentiable physics for learning and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Avila Belbute-Peres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentiable learning of submodular models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augmented neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Travacca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06315</idno>
		<title level="m">Implicit deep learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04866</idno>
		<title level="m">Deep declarative networks: A new hope</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FFJORD: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The implicit function theorem: History, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Parks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reviving and improving recurrent back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pitkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="page" from="45" to="46" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalization of back propagation to recurrent and higher order neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable differentiable physics for learning and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adjustment of an inverse matrix corresponding to a change in one element of a given matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FishNet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">UNet++: A nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
