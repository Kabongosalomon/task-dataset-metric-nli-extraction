<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu2016@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
							<email>yaojie2017@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
							<email>sunle@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions. In this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions, i.e., although a mention can nest other mentions, they will not share the same head word. Specifically, we propose Anchor-Region Networks (ARNs), a sequence-to-nuggets architecture for nested mention detection. ARNs first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures. Furthermore, we also design Bag Loss, an objective function which can train ARNs in an end-toend manner without using any anchor word annotation. Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER), or more generally entity mention detection 1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction <ref type="bibr" target="#b6">(GuoDong et al., 2005;</ref><ref type="bibr" target="#b17">Mintz et al., 2009)</ref>, event extraction <ref type="bibr" target="#b8">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b13">Li et al., 2013)</ref> and machine reading comprehension <ref type="bibr" target="#b21">(Rajpurkar et al., 2016;</ref>.</p><p>Previous approaches <ref type="bibr" target="#b34">(Zhou and Su, 2002;</ref><ref type="bibr" target="#b2">Chieu and Ng, 2002;</ref><ref type="bibr" target="#b0">Bender et al., 2003;</ref><ref type="bibr" target="#b22">Settles, 2004</ref>; <ref type="figure">Figure 1</ref>: An example of nested entity mentions. Due to the nested structure, "the","department","of" and "education" belong to both PER and ORG mentions. <ref type="bibr" target="#b12">Lample et al., 2016)</ref> commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions. For example in <ref type="figure">Figure 1</ref>, an Organization entity mention "the department of education" is nested in another Person entity mention "the minister of the department of education". Nested entity mentions are very common. For instance, in the well-known ACE2005 and RichERE datasets, more than 20% of entity mentions are nested in other mentions. Therefore, it is critical to consider nested mentions for real-world applications and downstream tasks.</p><p>In this paper, we propose a sequence-to-nuggets approach, named as Anchor-Region Networks (ARNs), which can effectively detect all entity mentions by modeling and exploiting the headdriven phrase structures <ref type="bibr" target="#b20">(Pollard and Sag, 1994;</ref><ref type="bibr" target="#b4">Collins, 2003)</ref> of them. ARNs originate from two observations. First, although an entity mention can nest other mentions, they will not share the same head word. And the head word of a mention can provide strong semantic evidence for its entity type <ref type="bibr" target="#b3">(Choi et al., 2018)</ref>. For example in <ref type="figure">Figure 1</ref>, although the ORG mention is nested in the PER mention, they have different head words "department" and "minister" respectively, and these head words strongly indicate their corresponding entity types to be ORG and PER. Second, entity men-  <ref type="figure">Figure 2</ref>: The overall architecture of ARNs. Here "minister" and "department" are detected anchor words for two mentions respectively.</p><p>tions mostly have regular phrase structures. For the two mentions in <ref type="figure">Figure 1</ref>, they share the same "DET NN of NP" structure, where the NN after DET are their head words. Based on above observations, entity mentions can be naturally detected in a sequence-to-nuggets manner by 1) identifying the head words of all mentions in a sentence; and 2) recognizing entire mention nuggets centered at detected head words by exploiting regular phrase structures of entity mentions.</p><p>To this end, we propose ARNs, a new neural network-based approach for nested mention detection. <ref type="figure">Figure 2</ref> shows the architecture of ARNs. First, ARNs employs an anchor detector network to identify whether each word is a head word of an entity mention, and we refer the detected words as anchor words. After that, a region recognizer network is used to determine the mention boundaries centering at each anchor word. By effectively capturing head-driven phrase structures of entity mentions, the proposed ARNs can naturally address the nested mention problem because different mentions have different anchor words, and different anchor words correspond to different mention nuggets.</p><p>Furthermore, because the majority of NER datasets are not annotated with head words, they cannot be directly used to train our anchor detector. To address this issue, we propose Bag Loss, an objective function which can be used to train ARNs in an end-to-end manner without any anchor word annotation. Specifically, our Bag Loss is based on at-least-one assumption, i.e., each mention should have at least one anchor word, and the anchor word should strongly indicate its entity type. Based on this assumption, Bag Loss can automatically select the best anchor word within each mention during training, according to the association between words and the entity type of the mention. For example, given an ORG training instance "the department of education", Bag Loss will select "department" as the anchor word of this mention based on its tight correlation with type ORG. While other words in the mention, such as "the" and "of", will not be regarded as anchor words, because of their weak association with ORG type.</p><p>We conducted experiments on three standard nested entity mention detection benchmarks, including ACE2005, GENIA and TAC-KBP2017 datasets. Experiments show that ARNs can effectively detect nested entity mentions and achieve the state-of-the-art performance on all above three datasets. For better reproduction, we openly release the entire project at github. com/sanmusunrise/ARNs.</p><p>Generally, our main contributions are:</p><p>• We propose a new neural network architecture named as Anchor-Region Networks. By effectively modeling and leveraging the headdriven phrase structures of entity mentions, ARNs can naturally handle the nested mention detection problem and achieve the stateof-the-art performance on three benchmarks. To the best of our knowledge, this is the first work which attempts to exploit the headdriven phrase structures for nested NER. • We design an objective function, named as Bag Loss. By exploiting the association between words and entity types, Bag Loss can effectively learn ARNs in an end-to-end manner, without using any anchor word annotation. • Head-driven phrase structures are widely spread in natural language. This paper proposes an effective neural network-based solution for exploiting this structure, which can potentially benefit many NLP tasks, such as semantic role labeling <ref type="bibr" target="#b35">(Zhou and Xu, 2015;</ref><ref type="bibr" target="#b7">He et al., 2017)</ref> and event extraction <ref type="bibr" target="#b1">(Chen et al., 2015;</ref><ref type="bibr" target="#b14">Lin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Nested mention detection requires to identify all entity mentions in texts, rather than only outmost mentions in conventional NER. This raises a critical issue to traditional sequential labeling models because they can only assign one label to each token. To address this issue, mainly two kinds of methods have been proposed.</p><p>Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different subsequences. For this, <ref type="bibr" target="#b5">Finkel and Manning (2009)</ref> regarded nodes of parsing trees as candidate subsequences. Recently, <ref type="bibr" target="#b31">Xu et al. (2017)</ref> and <ref type="bibr" target="#b23">Sohrab and Miwa (2018)</ref> tried to directly classify over all subsequences of a sentence. Besides,  proposed a transition-based method to construct nested mentions via a sequence of specially designed actions. Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences.</p><p>Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units. One representative direction is hypergraph-based methods <ref type="bibr" target="#b15">(Lu and Roth, 2015;</ref><ref type="bibr" target="#b11">Katiyar and Cardie, 2018;</ref>, where hypergraphbased tags are used to ensure nested mentions can be recovered from word-level tags. Besides, Muis and Lu (2017) developed a gap-based tagging schema to capture nested structures. However, these schemas should be designed very carefully to prevent spurious structures and structural ambiguity . But more expressive, unambiguous schemas will inevitably lead to higher time complexity during both training and decoding.</p><p>Different from previous methods, this paper proposes a new architecture to address nested mention detection. Compared with region-based approaches, our ARNs detect mentions by exploiting head-driven phrase structures, rather than exhaustive classifying over subsequences. Therefore ARNs can significantly reduce the size of candidate mentions and lead to much lower time complexity. Compared with schema-based approaches, ARNs can naturally address nested mentions since different mentions will have different anchor words. There is no need to design complex tagging schemas, no spurious structures and no structural ambiguity.</p><p>Furthermore, we also propose Bag Loss, which can train ARNs in an end-to-end manner without any anchor word annotation. The design of Bag Loss is partially inspired by multi-instance learning (MIL) <ref type="bibr" target="#b37">(Zhou and Zhang, 2007;</ref><ref type="bibr" target="#b36">Zhou et al., 2009;</ref><ref type="bibr" target="#b24">Surdeanu et al., 2012)</ref>, but with a different target. MIL aims to predict a unified label of a bag of instances, while Bag Loss is proposed to train ARNs whose anchor detector is required to predict the label of each instance. Therefore previous MIL methods are not suitable for training ARNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Anchor-Region Networks for Nested Entity Mention Detection</head><p>Given a sentence, Anchor-Region Networks detect all entity mentions in a two-step paradigm. First, an anchor detector network identifies anchor words and classifies them into their corresponding entity types. After that, a region recognizer network is applied to recognize the entire mention nugget centering at each anchor word. In this way, ARNs can effectively model and exploit head-driven phrase structures of entity mentions: the anchor detector for recognizing possible head words and the region recognizer for capturing phrase structures. These two modules are jointly trained using the proposed Bag Loss, which learns ARNs in an end-to-end manner without using any anchor word annotation. This section will describe the architecture of ARNs. And Bag Loss will be introduced in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Anchor Detector</head><p>An anchor detector is a word-wise classifier, which identifies whether a word is an anchor word of an entity mention of specific types. For the example in <ref type="figure">Figure 1</ref>, the anchor detector should identify that "minister" is an anchor word of a PER mention and "department" is an anchor word of an ORG mention. Formally, given a sentence x 1 , x 2 , ..., x n , all words are first mapped to a sequence of word representations x 1 , x 2 , ..., x n where x i is a combination of word embedding, part-of-speech embedding and character-based representation of word x i following <ref type="bibr" target="#b12">Lample et al. (2016)</ref>. Then we obtain a context-aware representation h A i of each word x i using a bidirectional LSTM layer:</p><formula xml:id="formula_0">− → h A i = LSTM(x i , − − → h A i−1 ) ← − h A i = LSTM(x i , ← − − h A i+1 ) h A i = [ − → h A i ; ← − h A i ]<label>(1)</label></formula><p>The learned representation h A i is then fed into a multi-layer perceptron(MLP) classifier, which computes the scores O A i of the word x i being an anchor word of specific entity types (or NIL if this word is not an anchor word):</p><formula xml:id="formula_1">O A i = MLP(h A i ) (2)</formula><p>where O A i ∈ R |C| and |C| is the number of entity types plus one NIL class. Finally a softmax layer is used to normalize O A i to probabilities:</p><formula xml:id="formula_2">P (cj|xi) = e O A ij |C| k=1 e O A ik (3) where O A ij is the j th element in O A i , P (c j |x i )</formula><p>is the probability of word x i being an anchor word of class c j . Note that because different mentions will not share the same anchor word, the anchor detector can naturally solve nested mention detection problem by recognizing different anchor words for different mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Region Recognizer</head><p>Given an anchor word, ARNs will determine its exact mention nugget using a region recognizer network. For the example in <ref type="figure">Figure 1</ref>, the region recognizer will recognize that "the minister of the department of education" is the mention nugget for anchor word "minister" and "the department of education" is the mention nugget for anchor word "department". Inspired by the recent success of pointer networks <ref type="bibr" target="#b26">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b29">Wang and Jiang, 2016)</ref>, this paper designs a pointer-based architecture to recognize the mention boundaries centering at an anchor word. That is, our region recognizer will detect the mention nugget "the department of education" for anchor word "department" by recognizing "the" to be the left boundary and "education" to be the right boundary.</p><p>Similar to the anchor detector, a bidirectional LSTM layer is first applied to obtain the contextaware representation h R i of word x i . For recognizing mention boundaries, local features commonly play essential roles. For instance, a noun before a verb is an informative boundary indicator for entity mentions. To capture such local features, we further introduce a convolutional layer upon h R i :</p><formula xml:id="formula_3">r i = tanh(W h R i−k:i+k + b)<label>(4)</label></formula><p>where h R i−k:i+k is the concatenation of vectors from h R i−k to h R i+k , W and b are the convolutional kernel and the bias term respectively. k is the (one-side) window size of convolutional layer. Finally, for each anchor word x i , we compute its left mention boundary score L ij and right mention boundary score R ij at word x j by</p><formula xml:id="formula_4">Lij = tanh(r T j Λ1h R i + U1r j + b1) Rij = tanh(r T j Λ2h R i + U2r j + b2)<label>(5)</label></formula><p>In the above two equations, the first term within the tanh function computes the score of word x j serving as the left/right boundary of a mention centering at word x i . And the second term models the possibility of word x j itself serving as the boundary universally. After that, we select the best left boundary word x j and best right boundary word x k for anchor word x i , and the nugget {x j , ..., x i , ..., x k } will be a recognized mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Learning with Bag Loss</head><p>This section describes how to train ARNs using existing NER datasets. The main challenge here is that current NER corpus are not annotated with anchor words of entity mentions, and therefore they cannot be directly used to train the anchor detector. To address this problem, we propose Bag Loss, an objective function which can effectively learn ARNs in an end-to-end manner, without using any anchor word annotation. Intuitively, one naive solution is to regard all words in a mention as its anchor words. However, this naive solution will inevitably result in two severe problems. First, a word may belong to different mentions when nested mentions exist. Therefore this naive solution will lead to ambiguous and noisy anchor words. For the example in <ref type="figure">Figure 1</ref>, it is unreasonable to annotate the word "department" as an anchor word of both PER and ORG mentions, because it has little association to PER type although the PER mention also contains it. Second, many words in a mention are just function words, which are not associated with its entity type. For example, words "the","of" and "education" in "the department of education" are not associated with its type ORG. Therefore annotating them as anchor words of the ORG mention will introduce remarkable noise.</p><p>To resolve the first problem, we observe that a word can only be the anchor word of the innermost mention containing it. This is because a mention nested in another mention can be regarded as a replaceable component, and changing it will not affect the structure of outer mentions. For the case in <ref type="figure">Figure 1</ref>, if we replace the nested mention "the department of education" by other ORG mention(e.g., changing it to "State"), the type of the  <ref type="figure">Figure 3</ref>: An illustration of bags. B i represents the bag where word x i is in. This sentence forms five bags, two of which correspond to two entity mentions and three of which correspond to NIL.</p><p>outer mention will not change. Therefore, words in a nested mention should not be regarded as the anchor word of outer mentions, and therefore a word can only be assigned as the anchor word of the innermost mention containing it.</p><p>To address the second problem, we design Bag Loss based on the at-least-one assumption, i.e., for each mention at least one word should be regarded as its anchor word. Specifically, we refer to all words belonging to the same innermost mention as a bag. And the type of the bag is the type of that innermost mention. For example, in <ref type="figure">Figure 3</ref>,{the, minister, of} will form a PER bag, and {the, department, of education} will form an ORG bag. Besides, each word not covered by any mention will form a one-word bag with NIL type. So there are three NIL bags in <ref type="figure">Figure 3</ref>, including {convened}, {a} and {meeting}.</p><p>Given a bag, Bag Loss will make sure that at least one word in each bag will be selected as its anchor word, and be assigned to the bag type. While other words in that bag will be classified into either the bag type or NIL. Bag Loss selects anchor words according to their associations with the bag type. That is, only words highly related to the bag type (e.g., "department" in "the department of education") will be trained towards the bag type, and other irrelevant words (e.g., "the" and "of" in the above example) will be trained towards NIL. Bag Loss based End-to-End Learning. For ARNs, each training instance is a tuple x = (x i , x j , x k , c i ), where x j , ..., x k is an entity mention with left boundary x j and right boundary x k . c j is its entity type and word x i is a word in this mention's bag 2 . For each instance, Bag loss considers two situations: 1) If x i is its anchor word, the loss will be the sum of the anchor detector loss (i.e., the loss of correctly classifying x i into its bag type c i ) and the region recognizer loss (i.e., the loss of correctly recognizing the mention boundary x j and x k ); 2) If x i is not its anchor word, the loss will be only the anchor detector loss (i.e., correctly classifying x i into NIL). The final loss for this instance is a weighted sum of the loss of these two situations, where the weight are determined using the association between word x i and the bag type c i compared with other words in the same bag. Formally, Bag Loss is written as:</p><formula xml:id="formula_5">L(xi; θ) = ωi · [− log P (ci|xi) + L R (xi; θ)] + (1 − ωi) · [− log P (N IL|xi)]<label>(6)</label></formula><p>where − log P (c i |x i ) is the anchor detector loss.</p><formula xml:id="formula_6">L R (x i ; θ) = L lef t (x i ; θ) + L right (x i ; θ)</formula><p>is the loss for the region recognizer measuring how preciously the region recognizer can identify the boundaries centered at anchor word x i . We define L lef t (x i ; θ) using max-margin loss:</p><formula xml:id="formula_7">L lef t (xi; θ) = 0, ci = N IL max(0, γ−Lij + max t =j Lit), ci = N IL (7)</formula><p>where γ is a hyper-parameter representing the margin, and L right (x i ; θ) is similarly defined.</p><p>Besides, ω i in Equation <ref type="formula" target="#formula_5">(6)</ref> measures the correlation between word x i and the bag type c i . Compared with other words in the same bag, a word x i should have larger w i if it has a tighter association with the bag type. Therefore, ω i can be naturally defined as:</p><formula xml:id="formula_8">ωi = [ P (ci|xi) maxx t ∈B i P (ci|xt) ] α .<label>(8)</label></formula><p>where B i denotes the bag x i belonging to, i.e., all words that share the same innermost mention with x i . α is a hyper-parameter controlling how likely a word will be regarded as an anchor word rather than regarded as NIL. α = 0 means that all words are annotated with the bag type. And α → +∞ means that Bag Loss will only choose the word with highest P (c i |x i ) as anchor word, while all other words in the same bag will be regarded as NIL. Consequently, Bag Loss guarantees that at least one anchor word (the one with highest P (c i |x i ), and its corresponding w i will be 1.0) will be selected for each bag. For other words that are not associated with the type (the ones with low P (c i |x i )), Bag Loss can make it to automatically learn towards NIL during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>We conducted experiments on three standard English entity mention detection benchmarks with nested mentions: ACE2005, GENIA and TAC-KBP2017 (KBP2017) datasets. For ACE2005 and GENIA, we used the same setup as previous work <ref type="bibr" target="#b10">(Ju et al., 2018;</ref><ref type="bibr" target="#b11">Katiyar and Cardie, 2018)</ref>. For KBP2017, we evaluated our model on the 2017 English evaluation dataset (LDC2017E55), using previous RichERE annotated datasets (LDC2015E29, LDC2015E68, LDC2016E31 and LDC2017E02) as the training set except 20 randomly sampled documents reserved as development set. Finally, there were 866/20/167 documents for KBP2017 train/dev/test set. In ACE2005, GENIA and KBP2017, there are 22%, 10% and 19% mentions nested in other mentions respectively. We used Stanford CoreNLP toolkit  to preprocess all documents for sentence splitting and POS tagging. Adadelta update rule (Zeiler, 2012) is applied for optimization. Word embeddings are initialized with pretrained 200-dimension Glove <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref> vectors 3 . Hyper-parameters are tuned on the development sets 4 apart from α in Equation <ref type="formula" target="#formula_8">(8)</ref>, which will be further discussed in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare ARNs with following baselines 5 :</p><p>• Conventional CRF models, including LSTM-CRF <ref type="bibr" target="#b12">(Lample et al., 2016)</ref> and Multi-CRF. LSTM-CRF is a classical baseline for NER, which doesn't consider nested mentions so only outmost mentions are used for training. Multi-CRF is similar to LSTM-CRF but learns one model for each entity type, and thus is able to recognize nested mentions if they have different types. • Region-based methods, including FOFE <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, Cascaded-CRF <ref type="bibr" target="#b10">(Ju et al., 2018</ref>) and a transition model (refered as Transition) proposed by . FOFE directly classifies over all sub-sequences of a sentence and thus all potential mentions can be considered. Cascaded-CRF uses several stacked CRF layers to recognize nested mentions at different levels. Transition constructs nested mentions through a sequence of actions. • Hypergraph-based methods, including the LSTM-Hypergraph (LH) model <ref type="bibr" target="#b11">(Katiyar and Cardie, 2018)</ref> and the Segmental Hypergraph (SH) by . LH used an LSTM model to learn features and then decode them into a hypergraph. SH further considered the transition between labels to alleviate labeling ambiguity, which is the state-of-the-art in both ACE2005 and GENIA 6 datasets. Besides, we also compared the performance of ARNs with the best system in TAC-KBP 2017 Evaluation <ref type="bibr" target="#b9">(Ji et al., 2017)</ref>. The same as all previous studies, models are evaluated using microaveraged Precision(P), Recall(R) and F1-score. To balance time complexity and performance,  proposed to restrict the maximum length of mentions to 6, which covers more than 95% mentions. So we also compared to baselines where the maximum length of mention is restricted or unrestricted. Besides, we also compared the decoding time complexity of different methods. <ref type="table" target="#tab_4">Table 1</ref> shows the overall results on ACE2005, GENIA and KBP2017 datasets. From this table, we can see that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall Results</head><p>1) Nested mentions have a significant influence on NER performance and are required to be specially treated. Compared with LSTM-CRF and Multi-CRF baselines, all other methods dealing with nested mentions achieved significant F1-score improvements. So it is critical to take nested mentions into consideration for real-world  <ref type="bibr" target="#b31">(Xu et al., 2017)</ref> 76.9 62.0 68.7 74.0 65.5 69.5 79.1 62.5 69.8 O(mn 2 ) Transition  74.5 71.5 73.0 78.0 70.2 73.9 74.7 67.0 70.1 O(mn) Cascaded-CRF <ref type="bibr" target="#b10">(Ju et al., 2018)</ref> 74.2 70.3 72.2 78.5 71.3 74.7 ----LH <ref type="bibr" target="#b11">(Katiyar and Cardie, 2018)</ref> 70  applications and downstream tasks.</p><p>2) Our Anchor-Region Networks can effectively resolve the nested mention detection problem, and achieved the state-of-the-art performance in all three datasets. On ACE2005 and GENIA, ARNs achieved the state-of-the-art performance on both the restricted and the unrestricted mention length settings. On KBP2017, ARNs outperform the top-1 system in the 2017 Evaluation by a large margin. This verifies the effectiveness of our new architecture.</p><p>3) By modeling and exploiting head-driven phrase structure of entity mentions, ARNs reduce the computational cost significantly. ARNs only detect nuggets centering at detected anchor words. Note that for each sentence, the number of potential anchor words k is significantly smaller than the sentence length n. Therefore the computational cost of our region recognizer is significantly lower than that of traditional region-based methods which perform classification on all sub-sequences, as well as hypergraphbased methods which introduced structural dependencies between labels to prevent structural ambiguity . Furthermore, ARNs are highly parallelizable if we replace the BiLSTM context encoder with other parallelizable context encoder architecture (e.g., Transformer <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effects of Bag Loss</head><p>In this section, we investigate effects of Bag Loss by varying the values of hyper-parameter α in Equation <ref type="formula" target="#formula_8">(8)</ref>  ure 4 shows the F1 curves on both ACE2005 and KBP2017 datasets when α varies. We can see that:</p><p>1) Bag Loss is effective for anchor word selection during training. In <ref type="figure" target="#fig_0">Figure 4</ref>, setting α to 0 significantly undermines the performance. Note that setting α to 0 is the same as ablating Bag Loss, i.e., the model will treat all words in the same innermost mention as anchor words. This result further verifies the necessity of Bag Loss. That is, because not all words in a mention are related to its type, it will introduce remarkable noise by regarding all words in mentions as anchor words.</p><p>2) Bag Loss is not sensitive to α when it is larger than a threshold. In <ref type="figure" target="#fig_0">Figure 4</ref>, our systems achieve nearly the same performance when α &gt; 0.8. We find that this is because our model can predict anchor word in a very sharp probability distribution, so slight change of α does  not make a big difference. Therefore, in all our experiments we empirically set α = 1 without special declaration. This also verified that Bag Loss can discover head-driven phrase structure steadily without using anchor word annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Further Discussion on Bag Loss and Marginalization-based Loss</head><p>One possible alternative solution for Bag Loss is to regard the anchor word as a hidden variable, and obtain the likelihood of each mention by marginalizing over all words in the mention nugget with</p><formula xml:id="formula_9">P (c, xj, x k ) = x i P (xi, c)P (xj, x k |xi, c).<label>(9)</label></formula><p>For P (x i , c), if we assume that the prior for each word being the anchor word is equal, it can be refactorized by P (xi, c) = P (c|xi)P (xi) ∝ P (c|xi).</p><p>However, we find that this approach does not work well in practice. This may because that, as we mentioned above, the prior probability of each word being the anchor word should not be equal. Words with highly semantic relatedness to the types are more likely to be the anchor word. Furthermore, this marginalization-based training object can only guarantee that words being regarded as the anchor words are trained towards the mention type, but will not encourage the other irrelevant words in the mention to be trained towards NIL. Therefore, compared with Bag Loss, the marginalization-based solution can not achieve the promising results for ARNs training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis on Anchor Words</head><p>To analyze the detected anchor words, <ref type="table" target="#tab_6">Table 2</ref> shows the most common anchor words for all entity types. Besides, words that frequently appear in a mention but being recognized as NIL are also   <ref type="figure">Figure 5</ref>: A representative error case of ARNs, where the right boundary of the PER mention is misclassified. Braces above the sentence indicate the output of ARNs, and brackets in the sentence represent the golden annotation. We find that the majority of errors occur because of the long-term dependencies stemming from postpositive attributive and attributive clauses.</p><p>presented. We can see that the top-10 anchor words of each type are very convincing: all these words are strong indicators of their entity types. Besides, we can see that frequent NIL words in entity mentions are commonly function words, which play significant role in the structure of mention nuggets (e.g., "the" and "a" often indicates the start of an entity mention) but have little semantic association with entity types. This supports our motivation and further verifies the effectiveness of Bag Loss for anchor word selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Error Analysis</head><p>This section conducts error analysis on ARNs. <ref type="table" target="#tab_8">Table 3</ref> shows the performance gap between the anchor detector and the entire ARNs. We can see that there is still a significant performance gap from the anchor detector to entire ARNs. That is, there exist a number of mentions whose anchor words are correctly detected by the anchor detector but their boundaries are mistakenly recognized by the region recognizer. To investigate the reason behind this above performance gap, we analyze these cases and find that most of these errors stem from the existence of postpositive attributive and attributive clauses. <ref type="figure">Figure 5</ref> shows an error case stemming from postpositive attributive. These cases are quite difficult for neural networks because long-term dependencies between clauses need to be carefully considered. One strategy to handle these cases is to introduce syntactic knowledge, which we leave as future work for improving ARNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>This paper proposes Anchor-Region networks, a sequence-to-nuggets architecture which can naturally detect nested entity mentions by modeling and exploiting head-driven phrase structures of entity mentions. Specifically, an anchor detector is first used to detect the anchor words of entity mentions and then a region recognizer is designed to recognize the mention boundaries centering at each anchor word. Furthermore, we also propose Bag Loss to train ARNs in an end-to-end manner without using any anchor word annotation. Experiments show that ARNs achieve the state-of-theart performance on all three benchmarks. As the head-driven structures are widely spread in natural language, the solution proposed in this paper can also be used for modeling and exploiting this structure in many other NLP tasks, such as semantic role labeling and event extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>on the system performance. Fig-The F1-score w.r.t. different α in Bag Loss on development sets. When α = 0, the model ablates Bag Loss and will treat all words in the same innermost mention as anchor words during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1906.03783v1 [cs.CL] 10 Jun 2019</figDesc><table><row><cell>Mention Nuggets</cell><cell cols="4">[The minister ... education] [the department of education] PER ORG</cell></row><row><cell>Region Recognizer</cell><cell>The minister</cell><cell cols="2">PER</cell><cell>... education…</cell></row><row><cell></cell><cell cols="2">…the department</cell><cell cols="2">ORG</cell><cell>of education…</cell></row><row><cell>Anchor words</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PER</cell><cell></cell><cell></cell><cell>ORG</cell></row><row><cell>Anchor Detector</cell><cell>…</cell><cell></cell><cell></cell><cell>…</cell></row><row><cell></cell><cell cols="4">… minister … department …</cell></row><row><cell>Sentence</cell><cell cols="4">The minister of the department of education convened a meeting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>[</head><label></label><figDesc>The minister of [ the department of education ] ORG ] PER convened a meeting. =B 1 =B 2 ={The, minister, of} → PER NIL NIL B 3 =B 4 =B 5 =B 6 ={the, department, of education} → ORG B 7 ={convened} → NIL B 8 ={a} → NIL B 9 ={meeting} → NIL</figDesc><table><row><cell>PER</cell><cell>ORG</cell><cell>NIL</cell></row><row><cell>B 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b12">Lample et al., 2016)</ref> 70.3 55.7 62.2 75.2 64.6 69.5 71.5 53.3 61.1</figDesc><table><row><cell></cell><cell></cell><cell>ACE2005</cell><cell></cell><cell></cell><cell>GENIA</cell><cell></cell><cell></cell><cell>KBP2017</cell><cell></cell><cell>Time</cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Complexity</cell></row><row><cell cols="11">LSTM-CRF (O(mn)</cell></row><row><cell>Multi-CRF</cell><cell cols="9">69.7 61.3 65.2 73.1 64.9 68.8 69.7 60.8 64.9</cell><cell>O(mn)</cell></row><row><cell>FOFE(c=6) (Xu et al., 2017)</cell><cell cols="9">76.5 66.3 71.0 75.4 67.8 71.4 81.8 62.0 70.6</cell><cell>O(mn 2 )</cell></row><row><cell>FOFE(c=n)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Overall experiment results on ACE2005, GENIA and KBP2017 datasets. c is the maximum length of mention and n refers to the length of sentence. For time complexity, m denotes the number of class and k denotes the average number of anchor words in each sentence(k &lt;&lt; n). The time complexity of Cascaded-CRF depends on datasets so is not listed here.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Type Most Frequent Anchor Words PER I, you, he, they, we, people, president, Mandela, family, officials ORG government, Apple, they, its, Nokia, company, Microsoft, military, party, bank FAC building, home, prison, house, store, factories, factory, school, streets, there GPE country, China, U.S., US, Cyprus, our, state, countries, Syria, Russia LOC world, moon, areas, space, European, Europe, area, region, places, border NIL the, a, of, 's, in, and, to, his, who, former</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The top-10 most frequent anchor words of each type on KBP2017 datasets. Line NIL shows most frequent words that appears in a mention but are not regarded as anchor words.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>F1-scores gap between the anchor detector and the entire ARNs (anchor + region).</figDesc><table><row><cell cols="2">… was [a man of [African] appearance, about 30</cell></row><row><cell>years old , with a small beard]</cell><cell>.</cell></row></table><note>PER LOC LOC PER</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For words not in any mention, we define xj = x k = xi and ci = NIL, but their boundary will not be considered during optimization according to Equation(7).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://nlp.stanford.edu/data/glove. 6B.zip 4 The hyper-parameter configures are openly released together with our source code at github.com/ sanmusunrise/ARNs. 5 As Wang and Lu (2018) reported, neural network-based baselines significantly outperform all non-neural methods. So we only compared with neural network-based baselines.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Even Sohrab and Miwa (2018) reported a higher performance on GENIA, their experimental settings are obviously different from other baselines. As they didn't release their dataset splits and source code, we are unable to compare it with listed baselines.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank the reviewers for their insightful comments and valuable suggestions. Moreover, this work is supported by the National Natural Science Foundation of China under Grants no. 61433015, 61572477 and 61772505; the Projects of the Chinese Language Committee under Grants no. WT135-24; and the Young Elite Scientists Sponsorship Program no. YESS20160177.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum entropy models for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="148" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition: a maximum entropy approach using global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="473" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of tac-kbp2017 13 languages entity discovery and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cash</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sydney</forename><forename type="middle">Informatics</forename><surname>Hub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Text Analysis Conference (TAC2017)</title>
		<meeting>the Tenth Text Analysis Conference (TAC2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural layered model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meizhi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nugget proposal networks for chinese event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1565" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Labeling gaps between words: Recognizing overlapping mentions with mention separators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><surname>Obaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2608" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Head-driven phrase structure grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition using conditional random fields and rich feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international joint workshop on natural language processing in biomedicine and its applications</title>
		<meeting>the international joint workshop on natural language processing in biomedicine and its applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golam</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural segmental hypergraphs for overlapping mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A neural transition-based model for nested mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1011" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A local detection approach for named entity recognition and mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sedtawut</forename><surname>Watcharawittayakul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1237" to="1247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Named entity recognition using an hmm-based chunk tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-instance learning by treating instances as noniid samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Yin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiinstance multi-label learning with application to scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1609" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
