<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIGHTWEIGHT MULTI-BRANCH NETWORK FOR PERSON RE-IDENTIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-26">26 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunbo</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hörmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LIGHTWEIGHT MULTI-BRANCH NETWORK FOR PERSON RE-IDENTIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-26">26 Jan 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-Identification</term>
					<term>Deep Learning</term>
					<term>Image Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person Re-Identification aims to retrieve person identities from images captured by multiple cameras or the same cameras in different time instances and locations. Because of its importance in many vision applications from surveillance to human-machine interaction, person re-identification methods need to be reliable and fast. While more and more deep architectures are proposed for increasing performance, those methods also increase overall model complexity. This paper proposes a lightweight network that combines global, part-based, and channel features in a unified multi-branch architecture that builds on the resource-efficient OSNet backbone. Using a well-founded combination of training techniques and design choices, our final model achieves state-of-the-art results on CUHK03 labeled, CUHK03 detected, and Market-1501 with 85.1% mAP / 87.2% rank1, 82.4% mAP / 84.9% rank1, and 91.5% mAP / 96.3% rank1, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Person Re-Identification (PREID) is an important computer vision task for video surveillance applications. Formally, the problem can be stated as follows <ref type="bibr">[1]</ref>. Given a probe image P, and a gallery of M images G = {Gi} M i=1 , all of which annotated with an associated identity id(Gi) ∈ N, the goal is to find a similarity measure sim (·) such that i * = arg max i=1,...,M sim(P, Gi) ⇒ id(P) = id(Gi * ).</p><p>(1)</p><p>While it is no surprise that the success of deep learning and the need for PREID as a processing step for person tracking has resulted in numerous approaches, the problem remains challenging, especially when it comes to balancing performance and low complexity of the models.</p><p>Recently, multiple-branch architectures have been proposed in particular <ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr" target="#b0">[5]</ref><ref type="bibr" target="#b1">[6]</ref>. These methods allow the network to focus on different person features in individual branches, e.g., on distinct spatial parts or channels. Although branching generally increases model performance, it comes with higher computational costs, especially if the number of branches or the total number of operations in them is increased. We claim that additional model complexity is not necessary and propose a network that outperforms other multi-branch approaches by using a suitable feature extractor and the right combination of training techniques.</p><p>The resulting network consists of three branches that optimize the global, partial, and channel-wise representations using simple computations, respectively. Despite this branching, we succeed in keeping the number of parameters low using OSNet <ref type="bibr" target="#b2">[7]</ref>, a lightweight feature extractor that has recently proven to be more efficient and accurate than other backbones for PREID tasks. Our deep neural network achieves state-of-the-art results on two important benchmark datasets, Market-1501 <ref type="bibr" target="#b3">[8]</ref> and CUHK03 <ref type="bibr" target="#b4">[9]</ref>. In detailed ablation studies, we demonstrate how the respective branches increase model performance, why our network performs better than other multi-branch approaches, and what training techniques are necessary to train a multi-branch architecture with OSNet backbone. Code and pretrained models of our research are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>While PREID has been studied as a computer vision task for a long time <ref type="bibr" target="#b5">[10]</ref>, deep learning accelerated the research progress and model performance significantly, dominating the scene ever since <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b2">7,</ref><ref type="bibr" target="#b6">[11]</ref><ref type="bibr" target="#b7">[12]</ref><ref type="bibr" target="#b8">[13]</ref>. PREID approaches can be categorized as follows. First, several methods focus on improving feature extraction for the global input images <ref type="bibr" target="#b2">[7,</ref><ref type="bibr" target="#b7">12,</ref><ref type="bibr" target="#b9">14]</ref>. Luo et al. <ref type="bibr" target="#b7">[12]</ref> contributed with comprehensive research of many training techniques and were able to find combinations that boost the overall performance. Zhou et al. <ref type="bibr" target="#b2">[7]</ref>, on the other hand, concentrated on the feature extraction itself, proposing OSNet, a multi-scale network designed explicitly for the PREID task that outperforms standard ResNet50 <ref type="bibr" target="#b10">[15]</ref> backbones despite a much lower number of parameters.</p><p>Another important research direction is finding spatial partitions of the persons' images [3, <ref type="bibr" target="#b8">13,</ref><ref type="bibr" target="#b11">16]</ref>. Usually, the input image is divided into disjoint parts, often horizontal stripes, to obtain partitioned features that are discriminative for person matching. Sun et al. <ref type="bibr" target="#b8">[13]</ref> utilized the idea of part pooling, where the partitioning is done via spatial pooling after the convolutional layers of the backbone. This idea has since been used in other architectures <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b11">16]</ref>. In this context, many multi-branch or multi-stage approaches have been developed [2, 3, 6]. They mostly try to learn global and spatial part features in individual branches or combine part, channel, and global features, either through pooling <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b11">16]</ref> or attention <ref type="bibr">[4,</ref><ref type="bibr" target="#b0">5,</ref><ref type="bibr" target="#b12">17,</ref><ref type="bibr" target="#b13">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Like all recent works on the problem, we design an end-to-end neural network architecture based on strong image feature extraction backbones pretrained on ImageNet <ref type="bibr" target="#b14">[19]</ref>. In this subsection, we describe the architecture and training of the proposed network to solve Eq. (1). Our goal is to utilize a multi-branch architecture similar to MGN [2] and SCR [3] that leverages global, part-based and channelbased features, while keeping the overall number of parameters and embeddings low. Consequently and as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our network consists of three branches: The global branch, the part branch, and the channel branch.</p><p>Let X ∈ R 384×128×3 be an input image. Before separating into distinct branches, the image X is passed through a truncated OSNet <ref type="bibr" target="#b2">[7]</ref> backbone, up until the first layer of the third block, i.e., conv3 0, as in <ref type="bibr" target="#b1">[6]</ref>. This concept has been employed before with ResNet50 [2, 3], using the first blocks up to conv4 0. We chose OS-Net over ResNet due to its superior performance and lower complexity for PREID tasks <ref type="bibr" target="#b1">[6,</ref><ref type="bibr" target="#b2">7]</ref>. After forwarding X through the initial layers, the network forms the three branches, which comprise the remaining layers of OSNet up to the fifth block. By this design, only the layers up to conv3 0 are shared by all the branches, and for each individual branch, we obtain a tensor of dimension 24 × 8 × 512.</p><p>In the global branch, we obtain two global representations as follows: First, we aggregate the information by applying 2D average pooling on the tensor, obtaining the 512-dimensional vector g. For the second global representation, the initial 24 × 8 × 512-tensor is used as an input for a drop block, inspired by <ref type="bibr" target="#b15">[20]</ref>. The drop block removes the highest activated horizontal regions from the tensor, forcing the network to emphasize on less discriminative regions, which increases the robustness of the resulting representation. Having removed the regions of highest activity, we apply 2D max pooling on the resulting tensor, obtaining another 512-dimensional vector gdrop.</p><p>In the channel branch, the initial 24 × 8 × 512-tensor is reduced to a 512-dimensional vector and then partitioned into two vectors of length 256 each. We use 1 × 1 convolutions to scale the representations back up, obtaining two 512-dimensional vectors c1 and c2.</p><p>Here, the parameters of the 1 × 1 convolutions are shared among both channel parts.</p><p>Finally, in the part branch, we transform the initial 24×8×512tensor into three representations. We use average pooling to obtain a volume of size 2 × 1 × 512 that we split into two 512-dimensional part-based representations p1 and p2, representing the upper and lower body, respectively. Additionally, we use max pooling on the initial volume, obtaining another 512-dimensional global representation pg within the part branch.</p><p>We use a BNNeck <ref type="bibr" target="#b7">[12]</ref> for all branch vector representations calculated in this way. Each BNNeck block consists of batch normalization and a fully connected to number-of-classes layer. The aim of this block is to optimize embeddings for two different metric spaces at the same time. Embeddings obtained before the batch normalization layer are used for optimization with respect to a ranking loss (e.g., triplet loss <ref type="bibr" target="#b16">[21]</ref>), while embeddings obtained after the fully connected layer are used for optimization with respect to an identity loss (e.g., Cross-Entropy (CE) loss). Embeddings obtained after the batch normalization but before the fully connected layer find a balance between the representations of the two different metric spaces (i.e., ranking space and identitiy space) and are therefore used for inference. From the resulting embeddings we form two sets, given by</p><formula xml:id="formula_0">I := {ĝ,ĝdrop,p1,p2,pg,ĉ1,ĉ1} , (2) R := {g, gdrop, pg} ,<label>(3)</label></formula><p>for training in identity and rank spaces, respectively, where· denotes the tensors of the BNNeck representations after the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training and Loss Functions</head><p>For training, we use a combination of CE loss and Multi-Similarity (MS) loss <ref type="bibr" target="#b17">[22]</ref>. The latter was designed to take advantage of existing pair-wise methods and sampling strategies by exploiting a soft weighting scheme that considers both self-similarity and relative similarity. We compute MS loss LMS for global embeddings R obtained before batch normalization, and CE loss LCE on all embeddings I obtained after applying softmax activation to the fully  <ref type="table" target="#tab_0">The table lists our results on the two most used benchmarks, Market-1501 and  CHUK03</ref>. The latter was evaluated on the labeled set (CHUK03-L) and the detection set (CHUK03-D) in multi-gallery-shot setting (cf. <ref type="bibr" target="#b23">[28]</ref>). Note that all results are reported without re-ranking (cf. <ref type="bibr" target="#b23">[28]</ref>). </p><formula xml:id="formula_1">LCE (f (X), y)) := i∈I LCE (i, y) ,<label>(5)</label></formula><p>where f (X) is our networks output when forwarding X. For CE loss LCE, we further use label smoothing <ref type="bibr" target="#b7">[12,</ref><ref type="bibr" target="#b18">23]</ref>, which is a regularization technique that encourages the model not to be too confident on the training data. It adds a uniform noise distribution in CE calculation to soften the ground truth labels, which helps to improve model generalization. Thus, the overall objective loss function is</p><formula xml:id="formula_2">L = λCELCE + λMSLMS,<label>(6)</label></formula><p>where λCE and λMS are suitable weights. Additionally, we use random erasing augmentation (REA) <ref type="bibr" target="#b19">[24]</ref>, which randomly substitutes a rectangle with the image's mean value. It has demonstrated to improve model generalization and to produce higher variance training data. Cosine annealing strategies are common in PREID networks <ref type="bibr" target="#b2">[7,</ref><ref type="bibr" target="#b20">25]</ref>. To further boost performance, we use warm-up cosine annealing <ref type="bibr" target="#b21">[26,</ref><ref type="bibr" target="#b22">27]</ref> as our learning rate strategy rather than traditional step learning rate schedules. The learning rate first grows linearly from 6 · 10 −5 to 6 · 10 −4 in 10 epochs, then cosine decay to 6 · 10 −7 is applied in the remaining epochs. The learning rate lr(t) at epoch t with T total epochs is given by</p><formula xml:id="formula_3">lr(t) = 6 · 10 −4 · t 10 ,</formula><p>if t ≤ 10 6 · 10 −4 · 1 2 1 + cos π t−10 T −10 , if 10 &lt; t ≤ T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>We evaluated the model on two of the most widely used large-scale datasets, Market-1501 <ref type="bibr" target="#b3">[8]</ref> and CUHK03 <ref type="bibr" target="#b4">[9]</ref>. The Market-1501 dataset contains 32,668 images of 1,501 persons across 6 cameras, whereas the CUHK03 dataset comprises 13,164 images of 1,360 person across 6 cameras. For CUHK03, we use the new 767-split protocol <ref type="bibr" target="#b23">[28]</ref>, obtaining results for the labeled (CUHK03-L) and detected (CUHK03-D) configurations separately. We did not evaluate on DukeMTMC-ReID since use of this dataset has been prohibited by the authors. Training Details. For training, input images are normalized to channel-wise zero-mean and a standard variation of 1 and spatial resolution of 384×128. Data augmentation is performed by resizing images to 105% width and height and random cropping, as well as random horizontal flip with a probability of 0.5. Models are trained for 140 epochs for Market-1501 and 180 epochs for CUHK03 with a batchsize of 48. A batch consists of 8 samples for 6 identities each. The parameters are optimized by using using the Adam optimizer <ref type="bibr" target="#b24">[29]</ref> with ǫ = 1e − 8, β1 = 0.9 and β2 = 0.999. The backbones are pre-trained on ImageNet <ref type="bibr" target="#b14">[19]</ref> and all experiments are implemented with PyTorch <ref type="bibr" target="#b25">[30]</ref>. To balance the losses we chose λCE = λMS = 0.5. Evaluation Details. Cosine distance is utilized to compute cumulative matching characteristics (CMC) <ref type="bibr" target="#b26">[31]</ref>. Query and gallery images are re-sized to 384 × 128 pixels and normalized. For a fair comparison with other existing methods, the CMC rank-1 accuracy (r1) and mean Average Precision (mAP) are reported as evaluation metrics. Results with the same identity and the same camera ID as the query image are not counted. The authors of <ref type="bibr" target="#b27">[32]</ref> state in their official code repository 2 that mAP values computed with recent PREID frameworks are about 1%-point higher than those computed by the original Matlab evaluation code of Market-1501 <ref type="bibr" target="#b3">[8]</ref>. We were able to reproduce this. For completeness and fair comparison, we also state the mAP values for our final models as computed by the original evaluation script. We hope to raise more awareness to this issue by providing both results. <ref type="table" target="#tab_0">Table 1</ref> compares the performance of our model with that of other recent methods. Our model achieves state-of-the-art results on Market-1501, CUHK-L and CUHK-D, both in terms of rank-1 accuracy and mAP. The large difference in performance with regard to the mAP on all datasets is particularly noticeable. Interestingly, despite its simplicity, our architecture achieves better performance than other multi-branch approaches. Architecturally, our model is closely re- , PLR-OSNet <ref type="bibr" target="#b1">[6]</ref>, and, in particular, SCR [3]. All of these approaches use a truncated backbone followed by branching. MGN relies on ResNet50 and only uses spatial partitions, whereas our model builds upon OSNet and also better exploits the PREID problem by additionally using channel partitions. In this regard, SCR is the most similar architecture since both spatial and channel partitions are used for multi-loss training. However, for good performance, SCR requires nearly twice as many embeddings as our model and creates part and channel partitions in the same branch, which could theoretically impede the branches' specialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-Arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Influence of Branches. When introducing branches to a neural network architecture, the parameter count can raise substantially. Thus, any such introduction has to be well-justified. <ref type="table" target="#tab_2">Table 2</ref> depicts our network's performance for different branch combinations. The results suggest that single branches perform similarly when the other two respective branches are deactivated. Among all branches, the channel branch has the lowest performance on CUHK03-D, indicating that global features are very important for generalization on this dataset. As can be seen by the pairwise combination of branches, the part branch influences the performance significantly on CUHK03-D. By using all three branches together, our model achieves state-ofthe-art results on both datasets. Influence of Backbones.  Influence of Loss Functions. We trained various modifications of our model with triplet loss instead of MS loss. Using MS loss in the final model slightly increases the rank-1 and mAP performance on CUHK03, but not on Market-1501. Thus, the choice of ranking loss function can be important for generalization on smaller datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have presented a multi-branch neural network that achieves stateof-the-art results on Market-1501 and CUHK03. Although branches increase the overall parameter count, we can keep the overall model complexity low by utilizing a lightweight OSNet backbone and suitable training techniques. The distinct branches of our network can capture the essential person features. Overall our research suggests that learning rate schedules and the backbone choice heavily influence the model performance and that drop blocks and MS loss assist the model in generalizing the smaller CUHK03 dataset. We conclude that multi-branch architectures should focus on the right combination of training techniques and OSNet feature extraction in favor of adding model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">REFERENCES</head><p>[1] Liang Zheng, Yi Yang, and Alexander G Hauptmann, "Person re-identification: Past, present and future," arXiv preprint arXiv:1610.02984, 2016.</p><p>[2] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou, "Learning discriminative features with multiple granularities for person re-identification," in Proceedings of the 26th ACM international conference on Multimedia, 2018, pp. 274-282.</p><p>[3] Hao Chen, Benoit Lagadec, and Francois Bremond, "Learning discriminative and generalizable representations by spatialchannel partition for person re-identification," in The IEEE Winter Conference on Applications of Computer Vision, 2020, pp. 2483-2492.</p><p>[4] Binghui Chen, Weihong Deng, and Jiani Hu, "Mixed highorder attention network for person re-identification," in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 371-381.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Structure of our network. After forwarding images through the first three blocks of an OSNet backbone, our network continues in three distinct branches to learn global, channel-based and part-based features. All volumes are forwarded to BNNeck layers to produce final embeddings suited for different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our method with state-of-the-art.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of branch influences. We investigate our models performance under the specified branch configurations, where G+C+P refers to our original model.</figDesc><table><row><cell></cell><cell cols="2">Market-1501</cell><cell cols="2">CUHK03-D</cell></row><row><cell>Branch</cell><cell>rank1</cell><cell>mAP</cell><cell>rank1</cell><cell>mAP</cell></row><row><cell>Global (G)</cell><cell>95.4</cell><cell>89.3</cell><cell>80.8</cell><cell>77.3</cell></row><row><cell>Channel (C)</cell><cell>95.9</cell><cell>88.8</cell><cell>74.7</cell><cell>71.2</cell></row><row><cell>Part (P)</cell><cell>95.9</cell><cell>90.2</cell><cell>80.3</cell><cell>77.9</cell></row><row><cell>C+P</cell><cell>96.1</cell><cell>91.2</cell><cell>82.7</cell><cell>79.8</cell></row><row><cell>G+C</cell><cell>96.0</cell><cell>90.9</cell><cell>82.0</cell><cell>79.7</cell></row><row><cell>G+P</cell><cell>96.1</cell><cell>91.2</cell><cell>83.4</cell><cell>81.3</cell></row><row><cell>G+C+P</cell><cell>96.3</cell><cell>91.5</cell><cell>84.9</cell><cell>82.4</cell></row><row><cell cols="3">lated to previous work such as MGN [2]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Influence of Learning Rate Schedule. As can be seen inTable 3, when substituting the cosine warmup annealing schedule with a constant schedule, performance decreases. For the constant schedule, we have reduced the initial learning rate of 6 × 10 −4 three times by a factor of 10 in the 50th, 80th and 110th epoch, respectively. The results indicate the importance of a suitable learning rate strategy for PRID on both datasets. Influence of Drop Block. The results inTable 3suggest that the drop block has hardly any influence on the performance on Market-1501. On the other hand, results on the CUHK03 dataset clearly show that the drop block can lead to better generalization on the test set and increases both metrics.</figDesc><table><row><cell>shows some examples of the dif-</cell></row><row><cell>ferent performances of ResNet50 and OSNet. The raw model with</cell></row><row><cell>ResNet50 (i.e., the one without beneficial additions) has the weak-</cell></row><row><cell>est performance among all models. Only with all possible additions</cell></row><row><cell>it is able to achieve similar performance of a raw model with OS-</cell></row><row><cell>Net backbone. The best configuration that can be achieved with</cell></row><row><cell>ResNet50 is still inferior than our final model. Our model with</cell></row><row><cell>OSNet backbone only has about 9 million parameters, compared to</cell></row><row><cell>about 23 million with ResNet50 backbone.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of training techniques. We investigate our models performance under the specified training modifications. Here, WCA indicates use of warmup cosine annealing, MS the use of MS loss over triplet loss, DB the use of drop block, and OSNet the use of OSNet over ResNet50 as backbone, respectively.</figDesc><table><row><cell></cell><cell cols="2">Configuration</cell><cell></cell><cell cols="2">Market-1501</cell><cell cols="2">CUHK03-D</cell></row><row><cell>OSNet</cell><cell>WCA</cell><cell>MS</cell><cell>DB</cell><cell>r1</cell><cell>mAP</cell><cell>r1</cell><cell>mAP</cell></row><row><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>95.4</cell><cell>87.9</cell><cell>71.7</cell><cell>70.3</cell></row><row><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>96.1</cell><cell>90.4</cell><cell>81.0</cell><cell>79.1</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>96.1</cell><cell>90.2</cell><cell>78.2</cell><cell>75.2</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>96.2</cell><cell>91.1</cell><cell>83.5</cell><cell>81.1</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>96.3</cell><cell>91.5</cell><cell>83.2</cell><cell>80.9</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>96.0</cell><cell>90.6</cell><cell>78.8</cell><cell>76.3</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>96.2</cell><cell>91.2</cell><cell>83.4</cell><cell>80.9</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>96.3</cell><cell>91.5</cell><cell>84.9</cell><cell>82.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/jixunbo/LightMBN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/VisualComputingInstitute/triplet-reid</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning diverse features with part-level resolution for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07442</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
		<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloofar</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04193</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person reidentification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canmiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compact network training for person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Hussam Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
	<note>Itamar Friedman, and Lihi Zelnik-Manor</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Top-db-net: Top dropblock for activation enhancement in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Pedrini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05435</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Voc-reid: Vehicle re-identification based on vehicle-orientation-camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="602" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Alban Desmaison, Luca Antiga, and Adam Lerer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Automatic differentiation in pytorch</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computational and performance aspects of pca-based face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjoon</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="321" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
