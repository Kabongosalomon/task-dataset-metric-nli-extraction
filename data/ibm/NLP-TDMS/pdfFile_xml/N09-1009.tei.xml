<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2009-06">June 2009. 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction</title>
					</analytic>
					<monogr>
						<title level="m">The 2009 Annual Conference of the North American Chapter of the ACL</title>
						<meeting> <address><addrLine>Boulder, Colorado</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="74" to="82"/>
							<date type="published" when="2009-06">June 2009. 2009</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a family of priors over probabilis-tic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis <ref type="bibr" target="#b4">(Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b7">Collins, 2003)</ref>, but are now commonly seen in applications like machine translation ( <ref type="bibr" target="#b28">Wu, 1997</ref>) and question answering ( <ref type="bibr" target="#b27">Wang et al., 2007</ref>). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning-both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors.</p><p>There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities ) to putting non-parametric priors over derivations <ref type="bibr" target="#b15">(Johnson et al., 2006</ref>) to learning the set of states in a grammar ( <ref type="bibr" target="#b10">Finkel et al., 2007;</ref><ref type="bibr" target="#b21">Liang et al., 2007)</ref>. Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational <ref type="bibr" target="#b17">(Johnson, 2007;</ref><ref type="bibr" target="#b20">Kurihara and Sato, 2006</ref>) or sampling techniques are applied <ref type="bibr" target="#b15">(Johnson et al., 2006</ref>).</p><p>Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability.</p><p>Following work by <ref type="bibr" target="#b1">Blei and Lafferty (2006)</ref> for topic models, <ref type="bibr" target="#b6">Cohen et al. (2008)</ref> proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex. <ref type="bibr">Cohen et al.</ref> used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules' weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions. <ref type="bibr">1</ref> The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN prior of <ref type="bibr" target="#b6">Cohen et al. (2008)</ref>, showing how to extend the use of the LN prior to tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary). Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in a new bilingual setting where grammars for two languages are learned at once (without parallel corpora).</p><p>Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes. We provide a variational EM algorithm for inference.</p><p>The rest of this paper is organized as follows. In §2, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to <ref type="bibr" target="#b19">Klein and Manning (2004)</ref>. In §3, we present our model and a variational inference algorithm for it. In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them. We discuss future work ( §5) and conclude in §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Grammars and Dependency Grammar Induction</head><p>A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process. HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state. Each "step" of the walk and each symbol emission corresponds to one derivation step. PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of "child" symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM). Each step or emission of an HMM and each rewriting operation of a PCFG is conditionally independent of the other rewriting operations given a single structural element (one HMM or PCFG state); this Markov property permits efficient inference for the probability distribution defined by the probabilistic grammar.</p><p>In general, a probabilistic grammar defines the joint probability of a string x and a grammatical derivation y:</p><formula xml:id="formula_0">p(x, y | θ) = K k=1 N k i=1 θ f k,i (x,y) k,i (1) = exp K k=1 N k i=1 f k,i (x, y) log θ k,i</formula><p>where f k,i is a function that "counts" the number of times the kth distribution's ith event occurs in the derivation. The θ are a collection of K multinomials θ 1 , ..., θ K , the kth of which includes N k events. Note that there may be many derivations y for a given string x-perhaps even infinitely many in some kinds of grammars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dependency Model with Valence</head><p>HMMs and PCFGs are the best-known probabilistic grammars, but there are many others. In this paper, we use the "dependency model with valence" (DMV), due to <ref type="bibr" target="#b19">Klein and Manning (2004)</ref>. DMV defines a probabilistic grammar for unlabeled, projective dependency structures. <ref type="bibr" target="#b19">Klein and Manning (2004)</ref> achieved their best results with a combination of DMV with a model known as the "constituent-context model" (CCM). We do not experiment with CCM in this paper, because it does not fit directly in a Bayesian setting (it is highly deficient) and because state-of-the-art unsupervised dependency parsing results have been achieved with DMV alone <ref type="bibr" target="#b24">(Smith, 2006</ref>).</p><p>Using the notation above, DMV defines x = x 1 , x 2 , ..., x n to be a sentence. x 0 is a special "wall" symbol, $, on the left of every sentence. A tree y is defined by a pair of functions y left and y right (both {0, 1, 2, ..., n} → 2 {1,2,...,n} ) that map each word to its sets of left and right dependents, respectively. Here, the graph is constrained to be a projective tree rooted at x 0 = $: each word except $ has a single parent, and there are no cycles or crossing dependencies. y left (0) is taken to be empty, and y right (0) contains the sentence's single head. Let y (i) denote the subtree rooted at position i. The probability P (y (i) | x i , θ) of generating this subtree, given its head word x i , is defined recursively, as described in <ref type="figure" target="#fig_0">Fig. 1 (Eq. 2)</ref>.</p><p>The probability of the entire tree is given by <ref type="figure">Figure 1</ref>: The "dependency model with valence" recursive equation. first y (j) is a predicate defined to be true iff x j is the closest child (on either side) to its parent x i . The probability of the tree p(x, y | θ) = P (y (0) | $, θ).</p><formula xml:id="formula_1">p(x, y | θ) = P (y (0) | $, θ). The θ are the multi- nomial distributions θ s (· | ·, ·, ·) and θ c (· | ·, ·). To P (y (i) | x i , θ) = D∈{left,right} θ s (stop | x i , D, [y D (i) = ∅])<label>(2)</label></formula><formula xml:id="formula_2">× j∈y D (i) θ s (¬stop | x i , D, first y (j)) × θ c (x j | x i , D) × P (y (j) | x j , θ)</formula><p>follow the general setting of Eq. 1, we index these distributions as θ 1 , ..., θ K . <ref type="bibr" target="#b13">Headden et al. (2009)</ref> extended DMV so that the distributions θ c condition on the valence as well, with smoothing, and showed significant improvements for short sentences. Our experiments found that these improvements do not hold on longer sentences. Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning DMV</head><p>Klein and Manning (2004) learned the DMV probabilities θ from a corpus of part-of-speech-tagged sentences using the EM algorithm. EM manipulates θ to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y). The likelihood surface is not globally concave, so EM only locally optimizes the surface. Klein and Manning's initialization, though reasonable and language-independent, was an important factor in performance.</p><p>Various alternatives to EM were explored by <ref type="bibr" target="#b24">Smith (2006)</ref>, achieving substantially more accurate parsing models by altering the objective function. Smith's methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters. In this paper, we consider only fully unsupervised methods, though we the Bayesian ideas explored here might be merged with the biasing approaches of Smith <ref type="formula" target="#formula_1">(2006)</ref> for further benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parameter Tying in the Bayesian Setting</head><p>As stated above, θ comprises a collection of multinomials that weights the grammar. Taking the Bayesian approach, we wish to place a prior on those multinomials, and the Dirichlet family is a natural candidate for such a prior because of its conjugacy, which makes inference algorithms easier to derive. For example, if we make a "mean-field assumption," with respect to hidden structure and weights, the variational algorithm for approximately inferring the distribution over θ and trees y resembles the traditional EM algorithm very closely <ref type="bibr" target="#b17">(Johnson, 2007)</ref>. In fact, variational inference in this case takes an action similar to smoothing the counts using the exp-Ψ function during the E-step. Variational inference can be embedded in an empirical Bayes setting, in which we optimize the variational bound with respect to the hyperparameters as well, repeating the process until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Logistic Normal Distributions</head><p>While Dirichlet priors over grammar probabilities make learning algorithms easy, they are limiting. In particular, as noted by <ref type="bibr" target="#b1">Blei and Lafferty (2006)</ref>, there is no explicit flexible way for the Dirichlet's parameters to encode beliefs about covariance between the probabilities of two events. To illustrate this point, we describe how a multinomial θ of dimension d is generated from a Dirichlet distribution with parameters α = α 1 , ..., α d :</p><formula xml:id="formula_3">1. Generate η j ∼ Γ(α j , 1) independently for j ∈ {1, ..., d}. 2. θ j ← η j / i η i .</formula><p>where Γ(α, 1) is a Gamma distribution with shape α and scale 1. Correlation among θ i and θ j , i = j, cannot be modeled directly, only through the normalization in step 2. In contrast, LN distributions <ref type="bibr" target="#b0">(Aitchison, 1986)</ref> provide a natural way to model such correlation. The LN draws a multinomial θ as follows: </p><formula xml:id="formula_4">1. Generate η ∼ Normal(µ, Σ). 2. θ j ← exp(η j )/ i exp(η i ). I 1 = {1:2, 3:6, 7:9} = { I 1,1 , I 1,2 , I 1,L1 } I 2 = {1:2, 3:6} = { I 2,1 , I 2,L2 } I 3 = {1:4, 5:7} = { I 3,1 , I 3,L3 } I N = {1:2} = { I 4,L4 } J 1 J 2 J K            partition struct. S η 1 = η 1,1 , η 1,2 , η 1,3 , η 1,4 , η 1,5 , η 1,6 , η 1,7 , η 1,8 , η 1,,1 ∼ Normal(µ 1 , Σ 1 ) η 2 = η 2,1 , η 2,2 , η 2,3 , η 2,4 , η 2,5 , η 2,,2 ∼ Normal(µ 2 , Σ 2 ) η 3 = η 3,1 , η 3,2 , η 3,3 , η 3,4 , η 3,5 , η 3,6 , η 3,,3 ∼ Normal(µ 3 , Σ 3 ) η 4 = η 4,1 , η 4,,4 ∼ Normal(µ 4 , Σ 4 )        sample η ˜ η 1 = 1 3 η 1,1 + η 2,1 + η 4,1 , η 1,2 + η 2,2 + η 4,2 ˜ η 2 = 1 3 η 1,3 + η 2,3 + η 3,1 , η 1,4 + η 2,4 + η 3,2 , η 1,5 + η 2,5 + η 3,3 , η 1,6 + η 2,6 + η 3,4 ˜ η 3 = 1 2 η 1,7 + η 3,5 , η 1,8 + η 3,6 , η 1,9 + η 3,7    combine η θ 1 = (exp˜ηexp˜ exp˜η 1 ) N1 i =1 exp˜ηexp˜ exp˜η 1,i θ 2 = (exp˜ηexp˜ exp˜η 2 ) N2 i =1 exp˜ηexp˜ exp˜η 2,i θ 3 = (exp˜ηexp˜ exp˜η 3 ) N3 i =1 exp˜ηexp˜ exp˜η 3,i          softmax</formula><formula xml:id="formula_5">K = 3 multinomials; L 1 = 3, L 2 = 2, L 3 = 2, L 4 = 1, 1 = 9, 2 = 6, 3 = 7, 4 = 2, N 1 = 2, N 2 = 4</formula><p>, and N 3 = 3. This figure is best viewed in color.</p><p>Blei and Lafferty <ref type="formula" target="#formula_1">(2006)</ref>  In that work, we obtained improvements even without specifying exactly which grammar probabilities covaried. While empirical Bayes learning permits these covariances to be discovered without supervision, we found that by initializing the covariance to encode beliefs about which grammar probabilities should covary, further improvements were possible. Specifically, we grouped the Penn Treebank part-of-speech tags into coarse groups based on the treebank annotation guidelines and biased the initial covariance matrix for each child distribution θ c (· | ·, ·) so that the probabilities of child tags from the same coarse group covaried. For example, the probability that a past-tense verb (VBD) has a singular noun (NN) as a right child may be correlated with the probability that it has a plural noun (NNS) as a right child. Hence linguistic knowledge-specifically, a coarse grouping of word classes-can be encoded in the prior.</p><p>A per-distribution LN distribution only permits probabilities within a multinomial to covary. We will generalize the LN to permit covariance among any probabilities in θ, throughout the model. For example, the probability of a past-tense verb (VBD) having a noun as a right child might correlate with the probability that other kinds of verbs (VBZ, VBN, etc.) have a noun as a right child.</p><p>The partitioned logistic normal distribution (PLN) is a generalization of the LN distribution that takes the first step towards our goal <ref type="bibr" target="#b0">(Aitchison, 1986)</ref>. Generating from PLN involves drawing a random vector from a multivariate normal distribution, but the logistic transformation is applied to different parts of the vector, leading to sampled multinomial distributions of the required lengths from different probability simplices. This is in principle what is required for arbitrary covariance between grammar probabilities, except that DMV has O(t 2 ) weights for a part-of-speech vocabulary of size t, requiring a very large multivariate normal distribution with O(t 4 ) covariance parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shared Logistic Normal Distributions</head><p>To solve this problem, we suggest a refinement of the class of PLN distributions. Instead of using a single normal vector for all of the multinomials, we use several normal vectors, partition each one and then recombine parts which correspond to the same multinomial, as a mixture. Next, we apply the logisitic transformation on the mixed vectors (each of which is normally distributed as well). <ref type="figure" target="#fig_0">Fig. 2</ref> gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</p><p>We now formalize this notion. For a natural number N , we denote by 1:N the set {1, ..., N }. For a vector in v ∈ R N and a set I ⊆ 1:N , we denote by v I to be the vector created from v by using the coordinates in I. Recall that K is the number of multinomials in the probabilistic grammar, and N k is the number of events in the kth multinomial. Definition 1. We define a shared logistic normal distribution with N "experts" over a collection of K multinomial distributions. Let η n ∼ Normal(µ n , Σ n ) be a set of multivariate normal variables for n ∈ 1:N , where the length of η n is denoted n . Let I n = {I n,j } Ln j=1 be a partition of 1: n into L n sets, such that ∪ Ln j=1 I n,j = 1: n and I n,j ∩ I n,j = ∅ for j = j . Let J k for k ∈ 1:K be a collection of (disjoint) subsets of {I n,j | n ∈ 1:N, j ∈ 1: n , |I n,j | = N k }, such that all sets in J k are of the same size, N k . Let˜ηLet˜ Let˜η k = 1 |J k | I n,j ∈J k η n,I n,j , and θ k,i = exp(˜ η k,i ) i exp(˜ η k,i ) . We then say θ distributes according to the shared logistic normal distribution with partition structure S = ({I n } N n=1 , {J k } K k=1 ) and normal experts {(µ n , Σ n )} N n=1 and denote it by θ ∼ SLN(µ, Σ, S).</p><p>The partitioned LN distribution in Aitchison (1986) can be formulated as a shared LN distribution where N = 1. The LN collection used by <ref type="bibr" target="#b6">Cohen et al. (2008)</ref> is the special case where N = K, each L n = 1, each k = N k , and each J k = {I k,1 }.</p><p>The covariance among arbitrary θ k,i is not defined directly; it is implied by the definition of the normal experts η n,I n,j , for each I n,j ∈ J k . We note that a SLN can be represented as a PLN by relying on the distributivity of the covariance operator, and merging all the partition structure into one (perhaps sparse) covariance matrix. However, if we are interested in keeping a factored structure on the covariance matrices which generate the grammar weights, we cannot represent every SLN as a PLN.</p><p>It is convenient to think of each η i,j as a weight associated with a unique event's probability, a certain outcome of a certain multinomial in the probabilistic grammar. By letting different η i,j covary with each other, we loosen the relationships among θ k,j and permit the model-at least in principleto learn patterns from the data. Def. 1 also implies that we multiply several multinomials together in a product-of-experts style <ref type="bibr" target="#b14">(Hinton, 1999)</ref>, because the exponential of a mixture of normals becomes a product of (unnormalized) probabilities.</p><p>Our extension to the model in <ref type="bibr" target="#b6">Cohen et al. (2008)</ref> follows naturally after we have defined the shared LN distribution. The generative story for this model is as follows:</p><p>1. Generate θ ∼ SLN(µ, Σ, S), where θ is a collection of vectors θ k , k = 1, ..., K.</p><p>2. Generate x and y from p(x, y | θ) (i.e., sample from the probabilistic grammar).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>In this work, the partition structure S is known, the sentences x are observed, the trees y and the grammar weights θ are hidden, and the parameters of the shared LN distribution µ and Σ are learned. <ref type="bibr">2</ref> Our inference algorithm aims to find the posterior over the grammar probabilities θ and the hidden structures (grammar trees y). To do that, we use variational approximation techniques ( <ref type="bibr" target="#b18">Jordan et al., 1999</ref>), which treat the problem of finding the posterior as an optimization problem aimed to find the best approximation q(θ, y) of the posterior p(θ, y | x, µ, Σ, S). The posterior q needs to be constrained to be within a family of tractable and manageable distributions, yet rich enough to represent good approximations of the true posterior. "Best approximation" is defined as the KL divergence between q(θ, y) and p(θ, y | x, µ, Σ, S).</p><p>Our variational inference algorithm uses a meanfield assumption: q(θ, y) = q(θ)q(y). The distribution q(θ) is assumed to be a LN distribution with</p><formula xml:id="formula_6">log p(x | µ, Σ, S) ≥ N n=1 E q [log p(η k | µ k , Σ k )] + K k=1 N k i=1˜f i=1˜ i=1˜f k,i ˜ ψ k,i + H(q) B (3) ˜ f k,i y q(y)f k,i (x, y)<label>(4)</label></formula><formula xml:id="formula_7">˜ ψ k,i ˜ µ C k,i − log˜ζlog˜ log˜ζ k + 1 − 1 ˜ ζ k N k i =1 exp˜µ exp˜ exp˜µ C k,i + (˜ σ C k,i ) 2 2<label>(5)</label></formula><formula xml:id="formula_8">˜ µ C k 1 |J k | I r,j ∈J k ˜ µ r,I r,j<label>(6)</label></formula><formula xml:id="formula_9">(˜ σ C k ) 2 1 |J k | 2 I r,j ∈J k ˜ σ 2</formula><p>r,I r,j (7) <ref type="figure">Figure 3</ref>: Variational inference bound. Eq. 3 is the bound itself, using notation defined in Eqs. 4-7 for clarity. Eq. 4 defines expected counts of the grammar events under the variational distribution q(y), calculated using dynamic programming. Eq. 5 describes the weights for the weighted grammar defined by q(y). Eq. 6 and Eq. 7 describe the mean and the variance, respectively, for the multivariate normal eventually used with the weighted grammar. These values are based on the parameterization of q(θ) by˜µby˜ by˜µ i,j and˜σand˜ and˜σ 2 i,j . An additional set of variational parameters is˜ζis˜ is˜ζ k , which helps resolve the non-conjugacy of the LN distribution through a first order Taylor approximation.</p><p>all off-diagonal covariances fixed at zero (i.e., the variational parameters consist of a single meañ µ k,i and a single variance˜σvariance˜ variance˜σ 2 k,i for each θ k,i ). There is an additional variational parameter, ˜ ζ k per multinomial, which is the result of an additional variational approximation because of the lack of conjugacy of the LN distribution to the multinomial distribution. The distribution q(y) is assumed to be defined by a DMV with unnormalized probabilities˜ψprobabilities˜ probabilities˜ψ.</p><p>Inference optimizes the bound B given in <ref type="figure">Fig. 3</ref> (Eq. 3) with respect to the variational parameters. Our variational inference algorithm is derived similarly to that of <ref type="bibr" target="#b6">Cohen et al. (2008)</ref>. Because we wish to learn the values of µ and Σ, we embed variational inference as the E step within a variational EM algorithm, shown schematically in <ref type="figure" target="#fig_1">Fig. 4</ref>. In our experiments, we use this variational EM algorithm on a training set, and then use the normal experts' means to get a point estimate for θ, the grammar weights. This is called empirical Bayesian estimation. Our approach differs from maximum a posteriori (MAP) estimation, since we re-estimate the parameters of the normal experts. Exact MAP estimation is probably not feasible; a variational algorithm like ours might be applied, though better performance is expected from adjusting the SLN to fit the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank <ref type="bibr" target="#b22">(Marcus et al., 1993</ref>) and the Chinese treebank ( <ref type="bibr" target="#b29">Xue et al., 2004</ref>). In both cases, following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure. For English, we train on §2-21, tune on §22 (without using annotated data), and report final results on §23. For Chinese, we train on §1-270, use §301-1151 for development and report testing results on §271-300. <ref type="bibr">3</ref> To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus. This performance measure is also known as attachment accuracy. We considered two parsing methods after extracting a point estimate for the grammar: the most probable "Viterbi" parse (argmax y p(y | x, θ)) and the minimum Bayes risk (MBR) parse (argmin y E p(y |x,θ) [(y; x, y )]) with dependency attachment error as the loss function <ref type="bibr" target="#b11">(Goodman, 1996)</ref>. Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Nouns, Verbs, and Adjectives</head><p>In this paper, we use a few simple heuristics to decide which partition structure S to use. Our heuris-Input: initial parameters µ (0) , Σ (0) , partition structure S, observed data x, number of iterations T Output: learned parameters µ, Σ t ← 1 ; while t ≤ T do E-step (for = 1, ..., M ) do: repeat optimize B w.r. tics rely mainly on the centrality of content words: nouns, verbs, and adjectives. For example, in the English treebank, the most common attachment errors (with the LN prior from <ref type="bibr" target="#b6">Cohen et al., 2008</ref>) happen with a noun (25.9%) or a verb (16.9%) parent. In the Chinese treebank, the most common attachment errors happen with noun (36.0%) and verb (21.2%) parents as well. The errors being governed by such attachments are the direct result of nouns and verbs being the most common parents in these data sets.</p><p>Following this observation, we compare four different settings in our experiments (all SLN settings include one normal expert for each multinomial on its own, equivalent to the regular LN setting from Cohen et al.):</p><p>• TIEV: We add normal experts that tie all probabilities corresponding to a verbal parent (any parent, using the coarse tags of <ref type="bibr" target="#b6">Cohen et al., 2008)</ref>. Let V be the set of part-of-speech tags which belong to the verb category. For each direction D (left or right), the set of multinomials of the form θ c (· | v, D), for v ∈ V , all share a normal expert. For each direction D and each boolean value B of the predicate first y (·), the set of multinomials θ s (· | x, D, v), for v ∈ V share a normal expert.</p><p>• TIEN: This is the same as TIEV, only for nominal parents.</p><p>• TIEV&amp;N: Tie both verbs and nouns (in separate partitions). This is equivalent to taking the union of the partition structures of the above two settings.</p><p>• TIEA: This is the same as TIEV, only for adjectival parents.</p><p>Since inference for a model with parameter tying can be computationally intensive, we first run the inference algorithm without parameter tying, and then add parameter tying to the rest of the inference algorithm's execution until convergence.</p><p>Initialization is important for the inference algorithm, because the variational bound is a nonconcave function. For the expected values of the normal experts, we use the initializer from <ref type="bibr" target="#b19">Klein and Manning (2004)</ref>. For the covariance matrices, we follow the setting in <ref type="bibr" target="#b6">Cohen et al. (2008)</ref> in our experiments also described in §3.1. For each treebank, we divide the tags into twelve disjoint tag families. <ref type="bibr">4</ref> The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0.5 between tags which belong to the same family, and 0 otherwise. This initializer has been shown to be more successful than an identity covariance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monolingual Experiments</head><p>We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.</p><p>The attachment accuracy for this set of experiments is described in <ref type="table">Table 1</ref>. The baselines include right attachment (where each word is attached to the word to its right), MLE via EM ( <ref type="bibr">Klein and Man- ning, 2004</ref>), and empirical Bayes with Dirichlet and LN priors <ref type="bibr" target="#b6">(Cohen et al., 2008</ref>). We also include a "ceiling" (DMV trained using supervised MLE from the training sentences' trees). For English, we see that tying nouns, verbs or adjectives improves performance compared to the LN baseline. Tying both nouns and verbs improves performance a bit more.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bilingual Experiments</head><p>Leveraging information from one language for the task of disambiguating another language has received considerable attention <ref type="bibr" target="#b8">(Dagan, 1991;</ref><ref type="bibr" target="#b23">Smith and Smith, 2004;</ref><ref type="bibr" target="#b25">Snyder and Barzilay, 2008;</ref><ref type="bibr">Bur- kett and Klein, 2008)</ref>. Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages. <ref type="bibr">5</ref> Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar-5 <ref type="bibr" target="#b12">Haghighi et al. (2008)</ref> presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. ing information between those two models is done by softly tying grammar weights in the two hidden grammars.</p><p>We first merge the models for English and Chinese by taking a union of the multinomial families of each and the corresponding prior parameters. We then add a normal expert that ties between the parts of speech in the respective partition structures for both grammars together. Parts of speech are matched through the single coarse tagset (footnote 4). For example, with TIEV, let V = V Eng ∪ V Chi be the set of part-of-speech tags which belong to the verb category for either treebank. Then, we tie parameters for all part-of-speech tags in V . We tested this joint model for each of TIEV, TIEN, TIEV&amp;N, and TIEA. After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately. <ref type="table">Table 1</ref> includes the results for these experiments. The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&amp;N. Performance with Chinese is also the highest in the bilingual setting, with TIEA and TIEV&amp;N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models <ref type="bibr" target="#b26">(Teh, 2006</ref>) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results. We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample K = 3 multinomials; L 1 = 3, L 2 = 2, L 3 = 2, L 4 = 1, 1 = 9, 2 = 6, 3 = 7, 4 = 2, N 1 = 2, N 2 = 4, and N 3 = 3. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Main details of the variational inference EM algorithm with empirical Bayes estimation of µ and Σ. B is the bound defined in Fig. 3 (Eq. 3). N is the number of normal experts for the SLN distribution defining the prior. M is the number of training examples. The full algorithm is given in Cohen and Smith (2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>attachment</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> Although the task, underlying model, and weights being tied were different, Eisner (2002) also showed evidence for the efficacy of parameter tying in grammar learning.</note>

			<note place="foot" n="2"> In future work, we might aim to learn S.</note>

			<note place="foot" n="3"> Unsupervised training for these datasets can be costly, and requires iteratively running a cubic-time inside-outside dynamic programming algorithm, so we follow Klein and Manning (2004) in restricting the training set to sentences of ten or fewer words in length. Short sentences are also less structurally ambiguous and may therefore be easier to learn from.</note>

			<note place="foot" n="4"> These are simply coarser tags: adjective, adverb, conjunction, foreign word, interjection, noun, number, particle, preposition, pronoun, proper noun, verb.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSF IIS-0836431. The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Statistical Analysis of Compositional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aitchison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two languages are better than one (for syntactic parsing)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarse-to-fine nbest parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inference for probabilistic grammars with shared logistic normal distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two languages are more informative than one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformational priors over grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The infinite tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Products of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICANN</title>
		<meeting>of ICANN</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian inference for PCFGs via Markov chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Why doesn&apos;t EM find good HMM POS-taggers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Corpus-based induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variational Bayesian grammar induction for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICGI</title>
		<meeting>of ICGI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The infinite PCFG using hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilingual parsing with factored estimation: Using English to parse Korean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual learning for morphological segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian language model based on Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING-ACL</title>
		<meeting>of COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? a quasi-synchronous grammar for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Ling</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="404" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
