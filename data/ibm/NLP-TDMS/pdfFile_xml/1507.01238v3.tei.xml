<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subspace clustering methods based on 1 , 2 or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, 1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, 2 and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many computer vision applications, such as motion segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28]</ref>, hand written digit clustering <ref type="bibr" target="#b40">[41]</ref> and face clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>, data from different classes can be well approximated by a union of low dimensional subspaces. In these scenarios, the task is to partition the data according to the membership of data points to subspaces.</p><p>More formally, given a set of points X = {x j ∈ R D } N j=1 lying in an unknown number n of subspaces {S i } n i=1 of unknown dimensions {d i } n i=1 , subspace clustering is the problem of clustering the data into groups such that each group contains only data points from the same subspace. This problem has received great attention in the past decade and many subspace clustering algorithms have been developed, including iterative, algebraic, statistical, and spectral clustering based methods (see <ref type="bibr" target="#b32">[33]</ref> for a review). Sparse and Low Rank Methods. Among existing tech-niques, methods based on applying spectral clustering to an affinity matrix obtained by solving an optimization problem that incorporates 1 , 2 or nuclear norm regularization have become extremely popular due to their simplicity, theoretical correctness, and empirical success. These methods are based on the so-called self-expressiveness property of data lying in a union of subspaces, originally proposed in <ref type="bibr" target="#b12">[13]</ref>. This property states that each point in a union of subspaces can be written as a linear combination of other data points in the subspaces. That is,</p><p>x j = Xc j and c jj = 0, or equivalently X = XC and diag(C) = 0,</p><p>where X = x 1 , . . . , x N ∈ R D×N is the data matrix and C = c 1 , . . . , c N ∈ R N ×N is the matrix of coefficients. While (1) may not have a unique solution for C, there exist solutions whose entries are such that if c ij = 0, then x i is in the same subspace as x j . For example, a point x j ∈ S i can always be written as a linear combination of d i other points in S i . Such solutions are called subspace preserving since they preserve the clustering of the subspaces. Given a subspace preserving C, one can build an affinity matrix W between every pair of points x i and x j as w ij = |c ij |+|c ji |, and apply spectral clustering <ref type="bibr" target="#b35">[36]</ref> to W to cluster the data.</p><p>To find a subspace preserving C, existing methods regularize C with a norm · , and solve a problem of the form:</p><formula xml:id="formula_1">C * = arg min C C s.t. X = XC, diag(C) = 0. (2)</formula><p>For instance, the sparse subspace clustering (SSC) algorithm <ref type="bibr" target="#b12">[13]</ref> uses the 1 norm to encourage the sparsity of C. Prior work has shown that SSC gives a subspace-preserving solution if the subspaces are independent <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, or if the data from different subspaces satisfy certain separation conditions and data from the same subspace are well spread out <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b29">30]</ref>. Similar results exist for cases where data is corrupted by noise <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31]</ref> and outliers <ref type="bibr" target="#b29">[30]</ref>. Other selfexpressiveness based methods use different regularizations on the coefficient matrix C. Least squares regression (LSR) <ref type="bibr" target="#b24">[25]</ref> uses 2 regularization on C. Low rank representation (LRR) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> and low rank subspace clustering (LRSC) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref> use nuclear norm minimization to encourage C to be low-rank. Based on these, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39</ref>] study regularizations that are a mixture of 1 and 2 , and <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref> propose regularizations that are a blend of 1 and the nuclear norm.</p><p>The advantage of 2 regularized LSR and nuclear norm regularized LRR and LRSC over sparsity regularized SSC is that the solution for C can be computed in closed form from the SVD of the (noiseless) data matrix X, thus they are computationally more attractive. However, the resulting C is subspace preserving only when subspaces are independent and the data is uncorrupted. Thus, there is a need for methods that both guarantee a subspace-preserving affinity under broad conditions and are computationally efficient.</p><p>Paper Contributions. In this work we study the selfexpressiveness based subspace clustering method that uses orthogonal matching pursuit (OMP) to find a sparse representation in lieu of the 1 -based basis pursuit (BP) method. The method is termed SSC-OMP, for its kinship to the original SSC, which is referred to as SSC-BP in this paper.</p><p>The main contributions of this paper are to find theoretical conditions under which the affinity produced by SSC-OMP is subspace preserving and to demonstrate its efficiency for large scale problems. Specifically, we show that:</p><p>1. When the subspaces and the data are deterministic, SSC-OMP gives a subspace-preserving C if the subspaces are independent, or else if the subspaces are sufficiently separated and the data is well distributed.</p><p>2. When the subspaces and data are drawn uniformly at random, SSC-OMP gives a subspace-preserving C if the dimensions of the subspaces are sufficiently small relative to the ambient dimension by a factor controlled by the sample density and the number of subspaces.</p><p>3. SSC-OMP is orders of magnitude faster than the original SSC-BP, and can handle up to 100,000 data points.</p><p>Related work. It is worth noting that the idea of using OMP for SSC had already been considered in <ref type="bibr" target="#b11">[12]</ref>. The core contribution of our work is to provide much weaker yet more succinct and interpretable conditions for the affinity to be subspace preserving in the case of arbitrary subspaces. In particular, our conditions are naturally related to those for SSC-BP, which reveal insights about the relationship between these two sparsity-based subspace clustering methods. Moreover, our experimental results provide a much more detailed evaluation of the behavior of SSC-OMP for large-scale problems. It is also worth noting that conditions under which OMP gives a subspace-preserving representation had also been studied in <ref type="bibr" target="#b39">[40]</ref>. Our paper presents a much more comprehensive study of OMP for the subspace clustering problem, by providing results under deterministic independent, deterministic arbitrary and random subspace models. In particular, our result for deterministic arbitrary models is much stronger than the main result in <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SSC by Orthogonal Matching Pursuit</head><p>The SSC algorithm approaches the subspace clustering problem by finding a sparse representation of each point in terms of other data points. Since each point in S i can be expressed in terms of at most d i N other points in S i , such a sparse representation always exists. In principle, we can find it by solving the following optimization problem:</p><formula xml:id="formula_2">c * j = arg min cj c j 0 s.t. x j = Xc j , c jj = 0,<label>(3)</label></formula><p>where c 0 counts the number of nonzero entries in c.</p><p>Since this problem is NP hard, the SSC method in <ref type="bibr" target="#b12">[13]</ref> relaxes this problem and solves the following 1 problem:</p><formula xml:id="formula_3">c * j = arg min cj c j 1 s.t. x j = Xc j , c jj = 0. (4)</formula><p>Since this problem is called the basis pursuit (BP) problem, we refer to the SSC algorithm in <ref type="bibr" target="#b12">[13]</ref> as SSC-BP. The optimization problems <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula">(4)</ref> have been studied extensively in the compressed sensing community, see, e.g., the tutorials <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, and it is well known that, under certain conditions on the dictionary X, their solutions are the same. However, results from compressed sensing do not apply to the subspace clustering problem because when the columns of X lie in a union of subspaces the solution for C need not be unique (see Section 3 for more details). This has motivated extensive research on the conditions under which the solutions of (3) or (4) are useful for subspace clustering.</p><p>It is shown in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> that when the subspaces are either independent or disjoint, and the data are noise free and well distributed, both (3) and (4) provide a sparse representation c j that is subspace preserving, as defined next.</p><p>Definition 1 (Subspace-preserving representation). A representation c ∈ R N of a point x ∈ S i in terms of the dictionary X = x 1 , . . . , x N is called subspace preserving if its nonzero entries correspond to points in S i , i.e. ∀j = 1, . . . , N, c j = 0 =⇒ x j ∈ S i .</p><p>In practice, however, solving N 1 -minimization problems over N variables may be prohibitive when N is large. As an alternative, consider the following program:</p><formula xml:id="formula_5">c * j = arg min cj x j − Xc j 2 2 s.t. c j 0 ≤ k, c jj = 0. (6)</formula><p>It is shown in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b10">11]</ref> that, under certain conditions, this problem can be solved using the orthogonal matching pursuit (OMP) algorithm <ref type="bibr" target="#b26">[27]</ref> (Algorithm 1). OMP solves the problem min c Ac − b 2 2 s.t. c 0 ≤ k greedily by selecting one column of A = a 1 , . . . , a M at a time (the one that maximizes the absolute value of the dot product with the residual in line 3) and computing the coefficients for the selected columns until k columns are selected. For subspace clustering purposes, the vector c * j ∈ R N (the jth column of</p><formula xml:id="formula_6">C * ∈ R N ×N ), is computed as OMP(X −j , x j ) ∈ R N −1 Algorithm 1 : Orthogonal Matching Pursuit (OMP) Input: A = [a 1 , . . . , a M ] ∈ R m×M , b ∈ R m , k max , .</formula><p>1: Initialize k = 0, residual q 0 = b, support set T 0 = ∅. 2: while k &lt; k max and q k 2 &gt; do 3:</p><formula xml:id="formula_7">T k+1 = T k {i * }, where i * = arg max i=1,...,M |a T i q k | 1 . 4: q k+1 = (I − P T k+1 )b,</formula><p>where P T k+1 is the projection onto the span of the vectors {a j , j ∈ T k+1 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>k ← k + 1.  with a zero inserted in its jth entry, where X −j is the data matrix with the jth column removed. After C * is computed, the segmentation of the data is found by applying spectral clustering to the affinity matrix W = |C * | + |C * | as done in SSC-BP. The procedure is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical Analysis of SSC-OMP</head><p>OMP has been shown to be effective for sparse recovery, with the advantage over BP that it admits simple, fast implementations. However, note that existing conditions for the correctness of OMP for sparse recovery are too strong for the subspace clustering problem. In particular, note that the matrix X need not satisfy the mutual incoherence <ref type="bibr" target="#b31">[32]</ref> or restricted isometry properties <ref type="bibr" target="#b10">[11]</ref>, as two points in a subspace could be arbitrarily close to each other. More importantly, these conditions are not applicable here because our goal is not to recover a unique sparse solution. In fact, the sparse solution is not unique since any d i linearly independent points from S i can represent a point x j ∈ S i . Therefore, there is a need to find conditions under which the output of OMP (which need not coincide with the solution of <ref type="bibr" target="#b5">(6)</ref> or <ref type="formula" target="#formula_2">(3)</ref>) is guaranteed to be subspace preserving.</p><p>This section is devoted to studying sufficient conditions under which SSC-OMP gives a subspace-preserving representation. Our analysis assumes that the data is noiseless. The termination parameters of Algorithm 1 are = 0 and k max large enough (e.g., k max = M ). We also assume that the columns of X are normalized to unit 2 norm. To make our results consistent with state-of-the-art results, we first study the case where the subspaces are deterministic, including both independent subspaces as well as arbitrary subspaces. We then study the case where both the subspaces and the data points are drawn at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Independent Deterministic Subspace Model</head><p>We first consider the case where the subspaces are fixed, the data points are fixed, and the subspaces are independent.</p><formula xml:id="formula_8">Definition 2. A collection of subspaces {S i } n i=1 is called independent if dim i S i = i dim(S i ), where i S i is defined as the subspace { i x i : x i ∈ S i }.</formula><p>Notice that two subspaces are independent if and only if they are disjoint, i.e., if they intersect only at the origin. However, pairwise disjoint subspaces need not be independent, e.g., three lines in R 2 are disjoint but not independent. Notice also that any subset of a set of independent subspaces is also independent. Therefore, any two subspaces in a set of independent subspaces are independent and hence disjoint. In particular, this implies that if {S i } n i=1 are independent, then S i and S (−i) := m =i S m are independent.</p><p>To establish conditions under which SSC-OMP gives a subspace-preserving affinity for independent subspaces, it is important to note that when computing OMP(X −j , x j ), the goal is to select other points in the same subspace as x j . The process for selecting these points occurs in step 3 of Algorithm 1, where the dot products between all points x m , m = j, and the current residual q k are computed and the point with the highest product (in absolute value) is chosen. Since in the first iteration the residual is q 0 = x j , we could immediately choose a point x m in another subspace whenever the dot product of x j with a point in another subspace is larger than the dot product of x j with points in its own subspace. What the following theorem shows is that, even though OMP may select points in the wrong subspaces as the iterations proceed, the coefficients associated to points in other subspaces will be zero at the end. Therefore, OMP (with = 0 and k max = N − 1) is guaranteed to find a subspace-preserving representation. Theorem 1. If the subspaces are independent, OMP gives a subspace-preserving representation of each data point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. [Sketch only]</head><p>Assume that x j ∈ S i . Since = 0 and k max is large, OMP gives an exact representation, i.e., x j = Xc j and c jj = 0. Thus, since S i and S (−i) are independent, the coefficients of data points in S (−i) must be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Arbitrary Deterministic Subspace Model</head><p>We will now consider a more general class of subspaces, which need not be independent or disjoint, and investigate conditions under which OMP gives a subspace-preserving representation. In the following, X i ∈ R D×Ni denotes the submatrix of X containing the points in the ith subspace; for any x j ∈ S i , X i −j ∈ R D×(Ni−1) denotes the matrix X i with the point x j removed; X i and X i −j denote respectively the set of vectors contained in the columns of X i and X i −j . Now, it is easy to see that a sufficient condition for OMP(X −j , x j ) to be subspace preserving is that for each k in step 3 of Algorithm 1, the point that maximizes the dot product lies in the same subspace as x j . Since q 0 = x j and q 1 is equal to x j minus the projection of x j onto the subspace spanned by the selected point, sayx, it follows that if x j ,x ∈ S i then q 1 ∈ S i . By a simple induction argument, it follows that if all the selected points are in S i , then so are the residuals {q k }. This suggests that the condition for OMP(X −j , x j ) to be subspace preserving must depend on the dot products between the data points and a subset of the set of residuals (the subset contained in the same subspace as x j ). This motivates the following definition and lemma. . The set of OMP residual directions associated with matrix X i −j and point x j ∈ S i is defined as:</p><formula xml:id="formula_9">W i j := {w = q q 2 : q ∈ Q(X i −j , x j ), q = 0}.<label>(7)</label></formula><p>The set of OMP residual directions associated with the data matrix X i is defined as</p><formula xml:id="formula_10">W i := j:xj ∈Si W i j . Lemma 1. OMP gives a subspace-preserving representa- tion for point x j ∈ S i in at most d i iterations if ∀w ∈ W i j max x∈ k =i X k |w x| &lt; max x∈X i \{xj } |w x|. (8)</formula><p>Proof. [Sketch only] By using an induction argument, it is easy to see that the condition in <ref type="bibr" target="#b7">(8)</ref> implies that the sequence of residuals of OMP(X −j , x j ) is the same as that of the fictitious problem OMP(X i −j , x j ). Hence, the output of OMP(X −j , x j ) is the same as that of OMP(X i −j , x j ), which is, by construction, subspace-preserving.</p><p>Intuitively, Lemma 1 tells us that if the dot product between the residual directions for subspace i and the data points in all other subspaces is smaller than the dot product between the residual directions for subspace i and all points in subspace i other than x j ∈ S i , then OMP gives a subspace-preserving representation. While such a condition is very intuitive from the perspective of OMP, it is not as intuitive from the perspective of subspace clustering as it does not rely on the geometry of the problem. Specifically, it does not directly depend on the relative configuration of the subspaces or the distribution of the data in the subspaces. In what follows, we derive conditions on the subspaces and the data that guarantee that the condition in (8) holds. Before doing so, we need some additional definitions.</p><p>Definition 4. The coherence between two sets of points of unit norm, X and Y, is defined as µ(X , Y) = max x∈X ,y∈Y | x, y |.</p><p>The coherence measures the degree of "similarity" between two sets of points. In our case, we can see that the left hand side of (8) is bounded above by the coherence between the sets W i and k =i X k . As per <ref type="bibr" target="#b7">(8)</ref>, this coherence should be small, which implies that data points from different subspaces should be sufficiently separated (in angle).</p><p>Definition 5. The inradius r(P) of a convex body P is the radius of the largest Euclidean ball inscribed in P.</p><p>As shown in Lemma 2, the right hand side of (8) is bounded below by r(</p><formula xml:id="formula_11">P i −j ), where P i −j := conv ± X i −j</formula><p>is the symmetrized convex hull of the points in the ith subspace other than x j , i.e., X i −j . Therefore, <ref type="bibr" target="#b7">(8)</ref> suggests that the minimum inradius r i := min j r(P i −j ) should be large, which means the points in S i should be well-distributed.</p><formula xml:id="formula_12">Lemma 2. Let x j ∈ S i . Then, for all w ∈ W i j , we have: max x∈ k =i X k |w x| ≤ max k:k =i µ(W i , X k ) ≤ max k:k =i µ(X i , X k )/r i ; max x∈X i \{xj } |w x| ≥ r(P i −j ) ≥ r i .<label>(9)</label></formula><p>Proof. The proof can be found in the Appendix.</p><p>Lemma 2 allows us to make the condition of Lemma 1 more interpretable, as stated in the following theorem.</p><formula xml:id="formula_13">Theorem 2. The output of OMP is subspace preserving if ∀i = 1, . . . , n, max k:k =i µ(W i , X k ) &lt; r i .<label>(10)</label></formula><p>Corollary 1. The output of OMP is subspace preserving if ∀i = 1, . . . , n, max</p><formula xml:id="formula_14">k:k =i µ(X i , X k ) &lt; r 2 i .<label>(11)</label></formula><p>Note that points in W i are all in subspace S i , as step 4 of OMP(A := X i −j , b := x j ) has b and P T k+1 b both in S i . The conditions <ref type="formula" target="#formula_0">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref> thus show that for each subspace S i , a set of points (i.e., X i or W i ) in S i should have low coherence with all points from other subspaces, and that points in X i should be uniformly located in S i to have a large inradius. This is in agreement with the intuition that points from different subspaces should be well separated, and points within a subspace should be well distributed.</p><p>For a comparison of Corollary 1 and Theorem 2, note that due to Lemma 2 condition (10) is tighter than condition (11), making Theorem 2 preferable. Yet Corollary 1 has the advantage that both sides of condition (11) depend directly on the data points in X , while condition (10) depends on the residual points in W i , making it algorithm specific.</p><p>Another important thing to notice is that conditions <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_0">(11)</ref> can be satisfied even if the subspaces are neither independent nor disjoint. For example, consider the case where S i S k = 0. Then, the coherence µ(W i , X k ) could still be small as long as no points in W i and X k are near the intersection of S i and S k . Actually, even this is too strong of an assumption since the intersection is a subspace. Thus, x ∈ X k , y ∈ W i could both be very close to the intersection yet have low coherence. The same argument also works for condition <ref type="bibr" target="#b10">(11)</ref>. Admittedly, under specific distributions of points, it is possible that there exists x ∈ X k and y ∈ W i that are arbitrarily close to each other when they are near the intersection. However, this worst case scenario is unlikely to happen if we consider a random model, as discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Arbitrary Random Subspace Model</head><p>This section considers the fully random union of subspaces model in <ref type="bibr" target="#b29">[30]</ref>, where the basis elements of each subspace are chosen uniformly at random from the unit sphere of the ambient space and the data points from each subspace are uniformly distributed on the unit sphere of that subspace. Theorem 3 shows that the sufficient condition in (10) holds true with high probability (i.e. the probability goes to 1 as the density of points grows to infinity) given some conditions on the subspace dimension d, the ambient space dimension D, the number of subspaces n and the number of data points per subspace.</p><p>Theorem 3. Assume a random union of subspaces model where all subspaces are of equal dimension d and the number of data points in each subspace is ρd + 1, where ρ &gt; 1 is the "density", so that the total number data points in all subspaces is N (n, ρ, d) = n(ρd + 1). The output of OMP is subspace preserving with probability p &gt; 1 − 2d</p><formula xml:id="formula_15">N (n,ρ,d) − N (n, ρ, d)e − √ ρd if d &lt; c 2 (ρ) log ρ 12 D log N (n, ρ, d) ,<label>(12)</label></formula><p>where c(ρ) &gt; 0 is a constant that depends only on ρ.</p><p>One interpretation of the condition in <ref type="formula" target="#formula_0">(12)</ref> is that the dimension d of the subspaces should be small relative to the ambient dimension D. It also shows that as the number of subspaces n increases, the factor log N (n, ρ, d) also increases, making the condition more difficult to be satisfied. In terms of the density ρ, it is shown in <ref type="bibr" target="#b29">[30]</ref> that there exists a ρ 0 such that c(ρ) = 1/ √ 8 when ρ &gt; ρ 0 . Then, it is easy to see that when ρ &gt; ρ 0 , the term that depends on ρ is log ρ log N (n,ρ,d) = log ρ log(n(ρd+1)) , which is a monotonically increasing function of ρ. This makes the condition easier to be satisfied as the density of points in the subspaces increases. Moreover, the probability of success is 1 − 2d N (n,ρ,d) − N (n, ρ, d)e − √ ρd , which is also an increasing function of ρ when ρ is greater than a threshold value. As a consequence, as the density of the points increases, the condition in Theorem 3 becomes easier to satisfy and the probability of success also increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Relationships with Other Methods</head><p>In this section we compare our results for SSC-OMP with those for other methods of the general form in <ref type="bibr" target="#b1">(2)</ref>. These methods include SSC-BP <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref>, which uses the 1 norm as a regularizer, LRR <ref type="bibr" target="#b21">[22]</ref> and LRSC <ref type="bibr" target="#b33">[34]</ref>, which use the nuclear norm, and LSR <ref type="bibr" target="#b24">[25]</ref> which uses the 2 norm. We also compare our results to those of <ref type="bibr" target="#b11">[12]</ref> for SSC-OMP. The comparison is in terms of whether the solutions given by these alternative algorithms are subspace-preserving. Independent Subspaces. Independence is a strong assumption on the union of subspaces. Under this assumption, a subspace has a trivial intersection with not only every other subspace, but also the union of all other subspaces. This case turns out to be especially easy for a large category of self-expressive subspace clustering methods <ref type="bibr" target="#b24">[25]</ref>, and SSC-BP, LRR, LRSC and LSR are all able to give subspacepreserving representations. Thus, in this easy case, the proposed method is as good as state-of-the-art methods. Arbitrary Subspaces. To the best of our knowledge, when the subspaces are not independent, there is no guarantee of correctness for LRR, LRSC and LSR. For SSC-BP, as shown in <ref type="bibr" target="#b29">[30]</ref>, the representation is subspace-preserving if ∀i = 1, . . . , n, max</p><formula xml:id="formula_16">k:k =i µ(V i , X k ) &lt; r i ,<label>(13)</label></formula><p>where V i is a set of N i dual directions associated with X i . When comparing <ref type="bibr" target="#b12">(13)</ref> with our result in condition <ref type="formula" target="#formula_0">(10)</ref>, we can see that the right hand sides are the same. However, the left hand sides are not directly comparable, as no general relationship is known between the sets V i and W i . Nonetheless, notice that the number of points in these two sets are not the same since card(V i ) = N i and card(W i ) = N i d i . Therefore, if we assume that the points in V i and W i are distributed uniformly at random on the unit sphere, then µ(W i , X k ) is expected to be larger than µ(V i , X k ), making the condition for SSC-OMP less likely to be satisfied than that for SSC-BP. Now, when comparing (13) with our condition in <ref type="formula" target="#formula_0">(11)</ref>, we see that the left hand sides are comparable under a random model where both V i and X i contain N i points. However, the right hand side is r 2 i , which is less than or equal to r i since the data are normalized and r i ≤ 1. This again makes the condition for SSC-OMP more difficult to hold than that for SSC-BP. However, this difference is expected to vanish for large scale problems, and SSC-OMP is computationally more efficient, as we will see in Section 5. Random Subspaces. For the random model, <ref type="bibr" target="#b29">[30]</ref> shows that SSC-BP gives a subspace-preserving representation with probability p &gt; 1 − .</p><p>If we compare this result with that of Theorem 3, we can see that the condition under which both methods succeed with high probability is exactly the same. The difference between them is that SSC-BP has a higher probability of success than SSC-OMP when d &gt; 1. However, it is easy to see that the difference in probability goes to zero as the density ρ goes to infinity. This means that the performance difference vanishes as the scale of the problem increases.</p><p>Other Results for SSC-OMP. Finally, we compare our results with those in <ref type="bibr" target="#b11">[12]</ref> for SSC-OMP. Define the principal angle between two subspaces S i and S k as:</p><formula xml:id="formula_18">θ * i,k = min x∈Si x 2 =1 min y∈S k y 2=1 arccos x, y .<label>(15)</label></formula><p>It is shown in <ref type="bibr" target="#b11">[12]</ref> that the output of SSC-OMP is subspacepreserving if for all i = 1, . . . , n,</p><formula xml:id="formula_19">max k:k =i µ(X i , X k ) &lt; r i − 2 1 − (r i ) 2 4 √ 12 max k:k =i cos θ * i,k . (16)</formula><p>The merit of this result is that it introduces the subspace angles in the condition, and satisfies the intuition that the algorithm is more likely to work if the subspaces are far apart from each other. However, the RHS of the condition shows an intricate relationship between the intra-class property r i and the inter-class property θ * i,k , which greatly complicates the interpretation of the condition. More importantly, as is shown in the Appendix, the condition is more restrictive than (10), which makes Theorem 2 a stronger result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first verify our theoretical results for SSC-OMP and compare them with those for SSC-BP by doing experiments on synthetic data using the random model. Specifically, we show that even if the subspaces are not independent, the solution of OMP is subspace-preserving with a probability that grows with the density of data points. Second, we test the performance of the proposed method on clustering images of handwritten digits and human faces, and conclude that SSC-OMP achieves the best trade off between accuracy and efficiency. Methods. We compare the performance of state-of-the-art spectral subspace clustering methods, including LRSC <ref type="bibr" target="#b33">[34]</ref>, SSC-BP <ref type="bibr" target="#b14">[15]</ref>, LSR <ref type="bibr" target="#b24">[25]</ref>, and spectral curvature clustering (SCC) <ref type="bibr" target="#b7">[8]</ref>. In real experiments, we use the code provided by the respective authors for computing the representation matrix C * , where the parameters are tuned to give the best clustering accuracy. We then apply the normalized spectral clustering in <ref type="bibr" target="#b35">[36]</ref> to the affinity |C * | + |C * |, except for SCC which has its own spectral clustering step. Metrics. We use two metrics to evaluate the degree to which the subspace-preserving property is satisfied. The first one is a direct measure of whether the solution is subspace preserving or not. However, for comparing with state of the art methods whose output is generally not subspace preserving, the second one measures how close the coefficients are from being subspace preserving.</p><p>-Percentage of subspace-preserving representations (p%): this is the percentage of points whose representations are subspace-preserving. Due to inexactness in the solvers, coefficients with absolute value less than 10 −3 are considered zero. A subspace-preserving solution gives p = 100.</p><p>-Subspace-preserving representation error (e%) <ref type="bibr" target="#b14">[15]</ref>: for each c j in (1), we compute the fraction of its 1 norm that comes from other subspaces and then average over all j, i.e., e = 100</p><formula xml:id="formula_20">N j (1− i (ω ij ·|c ij |)/ c j 1 ), where ω ij ∈ {0, 1}</formula><p>is the true affinity. A subspace-preserving C gives e = 0. Now, the performance of subspace clustering depends not only on the subspace-preserving property, but also the connectivity of the similarity graph, i.e., whether the data points in each cluster form a connected component of the graph.</p><p>-Connectivity (c): For an undirected graph with weights W ∈ R N ×N and degree matrix D = diag(W · 1), where 1 is the vector of all ones, we use the second smallest eigenvalue λ 2 of the normalized Laplacian L = I − D −1/2 W D −1/2 to measure the connectivity of the graph; λ 2 is in the range [0, n−1 n ] and is zero if and only if the graph is not connected <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref>. In our case, we compute the algebraic connectivity for each cluster, λ i 2 , and take the quantity c = min i λ i 2 as the measure of connectivity. Finally, we use the following two metrics to evaluate the performance of subspace clustering methods.</p><p>-Clustering accuracy (a%): this is the percentage of correctly labeled data points. It is computed by matching the estimated and true labels as a = max π 100 N ij Q est π(i)j Q true ij , where π is a permutation of the n groups, Q est and Q true are the estimated and ground-truth labeling of data, respectively, with their (i, j)th entry being equal to one if point j belongs to cluster i and zero otherwise.</p><p>-Running time (t): for each clustering task using R Matlab.</p><p>The reported numbers in all the experiments of this section are averages over 20 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Experiments</head><p>We randomly generate n = 5 subspaces each of dimension d = 6 in an ambient space of dimension D = 9. Each subspace contains N i = ρd sample points randomly generated on the unit sphere, where ρ is varied from 5 to 3,333, so that the number of points varies from 150 to 99,990. For SSC-OMP, we set in Algorithm 1 to be 10 −3 and k max to be d = 6. For SSC-BP we use the 1 -Magic solver. Due to the computational complexity, SSC-BP is run for ρ ≤ 200.</p><p>The subspace-preserving representation percentage and error are plotted in <ref type="figure" target="#fig_8">Figure 1(a) and 1(b)</ref>. Observe that the probability that SSC-OMP gives a subspace-preserving solution grows as the density of data point increases. When     comparing with SSC-BP, we can see that SSC-OMP is outperformed. This matches our analysis that the condition for SSC-OMP to give a subspace-preserving representation is stronger (i.e., is more difficult to be satisfied).</p><p>From a subspace clustering perspective, we are more interested in how well the method performs in terms of clustering accuracy, as well as how efficient the method is in terms of running time. These results are plotted in <ref type="figure" target="#fig_8">Figure  1</ref>(d) and 1(e), together with the connectivity 1(c). We first observe that SSC-OMP does not have as good a connectivity as SSC-BP. This could be partly due to the fact that it has fewer correct connections in the first place as shown by the subspace-preserving percentage. For clustering accuracy, SSC-OMP is also outperformed by SSC-BP. This comes at no surprise as the sparse representations produced by SSC-OMP are not as subspace-preserving or as well connected as those of SSC-BP. However, we observe that as the density of data points increases, the difference in clustering accuracy also decreases, and SSC-OMP seems to achieve arbitrarily good clustering accuracy for large N . Also, it is evident from <ref type="figure" target="#fig_8">Figure 1</ref>(e) that SSC-OMP is significantly faster: it is 3 to 4 orders of magnitude faster than SSC-BP when clustering 6,000 points. We conclude that as N increases, the difference in clustering accuracy between SSC-OMP and SSC-BP reduces, yet SSC-OMP is significantly faster, which makes it preferable for large-scale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Clustering Images of Handwritten Digits</head><p>In this experiment, we evaluate the performance of different subspace clustering methods on clustering images of handwritten digits. We use the MNIST dataset <ref type="bibr" target="#b19">[20]</ref>, which contains grey scale images of handwritten digits 0 − 9.</p><p>In each experiment, N i ∈ {50, 100, 200, 400, 600} randomly chosen images for each of the 10 digits are chosen. For each image, we compute a set of feature vectors using a scattering convolution network <ref type="bibr" target="#b5">[6]</ref>. The feature vector is a concatenation of coefficients in each layer of the network, and is translation invariant and deformation stable. Each feature vector is of size 3,472. The feature vectors for all images are then projected to dimension 500 using PCA. The subspace clustering techniques are then applied to the projected features. The results are reported in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The numbers show that both SSC-OMP and SSC-BP give a much smaller subspace-preserving representation error than all other methods, with SSC-BP being better than SSC-OMP. This is consistent with our theoretical analysis as there is no guarantee that LSR or LRSC give a subspacepreserving representation for non-independent subspaces, and SSC-BP has a higher probability of giving a subspacepreserving representation than SSC-OMP.</p><p>In terms of clustering accuracy, SSC-OMP is better than SSC-BP, which in turn outperforms LSR and LRSC, while Considering the running time of the methods, SSC-BP requires much more computation, especially when the number of points is large. Though SSC-OMP is an iterative method, its computation time is about twice that of LSR and LRSC, which have closed form solutions. This again qualifies the proposed method for large scale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Clustering Face Images with Varying Lighting</head><p>In this experiment, we evaluate the performance of different subspace clustering methods on the Extended Yale B dataset <ref type="bibr" target="#b17">[18]</ref>, which contains frontal face images of 38 individuals under 64 different illumination conditions, each of size 192 × 168. In this case, the data points are the original face images downsampled to 48 × 42 pixels. In each experiment, we randomly pick n ∈ {2, 10, 20, 30, 38} individuals and take all the images (under different illuminations) of them as the data to be clustered.</p><p>The clustering performance of different methods is reported in <ref type="table" target="#tab_1">Table 2</ref>. In terms of subspace-preserving recovery, we can observe a slightly better performance of SSC-BP over SSC-OMP in all cases. The other three methods have very large subspace-preserving representation errors especially when the number of subjects is n ≥ 10. In terms of clustering accuracy, all methods do fairly well when the number of clusters is 2 except for SCC, which is far worse than the others. As the number of subjects increases from 10 to 38, LSR and LRSC can only maintain an accuracy of about 60% and SCC is even worse, but SSC-OMP and SSC- BP maintain a reasonably good performance, although the accuracy also degrades gradually. We can see that SSC-BP performs slightly better when the number of subjects is 2 or 10, but SSC-OMP performs better when n &gt; 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We studied the sparse subspace clustering algorithm based on OMP. We derived theoretical conditions under which SSC-OMP is guaranteed to give a subspacepreserving representation. Our conditions are broader than those of state-of-the-art methods based on 2 or nuclear norm regularization, and slightly weaker than those of SSC-BP. Experiments on synthetic and real world datasets showed that SSC-OMP is much more accurate than state-ofthe-art methods based on 2 or nuclear norm regularization and about twice as slow. On the other hand, SSC-OMP is slightly less accurate than SSC-BP but orders of magnitude faster. Moreover, we are one of the few <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> that have demonstrated subspace clustering experiments on 100,000 points. Overall, SSC-OMP provided the best accuracy versus computation trade-off for large scale subspace clustering problems. We note that while the optimization algorithm for SSC-BP in <ref type="bibr" target="#b14">[15]</ref> is inefficient for large scale problems, our most recent work <ref type="bibr" target="#b38">[39]</ref> presents a scalable algorithm for elastic net based subspace clustering. A comparison with this work is left for future research.</p><p>In the appendices, we provide proofs for the theoretical results in the paper. We also provide the parameters of all the clustering methods studied in the handwritten digits and face image clustering experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>In Theorem 1 , we claim that the SSC-OMP gives subspace preserving representations if subspaces are independent. Here we provide the proof.</p><p>Theorem. If the subspaces are independent, OMP gives a subspace-preserving representation of every data point.</p><p>Proof. Consider a data point x j ∈ S i . We need to show that the output of OMP(X −j , x j ) is subspace-preserving. As an assumption, the termination parameters in OMP are set to be = 0 and k max = N − 1 (i.e., the total number of points in the dictionary X −j ). This means, in particular, that OMP always terminates with some iteration k * ≤ N −1 with q k * = 0, which can be seen to hold as follows. If the OMP algorithm computes q k = 0 for some k ≤ N − 2, then there is nothing to prove. Thus, to complete the proof, we suppose that q k = 0 for all 0 ≤ k ≤ N − 2, and proceed to prove that q N −1 = 0. In the OMP algorithm, the columns of X −j indexed by T k for any k are always linearly independent. This is evident from step 4 of Algorithm 1, as the residual vector q k is orthogonal to every column of X −j indexed by T k , thus when choosing a new entry to be added to T k in step 3 of Algorithm 1, points that are linearly dependent with the points indexed by T k would have zero inner product with q k , so would not be picked. Since all of the columns of X −j have been added by iteration N −1, we know that the columns of X −j are linearly independent and must contain at least d i linearly independent vectors from S i 1 . We conclude that q k * = q N −1 = 0 with k * = N − 1, as claimed. In light of this result and denoting T * := T k * , it follows from q k * = 0 that P T * · x j = x j by line 4 of Algorithm 1, so that x j is in the range of matrix X T * , which denotes the columns of X −j indexed by T * .</p><p>As a consequence of the previous paragraph, the final output of OMP, given by</p><formula xml:id="formula_21">c * = arg min c:Supp(c)⊆T * x j − X −j c 2 ,</formula><p>will satisfy x j = X −j · c * . We rewrite it as</p><formula xml:id="formula_22">x j − m:xm∈Si m∈T * x m · c * m = m:xm / ∈Si m∈T * x m · c * m .</formula><p>(A.1) <ref type="bibr" target="#b0">1</ref> We make the assumption that there are enough samples on each subspace. More specifically, ∀i, ∀x j ∈ S i , rank(X i −j ) = dim(S i ).</p><p>Observe that the left hand side of (A.1) is in subspace S i while the right hand side is in subspace S −i := m =i S m . By the assumption that the set of all subspaces is independent, we know S i and S −i are also independent, so they intersect only at the origin. As a consequence, we have</p><formula xml:id="formula_23">0 = m:xm / ∈Si m∈T * x m · c * m = m:xm / ∈Si x m · c * m , (A.2)</formula><p>where we also used the fact that c * m = 0 for all m / ∈ T * . Combining (A.2) with the early fact that the columns of X −j indexed by T k are linearly independent for all k (this includes k = k * ), we know that</p><formula xml:id="formula_24">c * m = 0 if x m / ∈ S i and m ∈ T * . (A.3)</formula><p>Finally, we use this to prove that c * is subspace-preserving.</p><p>To this end, suppose that c * j = 0, which from the definition of c * means that j ∈ T * . Using this fact, c * j = 0, and (A.3) allows us to conclude that c * j ∈ S i . Thus the solution c * is subspace-preserving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Lemma 1</head><p>In this section, we provide a detailed proof of Lemma 1. The proof follows straight forwardly by comparing inductively the steps of the procedure OMP(X −j , x j ) and the procedure of the fictitious problem OMP(X i −j , x j ). The idea is that these two procedures follow the same "path" if the condition of the lemma is satisfied. Proof. Let k * be the number of iterations computed by the procedure OMP(X −j , x j ) so that q k * = 0 (this was established in the first paragraph of the proof for Theorem 1). We prove that the solution to OMP(X −j , x j ) is subspace preserving by showing that T k * only contains indexes of points from the i-th subspace. This is shown by induction, in the way that T k contains points from the i-th subspace for every 0 ≤ k ≤ k * .</p><p>The set of residual directions W i j introduced in Definition 3 plays an essential role in this proof. For notational clarity, we denoteq k to be the residual vector generated at iteration k of the algorithm OMP(X i −j , x j ) (note that this is the fictitious problem). The residual vectors of OMP(X −j , x j ) are denoted by q k . In the induction, we also show that OMP(X i −j , x j ) does not terminate at any k &lt; k * , and that q k =q k whenever k ≤ k * .</p><p>First, in the case of k = 0, the argument that T 0 only contains indexes of points that are from subspace i is trivially satisfied since T 0 is empty. Also, q 0 =q 0 is satisfied because they are both set to be x j in line 1 of Algorithm 1. Now, given that q k =q k for some k &lt; k * and that T k contains points only from subspace S i , we show that q k+1 =q k+1 and that T k+1 contains indexes of points from subspace i. This could be shown by noticing that the added entry in step 3 of Algorithm 1 is given by arg max</p><formula xml:id="formula_25">m≤N,m =j |x m q k |.</formula><p>Here, since q k =q k , we have that q k / q k 2 is in the set W i j . Then, by using condition (B.1), we know that the arg max will give an index that corresponds to a point in S i . This guarantees that T k+1 only contains points from subspace S i . Moreover, the picked point is evidently the same as the point picked at iteration k of the OMP(X i −j , x j ). It then follows from step 4 of Algorithm 1 that the resultant residuals, q k+1 andq k+1 , are also equal. In the case of k + 1 &lt; k * , this means that q k+1 =q k+1 = 0, so the fictitious problem OMP(X i −j , x j ) does not terminate at this step. This finishes the mathematical induction.</p><p>The fact that OMP terminates in at most d i iterations follows from the following facts: (i) we have established that OMP(X −j , x j ) produces the same computations as does OMP(X i −j , x j ); (ii) the collection of vectors selected by OMP(X i −j , x j ) are linearly independent and contained in subspace S i ; and (iii) the dimension of S i is equal to d i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Lemma 2</head><p>In this section, we prove Lemma 2 in Section 3.</p><p>Lemma. Let x j ∈ S i . Then, for all w ∈ W i j , we have:</p><formula xml:id="formula_26">max x∈ k =i X k |w x| ≤ max k:k =i µ(W i , X k ) ≤ max k:k =i µ(X i , X k )/r i ; max x∈X i \{xj } |w x| ≥ r(P i −j ) ≥ r i . (C.1)</formula><p>Proof. Two of the inequalities need proofs while the other two follow directly from definitions. For the first one, we prove that max k:k =i µ(W i , X k ) ≤ max k:k =i µ(X i , X k )/r i . To do this, it suffices to show that for any k = i, µ(W i , X k ) ≤ µ(X i , X k )/r i . Notice that any pointŵ in W i is in the subspace S i , so it could be written as a linear combination of the points in X i , i.e.ŵ = X i · c for some c. Specifically, we pick a c that is given by the following optimization program:</p><formula xml:id="formula_27">c = arg min c c 1 s.t.ŵ = X i · c.</formula><p>(C.2)</p><p>Using (C.2), defining Y = X k · X i , letting the th column of Y by denoted by y , and using the Cauchy-Schwarz inequality, we can observe that</p><formula xml:id="formula_28">X k ŵ ∞ = X k · X i ·ĉ ∞ = Y ·ĉ ∞ = max |y ĉ| ≤ max y ∞ ĉ 1 = ĉ 1 max y ∞ ≤ ĉ 1 max µ(X i , X k ) = µ(X i , X k ) · ĉ 1 .</formula><p>To proceed, we need to provide a bound on ĉ 1 . Asĉ is defined by (C.2), it is shown that such a bound exists and is given by (see, e.g. lemma B.2 in <ref type="bibr" target="#b30">[31]</ref>)</p><formula xml:id="formula_29">ĉ 1 ≤ ŵ 2 /r(P i ) = 1/r(P i ),</formula><p>where P i := conv(±X i ), and we use the fact that every point in the set of residual directions W i is defined to have unit norm. Now, by the definitions we can get r(</p><formula xml:id="formula_30">P i ) ≥ r(P i −j ) ≥ r i , thus ĉ 1 ≤ 1/r(P i ) ≤ 1/r i , which gives X k ŵ ∞ ≤ µ(X k , X i )/r i . (C.3) Finally, since (C.3) holds for anyŵ in W i , the conclu- sion follows that µ(W i , X k ) ≤ µ(X i , X k )/r i .</formula><p>For the second part, we prove that for all w ∈ W i j , max x∈X i \{xj } |w x| ≥ r(P i −j ), or equivalently, X i −j · w ∞ ≥ r(P i −j ). The proof relies on the result (see definition 7.2 in <ref type="bibr" target="#b28">[29]</ref>) that for an arbitrary vector y ∈ S i ,</p><formula xml:id="formula_31">X i −j · y ∞ ≤ 1 ⇒ y 2 ≤ 1/r(P i −j ).</formula><p>It then follows that if (by contradiction) X i −j · w ∞ &lt; r(P i −j ), then X i −j · w ∞ = r(P i −j ) − &gt; 0 for some &gt; 0, and</p><formula xml:id="formula_32">X i −j w r(P i −j ) − ∞ = 1 ≤ 1 ⇒ w r(P i −j ) − 2 ≤ 1/r(P i −j ) ⇒ w 2 ≤ (r(P i −j ) − )/r(P i −j ) &lt; 1,</formula><p>which contradicts the fact that w is normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Theorem 2 and Corollary 1</head><p>We explicitly show the proof of Theorem 2 and Corollary 1. They follow from the previous two lemmas.</p><p>Theorem. The output of OMP is subspace preserving if ∀i = 1, . . . , n, max</p><formula xml:id="formula_33">k:k =i µ(W i , X k ) &lt; r i . (D.1)</formula><p>Corollary. The output of OMP is subspace preserving if ∀i = 1, . . . , n, max</p><formula xml:id="formula_34">k:k =i µ(X i , X k ) &lt; r 2 i . (D.2)</formula><p>Proof. Notice from Lemma 1 that the solution of SSC-</p><formula xml:id="formula_35">OMP for x j ∈ S i is subspace preserving if ∀w ∈ W i j max x∈ k =i X k |w x| &lt; max x∈X i \{xj } |w x|.</formula><p>(D.3) Lemma 2 provides bounds for both sides of (D.3) from which the theorem and the corollary follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Theorem 3</head><p>Theorem. Assume a random model where all subspaces are of equal dimension d and the number of data points in each subspace is ρd+1, where ρ &gt; 1 is the "density", so the total number data points in all subspaces is N = n(ρd+1). The output of OMP is subspace preserving with probability</p><formula xml:id="formula_36">p &gt; 1 − 2d N − N e − √ ρd if d &lt; c 2 (ρ) log ρ 12 D log N , (E.1)</formula><p>where c(ρ) &gt; 0 is a constant that depends only on ρ.</p><p>Proof. The proof goes by providing bounds for the left and right hand side of the inequality in Theorem 2, copied here for convenience of reference: ∀i = 1, . . . , n, max</p><formula xml:id="formula_37">k:k =i µ(W i , X k ) &lt; r i . (E.2)</formula><p>We first give a bound on the inradius r i . Denotē</p><formula xml:id="formula_38">r = c(ρ) √ log ρ √ 2d andp r = N e − √ ρd ,</formula><p>in which c(ρ) is a numerical constant depending on ρ. <ref type="bibr" target="#b28">[29]</ref> shows that since points in each subspace are independently distributed, it holds that P (r i ≥r for all i) ≥ 1 −p r .</p><p>Next we give a bound on the coherence. From an upper bound on the area of a spherical cap <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>, we have that if x, y ∈ R D are two random vectors that are distributed uniformly and independently on the unit sphere, then</p><formula xml:id="formula_39">P | x, y | ≥ 6 log N D ≤ 2 N 3 . (E.3)</formula><p>Under the random model, points x ∈ X k , ∀k are distributed uniformly at random on the unit sphere of R D by assumption. Any residual point w ∈ W i , ∀i also has uniform distribution on the unit sphere as it depends only on points in X i , which are independent and uniformly distributed. Furthermore, any pair of points x ∈ X k and w ∈ W i are distributed independently because points in X k and X i are independent. Thus the result of Equation (E.3) is applicable here. Since there are at most d × N 2 pairs of inner product in µ(W i , X k ), by using the union bound we can get P µ(W i , X k ) ≤μ for all i, k ≥ 1 −p µ dN 2 , where we have defined µ = 6 log N D andp µ = 2/N 3 .</p><p>As a consequence, if the condition (E.1) holds then we havē r &lt;μ. Applying again the union bound we get that condition (E.2) holds with probability p &gt; 1 −p µ d −p r . This finishes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with prior work on SSC-OMP</head><p>In Theorem 2 we give a sufficient condition for guaranteeing subspace-preserving of the SSC-OMP: ∀i = 1, . . . , n, max Prior to this work, <ref type="bibr" target="#b11">[12]</ref> gives another sufficient condition for SSC-OMP giving subspace-preserving representation, namely, We claim that Theorem 2 in this work is a stronger result than that provided in the work <ref type="bibr" target="#b11">[12]</ref>, as the sufficient condition of (F.3) implies (F.1). Here we give a rigorous argument for this claim.</p><p>Notice that the inequality in (F.3) implies that ∀k = i, µ(X i , X k ) &lt; r i − √ 2 − 2r i cos θ * i,k , (F.5)</p><p>see Lemma 1 in their paper. We show that condition (F.5) implies (F.1) when r i ≤ 1/2, and implies condition (F.2) when r i &gt; 1/2, which means that their result is weaker than our result that is based on condition (F.1). Case 1. If r i ≤ 1/2, then √ 2 − 2r i ≥ 1, thus (F.5) ⇒ µ(X i , X k ) &lt; r i − cos θ * i,k ⇒ cos θ * l,k &lt; r i ⇒ µ(X k , W i ) &lt; r i ⇔ (F.1). Case 2. If r i &gt; 1/2, then</p><formula xml:id="formula_40">(F.5) ⇒ µ(X i , X k ) &lt; r i − √ 2 − 2r i µ(X i , X k ) ⇒ µ(X i , X k ) &lt; r i /(1 + √ 2 − 2r i ) ⇒ µ(X i , X k ) &lt; r i /(1 + (2 − 2r i )) ⇒ µ(X i , X k ) &lt; (r i ) 2 ⇔ (F.2) ⇒ (F.1).</formula><p>So the condition in (F.1) is implied by (F.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Parameters for real experiments</head><p>For the purpose of reproducible results, we report the parameters used for all the methods in the real data experiments. For OMP, we set in Algorithm 1 to be 10 −3 , k max to be the true subspace dimension in the synthetic experiments, 10 in digit clustering and 5 in face clustering. For LSR, we use "LSR2" in <ref type="bibr" target="#b24">[25]</ref> with regularization λ = 60 for digit clustering and λ = 0.3 for face clustering. For LRSC, we use model "P 3 " in <ref type="bibr" target="#b33">[34]</ref>, with parameters τ = α = 0.1 for digit clustering and τ = α = 150 for face clustering. For SCC, we use dimension d = 8 for digit clustering and 5 for face clustering. We use 1 -Magic for SSC-BP in the synthetic experiments. For digit and face clustering, we use the noisy variation of SSC-BP in [15, sec. 3.1] for digit clustering with λ z = 80/µ z , and the sparse outlying entries variation of SSC-BP in [15, sec. 3.1] for face clustering with λ e = 30/µ e . For all algorithms, these constants were chosen to optimize performance For a fair comparison, we allow standard pre/postprocessing to be used whenever they improve the clustering accuracy. For preprocessing, we allow normalization of the original data points using the 2 norm, and for postprocessing, we allow normalization of the coefficient vectors using the ∞ norm. For experiments on synthetic data, we do not use any pre/post-processing. In digit clustering, preprocessing is applied to SSC-BP and SCC, and post-processing is used for SSC-OMP and SSC-BP. For face clustering, preprocessing is applied to SSC-OMP, LSR, LRSC and SCC, while post-processing is used for SSC-BP and LRSC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6 :</head><label>6</label><figDesc>end while Output: c * = arg min c:Supp(c)⊆T k b − Ac 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2 : 1 :</head><label>21</label><figDesc>Sparse Subspace Clustering by Orthogonal Matching Pursuit (SSC-OMP) Input: Data X = [x 1 , · · · , x N ], parameters k max , . Compute c * j from OMP(X −j , x j ) using Algorithm 1. 2: Set C * = [c * 1 , · · · , c * N ] and W = |C * | + |C * |. 3: Compute segmentation from W by spectral clustering. Output: Segmentation of data X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 3 .</head><label>3</label><figDesc>Let Q(A, b) be the set of all residual vectors computed in step 4 of OMP(A, b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 N</head><label>2</label><figDesc>(n,ρ,d) − N (n, ρ, d)e − √ ρd if d &lt; c 2 (ρ) log ρ 12 D log N (n, ρ, d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 .</head><label>1</label><figDesc>Performance of SSC-OMP and SSC-BP on synthetic data. The data are drawn from 5 subspaces of dimension 6 in ambient dimension 9. Each subspace contains the same number of points and the overall number of points is varied from 150 to 10 5 and is shown in log scale. For SSC-BP, however, the maximum number of points tested is 6,000 due to time limit. Notice that the bottom right figure also uses log scale in the y-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Lemma 3 .</head><label>3</label><figDesc>OMP gives a subspace-preserving representation for point x j ∈ S i in at most d i iterations if ∀w ∈ W i j max x∈ k =i X k |w x| &lt; max x∈X i \{xj } |w x|. (B.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>k:k =i µ(W i , X k ) &lt; r i , (F.1)and in Corollary 1 a stronger sufficient condition: ∀i = 1, . . . , n, maxk:k =i µ(X i , X k ) &lt; r 2 i . (F.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>4 √</head><label>4</label><figDesc>max k:k =i µ(X i , X k ) &lt; r i − 2 1 − (r i ) 2 12 max k:k =i cos θ * i,k , (F.3)in which the subspace angle is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of subspace clustering methods on the MNIST dataset. The data consists of a randomly chosen number Ni ∈ {50, 100, 200, 400, 600} of images for each of the 10 digits (i.e., 0-9), with features extracted from a scattering network and projected to dimension 500 using PCA.</figDesc><table><row><cell>No. points</cell><cell>500</cell><cell cols="2">1000 2000 4000</cell><cell>6000</cell></row><row><cell cols="4">e%: subspace-preserving representation error</cell></row><row><cell cols="5">SSC-OMP 42.13 38.73 36.20 34.22 33.22</cell></row><row><cell>SSC-BP</cell><cell cols="4">29.56 24.88 21.07 17.80 16.08</cell></row><row><cell>LSR</cell><cell cols="4">78.24 79.68 80.83 81.75 82.18</cell></row><row><cell>LRSC</cell><cell cols="4">81.33 81.99 82.67 83.15 83.27</cell></row><row><cell>SCC</cell><cell cols="4">89.89 89.87 89.85 89.81 89.81</cell></row><row><cell cols="4">a%: average clustering accuracy</cell></row><row><cell cols="5">SSC-OMP 83.64 86.67 90.60 91.22 91.25</cell></row><row><cell>SSC-BP</cell><cell cols="4">83.01 84.06 85.58 86.00 85.60</cell></row><row><cell>LSR</cell><cell cols="4">75.84 78.42 78.09 79.06 79.91</cell></row><row><cell>LRSC</cell><cell cols="4">75.02 79.76 79.44 78.46 79.88</cell></row><row><cell>SCC</cell><cell cols="4">53.45 61.47 66.43 71.46 70.60</cell></row><row><cell cols="2">t(sec.): running time</cell><cell></cell><cell></cell></row><row><cell>SSC-OMP</cell><cell>2.7</cell><cell>11.4</cell><cell cols="2">93.8 410.4 760.9</cell></row><row><cell>SSC-BP</cell><cell>20.1</cell><cell cols="3">97.9 635.2 4533 13605</cell></row><row><cell>LSR</cell><cell>1.7</cell><cell>5.9</cell><cell cols="2">42.4 136.1 327.6</cell></row><row><cell>LRSC</cell><cell>1.9</cell><cell>6.4</cell><cell cols="2">43.0 145.6 312.9</cell></row><row><cell>SCC</cell><cell>31.2</cell><cell cols="3">48.5 101.3 235.2 366.8</cell></row></table><note>SCC performs the worst among the algorithms tested.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of subspace clustering methods on EYaleB dataset. A 'NA' denotes that a running error was returned by the solver. The data consists of face images under 64 different illumination conditions of a randomly picked n = {2, 10, 20, 30, 38} individuals. Images are downsampled from size 192 × 168 to size 48 × 42 and used as the feature vectors (data points).</figDesc><table><row><cell>No. subjects</cell><cell>2</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>38</cell></row><row><cell cols="5">e%: subspace-preserving representation error</cell><cell></cell></row><row><cell>SSC-OMP</cell><cell cols="5">4.14 13.62 16.80 18.66 20.13</cell></row><row><cell>SSC-BP</cell><cell cols="5">2.70 10.33 12.67 13.74 14.64</cell></row><row><cell>LSR</cell><cell cols="5">22.77 67.07 79.52 84.94 87.57</cell></row><row><cell>LRSC</cell><cell cols="5">26.87 69.76 80.58 85.56 88.02</cell></row><row><cell>SCC</cell><cell>48.70</cell><cell>NA</cell><cell>NA</cell><cell cols="2">96.57 97.25</cell></row><row><cell cols="3">a%: average clustering accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSC-OMP</cell><cell cols="5">99.18 86.09 81.55 78.27 77.59</cell></row><row><cell>SSC-BP</cell><cell cols="5">99.45 91.85 79.80 76.10 68.97</cell></row><row><cell>LSR</cell><cell cols="5">96.77 62.89 67.17 67.79 63.96</cell></row><row><cell>LRSC</cell><cell cols="5">94.32 66.98 66.34 67.49 66.78</cell></row><row><cell>SCC</cell><cell>78.91</cell><cell>NA</cell><cell>NA</cell><cell cols="2">14.15 12.80</cell></row><row><cell cols="2">t(sec.): running time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSC-OMP</cell><cell>0.6</cell><cell>8.3</cell><cell>31.1</cell><cell cols="2">63.7 108.6</cell></row><row><cell>SSC-BP</cell><cell cols="5">49.1 228.2 554.6 1240 1851</cell></row><row><cell>LSR</cell><cell>0.1</cell><cell>0.8</cell><cell>3.1</cell><cell>8.3</cell><cell>15.9</cell></row><row><cell>LRSC</cell><cell>1.1</cell><cell>1.9</cell><cell>6.3</cell><cell>14.8</cell><cell>26.5</cell></row><row><cell>SCC</cell><cell>50.0</cell><cell>NA</cell><cell>NA</cell><cell cols="2">520.3 750.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If arg max in step 3 of the algorithm gives multiple items, pick one of them in a deterministic way, e.g., pick the one with the smallest index.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments. Work supported by NSF grant 1447822.</head><p>Appendices</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic subspace clustering via sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="66" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linear-time subspace clustering via bipartite graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2234" to="2246" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An elementary introduction to modern convex geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Flavors of Geometry</title>
		<imprint>
			<publisher>Univ. Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lambertian reflection and linear subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An introduction to compressive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral curvature clustering (SCC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CBMS Regional Conference Series in Mathematics</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of orthogonal matching pursuit using the restricted isometry property</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Greedy feature selection for subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustering disjoint subspaces via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A closed form solution to robust subspace estimation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslovak Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="619" to="633" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient ksupport matrix pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="617" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Correlation adaptive subspace segmentation by trace lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Elastic net subspace clustering applied to pop/rock music structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: recursive function approximation with application to wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems and Computation</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low rank subspace clustering (LRSC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiframe motion segmentation with missing data using PowerFactorization, and GPCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Noisy sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Provable subspace clustering: When LRR meets SSC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Oracle based active set algorithm for scalable elastic net subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geometric conditions for subspacesparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1585" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hybrid linear modeling via local best-fit flats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-negative low rank and sparse graph for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2328" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
