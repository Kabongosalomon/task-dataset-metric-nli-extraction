<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">6-DoF Object Pose from Semantic Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
						</author>
						<title level="a" type="main">6-DoF Object Pose from Semantic Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance-and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instanceand class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>This paper addresses the task of estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single image. Despite its importance in a variety of applications, e.g., robotic manipulation, and its intense study, most solutions tend to treat objects on a case-by-case basis. For instance, approaches can be discerned by whether they address "sufficiently" textured objects with those that are textureless. Some approaches focus on instance-based object detection while others address object classes. In this work, we strive for an approach where the admissibility of objects considered is as wide as possible (examples in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>Our approach combines statistical models of appearance and the 3D shape layout of objects for pose estimation. It consists of two stages that first reasons about the 2D projected shape of an object captured by a set of 2D semantic keypoints and then estimates the 3D object pose consistent with the keypoints. These steps are presented in <ref type="figure">Fig. 2</ref>. In the first stage, we use a high capacity convolutional network (convnet) to predict a set of semantic keypoints. Here, the network takes advantage of its ability to aggregate appearance information over a wide-field of view, as compared to localized part models, e.g., <ref type="bibr" target="#b0">[1]</ref>, to make reliable predictions of the semantic keypoints. In the second stage, the semantic keypoint predictions are used to explicitly reason about the intra-class shape variability and the camera pose modeled by a weak or full perspective camera model. Pose estimates are realized by maximizing the geometric consistency between the parametrized deformable model and the 2D semantic keypoints. While this work focuses on RGB-based pose estimation, in the case where a corresponding point cloud is provided with the image, our method can provide a robust way to initialize the iterative closest point (ICP) algorithm <ref type="bibr" target="#b1">[2]</ref>, to further refine the pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Estimating the 6-DoF pose of an object from a single image has attracted significant study. Given a rigid 3D object model and a set of 2D-to-3D point correspondences, various solutions have been explored, e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. This is commonly referred to as the Perspective-n-Point problem (PnP). To relax the assumption of known 2D landmarks, a number of approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> have considered the detection of discriminative image keypoints, such as SIFT <ref type="bibr" target="#b7">[8]</ref>, with highly textured objects. A drawback with these approaches is that they are inadequate for addressing textureless objects and their performance is susceptible to scene clutter. An alternative to sparse discriminative keypoints is offered by dense methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, where every pixel or patch is voting for the object pose. These approaches are also applicable for textureless objects, however, the assumption that a corresponding instance-specific 3D model is available for each object limits their general applicability.</p><p>Holistic template-based approaches are one of the earliest approaches considered in the object detection literature. To accommodate appearance variation due to camera capture arXiv:1703.04670v1 [cs.CV] 14 Mar 2017 <ref type="figure">Fig. 2</ref>: Pipeline of our approach. Given a single RGB image of an object (a), we localize a set of class-specific keypoints using a convnet with the stacked hourglass design. The output of this step is a set of heatmaps for each keypoint, which are combined for visualization in (b), sometimes leading to false detections. In (c), green dots represent the detected keypoints and the corresponding blue dots (connected with an arrow) the groundtruth locations. For robustness against such localization errors, we solve a fitting problem to enforce global consistency of the keypoints, where the response of the heatmaps is used as a measure of certainty for each keypoint. The optimization recovers the full 6-DoF pose of the object (d).</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula><p>viewpoint, a set of template images of the object instance are captured about the view sphere and are compared to the input image at runtime. In recent years, template-based methods have received renewed interest due to the advent of accelerated matching schemes and their ability to detect textureless objects by way of focusing their model description on the object shape <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>. While impressive results in terms of accuracy and speed have been demonstrated, holistic template-based approaches are limited to instance-based object detection. To address class variability and viewpoint, various approaches have used a collection of 2D appearance-based part templates trained separately on discretized views <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Convolutional networks (convnets) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> have emerged as the method of choice for a variety of problems. Closest to the current work is their application in camera viewpoint and keypoint prediction. Convnets have been used to predict the camera's viewpoint with respect to the object by way of direct regression or casting the problem as classification into a set discrete views <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. While these approaches allow for object category pose estimation they do not provide fine-grained information about the 3D layout of the object. Convnet-based keypoint prediction for human pose estimation (e.g., <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>) has attracted considerable study, while limited attention has been given to their application with generic object categories <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Their success is due in part to the high discriminative capacity of the network. Furthermore, their ability to aggregate information over a wide field of view allows for the resolution of ambiguities (e.g., symmetry) and for localizing occluding joints.</p><p>Statistical shape-based models tackle recognition by aligning a shape subspace model to image features. While originally proposed in the context of 2D shape <ref type="bibr" target="#b30">[31]</ref> they have proven useful for modelling the 3D shape of a host of object classes, e.g., faces <ref type="bibr" target="#b31">[32]</ref>, cars <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and human pose <ref type="bibr" target="#b34">[35]</ref>. In recent work <ref type="bibr" target="#b35">[36]</ref>, data-driven discriminative landmark hypotheses were combined with a 3D deformable shape model and a weak perspective camera model in a convex optimization framework to globally recover the shape and pose of an object in a single image. Here, we adapt this approach and extend it with a perspective camera model, in cases where the camera intrinsics are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>In the light of previous work, the contributions of our work are as follows:</p><p>• We present an efficient approach that combines highly reliable (semantic) keypoints predicted by a convnet with a deformable shape model to estimate the continuous 6-DoF pose of an object. Unlike previous work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available image training data. Furthermore, the same approach can be applied to instance-and classbased pose recovery. • Empirically, we demonstrate that the proposed approach yields accurate 6-DoF pose estimates in scenes with cluttered backgrounds without the requirement of any pose initialization. State-of-the-art performance is shown on the large-scale PASCAL3D+ dataset <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TECHNICAL APPROACH</head><p>The proposed pipeline includes object detection, keypoint localization and pose optimization. As object detection has been a well studied problem, we assume that a bounding box around the object has been provided by an off-the-shelf object detector, e.g., Faster R-CNN <ref type="bibr" target="#b36">[37]</ref>, and focus on the keypoint localization and pose optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Keypoint localization</head><p>The keypoint localization step employs the "stacked hourglass" network architecture <ref type="bibr" target="#b27">[28]</ref> that has been shown to be particularly effective for 2D human pose estimation. Motivated by this success, we use the same network design and train the network for object keypoint localization.</p><p>Network architecture A high level overview of the main network components is presented in <ref type="figure" target="#fig_1">Fig. 3</ref>. The network takes as input an RGB image, and outputs a set of heatmaps, one per keypoint, with the intensity of the heatmap indicating the confidence of the respective keypoint to be located at this position. The network consists of two hourglass components, where each component can be further subdivided into two main processing stages. In the first stage, a series of convolutional and max-pooling layers are applied to the input. After each max-pooling layer, the resolution of the feature maps decreases by a factor of two, allowing the next convolutional layer to process the features at a coarser scale. This sequence of processing continues until reaching the lowest resolution (4 × 4 feature maps), which is illustrated by the smallest layer in the middle of each module in <ref type="figure" target="#fig_1">Fig. 3</ref>. Following these downsampling layers, the processing continues with a series of convolutional and upsampling layers. Each upsampling layer increases the resolution by a factor of two. This process culminates with a set of heatmaps at the same resolution as the input of the hourglass module.</p><p>A second hourglass component is stacked at the end of the first one to refine the output heatmaps. The groundtruth labels used to supervise the training are synthesized heatmaps based on a 2D Gaussian centered at each keypoint with a standard deviation set to one. The 2 loss is minimized during training.</p><p>Optionally, intermediate supervision can be applied at the end of the first module, which provides a richer gradient signal to the network and guides the learning procedure towards a better optimum <ref type="bibr" target="#b37">[38]</ref>. The heatmap responses of the last module are considered as the final output of the network and the peak in each heatmap indicates the most likely location for the corresponding keypoint.</p><p>Design benefits The most critical design element of the hourglass network is the symmetric combination of bottomup and top-down processing that each hourglass module performs. Given the large appearance changes of objects due to in-class and viewpoint variation, both local and global cues are needed to effectively decide the locations of the keypoints in the image. The consolidation of features across different scales in the hourglass architecture allows the network to successfully integrate both local and global appearance information, and commit to a keypoint location only after this information has been made available to the network. Moreover, the stacking of hourglass modules provides a form of iterative processing that has been shown to be effective with several other recent network designs <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b28">[29]</ref> and offers additional refinement of the network estimates. Additionally, the application of intermediate supervision at the end of each module has been validated as an effective training strategy, particularly ameliorating the practical issue of vanishing gradients when training a deep neural network <ref type="bibr" target="#b37">[38]</ref>. Finally, residual layers are introduced <ref type="bibr" target="#b39">[40]</ref>, which have achieved state-of-the-art results for many visual tasks, including object classification <ref type="bibr" target="#b39">[40]</ref>, instance segmentation <ref type="bibr" target="#b40">[41]</ref>, and 2D human pose estimation <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose optimization</head><p>Given the keypoint locations on the 3D model as well as their correspondences in the 2D image, one naive approach is to simply apply an existing PnP algorithm to solve for the 6-DoF pose. This approach is problematic because the keypoint predictions by the convnet can be rendered imprecise due to occlusions and false detections in the background. Moreover, the exact 3D model of the object instance in the testing image is often unavailable. To address these difficulties, we propose to fit a deformable shape model to the 2D detections while considering the uncertainty in keypoint predictions.</p><p>A deformable shape model is built for each object category using 3D CAD models with annotated keypoints. More specifically, the p keypoint locations on a 3D object model are denoted by S ∈ R 3×p and</p><formula xml:id="formula_1">S = B 0 + k i=1 c i B i ,<label>(1)</label></formula><p>where B 0 is the mean shape of the given 3D model and B 1 , . . . , B k are several modes of possible shape variability computed by Principal Component Analysis (PCA). Given detected keypoints in an image, which are denoted by W ∈ R 2×p , the goal is to estimate the rotation R ∈ R 3×3 and translation T ∈ R 3×1 between the object and camera frames as well as the coefficients of the shape deformation c = [c 1 , · · · , c k ] .</p><p>The inference is formulated as the following optimization problem:</p><formula xml:id="formula_2">min θ 1 2 ξ(θ)D 1 2 2 F + λ 2 c 2 2 ,<label>(2)</label></formula><p>where θ is the set of unknowns, ξ(θ) denotes the fitting residuals dependent on θ, and the Tikhonov regularizer c 2 2 is introduced to penalize large deviations from the mean shape.</p><p>To incorporate the uncertainty in 2D keypoint predictions, a diagonal weighting matrix D ∈ R p×p is introduced:</p><formula xml:id="formula_3">D =      d 1 0 · · · 0 0 d 2 · · · 0 . . . . . . . . . . . . 0 0 · · · d p      ,<label>(3)</label></formula><p>where d i indicates the localization confidence of the ith keypoint in the image. In our implementation, d i is assigned the peak value in the heatmap corresponding to the ith keypoint. As shown previously <ref type="bibr" target="#b27">[28]</ref>, the peak intensity of the heatmap provides a good indicator for the visibility of a keypoint in the image. The fitting residuals, ξ(θ), measure the differences between the given 2D keypoints, provided by the previous processing stage, and the projections of 3D keypoints. Two camera models are next considered.</p><p>1) Weak perspective model: If the camera intrinsic parameters are unknown, the weak perspective camera model is adopted, which is usually a good approximation to the full perspective case when the camera is relatively far away from the object. In this case, the reprojection error is written as</p><formula xml:id="formula_4">ξ(θ) = W − sR B 0 + k i=1 c i B i −T 1 ,<label>(4)</label></formula><p>where s is a scalar,R ∈ R 2×3 andT ∈ R 2 denote the first two rows of R and T , respectively, and θ = {s, c,R,T }. The problem in (2) is continuous and in principal can be locally solved by any gradient-based method. We solve it with a block coordinate descent scheme because of its fast convergence and the simplicity in implementation. We alternately update each of the variables while fixing the others. The updates of s, c andT are simply solved using closed-form least squares solutions. The update ofR should consider the SO(3) constraint. Here, the Manopt toolbox <ref type="bibr" target="#b41">[42]</ref> is used to optimizeR over the Stiefel manifold. As the problem in (2) is non-convex, we further adopt a convex relaxation approach <ref type="bibr" target="#b42">[43]</ref> to initialize the optimization. More specifically, we only estimate the pose parameters while fixing the 3D model as the mean shape in the initialization stage. By setting c = 0 and replacing the orthogonality constraint onR by the spectral norm regularizer, the problem in (2) can be converted to a convex program and solved with global optimality <ref type="bibr" target="#b42">[43]</ref>.</p><p>2) Full perspective model: If the camera intrinsic parameters are known, the full perspective camera model is used, and the residuals are defined as</p><formula xml:id="formula_5">ξ(θ) =W Z − R B 0 + k i=1 c i B i − T 1 ,<label>(5)</label></formula><p>whereW ∈ R 3×p represents the normalized homogeneous coordinates of the 2D keypoints and Z is a diagonal matrix:</p><formula xml:id="formula_6">Z =      z 1 0 · · · 0 0 z 2 · · · 0 . . . . . . . . . . . . 0 0 · · · z p      ,<label>(6)</label></formula><p>where z i is the depth for the ith keypoint in 3D. Intuitively, the distances from the 3D points to the rays crossing the corresponding 2D points are minimized. In this case, the unknown parameter set θ is given by {Z, c, R, T }. The optimization here is similar to the alternating scheme in the weak perspective case. The update of Z also admits a closed-form solution and the update of R can be analytically solved by the orthogonal Procrustes analysis. To avoid local minima, the optimization is initialized by the weak perspective solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Instance-based pose recovery: gas canister</head><p>This section considers the recovery of pose for a specific object instance. This case fits well with many robotics applications where the objects in the environment are known. Moreover, it allows us to establish the accuracy of our approach in a relatively simple setting before dealing with the more challenging object class scenario.</p><p>We collected a dataset of 175 RGB-D images of a textureless gas canister. The depth data was only used to generate the groundtruth. More specifically, a complete 3D model of the gas canister was reconstructed using KinectFusion <ref type="bibr" target="#b43">[44]</ref> and the groundtruth object pose for each image was calculated by ICP with careful manual initialization. Then, 10 keypoints were manually defined on the 3D model and projected to the images yielding groundtruth keypoint locations in 2D for training the convnet.</p><p>A random 85%/15% split was used for the training/test data. A stacked hourglass network with two hourglass modules was trained. The output heatmaps for the testing images are visualized in the second column of <ref type="figure">Fig. 4</ref>. As can be seen, the hourglass network is able to locate the keypoints reliably in the presence of viewpoint variety and occlusions. The non-visible keypoints are also well localized due to the network's ability to take global context into account. The estimated object poses are shown in the last two columns of <ref type="figure">Fig. 4</ref>. The projected 3D models align accurately with the image; the full-perspective solution is more precise than the weak-perspective one. It is worth noting that only 150 images were used to train the network from scratch. Overfitting might be an issue with such a small training set, but the empirical results suggest that the hourglass model captures the object appearance very well in this single instance case.</p><p>More challenging examples with large intra-class variability are considered in Section IV-B. The 6-DoF pose was estimated with the known 3D model and camera intrinsic parameters using the optimization in Section III-B. The following geodesic distance was used to measure the rotation error between a pose estimate, R 1 , and the groundtruth, R 2 :</p><formula xml:id="formula_7">∆(R 1 , R 2 ) = log(R T 1 R 2 ) F √ 2 .<label>(7)</label></formula><p>As a simple baseline, the following greedy approach was implemented: the maximum response locations in the heatmaps were selected as 2D keypoint locations and the standard PnP problem was solved by the EPnP algorithm <ref type="bibr" target="#b3">[4]</ref> to estimate the object pose. The results are presented in <ref type="table" target="#tab_0">Table I</ref>. While the weak-perspective solutions (Proposed WP) are on average worse than EPnP due to the inaccurate camera model, the full-perspective solutions (Proposed FP) are much more precise than those of EPnP. The remarkably small pose errors returned by the proposed approach based on a single RGB image are in the range suitable for a general grasping system. In the supplemental video 1 , we demonstrate the efficacy of our approach in this situation by way of a robot grasping experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Class-based pose recovery: PASCAL3D+</head><p>Moving to a more challenging scenario, we demonstrate the full strength of our approach using the large-scale PAS-CAL3D+ dataset <ref type="bibr" target="#b18">[19]</ref>. The stacked hourglass network was trained from scratch with the training set of PASCAL3D+. Instead of training separate models for different object classes, a single network was trained to output heatmap predictions for all of the 124 keypoints from all classes. Using a single network for all keypoints allows us to share features across the available classes and significantly decreases the number of parameters needed for the network. At test time, given the class of the test object, the heatmaps corresponding to the keypoints belonging to this class were extracted. For pose optimization, two cases were tested: (i) the CAD model for the test image was known; and (ii) the CAD model was unknown and the pose was estimated with a deformable model whose basis was learned by PCA on all CAD models for each class in the dataset. Two principal components were used (k = 2) for each class, which was sufficient to explain greater than 95% of the shape variation. The 3D model was fit to the 2D keypoints with a weak-perspective model as the camera intrinsic parameters were not available.</p><p>Semantic correspondences A crucial component of our approach is the powerful learning procedure that is particularly successful at establishing correspondences across the semantically related keypoints of each class. To demonstrate this network property, in <ref type="figure">Fig. 5</ref> we present a subset of the keypoints for each class along with the localizations of these keypoints in a randomly selected set of images among the ones with the top 50 responses. It is interesting to note that despite the large appearance differences due to extreme viewpoint and intra-class variability, the predictions are very consistent and preserve the semantic relation across various class instances.</p><p>Pose estimation The quantitative evaluation for pose estimation on PASCAL3D+ is presented in <ref type="table" target="#tab_0">Table II</ref>. Only the errors for rotations are reported as the 3D translation cannot be determined in the weak perspective case and the ground truth is not available as well. The rotational error is calculated using the geodesic distance, <ref type="bibr" target="#b6">(7)</ref>. The proposed method shows improvement across most categories with respect to the stateof-the-art. The best results are achieved in the case where the fine subclass for the object is known and there exists an accurate CAD model correspondence. The proposed method with uniform weights for all keypoints is also compared as a baseline, which is apparently worse than considering the confidences during model fitting. A subset of results of the proposed method are visualized in <ref type="figure" target="#fig_2">Fig. 6</ref>. <ref type="table" target="#tab_0">Table II</ref> we observed higher errors than the state-of-the-art for two classes, namely boat and TV monitor. For most images of TV monitor, there are only four coplanar keypoints. This makes pose estimation an ill-posed problem for the weak perspective case. <ref type="figure" target="#fig_3">Figure  7</ref> illustrates some failure cases because of this ambiguity. For boat we observed that in many cases the objects are very small and there are insufficient cues to discriminate between the front and the back, which makes the keypoint localization extremely hard. In these extreme cases, holistic and discrete viewpoint prediction might be more robust, which could in practice provide a prior to regularize our continuous prediction. We exclude the results for two classes from PASCAL3D+, namely dining table and motorbike, as we observed some inconsistency of the left-right definition in groundtruth annotations, which lead to erroneous training data. Since other approaches (e.g., <ref type="bibr" target="#b23">[24]</ref>) rely on discrete viewpoint annotations only, this issue is not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure cases In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Processing time</head><p>On a desktop with an Intel i7 3.4GHz CPU, 8G RAM and a GeForce GTX Titan X 6GB GPU, our pipeline needs around 0.2 seconds for the keypoint localization step and less than 0.1 seconds for the shape fitting step, for a total running time under 0.3 seconds. This makes our approach particularly suitable for applications where near real-time is desired. Moreover, further improvements in the running time are anticipated due to improvements in hardware, particularly with GPUs. <ref type="figure">Fig. 4</ref>: Qualitative results on the gas canister dataset. From left-to-right: RGB images with bounding boxes provided by Faster R-CNN <ref type="bibr" target="#b36">[37]</ref>, heatmaps from the convnet, projections of the 3D model with estimated poses using the weak-perspective model and full-perspective model, respectively. Note the better alignment near the handle with the full-perspective model. <ref type="figure">Fig. 5</ref>: Localization results for diverse keypoint categories. We visualize eight images selected randomly from the top 50 responses for each keypoint. The keypoint localization network is particularly successful at establishing semantic correspondences across the instances of a class, despite the significant intra-class variation and wide ranging camera viewpoints.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SUMMARY</head><p>In this paper, we proposed an efficient method to estimate the continuous 6-DoF pose of an object from a single RGB image. Capitalizing on the robust semantic keypoint predictions provided by a state-of-the-art convnet, we proposed a pose optimization scheme that fits a deformable shape model to the 2D keypoints and recovers the 6-DoF pose of the object. To ameliorate the effect of false detections, our pose optimization integrates the heatmap response values in the optimization scheme to model the certainty of each detection.</p><p>Both the weak perspective and the full perspective cases were investigated. The experimental validation included an instance-based scenario as well as full-scale evaluation on the PASCAL3D+ dataset, where we demonstrated state-ofthe-art results for viewpoint estimation. Additionally, our method is accompanied by an efficient implementation with a running time under 0.3 seconds, making it a good fit for near real-time robotics applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Given a single RGB image of an object (left), we estimate its 6-DoF pose. The corresponding CAD model is overlaid on the image (right) using the estimated pose. Our method deals with both instance-based (top) and class-based scenarios (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of the stacked hourglass architecture. Here, two hourglass modules are stacked together. The symmetric nature of the design allows for bottom-up processing (from high to low resolution) in the first half of the module, and top-down processing (from low to high resolution) in the second half. Intermediate supervision is applied after the first module. The heatmap responses of the second module represent the final output of the network that is used for keypoint localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Example results of our approach on PASCAL3D+. For each example from left-to-right: the RGB image of the object, heatmap responses for the keypoints of the specific class, the CAD model projected to 2D after pose estimation, and the CAD model visualized in 3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Failure cases for the TV monitor class of PAS-CAL3D+. Although the visible keypoints are localized successfully the pose optimization fails because the 4 points are coplanar and the problem becomes ill-posed. NSF-IIP-1439681 (I/UCRC), NSF-IIS-1426840, ARL MAST-CTA W911NF-08-2-0004, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, an ONR STTR (Robotics Research), NSERC Discovery, and the DARPA FLA program.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Pose estimation errors on the gas canister dataset.</figDesc><table><row><cell>Approach</cell><cell cols="4">Rotation (degree) Translation (mm) Mean Median Mean Median</cell></row><row><cell>Proposed WP</cell><cell>7.99</cell><cell>7.61</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Proposed FP</cell><cell>3.57</cell><cell>3.11</cell><cell>12.05</cell><cell>8.82</cell></row><row><cell>EPnP [4]</cell><cell>7.17</cell><cell>5.21</cell><cell>43.45</cell><cell>21.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Viewpoint Estimation Median Error (degrees) on PASCAL3D+.</figDesc><table><row><cell>Approach</cell><cell>aero</cell><cell>bike</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>chair</cell><cell>sofa</cell><cell cols="2">train TV monitor</cell><cell>boat</cell></row><row><cell>Tulsiani and Malik [24]</cell><cell cols="2">13.8 17.7</cell><cell>12.9</cell><cell>5.8</cell><cell>9.1</cell><cell>14.8</cell><cell>15.2</cell><cell>8.7</cell><cell>15.4</cell><cell>21.3</cell></row><row><cell>ours -PCA basis</cell><cell cols="2">11.2 15.2</cell><cell>13.1</cell><cell>4.7</cell><cell>6.9</cell><cell>12.7</cell><cell>21.7</cell><cell>9.1</cell><cell>38.5</cell><cell>37.9</cell></row><row><cell>ours -CAD basis</cell><cell>8.0</cell><cell>13.4</cell><cell>11.7</cell><cell>2.0</cell><cell>5.5</cell><cell>10.4</cell><cell>9.6</cell><cell>8.3</cell><cell>32.9</cell><cell>40.7</cell></row><row><cell>ours -uniform weights</cell><cell cols="2">16.3 17.8</cell><cell>14.1</cell><cell cols="2">11.7 30.7</cell><cell>17.6</cell><cell>32.4</cell><cell>20.8</cell><cell>25.0</cell><cell>72.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.seas.upenn.edu/%7Epavlakos/projects/object3d</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We gratefully appreciate support through the following grants: NSF-DGE-0966142 (IGERT),</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative mixture-of-templates for viewpoint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">EPnP: An accurate O(n) solution to the PnP problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object recognition and full pose registration from a single image for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The MOPED framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal blending for high-accuracy instance recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2214" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 6D object pose estimation using 3D object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6D pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recovering 6D object pose and predicting next-best-view in the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3583" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Global hypothesis generation for 6D object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02287</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">REIN -A fast, robust, scalable recognition infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2939" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminatively trained templates for 3D object detection: A real time scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rios-Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2048" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Real-time scalable 6DOF pose estimation for textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Banerjee</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D object detection and viewpoint estimation with a deformable 3D cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="620" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teaching 3D geometry to deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3362" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond PASCAL: A benchmark for 3D object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image 3D object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for joint object detection and pose estimation: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
		<idno>abs/1412.7190</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Render for CNN: viewpoint estimation in images using CNNs trained with rendered 3D model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active shape models-Their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D shape regression for real-time facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detailed 3D representations for object recognition and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2608" to="2623" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconstructing vechicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reconstructing 3D human pose from 2D image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single image pop-up from discriminatively learned parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="927" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Manopt, a Matlab toolbox for optimization on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1455" to="1459" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3D shape estimation from 2D landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
