<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Semantic Segmentation with High-and Low-level Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
						</author>
						<title level="a" type="main">Semi-Supervised Semantic Segmentation with High-and Low-level Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computer Vision</term>
					<term>Semi-supervised Learning</term>
					<term>Semantic Segmentation</term>
					<term>Generative Adversarial Networks !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks -PASCAL VOC 2012, PASCAL-Context, and Cityscapes -the approach achieves new state-of-the-art in semi-supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S EMANTIC segmentation is one of the key computer vision tasks important in various applications including autonomous driving, medical-imaging and robotics. Lately, Deep Convolutional Neural Networks <ref type="bibr" target="#b18">[19]</ref> have demonstrated great results on this task for different datasets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b39">[40]</ref>. However, this success usually comes at the cost of collecting dense pixel-wise annotations -a cumbersome process that involves much manual effort.</p><p>Attempting to alleviate the problem, several methods exploit weaker forms of supervision: image-level labels <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>, bounding boxes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b28">[29]</ref>, or scribbles <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Only two previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref> have considered true semi-supervised learning for semantic segmentation, which requires having a small subset of fully-labeled samples along with a larger set of completely annotation-free images.</p><p>In this work, we propose a dual-branch method for semisupervised semantic segmentation which can effectively learn from annotation-free samples given a very small set of fully-annotated samples. Our design is based on the observation that CNNs trained on limited data are subject to two typical modes of failure; see <ref type="figure" target="#fig_0">Figure 1</ref>(c-d). The first one appears as inaccuracy in low-level details, such as wrong object shapes, inaccurate boundaries, and incoherent surfaces. The second one is the misinterpretation of high-level information, which leads to assigning large image regions to wrong classes.</p><p>The two network branches are designed to separately address those two types of artifacts. To deal with lowlevel errors, we propose an improved GAN-based model, where the segmentation network acts as a generator. It is trained together with a discriminator that classifies between generated and ground-truth segmentation maps. Instead of using the original GAN loss, we propose to use the feature matching loss introduced by Salimans et al. <ref type="bibr" target="#b32">[33]</ref>. Moreover, we introduce the self-training procedure based on the discriminator score which improves the final performance via leveraging high-quality generator predictions as fully labeled samples. For the second type of artifacts, we propose a semi-supervised multi-label classification branch which decides on the classes present in the image and thus aids the segmentation network to make globally consistent decisions. To utilize extra image-level information from unlabeled images, we leverage the success of ensemble-based semisupervised classification (SSL) methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b36">[37]</ref>. The two branches act in a complementary manner and successfully fix both low-level and high-level errors; see <ref type="figure" target="#fig_0">Fig. 1</ref> for a typical example. We demonstrate the effectiveness of our approach on different amounts of labeled data across a range of popular semantic segmentation datasets: PASCAL VOC 2012 <ref type="bibr" target="#b8">[9]</ref>, PASCAL-Context <ref type="bibr" target="#b26">[27]</ref> and Cityscapes <ref type="bibr" target="#b5">[6]</ref>. We consistently achieve the best results compared to existing methods and define the new state of the art in semi-supervised semantic segmentation. Our approach proves particularly efficient when only very few training samples are available: with as little as 2% labeled data we report an 11% performance improvement over the state of the art (see <ref type="figure">Figure 2</ref>).</p><p>We further show that the approach can easily make use of extra image-level weak annotations when those are available. It compares favorably to the existing methods operating in the same setting. The source code of this paper is available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly-supervised and Semi-supervised Segmentation.</head><p>To reduce annotation effort, most existing approaches rely on weakly-and semi-supervised training schemes which use weak labels from the whole dataset like image-level class labels <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref>, bounding boxes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref> or scribbles <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref>, where semi-supervised schemes <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref> additionally use a few pixel-wise segmentation labels.</p><p>Only two recent works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref> consider true semisupervised learning, i.e., they improve semantic segmentation with completely annotation-free images. These methods, like ours, utilize a GAN-based model. However, both approaches use the GAN in a different manner. Souly et al. <ref type="bibr" target="#b33">[34]</ref> use the GAN to generate additional images to enhance the features learned by the segmentation network. They 1. Source code: https://github.com/sud0301/semisup-semseg further extend their semi-supervised method by generating additional class-conditional images.</p><p>Most related to ours is the work by Hung et al. <ref type="bibr" target="#b14">[15]</ref>. They also propose a GAN-based design which enables learning from unlabeled samples. However, our framework is substantially different in many details. Hung et al.use an FCN-based <ref type="bibr" target="#b22">[23]</ref> discriminator which yields a dense probabilistic map for each pixel, whereas we propose an imagewise discriminator. In contrast to the two-stage training process of <ref type="bibr" target="#b22">[23]</ref>, we propose an automatic integration of selftraining based on the GAN training dynamics. Moreover, we propose to use a feature matching loss, which is crucial for the stability of GAN training, especially when only few labeled samples are available. Finally, we add a semisupervised multi-label classification branch for resolving high-level inconsistencies.</p><p>Also Luc et al. <ref type="bibr" target="#b23">[24]</ref> share some common ground with our work, although their work does not comprise semisupervised learning. In their case the GAN replaces CRFpost-processing which enhance low-level consistency in the segmentation maps. Luc et al. <ref type="bibr" target="#b23">[24]</ref> optimize the original GAN loss to encourage predicted segmentation maps to be similar to the ground-truth maps and show that it improves the performance in a fully-supervised setting.</p><p>Semi-supervised Classification. In contrast to segmentation, many semi-supervised methods exist for image classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Oliver et al. <ref type="bibr" target="#b27">[28]</ref>, however, criticize that most of the work lacks realistic evaluation to address real-world conditions. They propose a new experimental methodology closer to the real-world settings. We find that consistency-based semi-supervised classification methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b36">[37]</ref> show improvement over the supervised baseline while satisfying at least two procedures mentioned by Oliver et al. <ref type="bibr" target="#b27">[28]</ref>. Firstly, those methods show improvement over the supervised setting while using a high-quality supervised baseline. Secondly, they can improve upon the pre-trained network using unlabeled data. We use the Mean-Teacher method <ref type="bibr" target="#b36">[37]</ref> in our approach.</p><p>Network Fusion. The approach to fuse spatial and class information by channel-wise selection is inspired by some recent works in other domains. Hu et al. <ref type="bibr" target="#b13">[14]</ref> proposed SE-Net for image classification, which learns to combine spatial and channel-wise information by calibrating channel-wise features maps. Following SE-Net, Zhang et al. <ref type="bibr" target="#b39">[40]</ref> proposed to incorporate class information in semantic segmentation to highlight class-dependent feature maps. Multiple works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> have explored the usage of classification methods, both in a shared and a decoupled manner to constructively use class information for semi-and weakly supervised semantic segmentation. In this work, we use a decoupled approach with late fusion of spatial and class information to remove false positive class channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We propose a two-branch approach to the task of semisupervised semantic segmentation as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The lower branch predicts pixel-wise class labels and is referred to as the Semi-Supervised Semantic Segmentation GAN (s4GAN). The upper branch performs image-level classification and is denoted as the Multi-Label Mean Teacher (MLMT). The core of the s4GAN branch is a standard segmentation network for generating per-pixel class labels given the input image. We combine conventional supervised training with adversarial training, which allows leveraging unlabeled data to improve the prediction quality. The segmentation network acts as a generator and is trained together with a discriminator responsible for distinguishing the ground truth segmentation maps from the generated ones. We additionally treat the outputs of the discriminator as a quality measure and use it to identify the best predictions which are further exploited for self-training.</p><p>The MLMT branch predicts image-level class labels used to filter the s4GAN outputs. Its core is a Mean Teacher classifier, which effectively removes false positive predictions of the segmentation network. The contributions of the two branches are complementary to each other. Their outputs are combined to produce the final result.</p><p>Notations: Our dataset D is split into the labeled part D = {x , y } and the unlabeled part D u = {x u }, where x are the input images and y are the pixel-wise segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">s4GAN for Semantic Segmentation</head><p>In our s4GAN model, the segmentation network S acts as a generator network that takes image x as input and predicts C segmentation maps, one for each class. The discriminator D gets the concatenated input of the original image and its corresponding predicted segmentation. Its task is to match the distribution statistics of the predicted and the real segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Training S</head><p>The segmentation network S is trained with loss L S , which is a combination of three losses: the standard cross-entropy loss, the feature matching loss, and the self-training loss.</p><p>Cross-entropy loss. This is a standard supervised pixelwise cross-entropy loss term L ce . The loss for the output S(x) of size H × W × C is evaluated only for the labeled samples x :</p><formula xml:id="formula_0">L ce = − h,w,c y (h, w, c) log S(x )(h, w, c),<label>(1)</label></formula><p>where y is the ground-truth segmentation mask. Feature matching loss. The feature matching loss L f m <ref type="bibr" target="#b32">[33]</ref> aims to minimize the mean discrepancy between the feature statistics of the predicted, S(x u ) and the groundtruth segmentation maps, y :</p><formula xml:id="formula_1">L f m = E (x ,y )∼D [D k (y ⊕ x )] − E x u ∼D u [D k (S(x u ) ⊕ x u )] ,<label>(2)</label></formula><p>where D k (·) is the intermediate representation of the discriminator network after the k th layer. Both ground-truth and predicted segmentation masks are concatenated with their corresponding input images. Intuitively, it encourages the generator to predict segmentation maps which have the same feature statistics as the ground truth, and therefore also qualitatively resemble the ground truth. This loss is used on the unlabeled samples x u , thus forcing plausible solutions even for cases where dense labels are unavailable. Self-training loss. During GAN training, the discriminator (D) and the generator (G) networks need to be balanced. If D starts off being too strong, it does not provide any useful learning signal for G. In order to facilitate such balanced dynamics, we introduce the self-training (ST) loss. The main idea is to pick the best generator outputs (i.e. those able to fool D) which do not have the corresponding ground truth, and reuse them for supervised training. Intuitively, this pushes G more to produce predictions which D cannot distinguish from the real ones. This impedes the progress of D and does not allow it to become too strong.</p><p>Technically, the output of D varies between 0 and 1, where 0 should be assigned to the predicted segmentation maps and 1 to the ground-truth segmentation maps. We use this score as a confidence measure for the quality of the predicted segmentations. High-quality predictions are used for supervised training, i.e. we calculate the standard crossentropy loss based on them. The self-training loss term L st is thus defined as:</p><formula xml:id="formula_2">L st =    − h,w,c y * log S(x u ), if D(S(x u ) ≥ γ 0, otherwise,<label>(3)</label></formula><p>where γ is the confidence threshold which controls how certain D needs to be about the prediction in order for it to be used in self-training; y * are the pseudo pixel-wise labels generated from the prediction S(x u ) of the segmentation network.</p><p>The final training objective L S is composed of the three described terms:</p><formula xml:id="formula_3">L S = L ce + λ f m L f m + λ st L st ,<label>(4)</label></formula><p>where λ f m , λ st &gt; 0 are the corresponding weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Training D</head><p>The discriminator network is trained with the original GAN objective as proposed by Goodfellow et al.</p><formula xml:id="formula_4">[10] L D =E (x ,y )∼D [log D(y ⊕ x )] + E x u ∼D u [log(1 − D(S(x u ) ⊕ x u ))],<label>(5)</label></formula><p>where ⊕ denotes concatenation along the channel dimension. Following the original GAN idea, D learns to differentiate between the real y and the fake segmentation masks S(x u ) concatenated with the corresponding input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-label Semi-supervised Classification</head><p>We extend an ensemble-based semi-supervised classification method (Mean Teacher) <ref type="bibr" target="#b36">[37]</ref> for semi-supervised multilabel image classification. This model consists of two networks: a student network G and a teacher network H. Both networks receive the same images under different small perturbations. The weights (θ ) of the teacher network are the exponential moving average (online ensemble) of the student network's weights (θ). The predictions made by the student model are encouraged to be consistent with the predictions of the teacher model using the consistency loss which is the mean-squared error between the two predictions.</p><p>We optimize the student network using the categorical cross-entropy loss L cce for labeled samples x , and using the consistency loss L cons for all available samples (x u, ):</p><formula xml:id="formula_5">L M T = − c z (c) log(G θ (x )(c)) Lcce + λ cons G θ (x (u, ) ) − H θ (x (u, ) ) 2 Lcons ,<label>(6)</label></formula><p>where x and x are differently augmented images for student and teacher network respectively, z is the multihot vector for ground-truth class labels. The parameter λ cons &gt; 0 controls the weight of the consistency loss in L M T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Fusion</head><p>The two described branches are trained separately. For evaluation, the output of the classification branch simply deactivates the segmentation maps of those classes not present in the input image:</p><formula xml:id="formula_6">S(x) c = 0 if G(x c ) ≤ τ S(x) c otherwise<label>(7)</label></formula><p>where S(x) c is the segmentation map for class c, G(x) c is the soft output of the MLMT-branch, and τ = 0.2 is a threshold on that soft output obtained by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>The proposed approach was evaluated on the PASCAL VOC 2012 segmentation benchmark, the PASCAL-Context dataset, and the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>PASCAL VOC 2012. The dataset consists of 20 foreground object classes and one background class. We use the augmented annotation set which consists of 10582 training images and 1449 validation images. The training set contains 1464 images from the original PASCAL data and 9118 extra images from the Segmentation Boundary Dataset (SBD) <ref type="bibr" target="#b10">[11]</ref>.</p><p>The training data augmentations include random resizing, cropping to 321×321, and horizontal flipping. All the results for the PASCAL VOC dataset are shown on the validation set.</p><p>PASCAL-Context. This is a whole scene parsing dataset containing 4,998 training and 5,105 testing images with dense semantic labels. Following the previous work <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b39">[40]</ref>, we used semantic labels for 60 most frequent classes including the background class. The training data augmentations were the same as for the PASCAL VOC dataset.</p><p>Cityscapes. This is a driving scene dataset with 2975, 500, 1525 densely annotated images for training, validation, and testing, and contains 19 classes. We downsample the original 1024 × 2048 images by a factor 2. The training data is augmented with random crops of size 256 × 512 and horizontal flipping. All the results on the Cityscapes dataset are shown on the validation set.</p><p>Evaluation Metric. We report mean Intersection-over-Union (mIoU) for all our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Network Architecture</head><p>Semi-supervised Segmentation GAN. We used DeepLabv2 <ref type="bibr" target="#b3">[4]</ref> as our main segmentation network. Due to memory constraints, we used a single-scale variant of it. The discriminator network of the GAN model was a standard binary classification network consisting of 4 convolutional layers with 4×4 kernels with {64, 128, 256, 512} channels, each followed by a Leaky-ReLU <ref type="bibr" target="#b24">[25]</ref> activation with negative slope of 0.2 and a dropout <ref type="bibr" target="#b34">[35]</ref> layer with dropout probability of 0.5. We found this high dropout rate to be crucial for stable GAN training. The last convolutional layer is followed by global average pooling and a fully-connected layer. The output vector representation produced after global average pooling is used for evaluating the feature matching loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Multi-label Classification Network.</head><p>We used ResNet-101 <ref type="bibr" target="#b11">[12]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b7">[8]</ref> as the base architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training details</head><p>Similar to <ref type="bibr" target="#b3">[4]</ref>, we used the poly-learning policy for both the segmentation and the discriminator networks of the GAN model, where the base learning rate was multiplied by a factor of ((1 − iter max iter ) pow ) in every iteration. In our setup, pow = 0.9. Following the learning scheme in <ref type="bibr" target="#b14">[15]</ref>, the segmentation network was optimized using the SGD optimizer with a base learning rate of 2.5e-4, momentum 0.9 and a weight decay of 5e-4. The discriminator network was optimized using the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with a base learning rate of 1e-4 and betas set to (0.9, 0.99). The model was trained for 35K iterations on the PAS-CAL VOC and Cityscapes dataset, and for 50K iterations on the PASCAL-Context dataset. All the learning hyperparameters remained the same for all datasets except for the Cityscapes dataset, where the base learning rate of the discriminator network was set to 1e-5. We used a batch size of 8 for both PASCAL datasets and a batch size of 5 for the Cityscapes dataset. Through cross-validation, we find the optimal loss weights: λ f m = 0.1, λ st = 1.0, λ cons = 1.0 and τ = 0.2. These hyper-parameters remained the same for all datasets, whereas we set γ = 0.6 for both PASCAL datasets and 0.7 for the Cityscapes dataset. Our implementation is based on the open source toolbox Pytorch <ref type="bibr" target="#b29">[30]</ref>. All the experiments were run on a Nvidia Tesla P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Baselines</head><p>We compare to the DeepLabv2 <ref type="bibr" target="#b3">[4]</ref> network as the fullysupervised baseline approach, which was trained only on the labeled part of the dataset. DeepLabv2 makes use of dilated convolutions to enlarge the receptive field size and incorporate larger context, and introduces atrous spatial pyramidal pooling to capture image context at multiple levels.</p><p>Our main semi-supervised baseline is the approach proposed by Hung et al. <ref type="bibr" target="#b14">[15]</ref>. Apart from the differences described in Sec. 2, they also use a two-stage GAN training. In the first stage, both D and G are trained only using labeled data. In the second stage, D's outputs are used to update G using unlabeled samples, while D itself is further trained only on the labeled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Semi-supervised Semantic Segmentation</head><p>We evaluated our approach with different ratios of labeled and unlabeled samples. 1/50, 1/20, 1/8, 1/4 are the fractions of the total training images in the dataset that are used as labeled data, the rest of the data was used without labels. The labeled samples in the data splits were randomly sampled from the whole dataset, and the same data splits were used for all the baselines.  <ref type="bibr" target="#b21">[22]</ref> dataset. We achieve improved results compared to the previous method for all data splits. Our method achieves a performance increase of 5% to 12% over the baseline for different data splits by utilizing unlabeled samples without pre-training the network on any segmentation dataset. Notably, the approach works well even with only 2% (1/50) of labeled data. <ref type="figure" target="#fig_2">Figure 4</ref> shows qualitatively how our method helps remove artifacts produced by other methods. We also validated our approach with COCO pre-training to directly compare with Hung et al. <ref type="bibr" target="#b14">[15]</ref>, and achieved an improvement of 6.1 mIoU points over them for the 1/50 split. We speculate that <ref type="bibr" target="#b14">[15]</ref> is inferior in the low-data regime due to the two-stage GAN training, where the discriminator is only updated based on the labeled samples. This effectively reduces the amount Original Ground truth Baseline Ours <ref type="figure">Fig. 5</ref>. Qualitative results on the PASCAL-Context dataset using 1/8 labeled samples. Our approach produces improved results compared to the baseline. We compare our ('Ours') results with the fully-supervised baseline which is trained only on the labeled subset of data.</p><p>of data it sees during training, which can easily lead to overfitting. We conducted our initial experiments using Deeplabv3+ as the backbone architecture. Deeplabv3+ is unstable in the low-data 'supervised only' setting. It is only superior, if there is much labeled data. Thus, for a more informative experiment, we rather used Deeplabv2. However, our semisupervised model achieves even better performance with Deeplabv3+ than with the DeepLabv2-based model, see <ref type="table" target="#tab_2">Table 2</ref>. The results were obtained with cross-validation to avoid hyper-parameter search on the evaluation set. We also submitted our results to the PASCAL test server. Due to the benchmark restrictions we could only submit one random split (5% labeled samples). The results are consistent with our previous conclusions: 50.1 mIoU for baseline DeepLabv2 vs 60.5 for our semi-supervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-Context Dataset.</head><p>Our approach successfully generalizes to the whole scene parsing PASCAL-Context dataset. <ref type="table" target="#tab_3">Table 3</ref> shows the performance on two splits (1/8 and 1/4 labeled data) of PASCAL-Context. Although this dataset is smaller and more difficult than PASCAL VOC, there is still an improvement over the baseline of 3.2% and 2.4% for the 1/8 and 1/4 splits, respectively. <ref type="figure">Fig. 5</ref> show qualitative results on the PASCAL-Context test set using 1/8 labeled samples and the remaining unlabeled samples. PASCAL-Context is a smaller and harder dataset as compared to PASCAL VOC, therefore the results are not as visually appealing. Still, there is a clear improvement over the baseline.  <ref type="table" target="#tab_4">Table 4</ref>. The distribution of different classes in this dataset is highly imbalanced. The vast majority of the classes are present in almost every image, and the few remaining classes occur only scarcely. In this situation, a classifier that eliminates labels of non-existing classes does not help, thus, our MLMT branch was ineffective for the Cityscapes dataset.</p><p>Orig GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head><p>Orig GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head><p>Orig GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head><p>Orig GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head><p>Orig GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head><p>Orig GT    <ref type="figure" target="#fig_3">6</ref> show qualitative results obtained using our approach with 1/8 labeled samples and the remaining unlabeled samples. The differences on the Cityscapes dataset are subtle, therefore we include the zoomed-in views of informative areas. On images from <ref type="figure" target="#fig_3">Fig. 6</ref> show our approach yields improvement over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Studies</head><p>All the experiments for the ablation studies are shown on the PASCAL VOC dataset without COCO pre-training.</p><p>Contribution of the two branches. <ref type="table" target="#tab_5">Table 5</ref> shows the contribution of the s4GAN branch and the MLMT branch. The s4GAN branch is able to extract extra dense information using unlabeled images. It improves the shape of the segmented objects, makes the segmentation prediction more coherent by filling small holes, and improves the fine boundaries between the foreground and background. We showcase these improvements in <ref type="figure" target="#fig_4">Figure  7</ref>(e).</p><p>The MLMT branch plays a complementary role and removes the false positives from the predictions. <ref type="figure" target="#fig_4">Figure 7(d</ref> shows the improvement using the 'MLMT branch only' with the segmentation baseline method and <ref type="figure" target="#fig_4">Figure 7</ref>(g) shows the improvement using the MLMT branch together with the s4GAN branch. The MLMT branch makes use of unlabeled images to extract image-level information about the presence of the certain classes in the image. For some cases, the s4GAN branch introduces new artifacts which are also filtered out by the MLMT branch. This effect is shown in the bottom-row example of <ref type="figure" target="#fig_4">Figure 7</ref>.</p><formula xml:id="formula_7">) (a) Original (b) GT (c) Baseline (d) Ours</formula><p>In certain situations our method produces imprecise predictions. Sometimes object classes with multiple protrusions like plant leaves, chair legs, etc. are under-segmented by the s4GAN branch, as shown in <ref type="figure" target="#fig_5">Figure 8(top)</ref>. Occasionally, our approach can identify certain ambiguous foreground objects as one of the classes, as shown in <ref type="figure" target="#fig_5">Figure 8</ref>(bottom). Also, there exist few cases where some true positive results are wrongly predicted by the classifier. However, both qualitative and quantitative results confirm that these failure cases are outweighed by the positive effect of the proposed techniques. In <ref type="figure">Fig. 9</ref>, we include a few failure cases for PASCAL-context dataset using our approach. <ref type="figure" target="#fig_0">Fig.10</ref> shows a few failure cases for Cityscapes dataset where few thin objects were not segmented properly using our approach. Different s4GAN branch loss terms. We trained the generator network with a combination of the cross-entropy (CE) loss, the feature matching (FM) loss, and the self-training (ST) loss.</p><p>To justify this configuration, we compare the system performance when using different loss terms; see <ref type="table" target="#tab_6">Table 6</ref>. There is a consistent performance increase when adding all the proposed loss terms. We found it crucial for the system stability to train using the FM loss and not the standard GAN loss.  <ref type="figure" target="#fig_0">Figure 11</ref> illustrates the effect of using our proposed selftraining loss. We plot how the discriminator score changes during the course of training. The scores are averaged over 100 iterations of fake (generated) and real (ground-truth) samples separately. As discussed in Sec. 3.1.1, adding the ST loss impedes the progress of the discriminator and does not allow it to become overly confident, that is, draws its predicted scores towards 0.5. This has a positive effect on the generator performance, in particular with few labeled samples, as can be seen from the last line of <ref type="table" target="#tab_6">Table 6</ref>.</p><p>Semi-supervised multi-label classification. In this experiment, we compared the performance of the proposed MLMT branch with a standard supervised classifier. <ref type="table" target="#tab_5">Table  5</ref> shows that we already get an improvement of 1.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Ground truth Baseline Ours <ref type="figure">Fig. 9</ref>. Qualitative results on the PASCAL-Context dataset using 1/8 labeled samples. Failure of our approach. We compare our ('Ours') results with the fully-supervised baseline which is trained only on the labeled subset of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orig GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Ours</head><p>Orig GT Base Ours <ref type="figure" target="#fig_0">Fig. 10</ref>. Qualitative results on the Cityscapes dataset using 1/8 labeled samples. Failures of our approach. We compare our ('Ours') results with the fully-supervised baseline ('Base') which is trained only on the labeled subset of data.</p><p>over the s4GAN performance just by using a CNN-based classifier <ref type="bibr" target="#b11">[12]</ref>, but when we further add the consistencybased semi-supervised classification approach, we observe that the performance improvement increases to 2%. More detailed comparison between the two classification methods is included in the supplementary file. We also conducted a simple heuristic experiment where we deactivate the predicted class channels which have pixel count less than a threshold. In <ref type="table" target="#tab_5">Table 5</ref>, 's4GAN + Threshold' refers to the case where a single threshold is set for all the classes and 's4GAN + Class-wise Threshold' refers to the case where each class has its best respective threshold. We search for the best performing thresholds on the validation set in the range from 1K to 12K pixels at an increment step of 1K. multi-label classification and MLMT-based semi-supervised multi-label classification independent of the segmentation model. <ref type="figure" target="#fig_0">Figure 12</ref> shows the comparison between the ROC curves of the two methods on the task of multi-label classification. The MLMT classifier obtains a lower false positive rate for the same true positive rate. The effect is even more pronounced when not using ImageNet pre-training; see <ref type="figure" target="#fig_0">Figure 12</ref>(b). This mode of operation is important for domains where ImageNet pre-training does not help, e.g. bio-medical image analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Semi-supervised Semantic Segmentation with Weaklabels</head><p>To further validate the effectiveness of our approach, we compare it to other semi-supervised segmentation methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref> that utilize extra weak image-level annotations.</p><p>Here, we compare the performance of our approach with methods that use extra image-level annotations i.e.1,464 strongly (w/ segmentation masks) annotated images from the original PASCAL VOC dataset and 9,118 weakly (imagelevel) annotated images from the augmented SBD dataset. To use extra image-level annotations, we train the MLMT branch using extra image-level labels for improved multilabel classification. The training procedure and hyperparameters remain exactly same as in the previous semisupervised setting.  semantic segmentation results with extra ∼9K image-level annotations. We achieve an improvement of 5.2% over the baseline. Unlike previous methods, our approach does not utilize the CRF post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we presented a two-branch approach to the task of semi-supervised semantic segmentation. The branches are designed to alleviate both low-level and high-level artifacts which often occur when working in a low-data regime. The effectiveness of this design is demonstrated in a series of extensive experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•Fig. 1 .</head><label>1</label><figDesc>The authors are with the Computer Science Department at the University of Freiburg, Freiburg im Breisgau, Germany. E-mail: {mittal, tatarchm, brox}@cs.uni-freiburg.de An image from the PASCAL VOC dataset (a) and its groundtruth segmentation mask (b). Prediction (c) is obtained with supervised training on 5% labeled samples. Using the other 95% unlabeled images, our GAN-based branch improves the shape estimation (d). The second branch adds high-level consistency by removing false positives (e). (f) shows the output when training on 100% pixel-wise labeled samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of our proposed semi-supervised segmentation approach. The s4GAN branch is a GAN-based model which improves the low-level details in the segmentation prediction. The MLMT branch performs semi-supervised multi-label classification to exploit class-level information for removing false-positive predictions from the segmentation map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results obtained using our semi-supervised segmentation approach on the PASCAL VOC dataset with 5% labeled data without COCO pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results on the Cityscapes dataset using 1/8 labeled samples without COCO pre-training. The proposed semi-supervised approach produces improved results compared to the baseline. We compare our ('Ours') results with the fully-supervised baseline ('Base') which is trained only on the labeled subset of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Ablation study on the PASCAL VOC dataset showing the contribution of the MLMT (d) and the s4GAN (e) branches individually. The s4GAN and the MLMT branches together show a complementary behaviour fixing both low and high-level artifacts (g). These results are obtained using 5% labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Failure cases. Sometimes, our approach can lead to undersegmentation of objects with multiple protrusions, shown in the top row. Bottom row shows a case where an ambiguous foreground object is falsely marked as one of the classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Figure 7(f) and (g) show the effect of adding a CNN-based classifier and an MT-based semi-supervised classifier respectively.We also analyze the performance of the CNN-based Evolution of the discriminator output during the course of training averaged over real and fake samples separately. Using the self-training loss (w/ ST) prevents D from becoming overly strong and results in better training dynamics compared to the case when self-training is disabled (w/o ST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>ROC curves for CNN-based classification and MT-based semisupervised classification method using 5% labeled data with (a) and without (b) ImageNet pre-training. MT produces fewer false positives, especially when training from scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Semi-supervised semantic segmentation results on the PASCAL VOC dataset without and with COCO pre-training.Table 1shows the segmentation results on the PASCAL VOC dataset with and without pretraining on the Microsoft COCO</figDesc><table><row><cell cols="3">without COCO pre-training</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Labeled Data</cell><cell></cell></row><row><cell>Method</cell><cell>1/50</cell><cell>1/20</cell><cell>1/8</cell><cell>Full</cell></row><row><cell>DeepLabv2</cell><cell>48.3</cell><cell>56.8</cell><cell>62.0</cell><cell>70.7</cell></row><row><cell>Hung et al. [15]</cell><cell>49.2</cell><cell>59.1</cell><cell>64.3</cell><cell>71.4</cell></row><row><cell>Ours (s4GAN only)</cell><cell>58.1</cell><cell>60.9</cell><cell>65.4</cell><cell>71.2</cell></row><row><cell>Ours (s4GAN + MLMT)</cell><cell>60.4</cell><cell>62.9</cell><cell>67.3</cell><cell>73.2</cell></row><row><cell cols="3">with COCO pre-training</cell><cell></cell><cell></cell></row><row><cell>DeepLabv2</cell><cell>53.2</cell><cell>58.7</cell><cell>65.2</cell><cell>73.6</cell></row><row><cell>Hung et al. [15]</cell><cell>57.2</cell><cell>64.7</cell><cell>69.5</cell><cell>74.9</cell></row><row><cell>Ours (s4GAN only)</cell><cell>60.9</cell><cell>66.4</cell><cell>69.8</cell><cell>73.9</cell></row><row><cell>Ours (s4GAN + MLMT)</cell><cell>63.3</cell><cell>67.2</cell><cell>71.4</cell><cell>75.6</cell></row><row><cell>PASCAL VOC Dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Results on PASCAL VOC without COCO pre-training using different backbone architectures.</figDesc><table><row><cell>Method</cell><cell>1/50</cell><cell>1/20</cell><cell>1/8 Full</cell></row><row><cell>Deeplabv2 (v2)</cell><cell>48.3</cell><cell>56.8</cell><cell>62.0 70.7</cell></row><row><cell>Ours v2 (s4GAN+ MLMT)</cell><cell>60.4</cell><cell>62.9</cell><cell>67.3 73.2</cell></row><row><cell>Deeplabv3+ (v3+)</cell><cell cols="3">unstable unstable 63.5 74.6</cell></row><row><cell>Ours v3+ (s4GAN+ MLMT)</cell><cell>62.6</cell><cell>66.6</cell><cell>70.4 74.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Semi-supervised semantic segmentation results on the PASCAL-Context dataset without COCO pre-training.</figDesc><table><row><cell></cell><cell></cell><cell>Labeled Data</cell><cell></cell></row><row><cell>Method</cell><cell>1/8</cell><cell>1/4</cell><cell>Full</cell></row><row><cell>DeepLabv2</cell><cell>32.1</cell><cell>35.4</cell><cell>41.0</cell></row><row><cell>Hung et al. [15]</cell><cell>32.8</cell><cell>34.8</cell><cell>39.1</cell></row><row><cell>Ours (s4GAN only)</cell><cell>34.4</cell><cell>37.1</cell><cell>40.8</cell></row><row><cell>Ours (s4GAN + MLMT)</cell><cell>35.3</cell><cell>37.8</cell><cell>41.1</cell></row><row><cell cols="4">Cityscapes Dataset. On the Cityscapes dataset, the s4GAN</cell></row><row><cell cols="4">branch yields an improvement over the baseline of 3.1% and</cell></row><row><cell cols="3">1.7% for the 1/8 and 1/4 data splits respectively; see</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Semi-supervised semantic segmentation results on the Cityscapes dataset without COCO pre-training.</figDesc><table><row><cell></cell><cell></cell><cell>Labeled Data</cell><cell></cell></row><row><cell>Method</cell><cell>1/8</cell><cell>1/4</cell><cell>Full</cell></row><row><cell>DeepLabv2</cell><cell>56.2</cell><cell>60.2</cell><cell>66.0</cell></row><row><cell>Hung et al. [15]</cell><cell>57.1</cell><cell>60.5</cell><cell>66.2</cell></row><row><cell>Ours (s4GAN only)</cell><cell>59.3</cell><cell>61.9</cell><cell>65.8</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Ablation study of the contribution of each branch. Results are shown for the 5:95 data split on the PASCAL VOC dataset.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell></cell><cell>mIoU</cell></row><row><cell></cell><cell cols="2">labeled(%) unlabeled(%)</cell><cell></cell></row><row><cell>DeepLabv2</cell><cell>5</cell><cell>None</cell><cell>56.8</cell></row><row><cell>s4GAN only</cell><cell>5</cell><cell>95</cell><cell>60.9</cell></row><row><cell>MLMT only</cell><cell>5</cell><cell>95</cell><cell>59.0</cell></row><row><cell>s4GAN + Threshold</cell><cell>5</cell><cell>95</cell><cell>61.2</cell></row><row><cell>s4GAN + Class-wise Threshold</cell><cell>5</cell><cell>95</cell><cell>61.5</cell></row><row><cell>s4GAN + CNN</cell><cell>5</cell><cell>95</cell><cell>62.2</cell></row><row><cell>s4GAN + MLMT</cell><cell>5</cell><cell>95</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Ablation study of different GAN loss terms for the generator on the PASCAL VOC dataset. SGAN refers to the standard GAN loss<ref type="bibr" target="#b9">[10]</ref>, FM refers to the feature-matching loss and ST refers to the self-training loss.</figDesc><table><row><cell></cell><cell></cell><cell>Labeled Data</cell><cell></cell></row><row><cell>Loss Terms</cell><cell>1/50</cell><cell>1/20</cell><cell>1/8</cell></row><row><cell>CE only</cell><cell>48.3</cell><cell>56.8</cell><cell>62.0</cell></row><row><cell>CE + SGAN [10]</cell><cell>54.0</cell><cell>57.1</cell><cell>62.5</cell></row><row><cell>CE + FM</cell><cell>55.4</cell><cell>58.4</cell><cell>63.9</cell></row><row><cell>CE + FM + ST</cell><cell>58.1</cell><cell>60.9</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Semi-supervised semantic segmentation results on the PASCAL VOC dataset using extra weak image-level annotations. Data splits (A/B/C) refers to the usage of A pixel-wise labeled samples, B image-level labeled samples and C unlabeled samples.</figDesc><table><row><cell></cell><cell cols="3">Data Split (Strong/Weak/Unlab)</cell></row><row><cell>Method</cell><cell cols="3">1.4K/0/9K 1.4K/9K/0 All/0/0</cell></row><row><cell>DeepLab-CRF-LargeFOV [3] WSSL (CRF) a [29] MDC a [39] MDC (CRF) a [39]</cell><cell>62.5 c ---</cell><cell>-64.6 62.7 65.7</cell><cell>67.6 c ---</cell></row><row><cell>DeepLabv2</cell><cell>65.7</cell><cell>-</cell><cell>70.7</cell></row><row><cell>Ours (s4GAN only) b</cell><cell>67.5</cell><cell>-</cell><cell>71.2</cell></row><row><cell>Ours (s4GAN + MLMT) b</cell><cell>69.6</cell><cell>70.9</cell><cell>72.9</cell></row></table><note>a Base network: DeepLab-LargeFOV,b Base network: DeepLabv2, c As reported in [29]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>summarizes the semi-supervised 0.000 0.025 0.050 0.075 0.100 0.125</figDesc><table><row><cell>0.75 0.80 0.85 0.90 0.95 1.00 True Positive Rate</cell><cell>CNN MLMT</cell><cell>ROC Curve</cell><cell>0.2 0.4 0.6 0.8 1.0 True Positive Rate</cell><cell cols="2">CNN MLMT</cell><cell>ROC Curve</cell></row><row><cell></cell><cell cols="2">False Positive Rate</cell><cell>0.0 0.0</cell><cell>0.2</cell><cell cols="2">0.4 False Positive Rate 0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This study was supported by the German Federal Ministry of Education and Research via the project Deep-PTL and by the Intel Network of Intelligent Systems. We also thank Facebook for their P100 server donation and gift funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving consistency-based semi-supervised learning with weight averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05594</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zisserman. The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribblesupervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Realistic evaluation of semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On regularized losses for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
