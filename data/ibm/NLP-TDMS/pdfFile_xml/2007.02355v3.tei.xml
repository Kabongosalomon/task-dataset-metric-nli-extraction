<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HoughNet: Integrating near and long-range evidence for bottom-up object detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
							<email>nermin@ceng.metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samet Hicsonmez</surname></persName>
							<email>samethicsonmez@hacettepe.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Hacettepe University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HoughNet: Integrating near and long-range evidence for bottom-up object detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Detection</term>
					<term>Voting</term>
					<term>Bottom-up recognition</term>
					<term>Hough Transform</term>
					<term>Image-to-image translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents HoughNet, a one-stage, anchor-free, votingbased, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a logpolar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves 46.4 AP (and 65.1 AP50), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in another task, namely, "labels to photo" image generation by integrating the voting module of HoughNet to two different GAN models and showing that the accuracy is significantly improved in both cases. Code is available at https://github.com/nerminsamet/houghnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has brought on remarkable improvements in object detection. Performance on widely used benchmark datasets, as measured by mean averageprecision (mAP), has at least doubled (from 0.33 mAP <ref type="bibr" target="#b14">[15]</ref> [11] to 0.80 mAP on PASCAL VOC <ref type="bibr" target="#b16">[17]</ref>; and from 0.2 mAP <ref type="bibr" target="#b27">[28]</ref> to around 0.5 mAP on COCO <ref type="bibr" target="#b26">[27]</ref>) in comparison to the previous generation (pre-deep-learning, shallow) methods. Current state-of-the-art, deep learning based object detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref> predominantly follow a top-down approach where objects are detected holistically via rectangular region classification. This was not the case with the pre-deeplearning methods. The bottom-up approach was a major research focus as exemplified by the prominent voting-based (the Implicit Shape Model <ref type="bibr" target="#b23">[24]</ref>) and part-based (the Deformable Parts Model <ref type="bibr" target="#b9">[10]</ref>) methods. However, today, among deep learning based object detectors, the bottom-up approach has not been sufficiently explored with a few exceptions (e.g. CornerNet <ref type="bibr" target="#b22">[23]</ref>, ExtremeNet <ref type="bibr" target="#b51">[52]</ref>). In addition to the local votes originating from the mouse itself, there are strong votes from nearby "keyboard" objects, which shows that HoughNet is able to utilize both short and long-range evidence for detection. More examples can be seen in <ref type="figure">Fig. 4</ref> In this paper, we propose Hough-Net, a one-stage, anchor-free, votingbased, bottom-up object detection method. HoughNet is based on the idea of voting, inspired by the Generalized Hough Transform <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. In its most generic form, the goal of GHT is to detect a whole shape based on its parts. Each part produces a hypothesis, i.e. casts its vote, regarding the location of the whole shape. Then, the location with the most votes is selected as the result. Similarly, in HoughNet, the presence of an object belonging to a certain class at a particular location is determined by the sum of the class-conditional votes cast on that location <ref type="figure" target="#fig_0">(Fig. 1)</ref>. HoughNet processes the input image using a convolutional neural network to produce an intermediate score map per class. Scores in these maps indicate the presence of visual structures that would support the detection of an object instance. These structures could be object parts, partial objects or patterns belonging to the same or other classes. We name these score maps as "visual evidence" maps. Each spatial location in a visual evidence map votes for target areas that are likely to contain objects. Target areas are determined by placing a log-polar grid, which we call the "vote field," centered at the voter location. The purpose of using a log-polar vote field is to reduce the spatial precision of the vote as the distance between voter location and target area increases. This is inspired by foveated vision systems found in nature, where the spatial resolution rapidly decreases from the fovea towards the periphery <ref type="bibr" target="#b21">[22]</ref>. Once all visual evidence is processed through voting, the accumulated votes are recorded in object presence maps, where the peaks indicate the presence of object instances.</p><p>Current state-of-the-art object detectors rely on local (or short-range) visual evidence to decide whether there is an object at that location (as in top-down methods) or an important keypoint such as a corner (as in bottom-up methods). On the other hand, HoughNet is able to integrate both short and long-range visual evidence through voting. An example is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the detected mouse gets strong votes from two keyboards, one of which is literally at the other side of the image. In another example ( <ref type="figure">Fig. 4</ref>, row 2, col 1), a ball on the right-edge of the image is voting for the baseball bat on the left-edge. On the COCO dataset, HoughNet achieves comparable results with the state-of-the-art bottom-up detector CenterNet <ref type="bibr" target="#b8">[9]</ref>, while being the fastest object detector among bottom-up detectors. It outperforms prominent one-stage (RetinaNet <ref type="bibr" target="#b26">[27]</ref>) and two-stage detectors (Faster RCNN <ref type="bibr" target="#b40">[41]</ref>, Mask RCNN <ref type="bibr" target="#b15">[16]</ref>). To further show the effectiveness of our approach, we used the voting module of HoughNet in another task, namely, "labels to photo" image generation. Specifically, we integrated the voting module to two different GAN models (CycleGAN <ref type="bibr" target="#b53">[54]</ref> and Pix2Pix <ref type="bibr" target="#b19">[20]</ref>) and showed that the performance is improved in both cases.</p><p>Our main contribution in this work is HoughNet, a voting-based bottom-up object detection method that is able to integrate near and long-range evidence for object detection. As a minor contribution, we created a mini training set called "COCO minitrain", a curated subset of COCO train2017 set, to reduce the computational cost of ablation experiments. We validated COCO minitrain in two ways by (i) showing that the COCO val2017 performance of a model trained on COCO minitrain is strongly positively correlated with the performance of the same model trained on COCO train2017, and (ii) showing that COCO minitrain set preserves the object instance statististics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Methods using log-polar fields/representations. Many biological systems have foveated vision where the spatial resolution decreases from the fovea (point of fixation) towards the periphery. Inspired by this phenomenon, computer vision researchers have used log-polar fields for many different purposes including shape description <ref type="bibr" target="#b3">[4]</ref>, feature extraction <ref type="bibr" target="#b0">[1]</ref> and foveated sampling/imaging <ref type="bibr" target="#b43">[44]</ref>.</p><p>Non-deep, voting-based object detection methods. In the pre-deep learning era, generalized Hough Transform (GHT) based voting methods have been used for object detection. The most influential work was the Implicit Shape Model (ISM) <ref type="bibr" target="#b23">[24]</ref>. In ISM, Leibe et al. <ref type="bibr" target="#b23">[24]</ref> applied GHT for object detection/recognition and segmentation. During the training of the ISM, first, interest points are extracted and then a visual codebook (i.e. dictionary) is created using an unsupervised clustering algorithm applied on the patches extracted around interest points. Next, the algorithm matches the patches around each interest point to the visual word with the smallest distance. In the last step, the positions of the patches relative to the center of the object are associated with the corresponding visual words and stored in a table. During inference, patches extracted around interest points are matched to closest visual words. Each matched visual word casts votes for the object center. In the last stage, the location that has the most votes is identified, and object detection is performed using the patches that vote for this location. Later, ISM was further extended with discriminative frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>. Okada <ref type="bibr" target="#b32">[33]</ref> ensembled randomized trees using image patches as voting elements. Similarly, Gall and Lempitsky <ref type="bibr" target="#b13">[14]</ref> proposed to learn a mapping between image patches and votes using random forest framework. In order to fix the accumulation of inconsistent votes of ISM, Razavi et al. <ref type="bibr" target="#b36">[37]</ref> augmented the Hough space with latent variables to enforce consistency between votes. In Max-margin Hough Transform <ref type="bibr" target="#b31">[32]</ref>, Maji and Malik showed the importance of learning visual words in a discriminative max-margin framework. Barinova et al. <ref type="bibr" target="#b2">[3]</ref> detected multiple objects using energy optimization instead of non-maxima suppression peak selection of ISM.</p><p>HoughNet is similar to ISM and its variants described above only at the idea level as all are voting based methods. There are two major differences: (i) HoughNet uses deep neural networks for part/feature (i.e. visual evidence) estimation, whereas ISM uses hand-crafted features; (ii) ISM uses a discrete set of visual words (obtained by unsupervised clustering) and each word's vote is exactly known (stored in a table) after training. In HoughNet, however, there is not a discrete set of words and vote is carried through a log-polar vote field which takes into account the location precision as a function of target area.</p><p>Bottom-up object detection methods. Apart from the classical one-stage <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> vs. two-stage <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> categorization of object detectors, we can also categorize the current approaches into two: top-down and bottom-up. In the top-down approach <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>, a near-exhaustive list of object hypotheses in the form of rectangular boxes are generated and objects are predicted in a holistic manner based on these boxes. Designing the hypotheses space (e.g. parameters of anchor boxes) is a problem by itself <ref type="bibr" target="#b44">[45]</ref>. Typically, a single template is responsible for the detection of the whole object. In this sense, recent anchorfree methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b50">51]</ref> are also top-down. On the other hand, in the bottom-up approach, objects emerge from the detection of parts (or sub-object structures). For example, in CornerNet <ref type="bibr" target="#b22">[23]</ref>, top-left and bottom-right corners of objects are detected first, and then, they are paired to form whole objects. Following Cor-nerNet, ExtremeNet <ref type="bibr" target="#b51">[52]</ref> groups extreme points (e.g. left-most, etc.) and center points to form objects. Together with corner pairs of CornerNet <ref type="bibr" target="#b22">[23]</ref>, Center-Net <ref type="bibr" target="#b8">[9]</ref> adds center point to model each object as a triplet. HoughNet follows the bottom-up approach based on a voting strategy: object presence score is voted (aggregated) from a wide area covering short and long-range evidence.</p><p>Deep, voting-based object detection methods. Qi et al. <ref type="bibr" target="#b35">[36]</ref> apply Hough voting for 3D object detection in point clouds. Sheshkus et al. <ref type="bibr" target="#b41">[42]</ref> utilize Hough transform for vanishing points detection in the documents. For automatic pedestrian and car detection, Gabriel et al. <ref type="bibr" target="#b12">[13]</ref> proposed using discriminative generalized Hough transform for proposal generation in edge images, later to further refine the boxes, they fed these proposals to deep networks. In the deep learning era, we are not the first to use a log-polar vote field in a voting-based model. Lifshitz et al. <ref type="bibr" target="#b24">[25]</ref> used a log-polar map to estimate keypoints for single person human pose estimation. Apart from the fact that <ref type="bibr" target="#b24">[25]</ref> is tackling the human pose estimation task, there are several subtle differences. First, they prepare ground truth voting maps for each keypoint such that keypoints vote for every other one depending on its relative position in the log polar map. This requires manually creating static voting maps. Specifically, their model learns H × W × R × C voting map, where R is the number of bins and C is the augmented keypoints. In order to produce keypoint heatmaps they perform vote agregation at test phase. Second, this design restricts the model to learn only the keypoint locations as  voters. When we consider the object detection task and its complexity, it is not trivial to decide the voters of the objects and prepare supervised static voting maps as in human pose estimation. Moreover, this design limits the voters to reside only inside of the object (e.g. person) unlike our approach where an object could get votes from far away regions. To overcome these issues, unlike their model we apply vote aggregation during training (they perform vote agregation only at test phase). This allows us to expose the latent patterns between objects and voters for each class. In this way, our voting module is able to get votes from non-labeled objects (see the last row of <ref type="figure">Fig. 4</ref>). To the best of our knowledge, we are the first to use a log-polar vote field in a voting-based deep learning model to integrate the long range interactions for object detection. Similar to HoughNet, Non-local neural networks (NLNN) <ref type="bibr" target="#b45">[46]</ref> and Relation networks (RN) <ref type="bibr" target="#b18">[19]</ref> integrate long-range features. As a fundamental difference, in NLNN, the relative displacement between interacting features is not taken into account. However, HoughNet uses this information encoded through the regions of the log-polar vote field. RN models object-object relations explicitly for proposal-based two-stage detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HoughNet: the method and the models</head><p>The overall processing pipeline of our method is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. To give a brief overview, the input image first passes through a backbone CNN, the output of which is connected to three different branches carrying out the predictions of (i) visual evidence scores, (ii) objects' bounding box dimensions (width and height), and (iii) objects' center location offsets. The first branch is where the voting occurs. Before we describe our voting mechanism in detail, we first introduce the log-polar vote field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The log-polar "vote field"</head><p>We use the set of regions in a standard log-polar coordinate system to define the regions through which votes are collected. A log-polar coordinate system is defined by the number and radii of eccentricity bins (or rings) and the number of angle bins. We call the set of cells or regions formed in such a coordinate system as the "vote field" <ref type="figure" target="#fig_2">(Fig. 3)</ref>. In our experiments, we used different vote fields with different parameters (number of angle bins, etc.) as explained in the Experiments section. In the following, R denotes the number of regions in the vote field and K r is the number of pixels in a particular region r. ∆ r (i) denotes the relative spatial coordinates of the i th pixel in the r th region, with respect to the center of the field. We implement the vote field as a fixed-weight (nonlearnable) transposed-convolution filter as further explained below. After the input image is passed through the backbone network and the "visual evidence" branch, the voting module of HoughNet re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Voting module</head><formula xml:id="formula_0">ceives C tensors E 1 , E 2 , . . . , E C , each of size H × W × R,</formula><p>where C is the number of classes, H and W are spatial dimensions and R is the number of regions in the vote field. Each of these tensors contains class-conditional (i.e. for a specific class) "visual evidence" scores. The job of the voting module is to produce</p><formula xml:id="formula_1">C "object presence" maps O 1 , O 2 , . . . , O C , each of size H × W .</formula><p>Then, peaks in these maps will indicate the presence of object instances. The voting process, which converts the visual evidence tensors (e.g. E c ) to object presence maps (e.g. O c ), works as described below.</p><p>Suppose we wanted to process the visual evidence at the i th row, j th column and the r th channel of an evidence tensor E. When we place our vote field on a 2D map, centered at location (i, j), the region r marks the target area to be voted on, whose coordinates can be computed by adding the coordinate offsets ∆ r (·) to (i, j). Then, we add the visual evidence score E(i, j, r) to the target area of the object presence map. Note that this operation can be efficiently implemented using the "transposed convolution" (or "deconvolution") operation. Visual evidence scores from locations other than (i, j) are processed in the same way and the scores are accumulated in the object presence map. We formally define this procedure in Algorithm 1, which takes in a visual evidence tensor as input and produces an object presence map 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network architecture</head><p>Our network architecture design follows that of "Objects as Points" (OAP) <ref type="bibr" target="#b50">[51]</ref>. HoughNet consists of a backbone and three subsequent branches which predict (i) visual evidence scores, (ii) bounding box widths and heights, and (iii) center offsets. Our voting module is attached to the visual evidence branch <ref type="figure" target="#fig_1">(Fig. 2)</ref>.</p><p>The output of our backbone network is a feature map of size H × W × D, which is a result of inputting an image of size 4H × 4W × 3. The backbone's output is fed to all three branches. Each branch has one convolutional layer with 3 × 3 filters followed by a ReLU layer and another convolutional layer with 1 × 1 filters. The visual evidence branch outputs H × W × C × R sized output where C and R correspond to the number of classes and vote field regions, respectively. The width/height prediction branch outputs H × W × 2 sized output which predicts heights and widths for each possible object center. Finally, center offset branch predicts relative displacement of center locations across the spatial axes.</p><p>Objective functions For the optimization of the visual evidence branch, we use the modified focal loss <ref type="bibr" target="#b26">[27]</ref> introduced in CornerNet <ref type="bibr" target="#b22">[23]</ref> (also used in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>). In order to recover the lost precision of the center points due to down-sampling operations through the network, center offset prediction branch outputs classagnostic local offsets of object centers. We optimize this branch using the L 1 loss as the other bottom-up detectors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> do. Finally, our width &amp; height prediction branch outputs class-agnostic width and height values of objects. For the optimization of this branch, we use L 1 loss by scaling the loss by 0.1 as proposed in OAP <ref type="bibr" target="#b50">[51]</ref>. The overall loss is the sum of the losses from all branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section presents the experiments we conducted to show the effectiveness of our proposed method. First, we studied how different parameters of the vote field affect the final object detection performance. Next, we present several performance comparisons between HoughNet and the current state-of-the-art methods, on the COCO dataset. After presenting sample visual results for qualitative inspection, we describe our experiments on the "labels to photo" task. We used PyTorch <ref type="bibr" target="#b34">[35]</ref> to implement HoughNet.</p><p>Training and inference details We ran our experiments on 4 V100 GPUs. For training, we used 512×512 images unless stated otherwise. The training setup is not uniform across different experiments, mainly due to different backbones.</p><p>The inference pipeline is common for all HoughNet models. We extract center locations by applying a 3 × 3 max pooling operation on object presence heatmaps and pick the highest scoring 100 points as detections. Then, we adjust these points using the predicted center offset values. Final bounding boxes are generated using the predicted width &amp; height values on these detections. For testing, we follow the other bottom-up methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> and use two modes: (i) single-scale, horizontal-flip testing (SS testing mode), and (ii) multi-scale, horizontal-flip testing (MS testing mode). In MS, we use the following scale values, 0.6, 1.0, 1.2, 1.5, 1.8. To merge augmented test results, we use Soft-NMS <ref type="bibr" target="#b4">[5]</ref>, and keep the top 100 detections. All tests are performed on a single V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mini COCO</head><p>For faster analysis in our ablation experiments, we created "COCO minitrain" as a statistically validated mini training set. It is a subset of the COCO train2017 dataset, containing 25K images (about 20% of train2017) and around 184K objects across 80 object categories. We randomly sampled these images from the full set while preserving the following three quantities as much as possible: (i) proportion of object instances from each class, (ii) overall ratios of small, medium and large objects, (iii) per class ratios of small, medium and large objects.</p><p>To validate COCO minitrain, we computed the correlation between the val2017 performance of a model when it is trained on minitrain with the same of when it is trained on train2017. Over six different object detectors (Faster R-CNN, Mask R-CNN, RetinaNet, CornerNet, ExtremeNet and HoughNet), the Pearson correlation coefficients turned out to be 0.74 and 0.92 for AP and AP 50 , respectively. These values indicate strong positive correlation. Further details on minitrain can be found at https://github.com/giddyyupp/coco-minitrain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation experiments</head><p>Here we analyze the effects of the number of angle and ring bins of the vote field on performance. Models are trained on COCO minitrain and evaluated on val2017 set with SS testing mode. The backbone is Resnet-101 <ref type="bibr" target="#b16">[17]</ref>. In order to get higher resolution feature maps, we add three deconvolution layers on top of the default Resnet-101 network, similar to <ref type="bibr" target="#b46">[47]</ref>. We add 3×3 convolution filters before each 4 × 4 deconvolution layer, and put batchnorm and ReLU layers after convolution and deconvolution filters. We trained the network with a batch size of 44 for 140 epochs with Adam optimizer <ref type="bibr" target="#b20">[21]</ref>. Initial learning rate 1.75 × 10 −4 was divided by 10 at epochs 90 and 120.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Angle bins</head><p>We started with a large, 65 by 65, vote field with 5 rings. We set the radius of these rings from the most inner one to the most outer one as 2, 8, 16, 32 and 64 pixels, respectively. We experimented with 60 • , 90 • , 180 • and 360 • bins. We do not split the center ring (i.e. region with id 1 in <ref type="figure" target="#fig_2">Fig. 3</ref>) into further regions. Results are presented in <ref type="table" target="#tab_2">Table 1a</ref>. For the 180 • experiment, we divide the vote field horizontally. 90 • yields the best performance considering both AP and AP 50 . We used this setting in the rest of the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of center and periphery</head><p>We conducted experiments to analyze the importance of votes coming from different rings of the vote field. Results are presented in <ref type="table" target="#tab_2">Table 1b</ref>. In the Only Center case, we only keep the center ring and disable the rest.</p><p>In this way, we only aggregate votes from features of the object center directly. This case corresponds to a traditional object detection paradigm where only local (short-range) evidence is used. This experiment shows that votes from outer rings help improve performance. For the No Center case, we only disable the center ring. We observe that there is only 0.2 decrease in AP. This suggests that the evidence for successful detection is embedded mostly around the object center not directly inside the object center. In order to observe the power of long-range votes, we conducted another experiment called "Only Context," where we disabled the two most inner rings and used only the three outer rings for vote aggregation. This model reduced AP by 1.0 point compared to the full model.</p><p>Ring count To find out how far an object should get votes from, we discard outer ring layers one by one as presented in <ref type="table" target="#tab_2">Table 1c</ref>. The models with 5 rings, 4 rings and 3 rings have 17, 13 and 9 voting regions and 65, 33 and 17 vote field sizes, respectively. The model with 3 rings yields the best performance on AP metric and is the fastest one at the same time. On the other hand, the model with 5 rings yields 0.2 AP 50 improvement over the model with 3 rings. From all these ablation experiments, we decided to use the model with 5 rings and 90 • as our Base Model. Considering both speed and accuracy, we decided to use the model with 3 rings and 90 • as our Light Model.</p><p>Voting module vs. dilated convolution Dilated convolution <ref type="bibr" target="#b47">[48]</ref>, which can include long-range features, could be considered as an alternative to our voting module. To compare performance, we trained models on train2017 and evaluated them on val2017 using the SS testing mode. Baseline: We consider OAP with ResNet-101-DCN backbone as baseline. The last 1×1 convolution layer of center prediction branch in OAP, receives H × W × D tensor and outputs object center heatmaps with a tensor of size H × W × C.</p><p>Baseline + Voting Module: We first adapt the last layer of center prediction branch in baseline to output H × W × C × R tensor, then attach our voting module on top of the center prediction branch. Adding the voting module increases parameters of the layer by R times. The log-polar vote field is 65 × 65, and has 5 rings (90 • ). With 5 rings and 90 • we end up with R = 17 regions.</p><p>Baseline + Dilated Convolution: We use dilated convolution with kernel size 4 × 4 and dilation rate 22 for the last layer of the center prediction branch in baseline. Using 4×4 kernel increases parameters 16 times which is approximately equal to R in the Baseline + Voting Module. Using dilation rate 22, the filter size becomes 67 × 67 which is close to 65 × 65 log-polar vote field.</p><p>For a fair comparison with Baseline, both Baseline + Voting Module and the Baseline + Dilated Convolution use Resnet-101-DCN backbone. Our voting module outperforms dilated convolution in all cases ( <ref type="table" target="#tab_3">Table 2)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance of HoughNet and comparison with baseline</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we present the performance of HoughNet for different backbone networks, initializations and our base-vs-light model, on the val2017 set. There is a significant speed difference between Base and Light models. Our light model with R-101-DCN backbone is the fastest one (14.3 FPS) achieving 37.2 AP and 56.5 AP 50 . We observe that initializing the backbone with a pretrained model improves the detection performance. In <ref type="table" target="#tab_5">Table 4</ref>, we compare HoughNet's performance with its baseline OAP <ref type="bibr" target="#b50">[51]</ref> for two different backbones. HoughNet is especially effective for small objects, it improves the baseline by 2.1 and 2.2 AP points for R-101-DCN and HG-104 backbones, respectively. We also provide results for the recently introduced moLRP [34] metric, which combines localization, precision and recall in a single metric. Lower values are better.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head><p>For comparison with the state-of-the-art, we use Hourglass-104 <ref type="bibr" target="#b22">[23]</ref> backbone. We train Hourglass model with a batch size of 36 for 100 epochs using the Adam optimizer <ref type="bibr" target="#b20">[21]</ref>. We set the initial learning rate to 2.5 × 10 −4 and divided it by 10 at epoch 90. We provide visualization of votes for sample detections of HoughNet for qualitative visual inspection <ref type="figure">(Fig. 4)</ref>. These detections clearly show that HoughNet is able to make use of long-range visual evidence. One task where long-range interactions could be useful is the task of image generation from a given label map. There are two main approaches to solve this task; using unpaired and paired data for training. We take Cy-cleGAN <ref type="bibr" target="#b53">[54]</ref> and Pix2Pix <ref type="bibr" target="#b19">[20]</ref> as our baselines for unpaired and paired approaches, respectively. We attach our voting module at the end of CycleGAN <ref type="bibr" target="#b53">[54]</ref> and Pix2Pix <ref type="bibr" target="#b19">[20]</ref> models. For quantitative comparison, we use the Cityscapes <ref type="bibr" target="#b6">[7]</ref> dataset. In <ref type="table" target="#tab_8">Table 6</ref>, we present FCN scores <ref type="bibr" target="#b30">[31]</ref> (which is used as the measure of success in this task) of CycleGAN and Pix2Pix with and without our voting module. To obtain the "without" result, we used the already trained model shared by the authors. We obtained the "with" result using the official training code from their repositories. In both cases evaluation was done using the official test and evaluation scripts from their repos. Results show that using the voting module improves FCN scores by large margins. Qualitative inspection also shows that when our voting module is attached, the generated images conform to the given input segmentation maps better <ref type="figure">(Fig. 5</ref>). This is the main reason for the quantitative improvement. Since Pix2Pix is trained with paired data, generated images follow input segmentation maps, however, Pix2Pix fails to generate small details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Using our voting module in another task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection</head><p>Voters Detection Voters Detection Voters <ref type="figure">Fig. 4</ref>: Sample detections of HoughNet and their vote maps. In the "detection" columns, we show a correctly detected object, marked with a yellow bounding box. In the "voters" columns, the locations that vote for the detection are shown.</p><p>Colors indicate vote strength based on the standard "jet" colormap (red is high, blue is low; <ref type="figure" target="#fig_0">Fig. 1</ref>). In the top row, there are three "mouse" detections. In all cases, in addition to the local votes (that are on the mouse itself), there are strong votes coming from nearby "keyboard" objects. This voting pattern is justified given that mouse and keyboard objects frequently co-appear. A similar behavior is observed in the detections of "baseball bat", "baseball glove" and "tennis racket" in the second row, where they get strong votes from "ball" objects that are far-away. Similarly, in the third row, "vase" detections get strong votes from the flowers. In the first example of the bottom row, "dining table" detection gets strong votes from the candle object, probably because they co-occur frequently. Candle is not among the 80 classes of COCO dataset. Similarly, in the second example in the bottom row, "dining table" has strong votes from objects and parts of a standard living room. In the last example, partially occluded bird gets strong votes (stronger than the local votes on the bird itself) from the tree branch Input CycleGAN + Voting Input Pix2Pix + Voting <ref type="figure">Fig. 5</ref>: Sample qualitative results for the "labels to photo" task. When integrated with CycleGAN, our voting module helps generate better images in the sense that the image conforms to the input label map better. In all three images, CycleGAN fails to generate sky, buildings and falsely generates vegetation in the last image. When used with Pix2Pix, it helps generate more detailed images. In the first row, cars and buildings can be barely seen for Pix2Pix. Similarly, a bus is generated as a car and a bicycle is silhouetted in the second and third images, respectively. Our voting module fixes these errors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented HoughNet, a new, one-stage, anchor-free, votingbased, bottom-up object detection method. HoughNet determines the presence of an object at a specific location by the sum of the votes cast on that location. Voting module of HoughNet is able to use both short and long-range evidence through its log-polar vote field. Thanks to this ability, HoughNet generalizes and enhances current object detection methodology, which typically relies on only local (short-range) evidence. We show that HoughNet performs on-par with the state-of-the-art bottom-up object detector, and obtains comparable results with one-stage and two-stage methods. To further validate our proposal, we used the voting module of HoughNet in an image generation task. Specifically, we showed that our voting module significantly improves the performance of two GAN models in a "labels to photo" task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(Left) A sample "mouse" detection, shown with yellow bounding box, by HoughNet. (Right) The locations that vote for this detection. Colors indicate vote strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overall processing pipeline of HoughNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>A log-polar "vote field" used in the voting module of HoughNet. Numbers indicate region ids. A vote field is parametrized by the number of angle bins, and the number and radii of eccentricity bins, or rings. In this particular vote field, there are a total of 13 regions, 6 angle bins and 3 rings. The radii of the rings are 2, 8 and 16, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Vote aggregation algorithmInput: Visual evidence tensor Ec, Vote field relative coordinates ∆ Output: Object presence map OcInitialize Oc with all zeros for each pixel (i, j, r) in Ec do /* Kr: number of pixels in the vote field region r */ for k = 1 to Kr do</figDesc><table><row><cell>(y, x) ← (i, j) + ∆r(k)</cell></row><row><cell>Oc(y, x) ← Oc(y, x) + 1 Kr Ec(i, j, r)</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation experiments for the vote field. (a) Effect of angle bins on performance. Vote field with 90 • has the best performance (considering AP and AP 50 ). (b) Effect of central and peripheral regions. Here, the angle bin is 90 • and the ring count is four. Disabling any of center or periphery hurts performance, cf. (a). (c) Effect of number of rings. Angle is 90 • and vote field size is updated according to the radius of the last ring. Using 3 rings yields the best result. It is also the fastest model Model AP AP 50 AP 75 AP S AP M AP L FPS 60 • 24.6 41.3 25.0 8.2 27.7 36.2 3.4 90 • 24.6 41.5 25.0 8.2 27.7 36.2 3.5 180 • 24.5 41.1 24.8 8.1 27.7 36.3 3.5 360 • 24.6 41.1 25.1 8.0 27.8 36.3 3.5 (a) Varying the number of angle bins Only Center 23.8 39.5 24.5 7.9 26.8 34.7 3.5</figDesc><table><row><cell>No Center</cell><cell>24.4 40.9 24.9 7.4 27.6 37.1 3.3</cell></row><row><cell cols="2">Only Context 23.6 39.7 24.2 7.4 26.4 35.9 3.4</cell></row><row><cell cols="2">(b) Effectiveness of votes from center or periphery</cell></row><row><cell>5 Rings</cell><cell>24.6 41.5 25.0 8.2 27.7 36.2 3.5</cell></row><row><cell>4 Rings</cell><cell>24.5 41.1 25.3 8.2 27.8 36.1 7.8</cell></row><row><cell>3 Rings</cell><cell>24.8 41.3 25.6 8.4 27.6 37.5 15.6</cell></row><row><cell></cell><cell>(c) Varying ring counts</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>MethodAP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell></cell><cell>Comparing our voting mod-</cell></row><row><cell cols="2">ule to an equivalent (in terms of num-</cell></row><row><cell cols="2">ber of parameters and the spatial fil-</cell></row><row><cell cols="2">ter size) dilated convolution filter on</cell></row><row><cell cols="2">COCO val2017 set. Models are trained</cell></row><row><cell cols="2">on COCO train2017 and results are</cell></row><row><cell cols="2">presented on SS testing mode</cell></row><row><cell>Baseline</cell><cell>36.2 54.8 38.7 16.3 41.6 52.3</cell></row><row><cell cols="2">+ Dilated Conv. 36.6 56.1 39.2 16.7 42.0 53.6</cell></row><row><cell cols="2">+ Voting Module 37.3 56.6 39.9 16.8 42.6 55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>HoughNet results on COCO val2017 set for different training setups. † indicates initialization with CornerNet weights, * indicates initialization with ExtremeNet weights. Results are given for SS and MS testing modes, respectively / 40.7 55.2 / 60.6 38.4 / 43.9 16.2 / 22.5 41.7 / 44.2 52.0 / 55.7 3.5 / 0.5 Base R-101-DCN 37.3 / 41.6 56.6 / 61.2 39.9 / 44.9 16.8 / 22.6 42.6 / 44.8 55.2 / 58.8 3.3 / 0.4 Light R-101-DCN 37.2 / 41.5 56.5 / 61.5 39.6 / 44.5 16.8 / 22.5 42.5 / 44.8 54.9 / 58.4 14.3 / 2.1 Light HG-104 40.9 / 43.7 59.2 / 61.9 44.1 / 47.3 23.8 / 27.5 45.3 / 45.9 52.6 / 56.2 6.1 / 0.8 Light HG-104 * 41.7 / 44.7 60.5 / 63.2 45.6 / 48.9 23.9 / 28.0 45.7 / 47.0 54.6 / 58.1 5.9 / 0.8 Light HG-104 * 43.0 / 46.1 62.2 / 64.6 46.9 / 50.3 25.5 / 30.0 47.6 / 48.8 55.8 / 59.7 5.7 / 0.8</figDesc><table><row><cell cols="2">Models Backbone</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>APS</cell><cell>APM</cell><cell>APL</cell><cell>FPS</cell></row><row><cell>Base</cell><cell>R-101</cell><cell>36.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with baseline (OAP) on val2017. Results are given for single scale and multi scale test modes, respectively</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>APS</cell><cell>APM</cell><cell>APL</cell><cell>moLRP ↓</cell></row><row><cell cols="8">Baseline w R-101-DCN 36.2 / 39.2 54.8 / 58.6 38.7 / 41.9 16.3 / 20.5 41.6 / 42.6 52.3 / 56.2 71.1 / 68.3</cell></row><row><cell>+ Voting Module</cell><cell cols="7">37.2 / 41.5 56.5 / 61.5 39.6 / 44.5 16.8 / 22.5 42.5 / 44.8 54.9 / 58.4 69.9 / 66.6</cell></row><row><cell>Baseline w HG-104</cell><cell cols="7">42.2 / 45.1 61.1 / 63.5 46.0 / 49.3 25.2 / 27.8 46.4 / 47.7 55.2 / 60.3 66.1 / 63.9</cell></row><row><cell>+ Voting Module</cell><cell cols="7">43.0 / 46.1 62.2 / 64.6 46.9 / 50.3 25.5 / 30.0 47.6 / 48.8 55.8 / 59.7 65.6 / 63.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art on COCO test-dev. The methods are divided into three groups: two-stage, one-stage top-down and one-st age bottom-up. The best results are boldfaced separately for each group. Backbone names are shortened: R is ResNet, X is ResNeXt, F is FPN and HG is HourGlass. indicates that the FPS values were obtained on the same AWS machine with a V100 GPU using the official repos in SS setup. The rest of the FPS are from their corresponding papers. F. R-CNN is Faster R-CNN ExtremeNet 512×512 ≤1.8× 46.4 65.1 50.7 29.1 48.5 58.1 -</figDesc><table><row><cell>Method</cell><cell cols="6">Backbone Initialize Train size Test size AP AP50 AP75 APS APM APL FPS</cell></row><row><cell>Two-stage detectors:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R-FCN [8]</cell><cell>R-101</cell><cell cols="4">ImageNet 800×800 600×600 29.9 51.9</cell><cell>-10.8 32.8 45.0 5.9</cell></row><row><cell>CoupleNet [55]</cell><cell>R-101</cell><cell>ImageNet</cell><cell>ori.</cell><cell>ori.</cell><cell cols="2">34.4 54.8 37.2 13.4 38.1 50.8 -</cell></row><row><cell cols="2">F. R-CNN+++ [17] R-101</cell><cell cols="5">ImageNet 1000×600 1000×600 34.9 55.7 37.4 15.6 38.7 50.9 -</cell></row><row><cell>F. R-CNN [26]</cell><cell>R-101-F</cell><cell cols="5">ImageNet 1000×600 1000×600 36.2 59.1 39.0 18.2 39.0 48.2 5.0</cell></row><row><cell>Mask R-CNN [16]</cell><cell>X-101-F</cell><cell cols="5">ImageNet 1300×800 1300×800 39.8 62.3 43.4 22.1 43.2 51.2 11.0</cell></row><row><cell>Cascade R-CNN [6]</cell><cell>R-101</cell><cell>ImageNet</cell><cell>-</cell><cell>-</cell><cell cols="2">42.8 62.1 46.3 23.7 45.5 55.2 12.0</cell></row><row><cell>PANet [29]</cell><cell>X-101</cell><cell cols="5">ImageNet 1400×840 1400×840 47.4 67.2 51.8 30.1 51.7 60.0 -</cell></row><row><cell>One-stage detectors:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top Down:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD [30]</cell><cell cols="6">VGG-16 ImageNet 512×512 512×512 28.8 48.5 30.3 10.9 31.8 43.5 -</cell></row><row><cell>YOLOv3 [39]</cell><cell>Darknet</cell><cell cols="5">ImageNet 608×608 608×608 33.0 57.9 34.4 18.3 35.4 41.9 20.0</cell></row><row><cell>DSSD513 [12]</cell><cell>R-101</cell><cell cols="5">ImageNet 513×513 513×513 33.2 53.3 35.2 13.0 35.4 51.1 -</cell></row><row><cell>RefineDet (SS) [49]</cell><cell>R-101</cell><cell cols="5">ImageNet 512×512 512×512 36.4 57.5 39.5 16.6 39.9 51.4 -</cell></row><row><cell>RetinaNet [27]</cell><cell>X-101-F</cell><cell cols="5">ImageNet 1300×800 1300×800 40.8 61.1 44.1 24.1 44.2 51.2 5.4</cell></row><row><cell cols="2">RefineDet (MS) [49] R-101</cell><cell cols="5">ImageNet 512×512 ≤2.25× 41.8 62.9 45.7 25.6 45.1 54.1 -</cell></row><row><cell>OAP (SS) [51]</cell><cell cols="3">HG-104 ExtremeNet 512×512</cell><cell>ori.</cell><cell cols="2">42.1 61.1 45.9 24.1 45.5 52.8 9.6  *</cell></row><row><cell>FSAF (SS) [53]</cell><cell>X-101</cell><cell cols="5">ImageNet 1300×800 1300×800 42.9 63.8 46.3 26.6 46.2 52.7 2.7</cell></row><row><cell>FSAF (MS) [53]</cell><cell>X-101</cell><cell cols="5">ImageNet 1300×800 ∼≤2.0× 44.6 65.2 48.6 29.7 47.1 54.6 -</cell></row><row><cell>FCOS [43]</cell><cell>X-101-F</cell><cell cols="5">ImageNet 1300×800 1300×800 44.7 64.1 48.4 27.6 47.5 55.6 7.0  *</cell></row><row><cell cols="2">FreeAnchor (SS) [50] X-101-F</cell><cell cols="5">ImageNet 1300×960 1300×960 44.9 64.3 48.5 26.8 48.3 55.9 -</cell></row><row><cell>OAP (MS) [51]</cell><cell cols="6">HG-104 ExtremeNet 512×512 ≤1.5× 45.1 63.9 49.3 26.6 47.1 57.7 -</cell></row><row><cell cols="2">FreeAnchor (MS) [50] X-101-F</cell><cell cols="5">ImageNet 1300×960 ∼≤2.0× 47.3 66.3 51.5 30.6 50.4 59.0 -</cell></row><row><cell>Bottom Up:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ExtremeNet (SS) [52] HG-104</cell><cell>-</cell><cell>511×511</cell><cell>ori.</cell><cell cols="2">40.2 55.5 43.2 20.4 43.2 53.1 3.0  *</cell></row><row><cell cols="2">CornerNet (SS) [23] HG-104</cell><cell>-</cell><cell>511×511</cell><cell>ori.</cell><cell cols="2">40.5 56.5 43.1 19.4 42.7 53.9 5.2  *</cell></row><row><cell cols="2">CornerNet (MS) [23] HG-104</cell><cell>-</cell><cell cols="4">511×511 ≤1.5× 42.1 57.8 45.3 20.8 44.8 56.7 -</cell></row><row><cell cols="2">ExtremeNet (MS) [52] HG-104</cell><cell>-</cell><cell cols="4">511×511 ≤1.5× 43.7 60.5 47.0 24.1 46.9 57.6 -</cell></row><row><cell>CenterNet (SS) [9]</cell><cell>HG-104</cell><cell>-</cell><cell>511×511</cell><cell>ori.</cell><cell cols="2">44.9 62.4 48.1 25.6 47.4 57.4 4.8  *</cell></row><row><cell>CenterNet (MS) [9]</cell><cell>HG-104</cell><cell>-</cell><cell cols="4">511×511 ≤1.8× 47.0 64.5 50.7 28.9 49.9 58.9 -</cell></row><row><cell>HoughNet (SS)</cell><cell>HG-104</cell><cell>-</cell><cell>512×512</cell><cell>ori.</cell><cell cols="2">40.8 59.1 44.2 22.9 44.4 51.1 6.4  *</cell></row><row><cell>HoughNet (MS)</cell><cell>HG-104</cell><cell>-</cell><cell cols="4">512×512 ≤1.8× 44.0 62.4 47.7 26.4 45.4 55.2 -</cell></row><row><cell>HoughNet (SS)</cell><cell cols="3">HG-104 ExtremeNet 512×512</cell><cell>ori.</cell><cell cols="2">43.1 62.2 46.8 24.6 47.0 54.4 6.4  *</cell></row><row><cell>HoughNet (MS)</cell><cell>HG-104</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>presents performances of HoughNet and several established state-of-the-art detectors. First, we compare HoughNet with OAP<ref type="bibr" target="#b50">[51]</ref> since it is the model on which we built HoughNet. In OAP, they did not present any results for "from scratch" training. Instead they fine-tuned their model from ExtremeNet weights. When we do the same (i.e. initialize HoughNet with Ex-tremeNet weights), we obtain better results than OAP. However as expected, HoughNet is slower than OAP. Among the one-stage bottom-up object detectors, HoughNet performs on-par with the best bottom-up object detector by achieving 46.4 AP against 47.0 AP of CenterNet<ref type="bibr" target="#b8">[9]</ref>. HoughNet outperforms CenterNet on AP 50 (65.1 AP 50 vs. 64.5 AP 50 ). Note that, since our model is initialized with ExtremeNet weights, which makes use of the segmentation masks in its own training, our model effectively uses more data compared to CenterNet. HoughNet is the fastest among one-stage bottom-up detectors. It is faster than CenterNet, CornerNet and more than twice as fast as ExtremeNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of FCN scores for the "labels to photo" task on the Cityscapes<ref type="bibr" target="#b6">[7]</ref> dataset Method Per-pixel acc. Per-class acc. Class IOU</figDesc><table><row><cell>CycleGAN</cell><cell>0.43</cell><cell>0.14</cell><cell>0.09</cell></row><row><cell>+ Voting</cell><cell>0.52</cell><cell>0.17</cell><cell>0.13</cell></row><row><cell>pix2pix</cell><cell>0.71</cell><cell>0.25</cell><cell>0.18</cell></row><row><cell>+ Voting</cell><cell>0.76</cell><cell>0.25</cell><cell>0.20</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We provide a step-by-step animation of the voting process at https://shorturl. at/ilOP2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by the Scientific and Technological Research Council of Turkey (TÜBİTAK) through the project titled "Object Detection in Videos with Deep Neural Networks" (grant #117E054). The numerical calculations reported in this paper were partially performed at TÜBİTAK ULAKBİM, High Performance and Grid Computing Center (TRUBA resources). We also gratefully acknowledge the support of the AWS Cloud Credits for Research program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection through search with a foveated visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1005743</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.993558</idno>
		<ptr target="https://doi.org/10.1109/34.993558" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis of the discriminative generalized hough transform as a proposal generator for a deep network in automatic pedestrian and car detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schramm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">51228</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Class-specific hough forests for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://people.cs.uchicago.edu/rbg/latent-release5/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V C</forename><surname>Hough</surname></persName>
		</author>
		<title level="m">Machine Analysis of Bubble Chamber Pictures C590914</title>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="page" from="554" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Looking and acting: vision and eye movements in natural behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object detection using a max-margin hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative generalized hough transform for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Localization recall precision (lrp): A new performance metric for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Latent hough transform for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheshkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ingacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arlazarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nikolaev</surname></persName>
		</author>
		<title level="m">Houghnet: neural network architecture for vanishing points detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A review of log-polar imaging for visual perception in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Traver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="398" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4126" to="4134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
