<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON CYBERNETICS, MM YY 1 FG-Net: Fast Large-Scale LiDAR Point Clouds Understanding Network Leveraging Correlated Feature Mining and Geometric-Aware Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangcheng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhi</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON CYBERNETICS, MM YY 1 FG-Net: Fast Large-Scale LiDAR Point Clouds Understanding Network Leveraging Correlated Feature Mining and Geometric-Aware Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Large-Scale Point Clouds Understanding</term>
					<term>Scene Understanding in Robotics</term>
					<term>3D Semantic Segmentation</term>
					<term>Weakly- Supervised Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents FG-Net, a general deep learning framework for large-scale point clouds understanding without voxelizations, which achieves accurate and real-time performance with a single NVIDIA GTX 1080 GPU. First, a novel noise and outlier filtering method is designed to facilitate subsequent high-level tasks. For effective understanding purpose, we propose a deep convolutional neural network leveraging correlated feature mining and deformable convolution based geometricaware modelling, in which the local feature relationships and geometric patterns can be fully exploited. For the efficiency issue, we put forward an inverse density sampling operation and a feature pyramid based residual learning strategy to save the computational cost and memory consumption respectively. Extensive experiments on real-world challenging datasets demonstrated that our approaches outperform state-of-the-art approaches in terms of accuracy and efficiency. Moreover, weakly supervised transfer learning is also conducted to demonstrate the generalization capacity of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D UE to the directness and robustness in obtaining 3D information, there has been an increasing proliferation of light detection and ranging (LiDAR) sensors which have been popularly deployed on a variety of intelligent agents such as unmanned ground vehicles (UGVs), unmanned aerial vehicles (UAVs) to perform localization, obstacle detection, exploration, etc. Consequently, efficient and effective largescale 3D LiDAR point clouds understanding is of great importance to facilitate machine perception, which bridges the gap between 3D points and any high-level information, structural or semantic, or both <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. However, due to the electrical and mechanical disturbances, and the reflectance property of targets, the point clouds often suffer from noise and outliers. Moreover, compared with the 2D raster image, the topological <ref type="bibr">Manuscript</ref>   <ref type="figure">Fig. 1</ref>. Semantic segmentation results of our method compared with the state-of-the-art method FKA-Conv <ref type="bibr" target="#b4">[5]</ref> on S3DIS <ref type="bibr" target="#b5">[6]</ref>. The top row shows the overall segmentation performance by our method. The bottom 2 rows show the detailed comparisons of segmentation performance highlighted by red circles. Our method achieves real-time segmentation performance of 0.021s per 10 5 points, which is better and faster than the state-of-the-art method FKA-Conv.</p><p>relationship between objects in 3D point clouds is much weakened, rendering the task of segmentation and understanding much more challenging. Therefore, autonomous large-scale point clouds understanding remains an open problem and requires urgent efforts to tackle the challenges, especially when both accuracy and efficiency are taken into account. Like any high-level task in 2D image domain such as object detection, segmentation, or classification, the point clouds understanding methods can also be classified into traditional category and deep learning based methods. In the traditional category, the representative histogram-based methods <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b8">[8]</ref> encode the k-Nearest-Neighbor (kNN) geometric features of a 3D point via calculating its surrounding multidimensional average curvature for local geometric variations descriptions. In signature-based <ref type="bibr" target="#b9">[9]</ref> and transform-based <ref type="bibr" target="#b3">[4]</ref> methods, handcrafted feature descriptions of point clouds have been proposed and exploited for semantic understanding. However, the performances of these methods are merely demonstrated in well-controlled conditions with ideal assumptions such as noise-free and homogeneous environments <ref type="bibr" target="#b3">[4]</ref>. On the other hand, deep learning based point clouds processing methods have been proposed with promising results in recent years. The mainstream point clouds understanding methods can be arXiv:2012.09439v1 [cs.CV] 17 Dec 2020 roughly divided into three categories: projection-based <ref type="bibr" target="#b10">[10]</ref>- <ref type="bibr" target="#b16">[16]</ref>, voxel-based <ref type="bibr" target="#b17">[17]</ref>- <ref type="bibr" target="#b21">[21]</ref>, and direct point-based <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[22]</ref>- <ref type="bibr" target="#b32">[32]</ref>. Such representative works of each category will be discussed in Section II, and the limitations of these methods are summarized here: First, the sampling operation of almost all these methods has high computational cost and memory consumption. For instance, the widely applied farthest point sampling (FPS) <ref type="bibr" target="#b24">[24]</ref>  <ref type="bibr" target="#b25">[25]</ref> takes more than 1000 seconds to subsample 10 5 points to about 10 3 points. Furthermore, their subsequent perception networks usually rely on the expensive operations, such as voxelizations <ref type="bibr" target="#b17">[17]</ref>  <ref type="bibr" target="#b18">[18]</ref>, or graph construction <ref type="bibr" target="#b22">[22]</ref>, etc. Second, nearly all the existing methods are designed for small-scale point clouds without considering noise and outliers which are inevitable in practice. Moreover, the large-scale point clouds typically suffer from great class imbalance in semantic categories, and the points obtained by LiDAR in complex dynamic environments are often irregular, orderless, and have distant distributed semantic information. For example, in autonomous driving scenarios, the typical objects exhibit diverse geometric shapes with varying object sizes (e.g., cyclists and persons) or have distribution across a long spatial range in a non-uniform way (e.g., road, buildings, and vegetations). However, to the best of our knowledge, the existing methods can hardly capture the complex geometry and the latent feature correlations in large-scale point clouds effectively.</p><p>To overcome the aforementioned challenges, we propose a general deep learning framework named FG-Net for largescale point clouds understanding. We adopt deformable convolution for modelling the geometric structure, and pointwise attentional aggregation for mining the correlated features among point clouds. It should be noted that the deformable convolutional modelling can effectively adapt to the local geometry of objects by deformed kernels that dynamically adapt to diverse local geometries while the correlated feature mining can capture the distributed contextual information in spatial locations and semantic features adaptively across a long spatial range. The modules in our network can be implemented with simple pointwise matrix multiplication and add operations, which can be easily parallelized by GPU for acceleration. As shown in <ref type="figure">Fig. 1</ref>, our method outperforms state-of-the-art ones in terms of both accuracy and efficiency, rendering it achievable to realize real-time perception performance on the large-scale point clouds. In summary, our work makes the following contributions: 1) We propose pointwise correlated feature mining and geometric-aware modelling module for large-scale point clouds understanding. Furthermore, we interpret the effectiveness of our network by visualizing the complementary features captured by our network modules. 2) We propose a feature pyramid based residual learning architecture to leverage patterns at different resolutions in a memory-efficient way. Extensive experiments on real-world challenging datasets demonstrated that our approaches outperform state-of-the-art ones in terms of accuracy and efficiency. 3) We propose a novel fast noise and outliers removal method and a points down-sampling strategy for largescale point clouds, which simultaneously enhances the performance and improve the efficiency of semantic understanding tasks in the large-scale scene.</p><p>The paper is organized as follows: Section II gives a detailed review of existing methods of point clouds understanding. In Section III, we illustrate our proposed framework thoroughly and gives our optimization function formulation and training details. We also propose point clouds filtering methods and sampling methods which are specially designed to speed up our framework on large-scale point clouds. Section IV gives substantial results of our experiments and ablation studies. Section V concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Advanced deep learning techniques in 2D image domain have been investigated extensively, and resulted in stunning performance <ref type="bibr" target="#b33">[33]</ref>- <ref type="bibr" target="#b37">[37]</ref>. Naturally, such deep learning techniques have been exploited for point clouds processing and understanding, and the published works can be roughly categorized into voxel-based <ref type="bibr" target="#b17">[17]</ref>- <ref type="bibr" target="#b21">[21]</ref>, projection-based [10]- <ref type="bibr" target="#b16">[16]</ref> and point-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[22]</ref>- <ref type="bibr" target="#b32">[32]</ref>. The voxel-based and projection-based methods transform point clouds into different representations while the point-based methods process point clouds directly. These methods are mainly designed and tested on the relative small-scale point clouds of less than 10 5 points with block partitioning. Directly extending them to deal with large-scale point clouds will result in prohibitively expensive computational costs. Here we discuss these methods thoroughly of their advantages and shortcoming, and the rationale that motivates our modifications and improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Voxel-based and Projection-based Methods for Point Clouds Understanding</head><p>The most recent and typical voxel-based methods are Spar-seConv <ref type="bibr" target="#b17">[17]</ref> and Minkowski CNN <ref type="bibr" target="#b18">[18]</ref>. The voxel-based methods use 3D convolutions which are intuitive extensions of 2D counterparts. The advantage of them is that the spatial relationship can be well reserved with high voxel resolutions, but these methods are quite computationally expensive. The computation cost and memory consumption of the voxel-based models increase cubically with the resolution of input point clouds. By contrast, the geometric information loss will be significant if we decrease the resolution of voxelization. In addition, the voxel-based methods rely on aggressive down sampling to achieve bigger receptive fields, resulting in even lower resolution in deeper network layers. Hence, it is quite hard to achieve real-time performance while considering the balance between accuracy and computational cost. The projection is also used to project point clouds into range images <ref type="bibr" target="#b10">[10]</ref>- <ref type="bibr" target="#b12">[12]</ref> or multi-view images <ref type="bibr" target="#b13">[13]</ref>- <ref type="bibr" target="#b16">[16]</ref>, to facilitate the use of 2D CNNs. However, such projection inevitably leads to the loss of geometrical information. In practice of dealing with large-scale point clouds, the drawbacks of voxel-based and projection-based methods become more prohibitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise and Outliers Filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Density</head><p>Sampling</p><formula xml:id="formula_0">FG-Conv … FG-Conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-Scale Point Clouds Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation Output</head><p>Road Grass High Veg. Low Veg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Buildings</head><p>Hard scape Artifacts Cars …  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Classification Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point-based Methods for Point Clouds Understanding</head><p>The pointNet <ref type="bibr" target="#b27">[27]</ref> is the pioneering work that extracts the pointwise feature directly using shared Multi-layer Perceptron (MLP). It is permutation invariant to point clouds orders because it uses max-pooling operations. The pointNet++ <ref type="bibr" target="#b24">[24]</ref> extracts the local features using pointNet and considers local geometric relationships with the hierarchical grouping and abstraction as well as the multi-scale and resolution grouping. More point-based methods <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b38">[38]</ref> have been proposed recently with complicated network design to aggregate local features. However, all these methods are not able to model intrinsic geometric structures of points or to capture the nonlocal distributed contextual correlations in spatial locations and semantic features effectively. There are also a series of new explorations on how to implement convolution on point clouds. The methods <ref type="bibr" target="#b28">[28]</ref>- <ref type="bibr" target="#b30">[30]</ref> focus on how to learn kernels which can better capture the local geometry of points. However, the proposed convolutional kernels are too complicated to be directly applied to deep neural networks for large-scale point clouds understanding. Motivated by the challenges above, we proposed a novel lightweight point-based method to consider distributed long-range dependencies and learn kernels to capture the local structures of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Efficient Large-Scale Point Clouds Understanding</head><p>It is till recently that more attention has been paid to efficient large-scale point clouds understanding. Previously, block partitioning <ref type="bibr" target="#b24">[24]</ref> [27] <ref type="bibr" target="#b32">[32]</ref> was utilized to divide largescale point clouds into small 1m × 1m sub-blocks before being fed to networks. However, such operation of partition is time-consuming and damages the spatial geometric contextual information among the objects of large-scale scene. Although several attempts <ref type="bibr" target="#b22">[22]</ref> [31] <ref type="bibr" target="#b39">[39]</ref> have been made on large-scale point clouds segmentation, there are still some major problems existing: Firstly, the farthest point sampling (FPS) adopted by most of the previous methods require large computational cost which increases quadratically <ref type="bibr" target="#b0">[1]</ref> with respect to the number of input points N . Secondly, block partitioning causes that large-scale point clouds semantics can not be inferred within one scan, which limits the volume of point clouds that can be processed. Some methods <ref type="bibr" target="#b31">[31]</ref> [40] also try to combine voxel-wise features with pointwise features to improve the performance. Analogous to the super-pixel conception in 2D image domain, the super-point <ref type="bibr" target="#b22">[22]</ref> method in point clouds is also introduced to apply graph convolutions on large-scale points. But due to the high computational cost of voxelization or graph construction, such methods can hardly achieve realtime performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODOLOGY</head><p>In this section, a fast deep learning method leveraging correlated feature mining and geometric-aware modelling is proposed for large-scale point clouds understanding. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, our FG-Net takes raw point clouds of a large-scale complex scene as input and gives the predictions of object classification and semantic segmentation simultaneously. The details are given as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Noise and Outliers Filtering</head><p>The point clouds obtained by the LiDAR sensors contain noise and outliers which are harmful to the following highlevel processing, thus we propose a novel filtering method for pre-processing. As shown in Algorithm 1, the set of input point clouds is defined as P in = {p i }, i = 1, 2, 3, ..., N in , p i = (x i , y i , z i ). The radius based nearest neighbour ball query is conducted to ensure the robustness to the density distribution variation of point clouds in sampling. Given a query point p i ∈ P in , we define the neighbouring points within radius r as = {P j | p j − p i ≤ r, j = 1, 2, 3..., N r }. The number of points N r is obtained, which can be regarded as an estimation of point density in radius r, and will be reused in the following inverse density based sampling (IDS). If N r ≤ Ω (Ω is a threshold depending on the density of points acquired by LiDAR), we regard the point as an isolated point and remove it from P in . Next, the distance between points is modelled as the Gaussian distribution, and the points are removed if the mean distance is outside the confidence interval according to Gaussian distribution. The mean µ and standard deviation σ of the distances between points can be computed as follows:</p><formula xml:id="formula_1">µ = 1 N in N r N in i=1 N r j=1 d ij (1) σ = 1 N in N r N in i=1 N r j=1 (d ij − µ) 2 (2)</formula><p>Algorithm 1: Radius and Gaussian Distribution Based Noise and Outliers Filtering Input: The input raw point clouds</p><formula xml:id="formula_2">P in = {p i }, i = 1, 2, ..., N in , p i = (x i , y i , z i ),</formula><p>where N in is the number of input points Output: The points after filtering:</p><formula xml:id="formula_3">P out = {p m }, m = 1, 2, ..., M out , p m = (x m , y m , z m ) for all points p i ∈ P in in parallel do</formula><p>Find the radius r neighbourhood of p i , which is the ball B r = {s ∈ R 3 , s − p i ≤ r}; Calculate the number of point N r in B r , the points can be represented as:</p><formula xml:id="formula_4">{p j }, j = 1, 2, ..., N r ; if The number of points N r ≤ Ω then Remove p i from P in ; N in = N in − 1; Continue for j=1 to N r in parallel do Calculate the distance d ij = j p i − p j ;</formula><p>Model the distances by the Gaussian distribution, which means</p><formula xml:id="formula_5">d ∼ N (µ, σ), if N r j=1 d ij ≥ µ + mσ or N r j=1 d ij ≤ µ − mσ then</formula><p>Remove p ij from P in ;</p><formula xml:id="formula_6">P out = P in ;</formula><p>return P out ;</p><p>The noise and outliers can be removed very effectively with a speed of 0.61s per million (10 6 ) points. This simple but effective method can be utilized to remove noise and outliers of point clouds while enhancing the performance of point clouds understanding in the meanwhile. The implementation details for acceleration of our framework will be introduced in subsection C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Network Architecture for Large-Scale Point Clouds Understanding</head><p>We design the network module FG-Conv to capture the feature correlations and model the local geometry of point clouds simultaneously. Leveraging feature pyramid based residual learning framework, FG-Conv can be integrated into deep network FG-Net as the core module for large-scale point clouds understanding. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the large-scale point clouds can be processed in parallel by our novel design leveraging feature-level correlation mining and geometric convolutional modelling. The core network module FG-Conv includes 3 components: pointwise correlated features mining (PFM), geometric convolutional modelling (GCM), and attentional aggregation (AG), which are detailed as follows:</p><p>1) Pointwise correlated features mining: The point clouds after filtering are represented as x-y-z coordinates with perpoint features. The input features can consist of raw RGB, surface normal information, intensity of point clouds, and even learnt latent features. Denote the input point clouds as the matrix P ∈ R N ×(3+f in ) , where N is the number of points and f in is the dimension of input features respectively. The i-th vector in P can be denoted as</p><formula xml:id="formula_7">p i = (x i , f i ) T , where x i ∈ R 3 , f i ∈ R f in , i = 1, 2, 3, ..., N . For the point p i , denote the k-th point vector in the spherical neighbourhood B r = {s ∈ R 3 , s − x i ≤ r} as p k = (x k , f k ) T , x k ∈ R 3 , f k ∈ R f in , k = 1, 2, 3, .</formula><p>.., K, in which r is the radius of the neighbourhood. The similarity score g k of p k and p i is calculated as:</p><formula xml:id="formula_8">g k = p T k p i p k p i (3)</formula><p>which is the inner product of p k and p i . It gives a good evaluation of the similarity of neighbouring points in spatial locations and features. For each of the K neighbouring points, g k can be calculated and they constitute a similarity score vector r k ∈ R K , k = 1, 2, 3, ..., K. However, the similarity scores are not relevant to any specific task such as classification or segmentation. Thus, the attentional technique is introduced to make the new similarity score z k adaptive to specific task by training of deep networks. which is calculated as:</p><formula xml:id="formula_9">z k = σ(w 1 r k ), z k ∈ R K<label>(4)</label></formula><p>where w 1 ∈ R K×K is the weight matrix to be learnt. σ is the softmax function to normalize the attentional weights.</p><formula xml:id="formula_10">All p k constitute the matrix P k = (p 1 , p 2 , p 3 , ..., p k ) T , P k ∈ R K×(3+f in )</formula><p>. Then each element of z k is multiplied with each row of P k through element-to-row multiplication to obtain the augmented attentional feature matrix P k ∈ R K×(3+f in ) . From now on, the augmented features (e.g., P k ) which encode both geometry and feature correlations are called feature for the sake of brevity. Next, P k is concatenated with their corresponding input feature P k to obtain the enhanced feature</p><formula xml:id="formula_11">F 1 k ∈ R K×f mid (f mid = 6 + 2f in ).</formula><p>In this way, the local contextual relationship can be captured, and the similarity of features is enhanced adaptively and selectively by the attentive weighting in a learnable way. The similar feature elements in latent space are enhanced while distinct ones are attenuated.</p><p>2) Geometric convolutional modelling: After the pointwise correlated features mining, the local correlated features can be largely captured, but the geometric structure of points can not be sufficiently modelled. Inspired by the great success of deformable convolution in the image recognition <ref type="bibr" target="#b41">[41]</ref>, we extend deformable convolutions from image to point clouds to model the irregular and unordered 3D structures. Similar to 2D deformable convolutions, the deformable 3D kernels in Euclidean space are a set of learnable points that conform to the local structures of point clouds, thus, the dominant local geometric shapes of the points can be activated by the corresponding kernels in the neighbourhood. Note that kernel deformations can adapt to the local geometry of points in a learnable way by elaborately designed optimization functions. As shown in the right bottom of <ref type="figure" target="#fig_1">Fig. 3</ref>, like convolutional neural networks in image processing, the convolution on points is defined as:</p><formula xml:id="formula_12">F 2 k (p k , p i ) = p k ∈Br K ( p k , p i ) p k<label>(5)</label></formula><p>: </p><formula xml:id="formula_13">× ( = 6 + 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pointwise Correlated Features Mining (PFM)</head><formula xml:id="formula_14">A : × 1 = { } = { } = { } ′ = { ′ } = { } = { } { } Concatenation A Attention Element-Wise Sum S Sum Element-to-Row Multiplication Element-Wise Multiplication Point features × (3 + ) Kernel weights × (3 + ) × ) Outputs ( × ) Summation (1× ) Correlation function ( , , )</formula><p>Kernel position = The core problem is that point clouds are unstructured and unordered, which makes it difficult for point convolutional kernel function K ( p k , p i ) to learn representative local geometric patterns. We design the correlation function to measure the correspondence between kernel points and local geometry. To be more specific, the closer kernels points are to input points, the higher the correlation value should be assigned. Denote the difference between x k and x i as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deformable Point Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FG-Conv Module</head><formula xml:id="formula_15">∆x k = x k − x i . The N s pseudo kernel points S i ⊂ B r centered at S o (S o = x i ) are</formula><p>designed so as to imitate the convolutional kernels in image processing, (i = 1, 2, 3, ..., N s ). The relative coordinates of pseudo kernel points S i and the center point S o are given as:</p><formula xml:id="formula_16">s i = S i − S o .</formula><p>We set the correlation function C(s i , p k , p i ) as the Gaussian function formulated as:</p><formula xml:id="formula_17">C(s i , p k , p i ) = 1 N s exp(− s i − ∆x k 2 mσ 2 )<label>(6)</label></formula><p>where N s is the number of kernel points, m is a constant, and σ is the parameter determining the influence distance of kernel points. Then the kernel function can be given as the sum of all relations with learnable weights as shown in <ref type="formula" target="#formula_18">Equation 7</ref>:</p><formula xml:id="formula_18">K(p k , p i ) = N s n=1 C(s i , p k , p i ) W ker = 1 N s N s n=1 exp(− s i − ∆x k 2 mσ 2 ) W ker<label>(7)</label></formula><p>where W ker ∈ R (3+f in )×f mid is the weight matrix of MLP layers, and 3+f in , f mid are input and output channel numbers respectively. During the optimization process, the kernel points are forced to adapt to the dominant structures in the local point clouds. Finally, the feature after deformable convolution F 2 k ∈ R K×f mid (f mid = 6 + 2f in ) can be obtained. In this way, the local geometric structures are well captured by convolutional kernels and the dominant structural features are enhanced.</p><p>3) Attentional aggregation: The attention mechanism is utilized to leverage the feature level and geometric level patterns without large information loss. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the integrated neighbouring features can be represented as</p><formula xml:id="formula_19">F i ∈ R K×f int (f int = 2f mid ).</formula><p>Then the attentive score for aggregation is defined as w 2 ∈ R K , which will adaptively learn the importance score of each feature. The weighted attentional feature f a ∈ R f int can be given as:</p><formula xml:id="formula_20">f a = sof tmax(w 2 f i )<label>(8)</label></formula><p>The summed feature can be given as: </p><formula xml:id="formula_21">f h = K i=1 f i , f h ∈ R f int .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Feature pyramid hierarchical residual architecture: The Resnet <ref type="bibr" target="#b42">[42]</ref> based architecture has achieved great success in image recognition. Motivated by the residual learning paradigm, we proposed a general deep network that is specially designed for classification and semantic segmentation of large-scale point clouds. To our knowledge, until recently, there are several methods <ref type="bibr" target="#b26">[26]</ref> [43] starting to use residual learning for point clouds recognition, but their attempts are limited to small-scale point clouds. Thus the fitting capacity of residual architecture can not be fully demonstrated. We propose a feature pyramid based multi-scale fusion strategy for adaptively aggregating features from different layers of the network. Leveraging deep residual structure, memory-efficient deep networks can be built.</p><p>As shown in <ref type="figure">Fig. 4</ref>, the encoder-decoder based network structure can be utilized to obtain point clouds at multiple resolutions. In image processing, the networks are supposed to extract large feature maps for small objects and small feature maps for big objects <ref type="bibr" target="#b44">[44]</ref>  <ref type="bibr" target="#b45">[45]</ref>. It should be noted that scale variation in images will not exist in 3D point clouds. Different from images in which the scale of objects will vary with the distance, the scale of point clouds will keep constant. Hence the deconvolution by interpolation must be conducted to recover the points to the original resolution. As shown in   the Subfigure (a) of <ref type="figure">Fig. 4</ref>, in residual learning block (RLB), denote the input dimension and output dimension of RLB as D in and D out respectively. Unlike some deep architectures which are memory consuming, we reduce the feature dimension in the original residual learning block to D out /M (M = 8 is adopted in our framework) by 1 × 1 convolution before feeding them into FG-Conv module, which reduce the parameters by 9.6 times. And the accuracy for classification and segmentation can also be maintained through residual learning, which will be given in experiments. Another 1 × 1 convolution will be applied to recover the feature dimension. At the block connecting two stages, 1 × 1 convolution should be applied in skip link for increasing the feature dimensions. Then the global feature extraction in Subsubsection 5 will be conducted to obtain the latent global features M out from M in , which can be directly utilized for classification predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FG-Net</head><p>The point clouds are all upsampled after the h (h = 5 in our case) convolutional blocks. Unlike previous methods which directly used the upsampled features for segmentation, we propose to fuse the predictions at different resolutions and use the supervised loss to guide the training process. It turned out the hierarchical structure will give better results for pointwise large-scale point clouds segmentation. 5) Point clouds global feature extraction: The global and long-range dependencies in point clouds should also be captured before doing upsampling and giving the pointwise predictions. Due to the limited receptive field of the neural layer mentioned above, the global contextual semantic patterns can not be fully obtained. We adopt the self-attentional module shown in <ref type="figure">Fig. 5</ref> to selectively enhance the closely relevant elements in the global feature M in . After the global relationship mining by this module, both the local and global relationships in features and geometry will be captured adaptively. Then the feature representation with combined local or non-local semantic contextual correlations will be adaptively obtained to facilitate the subsequent recognition task. Given the original local feature map F out = M in ∈ R N i ×L , N i = N 625 , L = 256 in our case) as shown in <ref type="figure">Fig. 4 and Fig. 5</ref>, the 1×1 convolution with weight W G ∈ L×C mid (C mid = 1 in our case) is used to transform the feature map into latent representations M 1 and M 2 for further obtaining the similarity of each two elements in F out . After M 1 and M 2 are obtained, the dot product between them can be conducted to obtain the relevant score matrix M A ∈ R N i ×N i which is given as:</p><formula xml:id="formula_22">M A = M 1 M T 2<label>(9)</label></formula><p>Each element m y,z in M A gives the relevance score between the representation M 1 and M 2 . Then the softmax is applied to normalize the latent attentional scores to obtain the final selfrelation weights S y, z ∈ R N i ×N i of the latent representation M in . Each element s y,z of S y, z can be represented as:</p><formula xml:id="formula_23">s y,z = exp(m y,z ) N i y=1 N i z=1 exp(m y,z )<label>(10)</label></formula><p>The attention weights s y,z reveals the correlations among all local and non-local features. The more related distributed feature relationships, even in the non-local region, can be effectively captured, and larger attention weights are assigned in s y,z to enhances their similar semantic contexts. Finally, the attention scores are applied to all elements in M in to produce the global attentional vector M g : M g = S y, z M in <ref type="bibr" target="#b11">(11)</ref> and the consolidated feature M out is the sum of M g and M in , which is given as:</p><formula xml:id="formula_24">M out = M g + M in .</formula><p>Ultimately, the global contextual representation M g is fused with the local aggregated representation M in for a comprehensive encoding of local and non-local correlated features. The predictions of classification can be directly obtained from the aggregated latent features M out and the segmentation results can also be learnt by up-sampling.</p><p>6) Optimization function formulation and data augmentation: As mentioned in Subsubsection 2, denote the relative coordinates of kernel points as s i , i = 1, , 2, ..., N s and the learnt deformation as ∆s i . The losses utilized for the deformable convolution are designed as:</p><formula xml:id="formula_25">L f it (∆s i ) = N s i=1 min ∆si ( ∆x k − (s i + ∆s i ) mσ 2 ) 2<label>(12)</label></formula><p>which is utilized to match the kernel positions with local geometries of point clouds.</p><formula xml:id="formula_26">L rep1 (∆s i ) = min ∆si N s i=1 N s j=1 1 s i + ∆s i − s j − ∆s j<label>(13)</label></formula><p>which is the repulsive loss utilized to keep distance between different kernels.</p><formula xml:id="formula_27">L rep2 (∆s i ) = min ∆si N s i=1 s i + ∆s i 2<label>(14)</label></formula><p>which is to keep the kernel points from diverging and make them inside the query ball. The kernel loss will be the sum of above 3 losses. i.e. L ker (∆s i ) = L f it (∆s i ) + L rep1 (∆s i ) + L rep2 (∆s i ). As shown in <ref type="figure">Fig. 4</ref>, the losses at different stages of the network are also summed, which can be formulated as the cross-entropy loss denoted as L 1 :</p><formula xml:id="formula_28">L 1 (W) = N n=1 ( H h=1 α (h)ŷ i log(P seg (p (h) i , W )) + βŷ f use i log(P seg (p (f used) i , W ))<label>(15)</label></formula><p>α (h) denotes the weight at stage h of the residual network, W denotes the weight of the entire network, p (h) i denotes the upsampled point clouds at stage h, p (f used) i denotes the fused point clouds,ŷ i andŷ f use i denote the segmentation ground truth of points at different stages and fused points respectively. And P seg denotes the segmentation prediction of the networks.</p><p>We also propose to use the contextual loss shown in <ref type="figure">Fig. 4</ref> to predict the presence of objects or not in the scene to consider semantic contexts of the scene, which can be given as:</p><formula xml:id="formula_29">L 2 (W) = I i=1ŷ pre i log(P cls (p i , W ))<label>(16)</label></formula><p>Whereŷ pre i indicates whether the object presents in the scene or not and P cls is the classification prediction. This loss helps the network equally consider all the semantic categories appearing in the scene. The total loss the network can be given as: L c (W, ∆s i ) = L 1 (W) + L 2 (W) + L ker (∆s i ). The kernel positions and network parameters are jointly optimized in an end-to-end manner.</p><p>C. Implementation Details for Acceleration of our Framework 1) IGSAM for fast learning-based sampling: The sampling methods play a very significant role in processing point clouds by convolutional neural networks. Directly using raw points for segmentation is rather inefficient due to large computational costs when feeding them all into networks. Thus, an effective sampling method is highly required when taking efficiency into consideration. To tackle the large computational overhead when processing millions of point clouds, we propose efficient sampling methods IGSAM leveraging the advantages of inverse density sampling (IDS) and gumbel softmax sampling (GSS) to achieve fast and effective point clouds understanding. We design a novel learning based GSS which adaptively selects the significant points based on optimization objectives. Leveraging inverse density sampling (IDS), point clouds can be sampled efficiently with density awareness while the meaningful information or feature for point clouds understanding is maintained. The sampling method is implemented with multi-thread parallelization of CPU for acceleration.</p><p>To achieve learning based sampling operation, the Gumbel-Softmax trick is utilized to transfer the non-differentiable sampling operation into differentiable selection of features and coordinates by reparameterization tricks <ref type="bibr" target="#b46">[46]</ref>. In this way, points that matter most for the task can be selected in a learnable way. Given the point clouds P ∈ R N ×(3+f in ) , with its coordinate and features, the probability score s ∈ R N of a point being sampled can be estimated by the MLP based 1 × 1 convolution, which is formulated as:</p><formula xml:id="formula_30">s = sof tmax(M LP (P))<label>(17)</label></formula><p>Then the gumbel noise <ref type="bibr" target="#b46">[46]</ref> g ∈ R N , i = 1, 2, 3, ..., N can be selected from the gumbel distribution Gumbel(0, 1). And the   </p><formula xml:id="formula_31">FG-Net-v1 Ground Truth FG-Net-v1 FG-Net-v0 FG-Net-v0</formula><p>Ground Truth gumbel softmax (GS) operation on P can be used to compute the sampled vector y i , where τ is the time constant, and s i and g i are the i-th element in s and g respectively: : GM (s i ) = one hot encode(argmax(log(s i ) + g)) <ref type="bibr" target="#b19">(19)</ref> In this way, we can facilitate the differentiable sampling in training while maintain a discrete and hard GM sampling in network inference, hence sampling operation can be integrated into deep networks. To be more specific, given the input points denoted as P ∈ R N ×(d+3) , the GSS can be conducted as:</p><formula xml:id="formula_32">y i = GS(s i ) = sof tmax((log(s i ) + g i )/τ )<label>(18)</label></formula><formula xml:id="formula_33">P s = GS(W s P T ) · P<label>(20)</label></formula><p>where W s ∈ R N s ×(d+3) is weights of MLP layers in Equation 17, and P s ∈ R N s ×(d+3) are the N s points to be selected. During the inference of the network, the discrete sampling can be realized by substitute GS with GM, which is formulated as:   <ref type="figure">Fig. 10</ref>. The multi-thread CPU parallel processing of point clouds streams. The noise and outliers filtering is done on the CPU while sampling and deep network processing are done on the GPU. S i stands for i-th CPU processing stream, pc j stands for j-th point clouds batch in a single CPU stream.</p><formula xml:id="formula_34">P s = GM (W s P T ) · P (21)</formula><p>Note that the matrix W s will be relatively memory consuming when the quantity of point clouds is large, hence, we only adopt it in deeper layers of network when the points are down sampled to a certain quantity, which is less than 10 5 . In our IGSAM, the IDS is conducted in the first four stages of the network while the GSS is adopted in the fifth layer of the network. In this way, the density awareness can be maintained while significant points can be selected in a learnable way before conducting local aggregation and mining the global relationship by the non-local attentional module as introduced above, which enforces the sampling process to select the significant points for the specific understanding task.</p><p>2) Acceleration by multi-thread parallel computation on CPU: The noise and outliers filtering is implemented on the CPU while deep network computations are run on the GPU. It should be noted that we have reused the radius based ball query in both point clouds filtering and network operations for accelerations. As shown in <ref type="figure">Fig. 10</ref>, we have preloaded the next stream of point clouds to CPU before the network computation on GPU is finished for acceleration. And the multi-thread computation on CPU is also utilized to accelerate the query process which reduces the idle period notably in subsequent CPU and GPU computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments Setup</head><p>We have tested our method extensively on 8 different largescale point clouds understanding datasets. We implemented the network in Tensorflow and optimized it with Adam optimizer and initial learning rate of 1e −4 . Also, the point clouds are randomly rotated around each axis x, y, z with an angle φ ∈ [0, 2π]. The scaling is also applied along x, y, z axis with a scalar µ ∈ [0.85, 1.15] for data augmentation. The network is trained and tested in parallel with 5 × 10 5 point clouds in each stream. The experiments are conducted on Nvidia GTX 1080 graphics card with 8 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments of Filtering and Sampling Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Noise and Outliers filtering:</head><p>We have tested the influence of proposed noise and outliers filtering on the semantic segmentation performance of S3DIS. Noted that we adopt 6fold cross-validation for S3DIS to guarantee the generality and robustness. The noise filtering results with mean intersection over unions (mIOUs) are shown in <ref type="table" target="#tab_7">Table I</ref>, it demonstrates that filtering has a boost on segmentation performance for diverse point clouds understanding methods. With unrelated isolated noise points removed, the meaningful semantics of point clouds will be retained which will boost the performance of segmentation.</p><p>2) Point clouds sampling methods: To compare the efficiency our proposed IGSAM with different sampling methods, we have experimented their GPU memory usage and processing time on a single GTX 1080 GPU with 8 GB memory. The sampling methods include Random Sampling (RS), Reinforcement Learning based Sampling (RLS) <ref type="bibr" target="#b48">[48]</ref>, GSS <ref type="bibr" target="#b46">[46]</ref>, IDS, Farthest Point Sampling (FPS), and Generative Network (GS) <ref type="bibr" target="#b49">[49]</ref> based Sampling. The point clouds are divided into   batches consisting of 10 2 , 10 3 , 10 4 , 10 5 , 10 6 and 10 7 points respectively, then the batches of points are down-sampled 5 times which imitates the down-sampling in our network shown in <ref type="figure">Fig. 4</ref>. The total time and memory consumption of sampling methods on different numbers of points are illustrated in <ref type="figure" target="#fig_9">Fig.  14.</ref> It can be demonstrated that RS has the fastest processing speed with the smallest memory consumption. However, RS will result in a stochastic loss in meaningful information, which will give unsatisfactory segmentation results. As shown in <ref type="table" target="#tab_7">Table II</ref>, mIOUs will drop significantly from 70.8% to 66.8% if RS is adopted. It should also be noted that GSS is not suitable for more than 10 6 points because the GPU Activation Score: <ref type="figure" target="#fig_5">Fig. 16</ref>. Visualization of the learnt features by pointwise correlated feature learning and geometric convolutions respectively, please zoom in for details. memory will increase greatly with the number of points. Hence, we only use GSS in the last layer of the network when the number of points is less than 10 5 . Leveraging IDS with adaptability to the local density of points, our IGSAM achieve the best performance among different sampling methods with only a marginal increase of computational cost compared with RS. The segmentation mIOUs using different sampling methods are also shown in <ref type="table" target="#tab_7">Table II</ref>. It can be seen that our sampling methods give the best performance among all sampling methods on the S3DIS benchmark with mIOUs of 70.8%, which demonstrates the effectiveness of our proposed sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments of Large-scale Scene Understanding</head><p>We have experimented our method extensively on nearly all existing large-scale point clouds understanding datasets including ModelNet40 <ref type="bibr" target="#b50">[50]</ref>, ShapeNet-Part <ref type="bibr" target="#b51">[51]</ref>, PartNet <ref type="bibr" target="#b52">[52]</ref>, S3DIS <ref type="bibr" target="#b5">[6]</ref>, NPM3D <ref type="bibr" target="#b53">[53]</ref>, Semantic3D <ref type="bibr" target="#b54">[54]</ref>, Semantic-KITTI <ref type="bibr" target="#b55">[55]</ref>, and Scannet <ref type="bibr" target="#b56">[56]</ref>. The detailed information of different datasets and the results with speed of our framework in point clouds understanding are shown in <ref type="table" target="#tab_7">Table III</ref>. The qualitative Segmented Activation Segmented Activation Segmented Activation   experiments of large-scale real-world scene parsing are shown in <ref type="figure" target="#fig_5">Fig. 6, Fig. 7, Fig. 8, Fig. 9, Fig. 11</ref>, and <ref type="figure" target="#fig_0">Fig. 12</ref>, respectively. The mIOUs of 78.2%, 70.8%, 82.3%, 58.2% are attained on Semantic3D <ref type="bibr" target="#b54">[54]</ref>, S3DIS <ref type="bibr" target="#b5">[6]</ref>, NPM3D <ref type="bibr" target="#b53">[53]</ref> and fine-grained segmentation dataset PartNet <ref type="bibr" target="#b52">[52]</ref> respectively with real time segmentation of about 18.6 Hz for each LiDAR scan with 5 × 10 5 points, which outperforms state-of-theart methods in terms of accuracy and efficiency. Denote the network with global attentive module in <ref type="figure">Fig. 5</ref> as FG-Net-v0 and the one without as FG-Net-v1. With global attention, the feature correlations across a long spatial range are effectively captured and distributed objects such as road and buildings are more precisely segmented in FG-Net-v1 compared with FG-Net-v0. The transfer learning results shown in <ref type="figure" target="#fig_1">Fig. 13</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Main Features Accuracy/ mIOUs (%) Time (s)/ 10 5 points ModelNet40 <ref type="bibr" target="#b50">[50]</ref> More than 12311 CAD models from 40 classes 93.1 0.0561 ShapeNet-Part <ref type="bibr" target="#b51">[51]</ref> 16,881 shapes from 16 classes, annotated with 50 parts in total 86.6 0.0552 PartNet <ref type="bibr" target="#b52">[52]</ref> More than 27000 3D models in 24 object classes with 18 parts per object 58.2 0.0501 S3DIS <ref type="bibr" target="#b5">[6]</ref> LiDAR scan of more than 6000 m 2 of 6 different areas from 3 buildings 70.8 0.0498 NPM3D <ref type="bibr" target="#b53">[53]</ref> Kilometer-scale point clouds captured from multiple city roads 82.3 0.0528 Semantic3D <ref type="bibr" target="#b55">[55]</ref> Largest-scale dataset of more than 4 billion annotated points with 8 classes 78.2 0.0523 Semantic-KITTI <ref type="bibr" target="#b55">[55]</ref> Largest-scale LiDAR dataset for autonomous driving 53.8 0.0512 Scannet <ref type="bibr" target="#b56">[56]</ref> Reconstructed 1513 indoor scenes from 707 different areas 68.5 0.0495 in <ref type="figure">Fig. 15</ref>. It can be seen that kernel points are adaptively deformed to capture different geometric structures in the original point clouds. Hence in the test phase, the specific geometric structures in the unseen scene will be effectively captured and described by deformable kernels. In this way, we can model the geometry of the scene in a learnable way to better enhance structural awareness in per-point based processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Visualization of the learnt features:</head><p>To figure out what has been learnt by our network, the inner activations of the 2 core network modules are visualized as is illustrated in <ref type="figure" target="#fig_5">Fig. 16</ref>. It should be noted that the activations of designed 2 branches is complementary to each other because the deformable convolution captures the coarse-grained continuous geometry features while the correlated features learning focuses on fine-grained isolated per-point features as shown in <ref type="figure" target="#fig_5">Fig. 16</ref>, which is also in accordance with our design.</p><p>3) Visualization of the non-local activation: In order to further demonstrate the effectiveness of the non-local module, we also visualize the non-local activation. <ref type="figure" target="#fig_6">Fig. 17</ref> demonstrates that the non-local module can capture the long-range dependencies of the same semantic category such as chairs or bookcases. The contexts that are far away from each other can be nicely modeled and captured. It can also be observed that the non-local activation can also give rough results of segmentation prediction of the category of the query point, which is advantageous for further semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Efficiency and Online Performance</head><p>To give a more convincing evaluation of the real-time performance of our method on large-scale real-scene point clouds segmentation, we have tested our method compared with others on the whole sequence of Semantic-KITTI <ref type="bibr" target="#b55">[55]</ref> dataset. The sequences are captured and fed into the networks at 25 Hz. The inference time and the GPU memory used are shown in <ref type="figure" target="#fig_7">Fig. 18, and</ref>   <ref type="bibr" target="#b39">[39]</ref>, the speed increase by 274% and 38.5% while the memory consumption reduce 46.5% and 8.6% respectively, which is a prominent progress in the speed and memory efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Study of Network Modules</head><p>Our designed network modules can be easily integrated or removed from existing point clouds processing architectures. Some ablation studies are also done to validate the effectiveness and necessity of our designed modules. As shown in <ref type="table">Table.</ref> IV, 4 core modules are removed from our network respectively and the mIOUs of 6-fold cross-validation on S3DIS dataset is recorded. From the results, removing geometric convolutional modelling results in 11% performance drop because learning the intrinsic geometric shape contexts of point clouds is vital for the recognition. On the other hand, removing global and local correlated feature mining results in 5.6% and 7.3% drop in mIOUs, which demonstrates both the local and long-range feature relationship capturing are also essential to the segmentation task. And not using the attentional aggregation will also decline the performance for not retaining some meaningful features. Furthermore, the experiments show the 5-stage (h = 5) network is superior to 4-stage or 6-stage networks because shallow networks have a poor fitting ability while deeper networks will result in oversampling of point clouds, which will all deteriorate the performance. We also tested M = 1 in the RLB, however the segmentation performance will not increase. Therefore M = 8 is adopted for the sake of memory efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we have proposed a general solution FG-Net to large-scale point clouds understanding with real-time speed and state-of-the-art performance. The filtering and sampling methods are specially designed for large-scale point clouds with high efficiency and they will boost the scene parsing performance. The network can effectively model the point clouds structures and find the feature correlations across a long spatial range. Leveraging feature pyramid based residual learning, hierarchical features at different resolutions can be fused in a memory efficient way. Experiments on challenging circumstances showed that our approach outperforms state-ofthe-art methods in terms of both accuracy and efficiency. He also serves as an associate editor of the journal Unmanned Systems. Since 2019, he has been supported by the distinguished professor program of Hubei Province and the National Young Talent Program, China. He has published more than 70 research papers on top journals and conferences, such as IJCV, IEEE T-PAMI, IEEE TIE, IEEE TGRS, IEEE T-ITS, ISPRS JPRS, Neurocomputing, IEEE TCSVT, CVPR, ECCV, ACCV, BMVC, etc. His research interests include computer vision, machine learning, remote sensing and their applications. In particular, he has strong interests in vision for intelligent systems and intelligent system based vision. Ben M. Chen is currently a Professor of Mechanical and Automation Engineering at the Chinese University of Hong Kong (CUHK) and a Professor of Electrical and Computer Engineering (ECE) at the National University of Singapore (NUS). He was a Provost's Chair Professor in the NUS ECE Department, where he also served as the Director of Control, Intelligent Systems and Robotics Area, and Head of Control Science Group, NUS Temasek Laboratories. He was an Assistant Professor at the State University of New York at Stony Brook, in 1992-1993. His current research interests are in unmanned systems, robust control and control applications. Dr. Chen is an IEEE Fellow. He has authored/co-authored more than 450 journal and conference articles, and a dozen research monographs in control theory and applications, unmanned systems and financial market modeling. He had served on the editorial boards of a dozen international journals including IEEE Transactions on Automatic Control and Automatica. He currently serves as an Editor-in-Chief of Unmanned Systems. Dr. Chen has received a number of research awards. His research team has actively participated in international UAV competitions and won many championships in the contests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overall system framework of proposed FG-Net, and the core module FG-Conv can be integrated into FG-Net with multi-resolution residual learning. 1 × 1 Conv stands for 1 × 1 convolutions. The network operations are done from top to bottom and from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed illustration of our proposed novel pointwise correlated feature and geometric convolutional modelling module (FG-Conv Module), the deformable convolution operation is illustrated at the right bottom corner, with the query point denoted as p i and the k-th neighbour point denoted as p k , the output vector of point convolution is the dot product of point features and kernel weights. The feature vector is summed in attentional aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Then the element-wise multiplication between f a and f h is utilized to obtain the learnt feature f c = f a f h . And the final features f b is the sum of original feature and learnt feature: f b = f h + f c . We apply the MLP layer to control the dimension of the output vector flexibly and give the meaningful aggregated feature f out ∈ R f out containing both local correlated features and enhanced local geometry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1×</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>The detailed residual learning block design of our FG-Net is shown in the subfigure (a), in which we show the original inefficient residual learning block and our designed residual learning block respectively, and the proposed feature pyramid residual learning network is shown in the subfigure (b). N cls and N seg stand for the number of classes in classification of the presence of objects or not, and number of classes in semantic segmentation respectively. Detailed illustration of the non-local global attentive module that functions as "Global feature Extraction" inFig. 4. The feature representations M in and M out in this module are corresponding to Subfigure (b) inFig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Segmentation results of PartNet (FG-Net-v0 is the network with global attention while FG-Net-v1 is the one without global attention).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Semantic3D segmentation results with the inconsistent predictions indicated by the black circles, please zoom in for details (FG-Net-v0 is network with global attention, while FG-Net-v1 is the one without global attention).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Detailed S3DIS segmentation results with the inconsistent predictions highlighted in the red circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>S3DIS segmentation results of the whole area of Area 1 to Area 6, please zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Comparison of computational cost (a) and memory consumption (b) of different sampling methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 17 .</head><label>17</label><figDesc>Visualization of the non-local activations. Left shows the segmentation predictions; Right shows the non-local activations. (The background is indicated in blue and the query point is indicated in yellow, and the red points are given large attentive weights), please zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 18 .</head><label>18</label><figDesc>Comparisons of the time and memory cost of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Feng</head><label></label><figDesc>Lin received the B.Eng. degree in Computer Science and Control, and the M.Eng. degree in system engineering from the Beihang University, Beijing, China, in 2000 and 2003, respectively. He received the Ph.D. degree in Computer &amp; Electrical Engineering from the National University of Singapore (NUS) in 2011. He was the recipient of the Best Application Paper Award, 8th World Congress on Intelligent Control and Automation, Jinan, China (2010). Dr. Lin has worked as a Senior Research Scientist at the Temasek Laboratories NUS, and a Research Assistant Professor of Department of Electrical &amp; Computer Engineering at the National University of Singapore from 2011 to 2019. Dr. Lin is currently working as an Associate Research Scientist at Peng Cheng Laboratory since 2019. His main research interests are unmanned aerial vehicles, vision-aided control and navigation, target tracking, robot vision as well as embedded vision systems. He has served on the editorial board for Unmanned Systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>received December 16, 2020. This work is supported in part by Hong Kong PhD Fellowship Scheme (PF17-07092), and in part by the Research Grants Council of Hong Kong SAR (Grant No: 14209020), and in part by Peng Cheng Laboratory. (Corresponding authors: Kangcheng Liu, Zhi Gao, and Feng Lin). Chen are with the Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong 999077, China. (email: kcliu@mae.cuhk.edu.hk, bm-chen@cuhk.edu.hk) 2 Z. Gao is with the School of Remote Sensing and Information Engineering, Wuhan University, Hubei 430070, China. (email: gaozhinus@gmail.com)</figDesc><table><row><cell cols="3">Large-Scale Point Clouds Input</cell><cell></cell><cell cols="2">Semantic Segmentation Output</cell><cell></cell></row><row><cell cols="2">Ground Truth</cell><cell cols="2">Ours (0.021s)</cell><cell></cell><cell cols="2">FKA-Conv (2.3s)</cell></row><row><cell>ceiling</cell><cell>floor</cell><cell>wall</cell><cell>beam</cell><cell>column</cell><cell>window</cell><cell>door</cell></row><row><cell>table</cell><cell>chair</cell><cell>sofa</cell><cell>bookcase</cell><cell>board</cell><cell>clutter</cell><cell></cell></row></table><note>1 K. Liu and B. M.3 F. Ling is with the Peng Cheng Laboratory, First Xingke Street, Nanshan, Shenzhen, Guangdong 518066, China. (email: truelinfeng@outlook.com)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Utilizing Equation 18, y i is differentiable with respect to s i , thus, the differentiable sampling can be realized for original point distributions in the training process. It should be noted that τ should start from a high value of 1.0 and gradually</figDesc><table /><note>anneals to a smaller value of 0.05 during training. When τ → 0, GS degenerates to the Gumbel-Max (GM) selection. And the discrete individual point candidate can be hard selected utilizing GM in inference as shown in Equation 19</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE I THE</head><label>I</label><figDesc>Visualization of the Semantic-KITTI segmentation results with the wrong predictions highlighted by the red circles, please zoom in for detailsFig. 12. NPM3D segmentation results, it should be noted that the color brown stands for unlabeled points, please zoom in for details.</figDesc><table><row><cell></cell><cell cols="2">Ground Truth</cell><cell cols="2">Prediction</cell><cell>Ground Truth</cell><cell cols="2">Prediction</cell><cell cols="2">Ground Truth</cell><cell>Prediction</cell></row><row><cell></cell><cell>car</cell><cell>bicycle</cell><cell></cell><cell>motorcycle</cell><cell>truck</cell><cell>other vehicles</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell></row><row><cell></cell><cell>parking</cell><cell>sidewalk</cell><cell cols="2">other ground</cell><cell>buildings</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell cols="2">Fig. 11. Ground Truth</cell><cell cols="2">FG-Net-v1</cell><cell></cell><cell cols="2">FG-Net-v0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>unlabeled</cell><cell>ground</cell><cell cols="2">buildings</cell><cell>poles</cell><cell>bollards</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trash cans</cell><cell>barriers</cell><cell cols="2">pedestrians</cell><cell>cars</cell><cell>natural</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">COMPARISON OF SEGMENTATION ON S3DIS WITH AND WITHOUT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FILTERING.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell cols="2">mIOUs (without) mIOUs (with)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPG [22]</cell><cell></cell><cell>62.1</cell><cell>63.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shellnet [3]</cell><cell></cell><cell>66.8</cell><cell>67.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PointCNN [32]</cell><cell></cell><cell>65.4</cell><cell>65.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kpconv [29]</cell><cell></cell><cell>67.1</cell><cell>68.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DGCNN [47]</cell><cell></cell><cell>56.1</cell><cell>58.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FKA-Conv [5]</cell><cell></cell><cell>68.1</cell><cell>68.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FG-Net (Ours) (best)</cell><cell>70.2</cell><cell>70.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE II THE</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ground Truth</cell><cell cols="2">Predictions</cell><cell>Ground Truth</cell><cell>Predictions</cell><cell>Ground Truth</cell><cell>Predictions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>S3DIS</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scannet</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scannet Labels:</cell><cell>window</cell><cell>floor</cell><cell>wall</cell><cell>cabinet</cell><cell>table</cell><cell>door</cell><cell>desk</cell><cell>curtain</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>others</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>unlabeled</cell><cell>counter</cell><cell>bookshelf</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Fig. 13. Transfer learning results between S3DIS and Scannet, please zoom</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">in for details</cell><cell></cell></row><row><cell cols="7">COMPARISON OF SEGMENTATION PERFORMANCE ON S3DIS WITH</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">DIVERSE SAMPLING METHODS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sampling Method</cell><cell>RS</cell><cell>RLS</cell><cell>IGSAM</cell><cell>IDS</cell><cell>FPS</cell><cell>GS</cell><cell></cell><cell></cell></row><row><cell>SPG [22]</cell><cell>61.7</cell><cell>62.2</cell><cell>63.2</cell><cell cols="3">62.6 61.8 62.5</cell><cell></cell><cell></cell></row><row><cell>Shellnet [3]</cell><cell>66.2</cell><cell>66.3</cell><cell>67.6</cell><cell cols="3">66.9 66.1 66.3</cell><cell></cell><cell></cell></row><row><cell>PointCNN [32]</cell><cell>64.9</cell><cell>65.3</cell><cell>66.8</cell><cell cols="3">66.1 65.9 65.3</cell><cell></cell><cell></cell></row><row><cell>Kpconv [29]</cell><cell>66.5</cell><cell>67.1</cell><cell>67.5</cell><cell cols="3">67.3 66.7 66.3</cell><cell></cell><cell></cell></row><row><cell>DGCNN [47]</cell><cell>55.2</cell><cell>56.0</cell><cell>58.4</cell><cell cols="3">57.5 57.9 56.0</cell><cell></cell><cell></cell></row><row><cell>FKA-Conv [5]</cell><cell>64.6</cell><cell>65.2</cell><cell>68.6</cell><cell cols="3">66.2 66.1 65.3</cell><cell></cell><cell></cell></row><row><cell cols="2">FG-Net (Ours) (best) 66.8</cell><cell>70.3</cell><cell>70.8</cell><cell cols="3">70.3 69.5 69.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>To better demonstrate the geometry adaptive capacity of the deformable convolutions, the deformable kernel are visualized</figDesc><table><row><cell>also</cell></row><row><cell>demonstrates our networks learn the underlying latent model</cell></row><row><cell>of feature representations that is able to generalize across new</cell></row><row><cell>scenes.</cell></row><row><cell>D. Visualization of the Network Modules</cell></row><row><cell>1) Visualization of the Deformable Convolutional Kernels:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE III THE</head><label>III</label><figDesc>COMPARISONS OF PERFORMANCE OF OUR METHOD ON DIFFERENT LARGE-SCALE POINT CLOUDS UNDERSTANDING DATASETS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>the PaiSeg [57] is also a recently developed method for point clouds segmentation. With the residual learning framework, our method can reach 16.89 Hz, 19.53 Hz, 19.31 Hz, and 18.69 Hz for LIDAR scan 02, 04, 05, and 09 respectively. Compared with RSSP [1] and RandlaNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE IV THE</head><label>IV</label><figDesc>SEGMENTATION PERFORMANCE OF ABLATED NETWORK ON S3DIS Zhi Gao received the B.Eng. and the Ph.D degrees from Wuhan University, China in 2002 and 2007 respectively. Since 2008, he joined the Interactive and Digital Media Institute, National University of Singapore (NUS), as a Research Fellow (A) and project manager. In 2014, he joined Temasek Laboratories in NUS (TL@NUS) as a Research Scientiest (A) and Principal Investigator. He is currently working as a full professor with the School of Remote Sensing and Information Engineering, Wuhan University.</figDesc><table><row><cell>Ablations</cell><cell>mIOUs (%)</cell></row><row><cell>Remove pointwise feature relation mining (PFM)</cell><cell>66.2</cell></row><row><cell>Remove geometric convolutional modelling (GCM)</cell><cell>59.8</cell></row><row><cell>Remove attentional aggregation (AG)</cell><cell>67.1</cell></row><row><cell>Remove global feature extraction</cell><cell>63.5</cell></row><row><cell>The full network framework</cell><cell>70.8</cell></row><row><cell>Replace the backbone with a 6-stage network</cell><cell>69.1</cell></row><row><cell>Replace the backbone with a 4-stage network</cell><cell>67.9</cell></row><row><cell>Without semantic context loss L 2 (W)</cell><cell>68.2</cell></row><row><cell>With 2 RLB2 in each convolutional block</cell><cell>69.7</cell></row><row><cell>Choose M = 1 in RLB1 and RLB2</cell><cell>70.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time 3-d semantic scene parsing with lidar sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speedup 3-d texture-less object recognition against self-occlusion for intelligent manufacturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3887" to="3897" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object recognition in cluttered scenes with local surface features: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2270" to="2287" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FKAConv: Feature-Kernel Alignment for Point Cloud Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3384" to="3391" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for roadobject segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13138</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end learning local multi-view descriptors for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1919" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning multiview 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1759" to="1769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vv-net: Voxel vae net with group convolutions for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8500" to="8508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7440" to="7449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive hierarchical down-sampling for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nezhadarya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spherical kernel for efficient graph convolution on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical deep embedding for aurora image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new aggregation of dnn sparse and dense labeling for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">I-keyboard: Fully imaginary keyboard on touch devices empowered by deep neural decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep category-level and regularized hashing with global semantic similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised deep learning for brain disease prognosis using mri and incomplete clinical scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pvrcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The importance of sampling inmeta-reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9280" to="9290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Samplenet: Differentiable point cloud sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7578" to="7588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Partnet: A large-scale benchmark for fine-grained and hierarchical partlevel 3d object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Paris-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semantic3d. net: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pai-conv: Permutable anisotropic convolutional networks for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13135</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">He is currently pursing his Ph.D. degree in the Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong. His research interests include power systems analysis and control, robotics, LIDAR-SLAM, machine learning and computer graphics for unmanned autonomous systems. Currently, he has strong interests in 3D deep learning and scene understanding for robotic perception</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Kangcheng Liu received his B.Eng. degree in Electrical Engineering and Automation at Harbin Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
