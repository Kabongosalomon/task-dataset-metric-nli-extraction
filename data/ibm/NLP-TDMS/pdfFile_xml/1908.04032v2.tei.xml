<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>Tang 5, 6, 7 . 2019. August 5, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@sjtu.edu.cnnie@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
							<affiliation key="aff4">
								<orgName type="institution">Mila-Quebec Institute for Learning Algorithms</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">HEC Montréal</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">CIFAR AI Research Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename></persName>
						</author>
						<title level="a" type="main">An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation</title>
					</analytic>
					<monogr>
						<title level="m">1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data (DLP-KDD&apos;19)</title>
						<imprint>
							<date type="published">Tang 5, 6, 7 . 2019. August 5, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3326937.3341257</idno>
					<note>ACM Reference Format: Anchorage, AK, USA. 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Information retrieval</term>
					<term>• Comput- ing methodologies → Neural networks</term>
					<term>KEYWORDS Knowledge Graph, Knowledge-enhanced Recommendation, Neigh- borhood-based Interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies graph-based recommendation, where an interaction graph is constructed from historical records and is leveraged to alleviate data sparsity and cold start problems. We reveal an early summarization problem in existing graph-based models, and propose Neighborhood Interaction (NI) model to capture each neighbor pair (between user-side and item-side) distinctively. NI model is more expressive and can capture more complicated structural patterns behind user-item interactions. To further enrich node connectivity and utilize high-order structural information, we incorporate extra knowledge graphs (KGs) and adopt graph neural networks (GNNs) in NI, called Knowledge-enhanced Neighborhood Interaction (KNI). Compared with the state-of-the-art recommendation methods, e.g., feature-based, meta path-based, and KG-based models, our KNI achieves superior performance in click-through rate prediction (1.1%-8.4% absolute AUC improvements) and outperforms by a wide margin in top-N recommendation on 4 real world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems have become increasingly important in various online services for helping users find the information they * Equal contribution. This work was done when the first and the second authors were visiting Mila and Université de Montréal. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). DLP-KDD'19, August 5, 2019, Anchorage, AK, USA © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6783-7/19/08. https://doi.org <ref type="bibr">/10.1145/3326937.3341257</ref> want. However, existing recommender systems are challenged by the problems of data sparsity and cold start, i.e., most items receive only a few feedbacks (e.g., ratings and clicks) or no feedbacks at all (e.g., for new items). To tackle these problems, the existing approaches usually utilize side information to learn better user/item representations <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16]</ref>, which then facilitate the learning of user-item interactions, and finally promote the recommendation quality. In many scenarios, knowledge graphs (KGs) can be used to provide general background knowledge as well as rich structural information <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30]</ref>.</p><p>Graph-based recommender systems build interaction graphs from historical feedbacks and side information, where the nodes can be users, items, or side information (e.g., tag, genre), and two nodes are linked together based on relevance or co-occurrence. Recently, graph-based models are becoming more powerful with advanced graph neural networks. For example, graph convolution networks <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b28">28]</ref> can integrate high-order neighborhood information in an end-to-end way, graph attention networks <ref type="bibr" target="#b26">[26]</ref> can simulate user preferences on knowledge graphs. Graph-based models are more expressive than traditional feature-based models, because they take the local structures (of the users, items, and relevant nodes) into consideration.</p><p>However, due to an "early summarization" issue, the existing graph-based models cannot fully utilize the local structures: these models usually compress user-and item-neighborhoods into single user/item embeddings before prediction. In this case, only two nodes and one edge are activated, yet other nodes and their connections are mixed and relayed. We consider the meticulous local structures are valuable, and a good system should be able to capture useful patterns, and filter out other noise. Here is an example. A system is recommending a film to a user, where the user has rated 5 stars for "La La Land" (Land) and "Interstellar" (Inter), and the film has 2 tags, "romance" and "fiction". We know that "Land" is a romance film, and "Inter" is a science fiction film. Thus the connections between ("Land", "romance") and ("Inter", "fiction") could be helpful for recommendation, meanwhile, the connection between ("Land", "fiction") is nonsense and not expected, which is regarded as noise. We argue that the local structures are hidden and not fully utilized in previous graph-based methods.</p><p>To address the early summarization problem, we extend useritem interactions to their neighbors, and propose a unified Neighborhood Interaction (NI) model. More specifically, we propose a bi-attention network to make prediction on the local structures directly, instead of compressing them into user/item embeddings. We also utilize graph neural networks (GNNs) to integrate high-order neighborhood information, and introduce knowledge graphs to increase the local connectivity. The final model, called Knowledgeenhanced Neighborhood Interaction (KNI), is evaluated on 4 realworld datasets and compared with 8 feature-based, meta path-based, and graph-based models. Our experiments show that KNI outperforms state-of-the-art models, including Wide&amp;Deep, MCRec, Pin-Sage and RippleNet, by 1.1%-8.4% of AUC in click-through rate prediction, and exceeds baseline models by a wide margin in top-N recommendation. We also provide a case study and statistical analysis to demonstrate our model.</p><p>The rest of this paper is organized as follows: we first define the problem and introduce our KNI model in Section 2. And then we demonstrate the experiments and discuss the results in Section 3. Related works are summarized in Section 4. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KNOWLEDGE-ENHANCED NEIGHBORHOOD INTERACTION</head><p>In this section, we introduce graph-based recommendation and the early summarization issue at first, and define Neighborhood Interaction (NI). Then, we extend NI with graph neural networks (GNNs) and knowledge graphs (KGs </p><formula xml:id="formula_0">= {(u, c, v)|u ∈ U , v ∈ V , c = 1}.</formula><p>In G r ec , users' neighbors are items, and items' neighbors are users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Graph-based Recommendation.</head><p>Most existing graph-based recommender systems utilize the graph structures via summarizing the neighborhood information into user/item representations. And then these models learn user-item interactions from the representations, which is usually formulated as inner product. Denote N u and N v as user and item neighborhoods, u and v as their representations.</p><formula xml:id="formula_1">u = agg(N u ) (1) v = agg(N v )<label>(2)</label></formula><formula xml:id="formula_2">y u,v = σ (⟨u, v⟩)<label>(3)</label></formula><p>where agg() is an aggregation function which maps a set of neighbor nodes into a single embedding vector, σ () is the sigmoid function. For simplicity, we omit σ in the following. The main difference of graph-based models is how to learn user/item representations from the graph structures, in another word, designing aggregation functions. The most popular and general agg() include averaging, attention <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b30">30]</ref>, etc.</p><formula xml:id="formula_3">Average: u = 1 |N u | i ∈N u x i (4) Attention: α u,i = softmax i (w ⊤ [x u , x i ] + b) (5) u = i ∈N u α u,i x i (6)</formula><p>x u is the embedding vector of user u, x v is of item v, x i is of node i, and α u,i is the user-side attention score produced by an attention network. w and b 1 are attention network parameters. In this paper, we mainly employ the attention network structure in Eq. (5), where [, ] means concatenation. Most previous methods summarize user/item neighborhood information before learning their interactions, which compresses the local structures into only two nodes and one edge, yet other nodes and their connections are mixed and relayed. The behavior may restrict model from exploring the local structures and obstruct to distinguish useful patterns from noise. We call it the early summarization issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Neighbor-Neighbor</head><p>Interaction. After expandingŷ u,v (before taking σ ),</p><formula xml:id="formula_4">Average:ŷ u,v = ⟨ 1 |N u | i ∈N u x i , 1 |N v | j ∈N v x j ⟩ (7) = i ∈N u j ∈N v 1 |N u ||N v | ⟨x i , x j ⟩<label>(8)</label></formula><formula xml:id="formula_5">Attention:ŷ u,v = ⟨ i ∈N u α u,i x i , j ∈N v α v, j x j ⟩ (9) = i ∈N u j ∈N v α u,i α v, j ⟨x i , x j ⟩<label>(10)</label></formula><p>we find there exists a general form of Eq. (8) and (10)  <ref type="figure">Figure 1</ref>: Model overview. Note: Red circles denote users. Green circles denote rated or unseen items. Blue circles denote nonitem entities. Dash circles denote user and item neighborhoods. In this example, a KIG is constructed at first, and then higher hop neighborhood information is aggregated into local neighbors. Finally, the user and item neighborhoods are collected to compute neighborhood interactions.</p><formula xml:id="formula_6">y = A ⊙ Z (11) s.t. i, j A i, j = 1 , Z i, j = ⟨x i , x j ⟩ (12) where A ∈ R |N u |×|N v | is a weight matrix summing up to 1, Z ∈ R |N u |×|N v | is</formula><p>We propose a bi-attention network to better utilize the neighborhood information, namely Neighborhood Interaction (NI).</p><formula xml:id="formula_7">α i, j = softmax i, j (w ⊤ [x u , x i , x v , x j ] + b)<label>(13)</label></formula><formula xml:id="formula_8">y u,v = i ∈N u j ∈N v α i, j ⟨x i , x j ⟩<label>(14)</label></formula><p>Different from Eq. (5), Eq. (13) takes both user-and item-side neighbors into consideration. In Eq. <ref type="bibr" target="#b14">(14)</ref>, different interaction terms are weighted distinctively. From the discussion above, NI can better utilize the local structures for recommendation, therefore, NI is supposed to address the early summarization problem of graphbased models.</p><p>It is worth noting that the average form (Eq. (8)) and attention form (Eq. (10)) are special cases of NI. The average form sets the weight matrix as a constant A = 1/|N u ||N v |, asnd the attention form approximates the weight matrix via 1st rank matrix decomposition A ∼ α u α ⊤ v , where α u and α v are column vectors of the user-/item-side attention scores (Eq. <ref type="formula">(5)</ref>).</p><p>Besides, we include the user and the item in their neighborhoods, i.e., u ∈ N u , v ∈ N v , thus the interactions between user and item (u, v), user and item neighbor (u, j), user neighbor and item (i, v), user neighbor and item neighbor (i, j) are all considered for prediction. The NI model is illustrated in <ref type="figure">Fig. 1 (d)</ref>, where the edges represent interactions among two neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Integrating High-order Neighborhood Information</head><p>In last section, the user neighbors are ever-rated items, and the item neighbors are historical audience. The interaction graph G r ec also contains high-order neighborhood information, for example, a film is a 2-hop neighbor of another film if they share the same tag.</p><p>Introducing high-order neighborhood information has shown effective <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28]</ref> in graph-based recommendation, thus we introduce graph convolution network (GCN) <ref type="bibr" target="#b15">[15]</ref> and graph attention network (GAT) <ref type="bibr" target="#b24">[24]</ref> to encode high-order neighborhood information for NI model. Graph convolution network computes high-order node representations by stacking several graph convolution layers. Each graph convolution layer computes a node representation according to its nearest neighbors and itself (equivalent to a self loop in the graph). For a node u, a 2-layer GCN computes</p><formula xml:id="formula_9">x 1 i = σ ( 1 |N i | j ∈N i w 1 x j + b 1 )<label>(15)</label></formula><formula xml:id="formula_10">x 2 u = σ ( 1 |N u | i ∈N u w 2 x 1 i + b 2 )<label>(16)</label></formula><p>where x j is the feature vector or initial embedding of node j, x 1 i and x 2 u are outputs of the 1st and 2nd graph convolution layers, N i and N u are neighborhoods of i and u, and w and b are parameters to be learned. Successive graph convolution layers are separated by non-linear transformation σ (), which is usually ReLU.</p><p>Graph attention network is similar to GCN except that node embeddings are computed by multi-head self attention networks. For a node u, a 2-layer GAT computes (single head)</p><formula xml:id="formula_11">x 1 i = σ ( j ∈N i α 1 i, j w 1 x j + b 1 )<label>(17)</label></formula><formula xml:id="formula_12">x 2 u = σ ( i ∈N u α 2 u,i w 2 x 1 i + b 2 )<label>(18)</label></formula><p>where α l i, j is the attention score of node j to node i, produced by the l-th layer attention network 2</p><formula xml:id="formula_13">α l i, j = exp(LeakyReLU(w l a ⊤ [x l −1 i , x l −1 j ] + b l a )) k ∈N i exp(LeakyReLU(w l a ⊤ [x l −1 i , x l −1 k ] + b l a ))<label>(19)</label></formula><p>where w l a and b l a are parameters of the attention network, other notations are the same as GCN. Note that the above attention network structure is suggested in <ref type="bibr" target="#b24">[24]</ref>.</p><p>For any target node i, we can generate node embeddings x l i containing high-order neighborhood information with GCN or GAT <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">24]</ref>. And {x l i , i ∈ G r ec } can replace feature vectors or initial embeddings in NI model (Eq. (14)), where graph network serves as an encoder. This process is demonstrated in <ref type="figure">Fig. 1 (b)</ref> and (c). In (b) the 2-hop neighbors are propagated to 1-hop neighbors, and the 1-hop neighbors are concentrated to the central node.</p><p>Neighbor Sampling (NS) <ref type="bibr" target="#b7">[7]</ref> is a sampling method to facilitate graph network computation on large graphs. The original graph networks, e.g., GCN and GAT, traverse all neighbor nodes to generate </p><formula xml:id="formula_14">x 1 i = σ ( 1 K j ∈Ñ i w 1 x j + b 1 )<label>(20)</label></formula><p>whereÑ i is drawn randomly from N i , containing exactly K elements. NS controls the number of high-order neighbors directly, thus restrains model's complexity. There are other sampling methods, including random walk-based sampling <ref type="bibr" target="#b28">[28]</ref>, importance sampling <ref type="bibr" target="#b1">[2]</ref>, etc. In this work, we mainly adopt NS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Integrating Knowledge Graphs</head><p>A knowledge graph consists of a large number of entity-relationentity triples G kд = {(h, r, t)|h, t ∈ E, r ∈ R}, where E is the entity set, R is the relation set. Using the item set V as initial queries, we can map items to corresponding entities in knowledge graph.</p><p>Using the newly added entities as queries, we repeat the expansion several times and obtain knowledge-enhanced interaction graph (KIG), G = G r ec ∪ G kд . The resulting KIG is shown in <ref type="figure">Fig. 1 (a)</ref>. In KIG, the users' and items' neighbors are extended to non-item entities, e.g., a movie star. We can recklessly replace G r ec with G without modifying NI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Overview</head><p>The training objective is log loss</p><formula xml:id="formula_15">L(Y ,Ŷ ) = − y u,v =1 log(ŷ u,v ) − y u,v =0 log(1 −ŷ u,v ) + λ∥θ ∥ 2 2 (21)</formula><p>where λ∥θ ∥ 2 2 is the L2 regularization term to control overfitting. We then revisit the whole framework of KNI as shown in <ref type="figure">Fig. 1</ref>. (a): We first build knowledge-enhanced interaction graph (KIG) with user feedbacks and knowledge graphs. (b) and (c): We then apply graph neural networks (GNNs) to propagate high-order neighborhood information to user/item neighbors. (d): The user and item neighborhoods are collected and fed to Neighborhood Interactions (NI). The framework is trained end-to-end with the loss term presented above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Datasets</head><p>We combine 4 recommendation datasets with 2 public knowledge graphs in our experiments. The datasets and experiment code are publicly available 3 for reproducibility and further study.</p><p>The first two smaller datasets are released by <ref type="bibr" target="#b26">[26]</ref>.</p><p>• C-Book combines Book Crossing 4 and Microsoft Satori 5 .</p><p>• Movie-1M combines MovieLens 6 -1M and and Microsoft Satori.</p><p>We follow the procedures of <ref type="bibr" target="#b26">[26]</ref> to process the other two larger datasets, which are then linked to Freebase <ref type="bibr" target="#b0">[1]</ref>. The linkages are studied and provided by KB4Rec <ref type="bibr" target="#b32">[32]</ref>. Note that another dataset LFM in KB4Rec is not included in our experiments, because it follows a quite different scheme from the others and does not contain any rating or click information. A-Book and Movie-20M are processed as follows. Since A-Book and Movie-20M are originally in rating format, we convert ratings to binary feedbacks: 4 and 5 stars are converted to positive feedbacks (denoted by "1") and the other ratings to negative feedbacks. For each user, we sample the same amount of negative samples (denoted by "0") as their positive samples from unseen items. We also drop low-frequency users and items. The threshold is 5 for A-Book and 20 for Movie-20M.</p><p>After the datasets are processed, we split each dataset into training/validation/test sets at 6:2:2. Then we map the items of training set to corresponding entities in Freebase, with the help of KB4Rec <ref type="bibr" target="#b32">[32]</ref>. For each dataset, we use the linked items as initial queries to find related non-item entities. These entities are added to KIG and used for further expansion. We repeat this process 4 times to ensure sufficient knowledge is included in the final dataset. We also remove entities appearing less than 5 times on A-Book (the threshold is 20 for Movie-20M), and relations appearing less than 5000 times (same for Movie-20M). The basic statistics of the 4 datasets are presented in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compared Models</head><p>We compare NI (without knowledge graph) and KNI (with knowledge graph) with 2 feature-based, 2 meta path-based, and 4 graphbased models. For fair comparison, we pre-train TransR models and extract structural features for feature-based models. Note that the main difference between NI models and baseline models is, NI models make prediction from graph structures directly, while others compress the structural information into two node embeddings. From another perspective, NI models further take the interactions between neighbors into consideration. libFM <ref type="bibr" target="#b21">[21]</ref> is a widely used feature-based model, and is well known for modeling feature interactions. In our experiments, we concatenate the user ID, item ID, and the average embedding of related entities learned from TransR <ref type="bibr" target="#b17">[17]</ref> as the input to libFM.</p><p>Wide&amp;Deep <ref type="bibr" target="#b2">[3]</ref> is another feature-based model, which takes advantages of both shallow models and deep models and achieves state-of-the-art recommendation results. We provide the same input as libFM to Wide&amp;Deep.</p><p>PER <ref type="bibr" target="#b29">[29]</ref> is a meta path-based model, which builds heterogeneous information network (HIN) on side information, and extracts meta path-based features from HIN. In our experiments, we use all item-attribute-item relations as meta paths.</p><p>MCRec <ref type="bibr" target="#b10">[10]</ref> is a co-attentive model built on HIN. MCRec learns context representations from meta-paths, and is a state-of-the-art recommendation model. Besides, their code is released <ref type="bibr" target="#b8">8</ref> .</p><p>CKE <ref type="bibr" target="#b30">[30]</ref> proposes a general framework to jointly learn structural/textual/visual embeddings from knowledge graph, texts and images for collaborative recommendation. We adopt the structural embedding and recommendation components of CKE.</p><p>DKN <ref type="bibr" target="#b27">[27]</ref> is another knowledge graph-based recommendation model. In our experiments, we use pre-trained TransR embeddings as the input for DKN, with code 9 .</p><p>PinSage <ref type="bibr" target="#b28">[28]</ref> uses GCN for web-scale recommendation. In our experiments, we use PinSage as a representative GCN approach and explore different network structures and sampling methods on PinSage.</p><p>RippleNet <ref type="bibr" target="#b26">[26]</ref> is a state-of-the-art knowledge graph-based recommendation model. RippleNet uses attention networks to simulate user preferences on KG. In our experiments, we use RippleNet as a representative GAT approach, with code <ref type="bibr" target="#b10">10</ref> .</p><p>It is worth noting that, Wide&amp;Deep, MCRec, PinSage and Rip-pleNet are recently proposed state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Setup and Evaluation</head><p>We evaluate these models on 2 tasks, click-through rate (CTR) prediction and top-N recommendation. For CTR prediction, we use the metrics Area Under Curve (AUC) and Accuracy (ACC), which are widely used in binary classification problems. For top-N recommendation, we use the best models obtained in CTR prediction to generate top-N items, which are compared with the test set to compute Precision@K, Recall@K, and F1@K. We repeat each experiment 5 times and report the average scores.</p><p>General hyper-parameters include learning rate, embedding size, regularization, etc. Graph-based models, including PinSage, Rip-pleNet and our models, are trained with graph network modules. For these models, 2 hyper-parameters are critical, i.e., the hop number and the sampling method.</p><p>A larger hop number indicates a larger neighborhood. In the graph construction stage, we expand the items 4 times on Freebase, thus an item needs 4 steps to visit certain neighbors. For graphbased models, we tune the hop number from 1 to 4. Sampling methods are mainly introduced to speed up training on large graphs, and sometimes influence model convergence and performance. We tune neighbor sampling (NS) and random walk-based sampling in experiments.</p><p>We then apply grid search on embedding dimension, learning rate, l2 regularization, etc., for all the compared models. The hyperparameters are chosen according to the AUC scores on validation sets, and the parameter settings are explained in Section 3.5.</p><p>We repeat evaluation several times and use the average scores to compute the metrics. We perform an empirical experiments to determine the number of repetitions, shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. According to the figure, we conclude the prediction becomes stable after sufficient evaluations. In the following experiments, we fix this number to 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiment Results</head><p>In this section, we present and analyze the evaluation results of CTR prediction <ref type="table">(Table 2)</ref> and top-N recommendation <ref type="figure" target="#fig_2">(Fig. 3, 4</ref>, 5, 6). From <ref type="table">Table 2</ref> we can observe:</p><p>(i) Meta path-based and graph-based models outperform featurebased models. MCRec, PinSage and RippleNet outperform the other baseline models.</p><p>(ii) Meta-path design requires much human expertise, and is not end-to-end. Even though MCRec achieves competitive results with RippleNet, it requires more efforts to manually design and pre-process meta-paths. This restricts the application of meta pathbased models on large graphs and scenarios with complex schema.</p><p>(iii) High-order neighborhood information contains much more noise. We increase the hop numbers of different models from 1 to 4, and find performance usually decreases with 3-or 4-hops. We attribute this problem to the noise brought by the huge amounts of high-order neighbors ( <ref type="table">Table 3)</ref>.</p><p>(iv) NI shows significant improvements over baseline models. To our surprise, NI outperforms PinSage and RippleNet even without knowledge graphs. This means the local neighborhood structures are more valuable than high-order neighbors. We also observe that high-order neighborhoods increase dramatically.</p><p>(v) Integrating knowledge graphs, KNI obtains even better results than NI. Compared with Wide&amp;Deep, MCRerc, PinSage, and Rip-pleNet, KNI achieves 1.1%-8.4% AUC improvements on 4 datasets.</p><p>(vi) From the data perspective, book datasets are more sparse (&gt; 99.9%) than movie datasets, according to <ref type="table">Table 3</ref>. However, KNI achieves better improvements on the 2 book datasets (4%-5% AUC improvements over best baselines) than the 2 movie datasets (1%-2% AUC improvements). This means KNI can better solve data sparsity.</p><p>DLP-KDD'19, August 5, 2019, Anchorage, AK, USA <ref type="table">Table 2</ref>: The results of CTR prediction. Note: "*" indicates the statistically significant improvements over the best baseline, with p-value smaller than 10 −6 in two-sided t-test. For the top-N recommendation task, we compare KNI with baseline models. From <ref type="figure" target="#fig_2">Fig. 3, 4</ref>, 5, and 6 we can observe:</p><p>(i) The top-N recommendation results are consistent with CTR prediction. Meta path-based and graph-based models perform better than feature-based models. KNI performs the best.</p><p>(ii) On the two book datasets, KNI performs much better than baselines when K is small, especially in top-1 recommendation. This indicates that KNI captures user preference very well. On the 2 movie datasets, KNI outperforms state-of-the-art baseline models by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Parameter Settings</head><p>For hop number, we tune RippleNet following <ref type="bibr" target="#b26">[26]</ref>, and find Rip-pleNet performs best with hop=3 (C-Book), hop=2 (Movie-1M), hop=1 (A-Book and Movie-20M). For PinSage and our models, we find 1-hop is good enough. After an analysis of the datasets, we found that the main reason for this problem is the explosive increase of high-order neighbors. From <ref type="table">Table 3</ref> we can see that the average neighborhood size increases dramatically when it goes from 1-hop to 2-hop, especially on the 2 movie datasets. This may be caused by some high degree nodes in the knowledge graph. The noise brought by the high-order neighbors increases training difficulties. Similar results can be found in many other studies. For example, <ref type="bibr" target="#b26">[26]</ref> shows that larger hop numbers may decrease model performance. In <ref type="bibr" target="#b23">[23]</ref>, the author claims that 1 layer GCN performs the best.</p><p>As for the sampling method, we tune NS and random walk-based sampling on PinSage. We find that random walk-based sampling  <ref type="table" target="#tab_4">Table 4</ref>. This result confirms that the model complexity of KNI (Section 2.4) could be well controlled through sampling and parallelization. We perform grid search on the embedding dimension, learning rate and l2 regularization for each model, and we find that the embedding dimension 128 is the best of {4, 8, 16, 32, 64, 128} (we do not try higher dimensions considering the memory size), and the learning rate 10 −3 is generally better than {10 −4 , 2 * 10 −4 , 5 * 10 −4 , 2 * 10 −3 , 5 * 10 −3 , 10 −2 } (different models vary slightly), and we set the L2 regularization differently on different datasets: 10 −5 (C-Book), 10 −7 (Movie-1M), 10 −7 (A-Book), 10 −8 (Movie-20M). For other hyper-parameters provided by open-source softwares, we tune them carefully in the grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Case Study</head><p>To show the early summarization problem discussed in Section 2.1, as well as to understand how NI model improves other models, we conduct a case study on in this section. We randomly choose 10k users, 6k items, and 250k responses from the MovieLens-20M dataset, and randomly split training/validation/test sets at 6:2:2. We compare attention aggregation model (AAM) (Eq.(10)) and NI <ref type="figure" target="#fig_3">(Eq. (14)</ref>) solely on the user-item interaction graph. Recall the general form of graph-based recommendation models in Eq. <ref type="formula">(11)</ref>        through user-side and item-side attention networks separately, i.e., A i, j = α u,i α v, j , yet NI learns from both sides, i.e., A i, j = α i, j . Since the elements in A sum to 1, a weight matrix can be regarded as a distribution. Thus we can calculate its entropy to quantitatively measure the information it contains. We calculate the entropy of the weight matrix A of each test sample and plot the histograms of entropy in <ref type="figure">Fig. 7</ref>. The x-axis represents the entropy value, the larger value it has, the more information it contains. We can see that the weight matrices in NI model have higher entropy, i.e., more informative. Besides, the average entropy of GAT is 2.12, and 3.18 for NI. Considering the significant improvements of NI over RippleNet (a special case of AAM) in <ref type="table">Table 2</ref>, these results confirm the early summarization problem, and our NI model has the capability to learn more informative neighborhood interactions.</p><p>We also randomly select a user-item pair ("u46", "m3993") from the test set and plots the weight matrix A and interaction matrix Z. We compare AAM and NI in <ref type="figure" target="#fig_8">Fig. 8</ref>. The x-axis represents the neighbors of item "m3993", and y-axis for the neighbors of user "u46". In user-item interaction graph, users are linked to items with positive feedbacks. Thus user neighbors are items, and item neighbors are users. Grids with darker colors have larger values.</p><p>We can observe that: (i) Comparing (a) and (c), we find AAM mainly focuses on a single neighbor "m2140" of the user, while NI focuses on many more other neighbor pairs. (ii) Comparing (a) and (b), we find AAM disregards those neighbor pairs with high interactions, e.g., , ("m552", "u1744"). While in (c) and (d), we find NI preserves more neighbor pairs with high interactions. (iii) Checking in training set, we find the pairs with high interactions in our NI model, such as ("m7155", "u1744"), ("m1031", "u7521") and ("m2140", "u1744") are positive samples, which should be fully considered in prediction. Based on the above observations, we conclude AAM may lose useful information after compressing neighborhood information into single representation, while NI can preserve more useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Our work is highly related with knowledge-enhanced recommendation, and graph representation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge-enhanced Recommendation</head><p>Traditional recommender systems mostly suffer from several inherent issues such as data sparsity and cold start problems. To address the above problems, researchers usually incorporate side information. The utilization of side information mainly categorizes into 3 groups.</p><p>The first is feature-based, which regards side information as plain features and concatenates those features with user/item IDs as model input, including Matrix factorization models <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b16">16]</ref>, DNN models <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>, etc. Feature-based models highly rely on manual feature engineering to extract structural information, which is not end-to-end and less efficient.</p><p>The second way is meta path-based, which builds heterogeneous information network (HIN) on the side information. For example, PER <ref type="bibr" target="#b29">[29]</ref> and FMG <ref type="bibr" target="#b31">[31]</ref> extract meta path/meta graph-based features to represent the connection between users and items along different types of relation paths. MCRec <ref type="bibr" target="#b10">[10]</ref> instead learns context representations from meta paths to facilitate recommendation. DeepCoevolve <ref type="bibr" target="#b4">[4]</ref> further leverages user-item ineteraction network in sequential recommendation. Though these models are more intuitive, they usually require much expertise in meta-path design, making them less applicable in scenarios with complex schema.</p><p>Compared with the previous 2 ways, external knowledge graph contains much more fruitful facts and connections about items <ref type="bibr" target="#b0">[1]</ref>. For example, CKE <ref type="bibr" target="#b30">[30]</ref> proposes a general framework to jointly learn from the auxiliary knowledge graph, textual and visual information. DKN <ref type="bibr" target="#b27">[27]</ref> is later proposed to incorporate knowledge embedding and text embedding for news recommendation. More recently, RippleNet <ref type="bibr" target="#b26">[26]</ref> is proposed to simulate user preferences over the set of knowledge entities. It automatically extends user preference along links in the knowledge graph, and achieves stateof-the-art performance in knowledge graph-based recommendation. The major difference between prior work and ours is that NI focuses more on the interactions between neighbor nodes, and predict from graph structures directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Representation</head><p>Graph representation learning aims to learn latent, low-dimensional representations of graph vertices, while preserving graph topology structure, node content, and other information. In general, there are two main types of graph representation methods: unsupervised and semi-supervised methods.</p><p>Most of the unsupervised graph representation algorithms focus on preserving graph structure for learning node representations <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22]</ref>. For example, DeepWalk <ref type="bibr" target="#b18">[18]</ref> uses random walks to generate node sequences and learn node representations. Node2vec <ref type="bibr" target="#b5">[5]</ref> further exploits a biased random walk strategy to capture more flexible contextual structures. LINE <ref type="bibr" target="#b22">[22]</ref> uses first-order and second-order proximity to model a joint probability distribution and a conditional probability distribution on connected vertices.</p><p>Another type is semi-supervised models <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24]</ref>. In this type, there exist some labeled vertices for representation learning. For example, LANE <ref type="bibr" target="#b12">[12]</ref> incorporates label information into the attributed network embedding while preserving their correlations. GCN <ref type="bibr" target="#b15">[15]</ref> utilizes a localized graph convolutions for a classification task. GAT <ref type="bibr" target="#b24">[24]</ref> uses self-attention network for information propagation, which utilizes a multi-head attention mechanism to increase model capacity. GCN and GAT are popular architectures of the general graph networks, and can be naturally regarded as plug-in graph representation modules in other supervised tasks. In this work, we mainly utilize graph networks to generate structural node embeddings for KIG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we review previous graph-based recommender systems and find an early summarization problem of the existing methods. We extend user-item interactions to neighbor-neighbor interactions, and propose Neighborhood Interaction (NI) to further explore the neighborhood structures of users and items. Integrating highorder neighborhood information with Graph Neural Networks and Knowledge Graphs into NI, we obtain an end-to-end model, namely Knowledge-enhanced Neighborhood Interaction (KNI). We compare KNI with state-of-the-art models on 4 real-world datasets, and the superior results of KNI on CTR prediction and top-N recommendation demonstrate its effectiveness. We also provide a case study to quantitatively measure the early summarization problem. In the future, a promising direction is extending neighborhood interactions to higher-orders. Another direction is integrating user-side information in KIG to adapt to more general scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>A-Book combines Amazon Book 7 and Freebase. Amazon Book [8] contains over 22.5 million ratings (ranging from 1 to 5) collected from 8 million users and 2.3 million items. • Movie-20M combines MovieLens-20M and Freebase. Movie-Lens-20M contains ratings (ranging from 1 to 5) collected from the MovieLens website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Evaluation stabilizes after sufficient evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F1@KFigure 3 :</head><label>3</label><figDesc>, i.e.,ŷ = σ (A ⊙ Z). AAM learns the weight matrix A Top-N recommendation results for C-Book.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F1@KFigure 4 :</head><label>4</label><figDesc>Top-N recommendation results for Movie-1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F1@KFigure 5 :</head><label>5</label><figDesc>Top-N recommendation results for A-Book.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F1@KFigure 6 :Figure 7 :</head><label>67</label><figDesc>Top-N recommendation results for Movie-20M. DLP-KDD'19, August 5, 2019, Anchorage, AK, USA Entropy histogram. Note: The x-axis represents the entropy of attention distribution.(a) A i, j of AAM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Case study of test case (u46, m3993). Note: In (a)-(d), the y-axis represents neighbors of user 46, and the x-axis represents neighbors of item 3993. AAM: attention aggregation model (Eq. (10)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics for the expanded datasets. Note: "entities" contain both items and non-item entities.</figDesc><table><row><cell>Datasets</cell><cell cols="4">C-Book Movie-1M A-Book Movie-20M</cell></row><row><cell># users</cell><cell>17,860</cell><cell>6,036</cell><cell>78,809</cell><cell>59,296</cell></row><row><cell># items</cell><cell>14,967</cell><cell>2,445</cell><cell>32,389</cell><cell>11,895</cell></row><row><cell cols="2"># interactions 139,746</cell><cell>753,772</cell><cell>1,181,684</cell><cell>9,104,038</cell></row><row><cell># entities</cell><cell>77,881</cell><cell>182,011</cell><cell>265,478</cell><cell>64,067</cell></row><row><cell># relations</cell><cell>10</cell><cell>12</cell><cell>22</cell><cell>38</cell></row><row><cell># triples</cell><cell>71,628</cell><cell>923,718</cell><cell>1,551,554</cell><cell>1,195,391</cell></row><row><cell cols="5">a node embedding, which is time consuming and not tractable for a</cell></row><row><cell cols="5">very large graph. NS proposes to sample a fixed number (e.g., K) of</cell></row><row><cell cols="5">neighbors for each node in forward computation. Combining GCN</cell></row><row><cell cols="2">and NS as an examplẽ</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Training time of RippleNet and KNI.</figDesc><table><row><cell>Models</cell><cell cols="4">C-Book Movie-1M A-Book Movie-20M</cell></row><row><cell cols="2">RippleNet 17.75s</cell><cell>66.85s</cell><cell>120.38s</cell><cell>937.92s</cell></row><row><cell>KNI</cell><cell>2.05s</cell><cell>11.58s</cell><cell>21.52s</cell><cell>166.72s</cell></row><row><cell cols="5">does not always produce better results than NS, besides, random</cell></row><row><cell cols="5">walk-based sampling requires more time. Thus we only apply NS</cell></row><row><cell cols="5">on the other models. The number of neighbors to be sampled is</cell></row><row><cell cols="5">tuned from {4, 8, 16, 32, 64, 128} (128 is not applicable on A-Book and</cell></row><row><cell cols="5">Movie-20M due to memory constraints), and we find 4 (C-Book),</cell></row><row><cell cols="5">32 (Movie-1M), 8 (A-Book), and 32 (Movie-20M) perform slightly</cell></row><row><cell cols="5">better. We also test the training speed of RippleNet and KNI. When</cell></row><row><cell cols="5">fixing the maximum neighbor size to be 32, KNI with NS could</cell></row><row><cell cols="5">be 5.6-8.6 times faster than RippleNet to train one iteration in the</cell></row><row><cell cols="3">same GPU environment, shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In general, the bias term is canceled because of softmax. In<ref type="bibr" target="#b24">[24]</ref>, the linear projection is followed by a LeakyReLU layer, therefore, the bias term is kept.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">GAT<ref type="bibr" target="#b24">[24]</ref> utilizes LeakyReLU transformation before softmax in its original form.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Atomu2014/KNI 4 http://www2.informatik.uni-freiburg.de/ cziegler/BX/ 5 https://searchengineland.com/library/bing/bing-satori 6 https://grouplens.org/datasets/movielens/ 7 http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/librahu/MCRec 9 https://github.com/hwwang55/DKN 10 https://github.com/hwwang55/RippleNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the support of National Natural Science Foundation of China (61632017, 61702327, 61772333), Shanghai Sailing Program (17YF1428200). Jian Tang is supported by the Natural Sciences and Engineering Research Council of Canada, as well as the Canada CIFAR AI Chair Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">FastGCN: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03675</idno>
		<title level="m">Deep coevolutionary network: Embedding user and item features for recommendation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenkui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">NAIS: Neural Attentive Item Similarity Model for Recommendation. TKDE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging meta-path based context for top-n recommendation with a neural co-attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Label informed attributed network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Field-aware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<editor>RecSys. ACM</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fism: factored item similarity models for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00311</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1050</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08267</idno>
		<title level="m">Graphgan: Graph representation learning with generative adversarial nets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08284</idno>
		<title level="m">DKN: Deep Knowledge-Aware Network for News Recommendation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01973</idno>
		<title level="m">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Metagraph based recommendation fusion over heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaole</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11141</idno>
		<title level="m">KB4Rec: A Dataset for Linking Knowledge Bases with Recommender Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
