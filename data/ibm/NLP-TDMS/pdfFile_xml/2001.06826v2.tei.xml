<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">BIIT Lab</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">BIIT Lab</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
							<email>jcguo@tju.edu.cnccloy@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">BIIT Lab</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our Zero-DCE to face detection in the dark are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many photos are often captured under suboptimal lighting conditions due to inevitable environmental and/or technical constraints. These include inadequate and unbalanced lighting conditions in the environment, incorrect placement of objects against extreme back light, and under-exposure during image capturing. Such low-light photos suffer from compromised aesthetic quality and unsatisfactory transmission of information. The former affects viewers' experience while the latter leads to wrong message being communicated, such as inaccurate object/face recognition. <ref type="bibr">*</ref> The first two authors contribute equally to this work. † Jichang Guo (jcguo@tju.edu.cn) is the corresponding author.</p><p>(a) Raw (b) Zero-DCE (c) Wang et al. <ref type="bibr" target="#b27">[28]</ref> (d) EnlightenGAN <ref type="bibr" target="#b11">[12]</ref> Figure 1: Visual comparisons on a typical low-light image. The proposed Zero-DCE achieves visually pleasing result in terms of brightness, color, contrast, and naturalness, while existing methods either fail to cope with the extreme back light or generate color artifacts. In contrast to other deep learning-based methods, our approach is trained without any reference image.</p><p>In this study, we present a novel deep learning-based method, Zero-Reference Deep Curve Estimation (Zero-DCE), for low-light image enhancement. It can cope with diverse lighting conditions including nonuniform and poor lighting cases. Instead of performing image-to-image mapping, we reformulate the task as an image-specific curve estimation problem. In particular, the proposed method takes a low-light image as input and produces high-order curves as its output. These curves are then used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. The curve estimation is carefully formulated so that it maintains the range of the enhanced image and preserves the contrast of neighboring pixels. Importantly, it is differentiable, and thus we can learn the adjustable parameters of the curves through a deep convolutional neural network. The proposed network is lightweight and it can be iteratively applied to approximate higher-order curves for more robust and accurate dynamic range adjustment.</p><p>A unique advantage of our deep learning-based method is zero-reference, i.e., it does not require any paired or even unpaired data in the training process as in existing CNN-based <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref> and GAN-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>. This is made possible through a set of specially designed nonreference loss functions including spatial consistency loss, exposure control loss, color constancy loss, and illumination smoothness loss, all of which take into consideration multi-factor of light enhancement. We show that even with zero-reference training, Zero-DCE can still perform competitively against other methods that require paired or unpaired data for training. An example of enhancing a lowlight image comprising nonuniform illumination is shown in <ref type="figure">Fig. 1</ref>. Comparing to state-of-the-art methods, Zero-DCE brightens up the image while preserving the inherent color and details. In contrast, both CNN-based method <ref type="bibr" target="#b27">[28]</ref> and GAN-based EnlightenGAN <ref type="bibr" target="#b11">[12]</ref> yield under-(the face) and over-(the cabinet) enhancement.</p><p>Our contributions are summarized as follows. 1) We propose the first low-light enhancement network that is independent of paired and unpaired training data, thus avoiding the risk of overfitting. As a result, our method generalizes well to various lighting conditions. 2) We design an image-specific curve that is able to approximate pixel-wise and higher-order curves by iteratively applying itself. Such image-specific curve can effectively perform mapping within a wide dynamic range.</p><p>3) We show the potential of training a deep image enhancement model in the absence of reference images through task-specific non-reference loss functions that indirectly evaluate enhancement quality.</p><p>Our Zero-DCE method supersedes state-of-the-art performance both in qualitative and quantitative metrics. More importantly, it is capable of improving high-level visual tasks, e.g., face detection, without inflicting high computational burden. It is capable of processing images in realtime (about 500 FPS for images of size 640×480×3 on GPU) and takes only 30 minutes for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conventional Methods. HE-based methods perform light enhancement through expanding the dynamic range of an image. Histogram distribution of images is adjusted at both global <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> and local levels <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. There are also various methods adopting the Retinex theory <ref type="bibr" target="#b12">[13]</ref> that typically decomposes an image into reflectance and illumination. The reflectance component is commonly assumed to be consistent under any lighting conditions; thus, light enhancement is formulated as an illumination estimation problem. Building on the Retinex theory, several methods have been proposed. Wang et al. <ref type="bibr" target="#b28">[29]</ref> designed a naturalness-and information-preserving method when handling images of nonuniform illumination; Fu et al. <ref type="bibr" target="#b7">[8]</ref> proposed a weighted variation model to simultaneously estimate the reflectance and illumination of an input image; Guo et al. <ref type="bibr" target="#b8">[9]</ref> first estimated a coarse illumination map by searching the maximum intensity of each pixel in RGB channels, then refining the coarse illumination map by a structure prior; Li et al. <ref type="bibr" target="#b18">[19]</ref> proposed a new Retinex model that takes noise into consideration. The illumination map was estimated through solving an optimization problem. Contrary to the conventional methods that fortuitously change the distribution of image histogram or that rely on potentially inaccurate physical models, the proposed Zero-DCE method produces an enhanced result through imagespecific curve mapping. Such a strategy enables light enhancement on images without creating unrealistic artifacts. Yuan and Sun <ref type="bibr" target="#b35">[36]</ref> proposed an automatic exposure correction method, where the S-shaped curve for a given image is estimated by a global optimization algorithm and each segmented region is pushed to its optimal zone by curve mapping. Different from <ref type="bibr" target="#b35">[36]</ref>, our Zero-DCE is a purely datadriven method and takes multiple light enhancement factors into consideration in the design of the non-reference loss functions, and thus enjoys better robustness, wider image dynamic range adjustment, and lower computational burden.</p><p>Data-Driven Methods. Data-driven methods are largely categorized into two branches, namely CNN-based and GAN-based methods. Most CNN-based solutions rely on paired data for supervised training, therefore they are resource-intensive. Often time, the paired data are exhaustively collected through automatic light degradation, changing the settings of cameras during data capturing, or synthesizing data via image retouching. For example, the LL-Net <ref type="bibr" target="#b19">[20]</ref> was trained on data simulated on random Gamma correction; the LOL dataset <ref type="bibr" target="#b31">[32]</ref> of paired low/normal light images was collected through altering the exposure time and ISO during image acquisition; the MIT-Adobe FiveK dataset <ref type="bibr" target="#b2">[3]</ref> comprises 5,000 raw images, each of which has five retouched images produced by trained experts.</p><p>Recently, Wang et al. <ref type="bibr" target="#b27">[28]</ref> proposed an underexposed photo enhancement network by estimating the illumination map. This network was trained on paired data that were retouched by three experts. Understandably, light enhancement solutions based on paired data are impractical in many ways, considering the high cost involved in collecting sufficient paired data as well as the inclusion of factitious and unrealistic data in training the deep models. Such constraints are reflected in the poor generalization capability of CNN-based methods. Artifacts and color casts are com-   monly generated, when these methods are presented with real-world images of various light intensities.</p><formula xml:id="formula_0">v E 0 / y 9 G n T b G N B 6 K s i d P 4 C A = " &gt; A A A C F n i c b Z D L S g M x F I Y z X m u 9 V V 2 6 C R a h Q l t m q q A g Q l W K L r q o Y i / Q q U M m T d v Q T G Z I M k I Z 5 i n c + C p u X C j i V t z 5 N q a X h b b + E P j 4 z z n k n N 8 N G J X K N L + N u f m F x a X l x E p y d W 1 9 Y z O 1 t V 2 T f i g w q W K f + a L h I k k Y 5 a S q q G K k E Q i C P J</formula><formula xml:id="formula_1">g R h Q f W t J h 4 i g b D S q R V 0 C P b s y / O k W a 3 Y x 5 X q z U m p Z m V x 5 G E f D u A I b D i F G l x D H R q A 4 R G e 4 R X e j C f j x X g 3 P q a t O S O b 2 Y U / M D 5 / A O c 0 l w w = &lt; / l a t e x i t &gt; A {R,G,B} 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 3 t V J c a Y u 9 K t 4 W O x V x P u g e l 5 9 8 8 = " &gt; A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x M 1 g E F 6 U k V d B l 1 Y U u q 9 g H N D F M p t N 2 6 G Q S Z i Z C C c G N v + L G h S J u / Q p 3 / o 2 T N g t t P X D h c M 6 9 3 H u P H z E q l W V 9 G 4 W F x a X l l e J q a W 1 9 Y 3 P L 3 N 5 p y T A W m D R x y E L R 8 Z E k j H L S V F Q x 0 o k E Q Y H P S N s f X W Z + + 4 E I S U N + p 8 Y R c Q M 0 4 L R P M V J a 8 s w 9 J 0 B q i B F L z t P 7 x E l u K 1 e V C y d N P d s z y 1 b V m g D O E z s n Z Z C j 4 Z l f T i / E c U C 4 w g x J 2 b W t S L k J E o p i R t K S E 0 s S I T x C A 9 L V l K O A S D e Z v J D C Q 6 3 0 Y D 8 U u r i C E / X 3 R I I C K c e B r z u z g + W s l 4 n / e d 1 Y 9 c / c h P I o V o T j 6 a J + z K A K Y Z Y H 7 F F B s G J j T R A W V N 8 K 8 R A J h J V O r a R D s G d f n i e t W t U + r t Z u T s p 1 K 4 + j C P b B A T g C N j g F d X A N G q A J M H g E z + A V v B l P x o v x b n x M W w t G P r M L / s D 4 / A G K w J b P &lt; / l a t e x i t &gt; A {R,G,B} 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j R K N y U s q / n S H w E A P m F / 2 w C S T 1 4 c = " &gt; A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x M 1 g E F 6 U k V d B l 1 Y U u q 9 g H N D F M p t N 2 6 G Q S Z i Z C C c G N v + L G h S J u / Q p 3 / o 2 T N g t t P X D h c M 6 9 3 H u P H z E q l W V 9 G 4 W F x a X l l e J q a W 1 9 Y 3 P L 3 N 5 p y T A W m D R x y E L R 8 Z E k j H L S V F Q x 0 o k E Q Y H P S N s f X W Z + + 4 E I S U N + p 8 Y R c Q M 0 4 L R P M V J a 8 s w 9 J 0 B q i B F L z t P 7 x E l u K 1 e V C y d N v Z p n l q 2 q N Q G c J 3 Z O y i B H w z O / n F 6 I 4 4 B w h R m S s m t b k X I T J B T F j K Q l J 5 Y k Q n i E B q S r K U c B k W 4 y e S G F h 1 r p w X 4 o d H E F J + r v i Q Q F U o 4 D X 3 d m B 8 t Z L x P / 8 7 q x 6 p + 5 C e V R r A j H 0 0 X 9 m E E V w i w P 2 K O C Y M X G m i A s q L 4 V 4 i E S C C u d W k m H Y M + + P E 9 a t a p 9 X K 3 d n J T r V h 5 H E e y D A 3 A E b H A K 6 u A a N E A T Y P A I n s E r e D O e j B f j 3 f i Y t h a M f G Y X / I H x + Q O M R J b Q &lt; / l a t e x i t &gt; … … I &lt; l a</formula><p>Unsupervised GAN-based methods have the advantage of eliminating paired data for training. EnlightenGAN <ref type="bibr" target="#b11">[12]</ref>, an unsupervised GAN-based and pioneer method that learns to enhance low-light images using unpaired low/normal light data. The network was trained by taking into account elaborately designed discriminators and loss functions. However, unsupervised GAN-based solutions usually require careful selection of unpaired training data.</p><p>The proposed Zero-DCE is superior to existing datadriven methods in three aspects. First, it explores a new learning strategy, i.e., one that requires zero reference, hence eliminating the need for paired and unpaired data. Second, the network is trained by taking into account carefully defined non-reference loss functions. This strategy allows output image quality to be implicitly evaluated, the results of which would be reiterated for network learning. Third, our method is highly efficient and cost-effective. These advantages benefit from our zero-reference learning framework, lightweight network structure, and effective non-reference loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We present the framework of Zero-DCE in <ref type="figure" target="#fig_2">Fig. 2</ref>. A Deep Curve Estimation Network (DCE-Net) is devised to estimate a set of best-fitting Light-Enhancement curves (LE-curves) given an input image. The framework then maps all pixels of the input's RGB channels by applying the curves iteratively for obtaining the final enhanced image. We next detail the key components in Zero-DCE, namely LE-curve, DCE-Net, and non-reference loss functions in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Light-Enhancement Curve (LE-curve)</head><p>Inspired by the curves adjustment used in photo editing software, we attempt to design a kind of curve that can map a low-light image to its enhanced version automatically, where the self-adaptive curve parameters are solely dependent on the input image. There are three objectives in the design of such a curve: 1) each pixel value of the enhanced image should be in the normalized range of [0,1] to avoid information loss induced by overflow truncation; 2) this curve should be monotonous to preserve the differences (contrast) of neighboring pixels; and 3) the form of this curve should be as simple as possible and differentiable in the process of gradient backpropagation.</p><p>To achieve these three objectives, we design a quadratic curve, which can be expressed as:</p><formula xml:id="formula_2">LE(I(x); α) = I(x) + αI(x)(1 − I(x)),<label>(1)</label></formula><p>where x denotes pixel coordinates, LE(I(x); α) is the enhanced version of the given input I(x), α ∈ [−1, 1] is the trainable curve parameter, which adjusts the magnitude of LE-curve and also controls the exposure level. Each pixel is normalized to [0, 1] and all operations are pixel-wise. We separately apply the LE-curve to three RGB channels instead of solely on the illumination channel. The threechannel adjustment can better preserve the inherent color and reduce the risk of over-saturation. We report more details in the supplementary material. An illustration of LE-curves with different adjustment parameters α is shown in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>. It is clear that the LEcurve complies with the three aforementioned objectives. In addition, the LE-curve enables us to increase or decrease the dynamic range of an input image. This capability is conducive to not only enhancing low-light regions but also removing over-exposure artifacts.</p><p>Higher-Order Curve. The LE-curve defined in Eq. (1) can be applied iteratively to enable more versatile adjustment to cope with challenging low-light conditions. Specifically,</p><formula xml:id="formula_3">LE n (x) = LE n−1 (x) + α n LE n−1 (x)(1 − LE n−1 (x)),<label>(2)</label></formula><p>where n is the number of iteration, which controls the curvature. In this paper, we set the value of n to 8, which can deal with most cases satisfactory. Eq. (2) can be degraded to Eq. (1) when n is equal to 1. <ref type="figure" target="#fig_2">Figure 2(c)</ref> provides an example showing high-order curves with different α and n, which have more powerful adjustment capability (i.e., greater curvature) than the curves in <ref type="figure" target="#fig_2">Figure 2(b)</ref>. Pixel-Wise Curve. A higher-order curve can adjust an image within a wider dynamic range. Nonetheless, it is still a global adjustment since α is used for all pixels. A global mapping tends to over-/under-enhance local regions. To address this problem, we formulate α as a pixel-wise parameter, i.e., each pixel of the given input image has a corresponding curve with the best-fitting α to adjust its dynamic range. Hence, Eq. (2) can be reformulated as:</p><formula xml:id="formula_4">LE n (x) = LE n−1 (x)+A n (x)LE n−1 (x)(1−LE n−1 (x)),<label>(3)</label></formula><p>where A is a parameter map with the same size as the given image. Here, we assume that pixels in a local region have the same intensity (also the same adjustment curves), and thus the neighboring pixels in the output result still preserve the monotonous relations. In this way, the pixel-wise higher-order curves also comply with three objectives.</p><p>We present an example of the estimated curve parameter maps of three channels in <ref type="figure" target="#fig_3">Fig. 3</ref>. As shown, the bestfitting parameter maps of different channels have similar adjustment tendency but different values, indicating the relevance and difference among the three channels of a lowlight image. The curve parameter map accurately indicates the brightness of different regions (e.g., the two glitters on the wall). With the fitting maps, the enhanced version image can be directly obtained by pixel-wise curve mapping. As shown in <ref type="figure" target="#fig_3">Fig. 3(e)</ref>, the enhanced version reveals the content in dark regions and preserves the bright regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DCE-Net</head><p>To learn the mapping between an input image and its best-fitting curve parameter maps, we propose a Deep Curve Estimation Network (DCE-Net). The input to the DCE-Net is a low-light image while the outputs are a set of pixel-wise curve parameter maps for corresponding higherorder curves. We employ a plain CNN of seven convolutional layers with symmetrical concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1 followed by the ReLU activation function. We discard the down-sampling and batch normalization layers that break the relations of neighboring pixels. The last convolutional layer is followed by the Tanh activation function, which produces 24 parameter maps for 8 iterations (n = 8), where each iteration requires three curve parameter maps for the three channels. The detailed architecture of DCE-Net is provided in the supplementary material. It is noteworthy that DCE-Net only has 79,416 trainable parameters and 5.21G Flops for an input image of size 256×256×3. It is therefore lightweight and can be used in computational resource-limited devices, such as mobile platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Non-Reference Loss Functions</head><p>To enable zero-reference learning in DCE-Net, we propose a set of differentiable non-reference losses that allow us to evaluate the quality of enhanced images. The following four types of losses are adopted to train our DCE-Net. Spatial Consistency Loss. The spatial consistency loss L spa encourages spatial coherence of the enhanced image through preserving the difference of neighboring regions between the input image and its enhanced version:</p><formula xml:id="formula_5">L spa = 1 K K i=1 j∈Ω(i) (|(Y i − Y j )| − |(I i − I j )|) 2 , (4)</formula><p>where K is the number of local region, and Ω(i) is the four neighboring regions (top, down, left, right) centered at the region i. We denote Y and I as the average intensity value of the local region in the enhanced version and input image, respectively. We empirically set the size of the local region to 4×4. This loss is stable given other region sizes. Exposure Control Loss. To restrain under-/over-exposed regions, we design an exposure control loss L exp to control the exposure level. The exposure control loss measures the distance between the average intensity value of a local region to the well-exposedness level E. We follow existing practices <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> to set E as the gray level in the RGB color space. We set E to 0.6 in our experiments although we do not find much performance difference by setting E within [0.4, 0.7]. The loss L exp can be expressed as:</p><formula xml:id="formula_6">L exp = 1 M M k=1 |Y k − E|,<label>(5)</label></formula><p>where M represents the number of nonoverlapping local regions of size 16×16, Y is the average intensity value of a local region in the enhanced image. Color Constancy Loss. Following Gray-World color constancy hypothesis <ref type="bibr" target="#b1">[2]</ref> that color in each sensor channel averages to gray over the entire image, we design a color constancy loss to correct the potential color deviations in the enhanced image and also build the relations among the three adjusted channels. The color constancy loss L col can be expressed as:  where J p denotes the average intensity value of p channel in the enhanced image, (p,q) represents a pair of channels. Illumination Smoothness Loss. To preserve the monotonicity relations between neighboring pixels, we add an illumination smoothness loss to each curve parameter map A. The illumination smoothness loss L tv A is defined as:</p><formula xml:id="formula_7">L col = ∀(p,q)∈ε (J p −J q ) 2 , ε = {(R, G), (R, B), (G, B)},<label>(6)</label></formula><formula xml:id="formula_8">(a) Input (b) A R n (c) A G n (d) A B n (e) Result</formula><formula xml:id="formula_9">L tv A = 1 N N n=1 c∈ξ (|∇ x A c n | + ∇ y A c n |) 2 , ξ = {R, G, B},<label>(7)</label></formula><p>where N is the number of iteration, ∇ x and ∇ y represent the horizontal and vertical gradient operations, respectively. Total Loss. The total loss can be expressed as:</p><formula xml:id="formula_10">L total = L spa + L exp + W col L col + W tv A L tv A , (8)</formula><p>where W col and W tv A are the weights of the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation Details. CNN-based models usually use self-captured paired data for network training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> while GAN-based models elaborately select unpaired data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>. To bring the capability of wide dynamic range adjustment into full play, we incorporate both low-light and over-exposed images into our training set. To this end, we employ 360 multi-exposure sequences from the Part1 of SICE dataset <ref type="bibr" target="#b3">[4]</ref> to train the proposed DCE-Net. The dataset is also used as a part of the training data in EnlightenGAN <ref type="bibr" target="#b11">[12]</ref>. We randomly split 3,022 images of different exposure levels in the Part1 subset <ref type="bibr" target="#b3">[4]</ref> into two parts (2,422 images for training and the rest for validation). We resize the training images to the size of 512×512.</p><p>We implement our framework with PyTorch on an NVIDIA 2080Ti GPU. A batch size of 8 is applied. The filter weights of each layer are initialized with standard zero mean and 0.02 standard deviation Gaussian function. Bias is initialized as a constant. We use ADAM optimizer with default parameters and fixed learning rate 1e −4 for our network optimization. The weights W col and W tv A are set to 0.5, and 20, respectively, to balance the scale of losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We perform several ablation studies to demonstrate the effectiveness of each component of Zero-DCE as follows. More qualitative and quantitative comparisons can be found in the supplementary material. Contribution of Each Loss. We present the results of Zero-DCE trained by various combinations of losses in <ref type="figure" target="#fig_4">Fig. 4</ref>. The result without spatial consistency loss L spa has relatively lower contrast (e.g., the cloud regions) than the full result. This shows the importance of L spa in preserving the difference of neighboring regions between the input and the enhanced image. Removing the exposure control loss L exp fails to recover the low-light region. Severe color casts emerge when the color constancy loss L col is discarded. This variant ignores the relations among three channels when curve mapping is applied. Finally, removing the illumination smoothness loss L tv A hampers the correlations between neighboring regions leading to obvious artifacts. Effect of Parameter Settings. We evaluate the effect of parameters in Zero-DCE, consisting of the depth and width of the DCE-Net and the number of iterations. A visual example is presented in <ref type="figure">Fig. 5</ref>. In <ref type="figure">Fig. 5(b)</ref>, with just three convolutional layers, Zero-DCE 3−32−8 can already produce satisfactory results, suggesting the effectiveness of zero-reference learning. The Zero-DCE 7−32−8 and Zero-DCE 7−32−16 produce most visually pleasing results with natural exposure and proper contrast. By reducing the number of iterations to 1, an obvious decrease in performance is observed on Zero-DCE 7−32−1 as shown in <ref type="figure">Fig. 5(d)</ref>. This is because the curve with only single iteration has limited adjustment capability. This suggests the need for higherorder curves in our method. We choose Zero-DCE 7−32−8 as the final model based given its good trade-off between efficiency and restoration performance.</p><p>Impact of Training Data. To test the impact of training data, we retrain the Zero-DCE on different datasets: 1) only 900 low-light images out of 2,422 images in the original training set (Zero-DCE Low ), 2) 9,000 unlabeled low-light images provided in the DARK FACE dataset <ref type="bibr" target="#b36">[37]</ref> (Zero-DCE LargeL ), and 3) 4800 multi-exposure images from the data augmented combination of Part1 and Part2 subsets in the SICE dataset <ref type="bibr" target="#b3">[4]</ref> (Zero-DCE LargeLH ). As shown in <ref type="figure">Fig. 6</ref>(c) and (d), after removing the over-exposed training data, Zero-DCE tends to over-enhance the well-lit regions (e.g., the face), in spite of using more low-light images, (i.e., Zero-DCE LargeL ). Such results indicate the rationality and necessity of the usage of multi-exposure training data in the training process of our network. In addition, the Zero-DCE can better recover the dark regions when more multiexposure training data are used (i.e., Zero-DCE LargeLH ), as shown in <ref type="figure">Fig. 6(e)</ref>. For a fair comparison with other deep learning-based methods, we use a comparable amount of training data with them although more training data can bring better visual performance to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark Evaluations</head><p>We compare Zero-DCE with several state-of-the-art methods: three conventional methods (SRIE <ref type="bibr" target="#b7">[8]</ref>, LIME <ref type="bibr" target="#b8">[9]</ref>, Li et al. <ref type="bibr" target="#b18">[19]</ref>), two CNN-based methods (RetinexNet <ref type="bibr" target="#b31">[32]</ref>, Wang et al. <ref type="bibr" target="#b27">[28]</ref> ), and one GAN-based method (Enlighten-GAN <ref type="bibr" target="#b11">[12]</ref>). The results are reproduced by using publicly available source codes with recommended parameters.</p><p>We perform qualitative and quantitative experiments on standard image sets from previous works including NPE <ref type="bibr" target="#b28">[29]</ref> (84 images), LIME <ref type="bibr" target="#b8">[9]</ref> (10 images), MEF <ref type="bibr" target="#b21">[22]</ref> (17 images), DICM <ref type="bibr" target="#b13">[14]</ref> (64 images), and VV ‡ (24 images). Besides, we quantitatively validate our method on the Part2 subset of SICE dataset <ref type="bibr" target="#b3">[4]</ref>, which consists of 229 multi-exposure sequences and the corresponding reference image for each multi-exposure sequence. For a fair comparison, we only use the low-light images of Part2 subset <ref type="bibr" target="#b3">[4]</ref> for testing, since baseline methods cannot handle over-exposed images well. Specifically, we choose the first three (resp. four) low-light images if there are seven (resp. nine) images in a multi-exposure sequence and resize all images to a size of 1200×900×3. Finally, we obtain 767 paired low/normal light images. We discard the low/normal light image dataset mentioned in <ref type="bibr" target="#b36">[37]</ref>, because the training datasets of RetinexNet <ref type="bibr" target="#b31">[32]</ref> and EnlightenGAN <ref type="bibr" target="#b11">[12]</ref> consist of some images from this dataset. Note that the latest paired training and testing dataset constructed in <ref type="bibr" target="#b27">[28]</ref> are not publicly available. We did not use the MIT-Adobe FiveK dataset <ref type="bibr" target="#b2">[3]</ref> as it is not primarily designed for underexposed photos enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Visual and Perceptual Comparisons</head><p>We present the visual comparisons on typical low-light images in <ref type="figure" target="#fig_6">Fig. 7</ref>. For challenging back-lit regions (e.g., the face in <ref type="figure" target="#fig_6">Fig. 7(a)</ref>), Zero-DCE yields natural exposure and clear details while SRIE <ref type="bibr" target="#b7">[8]</ref>, LIME <ref type="bibr" target="#b8">[9]</ref>, Wang et al. <ref type="bibr" target="#b27">[28]</ref>, and EnlightenGAN <ref type="bibr" target="#b11">[12]</ref> cannot recover the face clearly. RetinexNet <ref type="bibr" target="#b31">[32]</ref> produces over-exposed artifacts. In the second example featuring an indoor scene, our method enhances dark regions and preserves color of the input image simultaneously. The result is visually pleasing without obvious noise and color casts. In contrast, Li et al. <ref type="bibr" target="#b18">[19]</ref> oversmoothes the details while other baselines amplify noise and even produce color deviation (e.g., the color of wall).</p><p>We perform a user study to quantify the subjective visual quality of various methods. We process low-light images from the image sets (NPE, LIME, MEF, DICM, VV) by different methods. For each enhanced result, we display it on a screen and provide the input image as a reference. A total of 15 human subjects are invited to independently score the visual quality of the enhanced image. These subjects ‡ https://sites.google.com/site/vonikakis/ datasets (a) Input <ref type="figure">Figure 6</ref>: Ablation study on the impact of training data.</p><formula xml:id="formula_11">(b) Zero-DCE (c) Zero-DCE Low (d) Zero-DCE LargeL (e) Zero-DCE LargeLH</formula><p>(a) Inputs (b) SRIE <ref type="bibr" target="#b7">[8]</ref> (c) LIME <ref type="bibr" target="#b8">[9]</ref> (d) Li et al. <ref type="bibr" target="#b18">[19]</ref> (e) RetinexNet <ref type="bibr" target="#b31">[32]</ref> (f) Wang et al. <ref type="bibr" target="#b27">[28]</ref> (g) EnlightenGAN <ref type="bibr" target="#b11">[12]</ref> (h) Zero-DCE are trained by observing the results from 1) whether the results contain over-/under-exposed artifacts or over-/underenhanced regions; 2) whether the results introduce color deviation; and 3) whether the results have unnatural texture and obvious noise. The scores of visual quality range from 1 to 5 (worst to best quality). The average subjective scores for each image set are reported in <ref type="table" target="#tab_0">Table 1</ref>. As summarized in <ref type="table" target="#tab_0">Table 1</ref>, Zero-DCE achieves the highest average User Study (US) score for a total of 202 testing images from the above-mentioned image sets. For the MEF, DICM, and VV sets, our results are most favored by the subjects. In addition to the US score, we employ a non-reference perceptual index (PI) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> to evaluate the perceptual quality. The PI metric is originally used to measure perceptual quality in image super-resolution. It has also been used to assess the performance of other image restoration tasks, such as image dehazing <ref type="bibr" target="#b25">[26]</ref>. A lower PI value indicates better perceptual quality. The PI values are reported in <ref type="table" target="#tab_0">Table 1</ref> too. Similar to the user study, the proposed Zero-DCE is superior to other competing methods in terms of the average PI values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Quantitative Comparisons</head><p>For full-reference image quality assessment, we employ the Peak Signal-to-Noise Ratio (PSNR,dB), Structural Similarity (SSIM) <ref type="bibr" target="#b30">[31]</ref>, and Mean Absolute Error (MAE) metrics to quantitatively compare the performance of different methods on the Part2 subset <ref type="bibr" target="#b3">[4]</ref>. In <ref type="table" target="#tab_1">Table 2</ref>, the proposed Zero-DCE achieves the best values under all cases, despite that it does not use any paired or unpaired training data. Zero-DCE is also computationally efficient, benefited from the simple curve mapping form and lightweight network structure. <ref type="table">Table 3</ref> shows the runtime § of different methods averaged on 32 images of size 1200×900×3. For conventional methods, only the codes of CPU version are available.   <ref type="table">Table 3</ref>: Runtime (RT) comparisons (in second). The best result is in red whereas the second best one is in blue.</p><p>Method RT Platform SRIE <ref type="bibr" target="#b7">[8]</ref> 12.1865 MATLAB (CPU) LIME <ref type="bibr" target="#b8">[9]</ref> 0.4914 MATLAB (CPU) Li et al. <ref type="bibr" target="#b18">[19]</ref> 90.7859 MATLAB (CPU) RetinexNet <ref type="bibr" target="#b31">[32]</ref> 0.1200 TensorFlow (GPU) Wang et al. <ref type="bibr" target="#b27">[28]</ref> 0.0210 TensorFlow (GPU) EnlightenGAN <ref type="bibr" target="#b11">[12]</ref> 0.0078 PyTorch (GPU) Zero-DCE 0.0025 PyTorch (GPU)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Face Detection in the Dark</head><p>We investigate the performance of low-light image enhancement methods on the face detection task under lowlight conditions. Specifically, we use the latest DARK FACE dataset <ref type="bibr" target="#b36">[37]</ref> that composes of 10,000 images taken in the dark. Since the bounding boxes of test set are not publicly available, we perform evaluation on the training and validation sets, which consists of 6,000 images. A state-of-the-art deep face detector, Dual Shot Face Detector (DSFD) <ref type="bibr" target="#b17">[18]</ref>, trained on WIDER FACE dataset <ref type="bibr" target="#b33">[34]</ref>, is used as the baseline model. We feed the results of different low-light image enhancement methods to the DSFD <ref type="bibr" target="#b17">[18]</ref> and depict the precision-recall (P-R) curves in <ref type="figure">Fig. 8</ref>. Besides, we also compare the average precision (AP) by using the evaluation tool ¶ provided in DARK FACE dataset <ref type="bibr" target="#b36">[37]</ref>. ¶ https://github.com/Ir1d/DARKFACE_eval_tools</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Detection</head><p>Enhanced Detection <ref type="figure">Figure 8</ref>: The performance of face detection in the dark. P-R curves, the AP, and two examples of face detection before and after enhanced by our Zero-DCE.</p><p>As shown in <ref type="figure">Fig. 8</ref>, after image enhancement, the precision of DSFD <ref type="bibr" target="#b17">[18]</ref> increases considerably compared to that using raw images without enhancement. Among different methods, RetinexNet <ref type="bibr" target="#b31">[32]</ref> and Zero-DCE perform the best. Both methods are comparable but Zero-DCE performs better in the high recall area. Observing the examples, our Zero-DCE lightens up the faces in the extremely dark regions and preserves the well-lit regions, thus improves the performance of face detector in the dark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a deep network for low-light image enhancement. It can be trained end-to-end with zero reference images. This is achieved by formulating the low-light image enhancement task as an image-specific curve estimation problem, and devising a set of differentiable nonreference losses. Experiments demonstrate the superiority of our method against existing light enhancement methods. In future work, we will try to introduce semantic information to solve hard cases and consider the effects of noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 )LE 2 = 2 )</head><label>122</label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + H 0 y O U L S C c s O m s N d N x 0 s 1 3 n Z P w o = " &gt; A A A C D 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 W p U E p S B Q U R q l J U 6 K K K f U B T w 2 Q 6 b Y d O H s x M h B L y B 2 7 8 F T c u F H H r 1 p 1 / 4 6 T N Q q s H L h z O u Z d 7 7 7 F 9 R o X U 9 S 8 t N T M 7 N 7 + Q X s w s L a + s r m X X N x r C C z g m d e w x j 7 d s J A i j L q l L K h l p + Z w g x 2 a k a Q / P Y 7 9 5 T 7 i g n n s r R z 7 p O K j v 0 h 7 F S C r J y u 5 W K 5 Y B T 2 C 1 k r 8 6 N h 0 k B x i x 8 D S 6 C 8 3 w p n B R O D O j y D L 2 r G x O L + p j w L / E S E g O J K h Z 2 U + z 6 + H A I a 7 E D A n R N n R f d k L E J c W M R B k z E M R H e I j 6 p K 2 o i x w i O u H 4 n w j u K K U L e x 5 X 5 U o 4 V n 9 O h M g R Y u T Y q j O + W E x 7 s f i f 1 w 5 k 7 6 g T U t c P J H H x Z F E v Y F B 6 M A 4 H d i k n W L K R I g h z q m 6 F e I A 4 w l J F m F E h G N M v / y W N U t H Y L 5 a u D 3 J l P Y k j D b b A N s g D A x y C M r g E N V A H G D y A J / A C X r V H 7 V l 7 0 9 4 n r S k t m d k E v 6 B 9 f A N l P Z p V &lt; / l a t e x i t &gt; LE(LE 1 ; A {R,G,B} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E y G v n 8 X k H i Y R R d p 7 j V F Z L 2 7 U X J U = " &gt; A A A C E n i c b Z D L S g M x F I Y z 9 V b r r e r S T b A I L Z Q y U w U F E a p S d N F F F X u B z j h k 0 r Q N z V x I M k I Z 5 h n c + C p u X C j i 1 p U 7 3 8 a 0 n Y W 2 / h D 4 + M 8 5 5 J z f C R g V U t e / t d T C 4 t L y S n o 1 s 7 a + s b m V 3 d 5 p C j / k m D S w z 3 z e d p A g j H q k I a l k p B 1 w g l y H k Z Y z v B z X W w + E C + p 7 d 3 I U E M t F f Y / 2 K E Z S W X a 2 U K v a Z X g G a 9 W 8 I u P U d J E c Y M S i 8 / g + M q P b 4 l X x w o x j u 1 y w s z m 9 p E 8 E 5 8 F I I A c S 1 e 3 s l 9 n 1 c e g S T 2 K G h O g Y e i C t C H F J M S N x x g w F C R A e o j 7 p K P S Q S 4 Q V T U 6 K 4 Y F y u r D n c / U 8 C S f u 7 4 k I u U K M X E d 1 j j c W s 7 W x + V + t E 8 r e i R V R L w g l 8 f D 0 o 1 7 I o P T h O B / Y p Z x g y U Y K E O Z U 7 Q r x A H G E p U o x o 0 I w Z k + e h 2 a 5 Z B y W y j d H u Y q e x J E G e 2 A f 5 I E B j k E F X I M 6 a A A M H s E z e A V v 2 p P 2 o r 1 r H 9 P W l J b M 7 I I / 0 j 5 / A D 9 w m 0 0 = &lt; / l a t e x i t &gt; LE n = LE(LE n 1 ; A {R,G,B} n ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c z A b C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>e R u t u / H N b r D 0 R I 6 v M 7 N Q h I y 0 N d T j s U I 6 U t J 5 U r l x w O z 2 C 5 l N E U 8 Z w V n 9 o e U j 2 M W H Q e 3 0 d 2 d J u 9 y l 7 Y c e z w A y e V N v P m S H A W r A m k w U Q V J / V l t 3 0 c e o Q r z J C U T c s M V C t C Q l H M S J y 0 Q 0 k C h P u o S 5 o a O f K I b E W j s 2 K 4 r 5 0 2 7 P h C P 6 7 g y P 0 9 E S F P y o H n 6 s 7 h x n K 6 N j T / q z V D 1 T l p R Z Q H o S I c j z / q h A w q H w 4 z g m 0 q C F Z s o A F h Q f W u E P e Q Q F j p J J M 6 B G v 6 5 F m o F f L W Y b 5 w c 5 Q u m p M 4 E m A X 7 I E M s M A x K I J r U A F V g M E j e A a v 4 M 1 4 M l 6 M d + N j 3 D p n T G Z 2 w B 8 Z n z 8 y 6 p 2 A &lt; / l a t e x i t &gt; A {R,G,B} n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 6 y D r 0 f v w K W c u 1 z 0 Z a E T W v B M 0 2 E = " &gt; A A A C A n i c b V B N S 8 N A E J 3 U r 1 q / q p 7 E S 7 A I H k p J q q D H q g c 9 V r E f 0 M S y 2 W 7 b p Z t N 2 N 0 I J Q Q v / h U v H h T x 6 q / w 5 r 9 x 0 + a g r Q 8 G H u / N M D P P C x m V y r K + j d z C 4 t L y S n 6 1 s L a + s b l V 3 N 5 p y i A S m D R w w A L R 9 p A k j H L S U F Q x 0 g 4 F Q b 7 H S M s b X a Z + 6 4 E I S Q N + p 8 Y h c X 0 0 4 L R P M V J a 6 h b 3 H B + p I U Y s P k / u Y y e + L V + V L 5 w k 6 W q v Z F W s C c x 5 Y m e k B B n q 3 e K X 0 w t w 5 B O u M E N S d m w r V G 6 M h K K Y k a T g R J K E C I / Q g H Q 0 5 c g n 0 o 0 n L y T m o V Z 6 Z j 8 Q u r g y J + r v i R j 5 U o 5 9 T 3 e m B 8 t Z L x X / 8 z q R 6 p + 5 M e V h p A j H 0 0 X 9 i J k q M N M 8 z B 4 V B C s 2 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " e x k W Y H z 5 k A d k F b h P Q / 1 u 6 n c F Z a k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 7 2 1 Y G u h D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T x b D F Y h G r T k A 1 C i 6 x Z b g R 2 E k U 0 i g Q + B C M b 2 b + w x M q z W N 5 b y Y J + h E d S h 5 y R o 2 V m n f 9 c s W t u n O Q V e L l p A I 5 G v 3 y V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 1 E s 1 J p S N 6 R C 7 l k o a o f a z + a F T c m a V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 12 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 J h u A t v 7 x K 2 r W q d 1 G t N S 8 r d T e P o w g n c A r n 4 M E V 1 O E W G t A C B g j P 8 A p v z q P z 4 r w 7 H 4 v W g p P P H M M f O J 8 / m u G M v w = = &lt; / l a t e x i t &gt;… (a) The framework of Zero-DCE. A DCE-Net is devised to estimate a set of best-fitting Light-Enhancement curves (LE-curves) that iteratively enhance a given input image. (b, c) LE-curves with different adjustment parameters α and numbers of iteration n. In (c), α 1 , α 2 , and α 3 are equal to -1 while n is equal to 4. In each subfigure, the horizontal axis represents the input pixel values while the vertical axis represents the output pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>An example of the pixel-wise curve parameter maps. For visualization, we average the curve parameter maps of all iterations (n = 8) and normalize the values to the range of [0, 1]. A R n , A G n , and A B n represent the averaged best-fitting curve parameter maps of R, G, and B channels, respectively. The maps in (b), (c), and (d) are represented by heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(a) Input (b) Zero-DCE (c) w/o L spa (d) w/o L exp (e) w/o L col (f) w/o L tv AAblation study of the contribution of each loss (spatial consistency loss L spa , exposure control loss L exp , color constancy loss L col , illumination smoothness loss L tv A ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>16 Figure 5 :</head><label>165</label><figDesc>Ablation study of the effect of parameter settings. l-f -n represents the proposed Zero-DCE with l convolutional layers, f feature maps of each layer (except the last layer), and n iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visual comparisons on typical low-light images. Red boxes indicate the obvious differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>User study (US)↑/Perceptual index (PI)↓ scores on the image sets (NPE, LIME, MEF, DICM, VV). Higher US score indicates better human subjective visual quality while lower PI value indicates better perceptual quality. The best result is in red whereas the second best one is in blue under each case. .79 3.50/2.76 3.22/2.61 3.42/3.17 2.80/3.37 3.32/2.94 LIME [9] 3.78/3.05 3.95/3.00 3.71/2.78 3.31/3.35 3.21/3.03 3.59/3.04 Li et al. [19] 3.80/3.09 3.78/3.02 2.93/3.61 3.47/3.43 2.87/3.37 3.37/3.72 RetinexNet [32] 3.30/3.18 2.32/3.08 2.80/2.86 2.88/3.24 1.96/2.95 2.58/3.06 Wang et al. [28] 3.83/2.83 3.82/2.90 3.13/2.72 3.44/3.20 2.95/3.42 3.43/3.01 EnlightenGAN [12] 3.90/2.96 3.84/2.83 3.75/2.45 3.50/3.13 3.17/4.71 3.63/3.22 Zero-DCE 3.81/2.84 3.80/2.76 4.13/2.43 3.52/3.04 3.24/3.33 3.70/2.88</figDesc><table><row><cell>Method</cell><cell>NPE</cell><cell>LIME</cell><cell>MEF</cell><cell>DICM</cell><cell>VV</cell><cell>Average</cell></row><row><cell>SRIE [8]</cell><cell>3.65/2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons in terms of full-reference image quality assessment metrics. The best result is in red whereas the second best one is in blue under each case.</figDesc><table><row><cell>Method</cell><cell cols="3">PSNR↑ SSIM↑ MAE↓</cell></row><row><cell>SRIE [8]</cell><cell>14.41</cell><cell>0.54</cell><cell>127.08</cell></row><row><cell>LIME [9]</cell><cell>16.17</cell><cell>0.57</cell><cell>108.12</cell></row><row><cell>Li et al. [19]</cell><cell>15.19</cell><cell>0.54</cell><cell>114.21</cell></row><row><cell>RetinexNet [32]</cell><cell>15.99</cell><cell>0.53</cell><cell>104.81</cell></row><row><cell>Wang et al. [28]</cell><cell>13.52</cell><cell>0.49</cell><cell>142.01</cell></row><row><cell>EnlightenGAN [12]</cell><cell>16.21</cell><cell>0.59</cell><cell>102.78</cell></row><row><cell>Zero-DCE</cell><cell>16.57</cell><cell>0.59</cell><cell>98.78</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">§ Runtime is measured on a PC with an Nvidia GTX 2080Ti GPU and Intel I7 6700 CPU, except for Wang et al.<ref type="bibr" target="#b27">[28]</ref>, which has to run on GTX 1080Ti GPU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A spatial processor model for object colour perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gershon</forename><surname>Buchsbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input/output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koltun</forename><surname>Vladlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep photo enhancer: Unpaired learning for image enhancement from photographs with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuching</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manhsin</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yungyu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exact histogram specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinu</forename><surname>Coltuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Chassery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1143" to="1152" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Brightness preserving dynamic histogram equalization for image contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidi</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas Sia Pik</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1752" to="1758" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wespe: Weakly supervised photo enhancer for digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPRW</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EnlightenGAN: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="128" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2d histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Underwater image color correction based on weakly supervised color transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="323" to="327" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lightennet: a convolutional neural network for weakly illuminated image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dsfd: Dual shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyuen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structure-revealing low-light image enhancement via robust retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mading</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2828" to="2841" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adedotun</forename><surname>Kin Gwn Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment for multi-exposure image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3345" to="3356" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exposure fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCCGA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exposure fusion: A simple and practrical alterrnative to high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced pix2pix dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive image contrast enhancement using generalizations of histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="896" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning for free-hand sketch: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepexposure: Learning to expose photos with asynchronously reinforced adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic exposure correction of consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ug+ track 2: A collective benchmark effort for evaluating and advancing image understanding in poor visibility environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhangyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04474</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
