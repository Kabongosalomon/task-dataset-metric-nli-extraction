<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cluster-level Feature Alignment for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
							<email>jfan@uncc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC Charlotte</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cluster-level Feature Alignment for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Person Re-Identification</term>
					<term>Feature Alignment</term>
					<term>Metric Learn- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instance-level alignment is widely exploited for person reidentification, e.g. spatial alignment, latent semantic alignment and triplet alignment. This paper probes another feature alignment modality, namely cluster-level feature alignment across whole dataset, where the model can see not only the sampled images in local mini-batch but the global feature distribution of the whole dataset from distilled anchors. Towards this aim, we propose anchor loss and investigate many variants of cluster-level feature alignment, which consists of iterative aggregation and alignment from the overview of dataset. Our extensive experiments have demonstrated that our methods can provide consistent and significant performance improvement with small training efforts after the saturation of traditional training. In both theoretical and experimental aspects, our proposed methods can result in more stable and guided optimization towards better representation and generalization for well-aligned embedding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (ReID) is an essential component of intelligent computer vision systems. It has drawn increasing interest in many applications, such as surveillance, activity analysis and long-term tracking. Given an image of a person-of-interest captured by one camera, the goal is to re-identify this person from images captured by multiple cameras without overlapping viewpoints. As an instance-level recognition problem, the ReID task is inherently challenging. First, intra-class variations are typically huge due to significant changes of visual appearances caused by camera viewing conditions, human pose variations, occlusions, et al . Second, the inter-class variations can be quite small because people may wear similar clothes.</p><p>To address these challenges of intra-class diversity and inter-class inseparability, a lot of efforts have been devoted to deep learning for its strong capability on discriminative feature extraction. Most of existing methods put the training  process of person ReID under classification framework, where intermediate features are extracted to compute the similarity between query and gallery images during test. Some tailor-made neural networks are proposed to incorporate localization/attention or disentanglement for feature alignment. The former one aligns features in 2D spatial dimension and the latter one targets latent semantic alignment, but the essence of those approaches is instance-level alignment (c.f . <ref type="figure" target="#fig_1">Fig. 1 left)</ref>. In addition, a large variety of loss functions have been proposed for metric learning in person ReID. For example, the two most prevalent loss functions are classification loss, e.g. cross entropy loss, and metric-learning based loss, e.g. hard-negative mining triplet loss <ref type="bibr" target="#b10">[9]</ref>. For those advanced networks driven by such popular loss definitions, although successful, we argue that they can still be categorized as instance-level alignment (c.f . <ref type="figure" target="#fig_1">Fig. 1 left)</ref>. The intrinsic reason of such constraint is attributed to the adoption of general classification framework, where the interaction it builds can only dwell within the sampled mini-batch but cannot see more neighbors in the distribution of the whole dataset. As a result, it inhibits the growth of intra-class compactness and inter-class separability.</p><p>Aiming to break through the aforementioned limitations and step beyond the instance-level feature alignment, we propose a succinct and efficient method to enable cluster-level interaction in feature space, targeting the alignment from an overview of latent feature distribution (c.f . <ref type="figure" target="#fig_1">Fig. 1 right)</ref>. ReID is in essential a metric learning problem. When projected to the learned feature space, feature points are expected to gather into compact clusters respecting their labels and such cluster-level interaction may inhabit better formulation of the clusters. We define the center of each feature cluster as the anchor. In a computational efficient manner, the anchors generated from aggregation serve as a supervision from the distribution of whole dataset and enable the model to see other training images in the dataset indirectly. In practice, after the saturation of traditional training, we manipulate two iterative steps to further intensify the cluster compactness: (a) Aggregate cluster features across dataset, stepping beyond the limitation of a classification framework; (b) Align features under the guidance from aggregated anchors. We claim such cluster-level feature alignment is much more promising for identity-related representation.</p><p>Besides the view of feature alignment modality (c.f . <ref type="figure" target="#fig_1">Fig. 1</ref>), the proposed method, called anchor loss, provides consistent optimization for metric learning which benefits training as well as generalization process. The classification loss tries to align the features in orders according to the classification labels. Specifically, the inner product f i p j between classifier p j in the full-connected layer and the feature vector f i is increased as p j and f i pulling towards each other (c.f . <ref type="figure">Fig. 2</ref>. From the view of optimization: Anchor loss provides more stability and consistency for optimization. (a) Classification loss pulls feature sample fi towards the corresponding classifier vector pj in fully connected layer; (b) Triplet loss probes the interaction within sampled mini-batch (denoted as solid color); (c-d) Anchor loss enables the sampled mini-batch to see the anchors aggregated from all the siblings through iterative aggregation(c) and alignment(d).</p><formula xml:id="formula_0">(b) (a) (d) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2(a)</head><p>). It has promising convergence but unnecessary penalize the intra-class variance if classifier diversifies channel-wise focus on decision. Also, it handles training mini-batch samples by simply averaging individual losses, thus can only build sample connection from the identity implicitly. Triplet loss <ref type="bibr" target="#b10">[9]</ref> tries to align features in more explicitly way. It optimizes the intra-class and inter-class distance by mini-batch interaction with proper sampling. From <ref type="figure">Fig. 2</ref>(b), we can see that the optimization direction of triplet loss is highly dependent on the minibatch sampling and inevitably introduces uncertainty and inconsistency. On the other hand, anchor loss enables the sampled mini-batch to see the anchors a j aggregated from all the siblings, bearing more consistency (c.f . <ref type="figure">Fig. 2(c-d)</ref>). Anchors generated from aggregation provide strong guidance and propagate the global information from the distribution of dataset to local mini-batch training. Based on extensive experiments, we demonstrate that anchor loss can consistently boost the generalization.</p><p>Overall, in this proposed paper, we learn a metric to overcome intra-class diversity and inter-class confusion for person ReID based on anchor-based minbatch training. Although each mini-batch of samples is a small subset of the dataset, we can successfully capture the global information during training through anchors which play key roles in the proposed cluster-level feature alignment. A small number of representative anchors propagate rich knowledge from the distribution of dataset into local training batch in a computational efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Person Re-Identification A large group of person re-identification network focuses on feature alignment. In general, there are two kinds of feature alignment: (a) spatial feature alignment by attention and localization (b) latent feature alignment by disentanglement.</p><p>In spatial feature alignment, it can be categorized as self-supervised and extra-supervised methods. We consider hand-crafted splitting as one representation of self-supervision. Sun et al . <ref type="bibr" target="#b27">[26]</ref> propose PCB to split the intermediate features horizontally in order to align the feature in local spatial parts and is widely used by <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b41">40]</ref>. Quite some works <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b13">12]</ref> proposed similar and effective part-aligning CNN networks for locating context regions and then extract these regional features for ReID. Extra-supervision leverages human part detector <ref type="bibr" target="#b9">[8]</ref>, human pose <ref type="bibr" target="#b16">[15]</ref> or human body parsing <ref type="bibr" target="#b14">[13]</ref> to provide more accurate localization. For example, <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b36">35]</ref> incorporate external pose attention maps to align the feature in deformable spatial space of human body. SPReID <ref type="bibr" target="#b14">[13]</ref> utilizes a parsing model to generate five different predefined human part masks to compute more reliable part representations, which achieves promising results on various person ReID benchmarks. Dense semantic alignment <ref type="bibr" target="#b39">[38]</ref> went one step further, it addressed the body misalignment by leveraging the estimation of the dense semantics of a person image, and constructed a set of densely semantically aligned part images for re-identification. Other methods for spatial alignment include the attention from attribute <ref type="bibr" target="#b28">[27]</ref>, forground mask <ref type="bibr" target="#b29">[28]</ref>, et al .</p><p>For latent feature alignment, DG-net <ref type="bibr" target="#b44">[43]</ref> proposed a disentanglement solution by GAN and Autoencoder to decouple input into appearance code and structure code, and extract pose-invariant features. Whereas, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b37">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Learning: Center Loss and Triplet Loss</head><p>In end-to-end learning process, several methods propose to explore iteration within mini-batch for feature alignment . Zheng et al . <ref type="bibr" target="#b45">[44]</ref> propose a verification loss to align the pairwise features. Hermans et al . <ref type="bibr" target="#b10">[9]</ref> target the triplet samples and point out the triplet loss on hard examples mining is superior to batch-all triplet loss. In face recognition, parametric center loss <ref type="bibr" target="#b33">[32]</ref> is proposed to align the intra-class distance and it is used by <ref type="bibr" target="#b17">[16]</ref> for person reID, which looks similar to our proposed anchors. Whereas, our motivation is essentially different. Center loss treats the parametric center as an auxiliary decision factor similar to the classifier p j in <ref type="figure">Fig. 2(a)</ref>, which is jointly optimized under classification framework and only builds the connection from identity label implicitly. On the contrary, our proposed anchor loss complies the embedded feature distribution and distills the knowledge from sibling samples (c.f . <ref type="figure">Fig. 2</ref>(c)) to enable the interaction in cluster level explicitly. More recently, Wen et al . <ref type="bibr" target="#b34">[33]</ref> revisit center loss and propose to use the classifier layer as the center for each class, which further validates our motivation difference. Moreover, it suffers from large instability due to random initialization for parametric center. On the other hand, our method provides constant improvement because the aggregation distills knowledge from dataset distribution.</p><p>In summary, those methods never push towards the constraint of classification framework in mini-batch training and scrutinize the modality in cluster-level feature alignment. Probably due to the concern about training efficiency, crossdataset aggregation is not fully investigated in deep CNN methods. However, we conduct a comprehensive study on different variants of the cluster-level interaction, which has demonstrated our method could be trained effectively and efficiently with small training efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Person ReID aims to establish the identity correspondences between each query image and gallery images across different cameras. We use Convolutional neural network (CNN) to extract image features due to its strong representation power. To learn discriminative representation that is robust against intra-class variation and interclass confusion, we take advantage of three different loss functions to train the model: (1) cross-entropy classification loss L cls <ref type="figure">(Fig. 2(a)</ref>); (2) triplet loss L trip <ref type="figure">(Fig. 2(b)</ref>); (3) anchor loss L anchor <ref type="figure">(Fig. 2(d)</ref>). L cls and L trip <ref type="bibr" target="#b10">[9]</ref> are widely used for person ReID, and they are both instance-level optimization as illustrated in <ref type="figure">Fig. 2</ref>(a-b). To propagate rich knowledge from outside samples into each mini-batch, the designed anchor loss L anchor targets cluster-level supervision which has two options:</p><p>(a) Anchor Loss for Intra-Class Compactness: L anchor pulls the feature vector towards the anchor where its label belongs:</p><formula xml:id="formula_1">L anchor = 1 |B| i∈B C j=1 δ(y i = j)D(f i , a j )<label>(1)</label></formula><p>where |B| denotes the number of samples in the mini-batch B, and C is the number of classes. f i and y i are the feature vector and the label of the sample i in B, respectively. a j is the anchor for the j−th class. D(f i , a j ) is the distance between the sample f i and the anchor a j . δ(y i = j) = 1 if the condition is satisfied, i.e. the label of sample i in the mini-batch equals j; otherwise, δ(y i = j) = 0. (b) Triplet Anchor Loss for Intra-Class Compactness &amp; Inter-Class Separability: Motivated by hard sample mining <ref type="bibr" target="#b10">[9]</ref>, we add extra inter-class penalty to target the hard/confused anchor mining:</p><formula xml:id="formula_2">L T ripletAnchor = 1 |B| i∈B C j=1 δ(y i = j)[D(f i , a j ) − min k =j D(f i , a k ) + margin] (2)</formula><p>Triplet anchor loss not only pulls the samples to the anchor in the same class close, but also pushes the negative samples further away than the distance between anchor and positive samples. It could be more discriminative by simultaneously taking into account intra-class compactness and inter-class separability.</p><p>On the other hand, it may import inconsistency during the optimization in a similar way as triplet loss (c.f . <ref type="figure">Fig. 2(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-Staged Training</head><p>As shown in <ref type="figure">Fig. 2</ref>, anchors generated from aggregation contribute more consistent optimization and less noisy guidance during training process. However, it is based on the assumption that distribution of embedded features is approximately cluster-formed. During the early training phase when the feature distribution is still random and stochastic, such aggregated anchors may contain misleading information and impair the training process. The cluster-level supervision could be more effective after the saturation of traditional training stage, and therefore our training consists of two stages: Stage I: Train the model in the traditional manner with loss function L = L cls + L trip , cultivating the initial formulation of clusters (c.f . <ref type="figure" target="#fig_2">Fig. 3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(left));</head><p>Stage II: Train the model under the cluster level supervision with loss function L = L cls + L anchor , capturing distribution of whole dataset in embedded feature space (c.f . <ref type="figure" target="#fig_2">Fig. 3</ref>(right)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generation and Update of Anchors</head><p>During the learning process, we take the anchors as the global supervision from data distribution to align feature towards a better representative embedding in local mini-batch training, i.e. pushing towards the target anchor (Eq. (1)) and pulling samples away from the confusion anchor (Eq. <ref type="formula">(2)</ref>). Aggregation and alignment are iteratively performed to further reduce the intra-class variance and inter-class entanglement.</p><p>Generation When the latent features are extracted over training dataset respecting their labels, we consider two approaches to estimate optimal anchors during aggregation:</p><p>(a) Average aggregation: When the embedded features are approximately cluster-formed, aggregate the embedded features f i for each class in training dataset T :</p><formula xml:id="formula_3">a j = i∈T δ(y i = j)f i i∈T δ(y i = j)<label>(3)</label></formula><p>(b) Voting by confidence: Taking the prediction probability P (j|i) for class j as the contribution, aggregate the embedded features by a weighted mean:</p><formula xml:id="formula_4">a j = i∈T δ(y i = j)P (j|i)f i i∈T δ(y i = j)P (j|i)<label>(4)</label></formula><p>Eq.</p><p>(3) treats each sample's contribution to anchor equally and could be an effective estimation to eliminate the variance of latent feature distribution caused by pose, camera view condition, background, et al . It may work well supposing the training samples are equally distributed in terms of variance. Eq. (4) takes the classifier confidence as the contribution and help to revealing the early portrait of anchors. Intuitively, when the feature cluster distribution is still stochastic in the early training stage, easy samples, which may contains less noise and variance thus converges faster, could be close to optimal anchors and guide the hard samples moving towards the estimated optimal centers. Update Frequency Ideally, we should calculate anchors by either Eq. (3) or Eq. (4) after each forward and backward process when the training parameters are updated. However, such a process is unrealistic in terms of training efficiency and thus we consider three options for the update frequency: (c) Each Iteration (Algorithm 3): In each iteration trained with anchor loss, the anchor for class j is updated as:</p><formula xml:id="formula_5">a t+1 j = [1 − η · i∈B δ(y i = j)] · a t j + η · i∈B δ(y i = j)f i<label>(5)</label></formula><p>where a t j and a t+1 j are the anchors for class j at the t−th and t + 1−th iterations respectively. To approximate the anchors calculated per iteration, we design the weight as η = 1 i∈T δ(yi=j) where T is the total training dataset. Option (a) take the anchors calculated from the initial convergence as the optimal one and only one aggregation process is performed. It is based on the observation that the clusters are almost formed after the convergence of Stage I training and thus may provide stable optimization. Option (b) adaptively updates the anchors after each training epoch in a manner similar to EM optimization: Estimate the anchor location according to the current feature cluster distribution in aggregation step and maximize the cluster compactness in alignment step. Option (c) is a trade-off option between training efficiency and adaptive estimation for anchors, which can be viewed as an approximate approach to option (b). From our experiments demonstrated later, we show this method could have comparable performance with option (b) and thus provides an alternative when the size of training samples are large or in the context of online learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>Experiment Setup We adopt the bag of tricks proposed by <ref type="bibr" target="#b17">[16]</ref>, i.e. warm-up learning rate scheduler, random erasing augmentation <ref type="bibr" target="#b46">[45]</ref>, label smoothing, no stride down-sampling in last bottleneck of ResNet50 and bnneck (one additional batch normalization layer after classifier). We use L 2 distance for the anchor loss and its variants, which benefits stable training. We experiment our methods on three datasets, Market1501 <ref type="bibr" target="#b42">[41]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b21">[20]</ref> and CUHK03 <ref type="bibr" target="#b15">[14]</ref>. Market-1501 have 12,936 training images with 751 different identities. Gallery and query sets have 19,732 and 3,368 images respectively with another 750 identities. DukeMTMC-ReID includes 16,522 training images of 702 identities, 2,228 query and 17,661 gallery images of another 702 identities. CUHK03-NP is a new training-testing split protocol for CUHK03, it contains two subsets which provide labeled and detected (from a person detector) person images. The detected CUHK03 set includes 7,365 training images, 1,400 query images and 5,332 gallery images. The labeled set contains 7,368 training, 1,400 query and 5,328 gallery images respectively. The new protocol splits the training and testing sets into 767 and 700 identities. We note that our method is only used during the training stage and the evaluation methods stay the same with previous approaches.</p><p>Firstly, we present the experimental comparison without triplet loss to analyze three factors: starting time, aggregation methods and anchor loss functions. Secondly, we delve into the effects of aggregation anchors from a reconstruction experiments. Thirdly, the comparison, when triplet loss is incorporated in the Stage I training until convergence, will be further analysed. Lastly, we demonstrate the advantages of our method comparing to the parametric center loss <ref type="bibr" target="#b33">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study for Three Factors</head><p>From the experimental results in <ref type="table" target="#tab_0">Table 1</ref>, we conclude the impacts of three factors: (a) When to start Attributed to better cluster distribution, stating aggregation and alignment after the convergence of initial training results in better generalization. From <ref type="figure" target="#fig_5">Fig. 4</ref>, one can see the anchors change rapidly during the early training phase. The transition becomes steady as the training process towards saturation. It is in line with our analysis that anchor loss may impose unexpected prior and abet densely distributed clusters when applied early during training, impairing the generalization consequently. (b) How to calculate anchors When the feature alignment is still stochastic, i.e. early training phase, calculating anchors with probability contribution (Eq. (4)) produces better results. Easy samples, which converge earlier, may contain less noises and variance, revealing the approximation of optimal anchors, e.g. the anchors generated from reconstruction in <ref type="figure">Fig. 5</ref>. After the initial training approaches convergence, the benefits of voting by confidence become less significant.   <ref type="formula" target="#formula_4">(4)</ref>) and anchor loss choices(Eq. (1)&amp;Eq. <ref type="formula">(2)</ref>). f and y denote the extracted feature and its label. Before Estart L = L cls is used in the first stage training. Afterwards, either L = L cls + L Anchor or L = L cls + L T ripletAnchor is applied. We updates the anchors each epoch as in Algorithm 2. intra-class anchor loss performs slightly better, implying the consistency weights more in training Stage II. In experiments, we also find triplet anchor loss needs a proper tuning for the margin hyper-parameter. Training is unstable when set to a high margin while easily saturated when set to a low one. The best results are found when margin = 0, where however training converges slower comparing to intra-class anchor loss. It validates our initial assumption that, when the anchors are well aggregated after Stage I training convergence, optimization with consistency provides more benefits for generalization.  <ref type="figure">Fig. 7</ref>. Test result of anchor loss from checkpoints: Without bells and whistles, anchor losses boost the performance significantly after the initial training is saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>entropy loss cross entropy loss + anchor loss cross entropy loss + triplet loss cross entropy loss + anchor loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">What Would the Anchors Look Like</head><p>In order to further validate our assumption about the benefits of aggregation, we train a decoder network to reconstruct the images from the feature maps before GAP (global average pooling), where encoder is the well-trained model without anchor loss. A decoder structure slightly modified from DG-Net <ref type="bibr" target="#b44">[43]</ref> is used. Then we generate anchors in image space by feeding the average aggregated feature maps before GAP into trained decoder (c.f . <ref type="figure">Fig. 6</ref>). We note that GAP applied anchor feature map produce the same as anchor feature vector and 2D maps are used to preserve the spatial information for better reconstruction. This anchor feature vector can be taken as the initial anchor during the start of training Stage II. As the results in <ref type="figure">Fig. 5</ref>, the anchors could be a feature vector which dissects the view-variance, pose-variance as well as backgroundvariance. Comparing to the sample images from same class (c.f . <ref type="figure">Fig. 6</ref>), anchors generated from aggregation acts like a implicit regularization to remove noise and variance. It complies that the average aggregation distills the constitutional id-related features over the sampling distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Discussion</head><p>Triplet Loss From the results in <ref type="table">Table 3</ref>, our methods consistently improve the results over the original model no matter which variant is chosen. As in <ref type="figure">Fig. 7</ref>, our methods boost the generalization after the training Stage I converges, where the triplet loss reaches saddle point and there is still room to further intensify the cluster compactness ( <ref type="figure" target="#fig_7">Fig. 8 left)</ref>. After the Stage II training, the intraclass distance is further reduced <ref type="figure" target="#fig_7">(Fig. 8 right)</ref> and boost the generalization in terms of both rank@1 accuracy and mAP ( <ref type="table">Table 3</ref>). As illustrated in <ref type="figure">Fig. 2</ref> and the experiments in Section 4.1, our methods perform more effectively after the training Stage I is converged, where a stable feature distribution is provided for aggregation. On the other hand, triplet loss may provide beneficial effects during the initial stochastic training process. Triplet loss can inhabit more compacted feature embedding for each class in euclidean space than cross-entropy loss, which has been discussed in BNNeck <ref type="bibr" target="#b17">[16]</ref>. Consequently, the improvement that the proposed anchor loss brings, is more significant for the model trained with L trip than the one trained without L trip (c.f . <ref type="table">Table 3&amp;Fig</ref>. 7). In summary, anchor loss stimulates stable and effective optimization to find better local optimal when triplet loss suffers from stochastic saddle point (c.f . <ref type="figure" target="#fig_7">Fig. 8</ref>). Applicability In terms of the comparison about update frequency for anchors, three methods (Algorithm 1,Algorithm 2,Algorithm 3) derive comparable results as shown in <ref type="table">Table 2</ref>. Considering the cluster formulation, the anchors generated at the end of traditional training are already well-complied with data distribution regarding their identity labels (c.f . <ref type="figure" target="#fig_7">Fig. 8 left)</ref>. After Stage II fine-tuning, the aggregated anchors stay close to the initial one since all the samples are pulled towards their anchors in the optimization (c.f . <ref type="figure" target="#fig_7">Fig. 8 right)</ref>. From another perspective, those three methods in <ref type="table">Table 2</ref>  line learning, Algorithm 1 and Algorithm 3 would be preferred with little sacrifice of performance. Hence, our proposed method could be tremendously flexible and widely applicable in terms of training efficiency. Robustness A natural question about the Stage II fine-tuning is that whether the further cluster-level alignment tends to be domain dependent and overfit the domain distribution. From the cross-domain testing experiments in <ref type="table">Table 4</ref>, our methods invariably outperform the baseline model. It verifies the proposed methods are an effective and robust approach to embed images into identityrelated space for metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Non-Parametric Anchor vs Parametric Center</head><p>Although anchors in our work looks like the centers proposed in <ref type="bibr" target="#b33">[32]</ref>, they are intrinsically not the same: the former is non-parametric while the latter is parametric. In fact there is no essential difference between the center loss min cj ||f i −c j || 2 and the classification loss max p j (f i p j ), both of which are distance metrics, i.e. L 2 distance and inner product. The role played by centers c j <ref type="bibr" target="#b33">[32]</ref> corresponds to the role of the hyperplanes p j in traditional classification. As a result, parametric center loss still conforms to instance-level alignment similar to classification loss under the local mini-batch training framework, and cannot build the global connection in cluster level. On the other hand, in the proposed anchor loss (Eq. (1)&amp;Eq. (2)), anchors a j are not optimization variables but calculated from cluster distribution instead. The anchors are iteratively updated from the aggregation of dataset features, which enables them to have the global view of feature distribution during the local mini-batch training.</p><p>We train 12 models independently using the the proposed anchor loss in comparison with another 12 independently trained models using the center loss <ref type="bibr" target="#b33">[32]</ref>   <ref type="figure">Fig. 9</ref>. Test result comparison between our method and parametric center loss: We train 12 models independently for each method on Market1501 dataset. on Market1501 dataset. <ref type="figure">Fig. 9</ref> illustrates the model performance histogram in terms of mAP (left) and rank1 (right). As can be observed, our proposed anchor loss consistently outperform those center loss <ref type="bibr" target="#b33">[32]</ref>. It validates that anchor loss distills the knowledge in latent feature space from the images belonging to the same identity and aggregate them into anchors to guide the training towards well-aligned embedding. Such embedding complies intrinsic feature distribution and thus helps the both training and generalization. Furthermore, the result variance of parametric center loss is much higher than anchor loss, implying the dependency to random initialization for parametric center which may impose some stochastic prior to the cluster formulation. On the contrary, our methods consistently outperform center loss with small variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison with the State-of-the-Art Methods</head><p>We compare our method with the recent state-of-the-art methods in <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref>. Comparing to spatial alignment by either attention or localization <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b5">4]</ref>, our method is much succinct without extra modules or classifier heads to handle subspace alignment. We have made several attempts to incorporate spatial alignment in our baseline model, e.g. PCB <ref type="bibr" target="#b27">[26]</ref>, only to find slightly worse results. Based on our observation from reconstruction experiment (c.f . <ref type="figure">Fig. 6</ref>), we notice that decoder can be trained well to reconstruct images in both training and test dataset, implying spatially diversified features are preserved in feature maps before GAP (global average pooling). However, the feature vectors aggregated after GAP is aligned, meaning GAP could effectively eliminate the spatial variance while preserving the channel-wise diversity when a unified strong model is well trained, e.g. the strong baseline model <ref type="bibr" target="#b17">[16]</ref>. Hence, we infer that the benefits of spatial alignment is marginal in our context. Comparing to channel attention and several variants <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b3">2,</ref><ref type="bibr" target="#b4">3]</ref> which aim to generate diverse and uncorrelated feature embedding with the efforts on convolution filters in self-supervised manner, our methods achieve the cluster encoding with focus on feature space by an explicit supervision of aggregated anchors from other images. DG-Net <ref type="bibr" target="#b44">[43]</ref> endeavors to disentangle appearance code and structure code by GAN and auto-encoder, our method accomplishes similar effect that dissects the variance and preserves the identity-related features in the direction towards aggregated anchors (c.f . <ref type="figure">Fig. 5</ref>). IBN-Net <ref type="bibr" target="#b19">[18]</ref> is proposed to reduce appearance variance and keep discriminative feature extraction by unifying both instance batch normalization and batch normalization. Luo et al . <ref type="bibr" target="#b17">[16]</ref> apply it as the backbone network for person re-identification and we report our implementation result in <ref type="table">Table 6</ref>. We note that resnet50-ibn-a network has the same parameter size and computational cost with original resnet50. Without bells and whistles, our method consistently boosts the performance in terms of both Rank@1 and mAP comparing to corresponding baseline (resnet50 and resnet50ibn-a <ref type="bibr" target="#b17">[16]</ref>), achieving the state-of-the-art results on Market-1501, DukeMTMC-reID <ref type="table">(Table 6</ref>) and CUHK03 <ref type="table">(Table 5)</ref> datasets.. Specially, due to further reduce of intra-class variance towards a compact cluster in latent feature space, our method improves mAP significantly and benefits the robustness for the application of person re-identification. In summary, comparing to the recent state-ofthe-art methods, our methods visit another modality of alignment, cluster-level alignment, validating that exploration of interaction of clusters observed from dataset feature distribution improves both training and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigate the person re-identification from the view of alignment and find an interesting and effective approach to delve in another scale of feature alignment, cluster level. By performing aggregation and alignment iteratively, our proposed anchor loss is enabled to interact with more images indirectly from aggregated anchors, which pass the distilled knowledge from the feature distribution and provide a consistent optimization to further boost the performance significantly after traditional training convergence. It shows the cluster-level alignment guided by the aggregation of dataset distribution, which steps beyond the general classification framework, is essential and beneficial for identity-related embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>From the view of alignment modality: instance-level alignment (left) and cluster-level alignment (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Two training stages of the proposed metric learning framework. Stage I (left): instance-level alignment with L cls + Ltrip. Stage II (right): feature aggregation respecting class label to generate anchors, and cluster-level alignment with L cls + L anchor .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Constant (Algorithm 1): When the model is trained until initial convergence, the cluster-level feature aggregation is calculated and serves as the fixed anchors during the following fine-tuning process; (b) Each Epoch (Algorithm 2): When the model is trained until some epoch, E start , the anchors c j are updated after each following training epoch by either Eq. (3) or Eq. (4);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(c) How to look at anchors Triplet anchor loss and intra-class anchor loss achieve comparable results when starting the aggregation and alignment in the intermediate stage of training convergence. After the initial training saturation,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The change of sampled anchors (id = 0,1,...,7) along training epochs (i.e. 0, 10, 20, ... 120): Each image corresponds to an ID (0,1,...,7). Each row in image represents the anchor feature vector (2048 dimension) in the sampled epoch. Total 13 epochs from 0 to 120 with step of 10 is sampled. Zoom in to see the details. The sampled anchors are calculated from the checkpoints of training without anchor loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Reconstruction results of anchors in the training dataset (ID=0,1,2,3...,<ref type="bibr" target="#b20">19)</ref>. Reconstruction pipeline of anchors in image space: Encoder is transfered and frozen during the training of decoder. During the inference, the decoder reconstructs the anchor into image space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>T-SNE visualization of the samples (ID=0,1,...99) on training dataset: Training result of Stage I with Ltrip (left) v.s. Stage II with L anchor (right). Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Fix Anchors for i ← 1 to StartEpoch Estart do Update model parameter W by L = Ltrip + Lcls; Update cj for each class j; for i ← Estart to EndEpoch Eend do for each iteration do Update model parameter W by L = Lanchor + Lcls;</figDesc><table><row><cell>Algorithm 2: Update Anchors</cell><cell>Algorithm 3: Update Anchors</cell></row><row><cell>Each Epoch</cell><cell>Each Iteration</cell></row><row><cell>for i ← 1 to StartEpoch Estart</cell><cell></cell></row><row><cell>do</cell><cell></cell></row><row><cell>Update model parameter W</cell><cell></cell></row><row><cell>by L = Ltrip + Lcls;</cell><cell></cell></row><row><cell>for i ← Estart to EndEpoch Eend</cell><cell></cell></row><row><cell>do</cell><cell></cell></row><row><cell>for each iteration do</cell><cell></cell></row><row><cell>Update model parameter</cell><cell></cell></row><row><cell>W by L = Lanchor + Lcls;</cell><cell></cell></row><row><cell>Update cj for each class j;</cell><cell></cell></row></table><note>for i ← 1 to StartEpoch Estart do Update model parameter W by L = Ltrip + Lcls; for i ← Estart to EndEpoch Eend do for each iteration do Update model parameter W by L = Lanchor + Lcls; Update cj for each class j;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>LAnchor(f, y, aavg) LAnchor(f, y, aweighted) LT ripletAnchor (f, y, aavg) LAnchor(f, y, aavg) LAnchor(f, y, aweighted) LT ripletAnchor (f, y, aavg) Ablation study for three factors on Market1501 dataset: starting epoch Estart, aggregation methods (Eq. (3)&amp;Eq.</figDesc><table><row><cell>Estart</cell><cell></cell><cell>Rank@1</cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell></row><row><cell>-</cell><cell>93.79%</cell><cell>93.79%</cell><cell>93.79%</cell><cell>84.69%</cell><cell>84.69%</cell><cell>84.69%</cell></row><row><cell>0</cell><cell>93.85%</cell><cell>94.06%</cell><cell>83.86%</cell><cell>83.93%</cell><cell>84.95%</cell><cell>67.48%</cell></row><row><cell>10</cell><cell>93.32%</cell><cell>93.29%</cell><cell>93.29%</cell><cell>83.77%</cell><cell>83.59%</cell><cell>83.65%</cell></row><row><cell>40</cell><cell>93.97%</cell><cell>93.91%</cell><cell>94.09%</cell><cell>85.49%</cell><cell>85.45%</cell><cell>85.43%</cell></row><row><cell>70</cell><cell>94.09%</cell><cell>94.09%</cell><cell>94.15%</cell><cell>85.75%</cell><cell>85.89%</cell><cell>85.81%</cell></row><row><cell>120</cell><cell>94.18%</cell><cell>94.03%</cell><cell>94.09%</cell><cell>85.98%</cell><cell>85.96%</cell><cell>85.90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ..Table 4 .</head><label>24</label><figDesc>are alternatives concerning computational cost and training availability while provide comparable result. For example, when training with large training dataset or in the context of on-34% 87.91% 88.3% 78.9% epoch 95.37% 87.99% 88.3% 79.1% iteration 95.25% 88.11% 88.5% 79.1% Ablation study for frequency to update anchors Ablation study of applying anchor loss after the initial training stage, i.e. The performance of different models is evaluated on cross-domain datasets. Market1501 − → DukeMTMC means that we train the model on Market1501 and evaluate it on DukeMTMC-reID. () denotes the models trained and tested with input size 384 × 192.</figDesc><table><row><cell>Frequency constant 95.Stage I Market1501 DukeMTMC reID rank@1 mAP rank@1 mAP Lcls Lcls + Ltriplet</cell><cell>Stage II -Lcls + LAnchor -Lcls + LAnchor Lcls + LAnchor Lcls + LT ripletAnchor Lcls + LT ripletAnchor</cell><cell cols="2">aggregation method Rank@1 mAP -93.79% 84.69% avg 94.18% 85.98% -94.42% 86.18% avg 95.37% 87.99% weighted 95.04% 87.95% avg 95.25% 87.84% weighted 95.13% 87.87%</cell></row><row><cell></cell><cell>Lcls + LAnchor + Ltriplet</cell><cell>avg</cell><cell>95.16% 87.87%</cell></row><row><cell></cell><cell>Lcls + LT ripletAnchor + Ltriplet</cell><cell>avg</cell><cell>95.04% 87.94%</cell></row><row><cell></cell><cell>Lcls + LT tripletAnchor + Ltriplet</cell><cell>weighted</cell><cell>95.28% 88.07%</cell></row><row><cell>Table 3epoch 120.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison with SOTA on CUHK03: CUHK03 evaluation with the setting of 767/700 training/test split on both the labeled and detected images. * denotes our implementation. Comparison of SOTA on Market1501 dataset and DukeMTMC-reID dataset. () denotes the results with a larger input size 384 × 192.</figDesc><table><row><cell>Methods</cell><cell>alignment method</cell><cell cols="2">Market-1501 Rank@1 mAP</cell><cell cols="2">DukeMTMC-reID Rank@1 mAP</cell></row><row><cell>Mancs (ECCV2018)</cell><cell>-</cell><cell>93.1%</cell><cell>82.3%</cell><cell>84.9%</cell><cell>71.8%</cell></row><row><cell cols="2">PCB+RPP [26] (ECCV2018) vertical partition</cell><cell>93.8%</cell><cell>81.6%</cell><cell>83.3%</cell><cell>69.2%</cell></row><row><cell>VPM [25] (CVPR2019)</cell><cell>soft vertical partition</cell><cell>93.8%</cell><cell>80.8%</cell><cell>83.6%</cell><cell>72.6%</cell></row><row><cell cols="2">AANet152 [27] (CVPR2019) attribute attention</cell><cell>93.9%</cell><cell>83.4%</cell><cell>87.7%</cell><cell>74.3%</cell></row><row><cell>IANet [10](CVPR2019)</cell><cell>spatial semantic</cell><cell>94.4%</cell><cell>83.1%</cell><cell>87.1%</cell><cell>73.4%</cell></row><row><cell>MltB [37] (CVPR2019)</cell><cell>CAM</cell><cell>94.7%</cell><cell>84.5%</cell><cell>85.8%</cell><cell>72.9 %</cell></row><row><cell>DG-Net [43] (CVPR2019)</cell><cell>latent code</cell><cell>94.8%</cell><cell>86.0%</cell><cell>86.6%</cell><cell>74.8 %</cell></row><row><cell>MVP Loss [24] (ICCV2019)</cell><cell>-</cell><cell>91.4%</cell><cell>80.5%</cell><cell>83.4%</cell><cell>70.0%</cell></row><row><cell>OSNet [46] (ICCV2019)</cell><cell>channel attention</cell><cell>94.8%</cell><cell>84.9%</cell><cell>88.6%</cell><cell>73.5%</cell></row><row><cell>MHN-6 [2] (ICCV2019)</cell><cell>high-order attention</cell><cell>95.1%</cell><cell>85.0%</cell><cell>89.1%</cell><cell>77.2%</cell></row><row><cell>P 2 -Net [7] (ICCV2019)</cell><cell>part</cell><cell>95.2%</cell><cell>85.6%</cell><cell>86.5%</cell><cell>73.1%</cell></row><row><cell>BDB+Cut [4] (ICCV2019)</cell><cell>partition</cell><cell>95.3%</cell><cell>86.7%</cell><cell>89.0%</cell><cell>76.0%</cell></row><row><cell>ABD-Net [3] (ICCV2019)</cell><cell>diverse attention</cell><cell>95.6%</cell><cell>88.3%</cell><cell>89.0%</cell><cell>78.6%</cell></row><row><cell>Resnet50 [16]</cell><cell>-</cell><cell cols="4">94.1%(93.6%) 85.7%(85.8%) 86.2%(86.9%) 75.9%(76.8%)</cell></row><row><cell>Resnet50 [16]</cell><cell>parametric center</cell><cell>94.5%</cell><cell>85.9%</cell><cell>86.4%</cell><cell>76.4%</cell></row><row><cell>Resnet50(ours)</cell><cell>cluster anchor</cell><cell>95.4%(94.9%)</cell><cell>88.0%</cell><cell cols="2">88.3%(89.1%) 79.1%(79.6%)</cell></row><row><cell>Resnet50-ibn-a [16]</cell><cell>-</cell><cell cols="4">95.2%(95.5%) 87.2%(88.2%) 89.0%(89.7%) 79.4%(80.0%)</cell></row><row><cell>Resnet50-ibn-a [16]</cell><cell>parametric center</cell><cell>95.0%</cell><cell>87.2%</cell><cell>89.4%</cell><cell>78.8%</cell></row><row><cell>Resnet50-ibn-a(ours)</cell><cell>cluster anchor</cell><cell cols="4">95.7%(95.8%) 88.9%(89.7%) 90.2%(91.0%) 80.6%(81.8%)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mancs</surname></persName>
		</author>
		<idno>ECCV2018) [29] 69.0% 53.9% 65.5% 60.5%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet50</surname></persName>
		</author>
		<idno>ours) 76.36% 74.50% 72.36% 70.32%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dangwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiaotang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shuyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Junjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiaogang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiaoou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction-andaggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huaming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wanli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a partaware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saquib Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Slawomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiaogang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Co-segmentation inspired attention networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mvp matching: A maximum-value perfect matching for mining hard samples, with application to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person reidentifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Eliminating background-bias for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shaogang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive study on center loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial open-world person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ancong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei-Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-pseudo regularized label for generated samples in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jinsong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhedong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhaoxiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhedong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Re-identification with consistent attentive siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
