<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Département d&apos;informatique de l&apos;ENS</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Valeo.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Valeo.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Département d&apos;informatique de l&apos;ENS</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object discovery</term>
					<term>large-scale</term>
					<term>optimization</term>
					<term>region propos- als</term>
					<term>unsupervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of discovering the objects present in a collection of images without any supervision. We build on the optimization approach of Vo et al. <ref type="bibr" target="#b33">[34]</ref> with several key novelties: (1) We propose a novel saliency-based region proposal algorithm that achieves significantly higher overlap with ground-truth objects than other competitive methods. This procedure leverages off-the-shelf CNN features trained on classification tasks without any bounding box information, but is otherwise unsupervised. (2) We exploit the inherent hierarchical structure of proposals as an effective regularizer for the approach to object discovery of [34], boosting its performance to significantly improve over the state of the art on several standard benchmarks. (3) We adopt a two-stage strategy to select promising proposals using small random sets of images before using the whole image collection to discover the objects it depicts, allowing us to tackle, for the first time (to the best of our knowledge), the discovery of multiple objects in each one of the pictures making up datasets with up to 20,000 images, an over five-fold increase compared to existing methods, and a first step toward true large-scale unsupervised image interpretation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object discovery, that is finding the location of salient objects in images without using any source of supervision, is a fundamental scientific problem in computer vision. It is also potentially an important practical one, since any effective solution would serve as a reliable free source of supervision for other tasks such as object categorization, object detection and the like. While many of these tasks can be tackled using massive amounts of annotated data, the manual annotation process is complex and expensive at large scales. Combining the discovery results with a limited amount of annotated data in a semi-supervised setting is a promising alternative to current data-hungry supervised approaches <ref type="bibr" target="#b34">[35]</ref>.</p><p>Vo et al. <ref type="bibr" target="#b33">[34]</ref> posit that image collections possess an implicit graph structure. The pictures themselves are the nodes, and an edge links two images when they arXiv:2007.02662v2 [cs.CV] 25 Aug 2020 share similar visual content. They propose the object and structure discovery framework (OSD) to localize objects and find the graph structure simultaneously by solving an optimization problem. Though demonstrating promising results, <ref type="bibr" target="#b33">[34]</ref> has several shortcomings, e.g., the use of supervised region proposals, the limitation in addressing large image collections (See Section 2). Our work is built on OSD, aims to alleviate its limitations and improves it to effectively discover multiple objects in large image collections. Our contributions are:</p><p>• We propose a simple but effective method for generating region proposals directly from CNN features (themselves trained beforehand on some auxiliary task <ref type="bibr" target="#b28">[29]</ref> without bounding boxes) in an unsupervised way (Section 3.1). Our algorithm gives on average half the number of region proposals per image compared to selective search <ref type="bibr" target="#b32">[33]</ref>, edgeboxes <ref type="bibr" target="#b39">[40]</ref> or randomized Prim <ref type="bibr" target="#b22">[23]</ref>, yet significantly outperforms these off-the-shelf region proposals in object discovery ( <ref type="table" target="#tab_2">Table 3</ref>).</p><p>• Leveraging the intrinsic structure of region proposals generated by our method allows us to add an additional constraint into the OSD formulation that acts as a regularizer on its behavior (Section 3.2). This new formulation (rOSD) significantly outperforms the original algorithm and allows us to effectively perform multi-object discovery, a setting never studied before (to the best of our knowledge) in the literature.</p><p>• We propose a two-stage algorithm to make rOSD applicable to large image collections (Section 3.3). In the first stage, rOSD is used to choose a small set of good region proposals for each image. In the second stage, these proposals and the full image collection are fed to rOSD to find the objects and the image graph structure.</p><p>• We demonstrate that our approach yields significant improvements over the state of the art in object discovery <ref type="table" target="#tab_4">(Tables 4 and 5</ref>). We also run our two-stage algorithm on a new and much larger dataset with 20,000 images and show that it significantly outperforms plain OSD in this setting ( <ref type="table" target="#tab_6">Table 7)</ref>.</p><p>The only supervisory signal used in our setting are the image labels used to train CNN features in an auxiliary classification task (see <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref> for similar approaches in the related colocalization domain). We use CNN features trained on ImageNet classification <ref type="bibr" target="#b28">[29]</ref>, without any bounding box information. Our region proposal and object discovery algorithms are otherwise fully unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Region proposals have been used in object detection/discovery to serve as object priors and reduce the search space. In most cases, they are found either by a bottom-up approach in which low-level cues are aggregated to rank a large set of boxes obtained with sliding window approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> and return the top windows as proposals, or by training a model to classify them (as in randomized Prim <ref type="bibr" target="#b22">[23]</ref>, see also <ref type="bibr" target="#b25">[26]</ref>), with bounding box supervision. Edgeboxes <ref type="bibr" target="#b39">[40]</ref> and selective search <ref type="bibr" target="#b32">[33]</ref> are popular off-the-shelf algorithms that are used to generate region proposals in object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, weakly supervised object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> or image colocalization <ref type="bibr" target="#b20">[21]</ref>. Note, however, that the features used to generate proposals in these algorithms and those representing them in the downstream tasks are generally different in nature: Typically, region proposals are generated from low-level features such as color and texture <ref type="bibr" target="#b32">[33]</ref> or edge density <ref type="bibr" target="#b39">[40]</ref>, but CNN features are used to represent them in downstream tasks. However, the Region Proposal Network in Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> shows that proposals generated directly from the features used in the object detection task itself give a great boost in performance. In the object discovery setting, we therefore propose a novel approach for generating region proposals in an unsupervised way from CNN features trained on an auxiliary classification task without bounding box information. Features from CNNs trained on large-scale image classification have also been used to localize object in the weakly supervised setting. Zhou et al. <ref type="bibr" target="#b38">[39]</ref> and Selvaraju et al. <ref type="bibr" target="#b27">[28]</ref> fine-tune a pre-trained CNN to classify images and construct class activation maps, as weighted sums of convolutional feature maps or their gradient with respect to the classification loss, for localizing objects in these images. Tang et al. <ref type="bibr" target="#b31">[32]</ref> generate region proposals to perform weakly supervised object detection on a set of labelled images by training a proposal network using the images' labels as supervision. Contrary to these works, we generate region proposals using only pre-trained CNN features without fine-tuning the feature extractor. Moreover, our region proposals come with a nice intrinsic structure which can be exploited to improve object discovery performance.</p><p>Early work on object discovery <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref> focused on a restricted setting where images are from only a few distinctive object classes. Cho et al. <ref type="bibr" target="#b5">[6]</ref> propose an approach for object and structure discovery by combining a part-based matching technique and an iterative match-then-localize algorithm, using off-theshelf region proposals as primitives for matching. Vo et al. <ref type="bibr" target="#b33">[34]</ref> reformulate <ref type="bibr" target="#b5">[6]</ref> in an optimization framework and obtain significantly better performance. Image colocalization can be seen as a narrow setting of object discovery where all images in the collection contain objects from the same class. Observing that supervised object detectors often assign high scores to only a small number of region proposals, Li et al. <ref type="bibr" target="#b20">[21]</ref> propose to mimic this behavior by training a classifier to minimize the entropy of the scores it gives to region proposals. Wei et al. <ref type="bibr" target="#b34">[35]</ref> localize objects by clustering pixels with high activations in feature maps from CNNs pre-trained in ImageNet. All of the above works, however, focus on discovering only the main object in the images and target small-to-medium-scale datasets. Our approach is based on a modified version of the OSD formulation of Vo et al. <ref type="bibr" target="#b33">[34]</ref> and pre-trained CNN features for object discovery, offers an effective and efficient solution to discover multiple objects in images in large-scale datasets. The recent work of Hsu et al. <ref type="bibr" target="#b17">[18]</ref> for instance co-segmentation can also be adapted for localizing multiple objects in images. However, it requires input images to contain an object of a single dominant class while images may instead contain several objects from different categories in our setting.</p><p>Object and structure discovery (OSD) <ref type="bibr" target="#b33">[34]</ref>. Since our work is built on <ref type="bibr" target="#b33">[34]</ref>, we give a short recap of this work in this section. Given a collection of n images, possibly containing objects from different categories, each equipped with p region proposals (which can be obtained using selective search <ref type="bibr" target="#b32">[33]</ref>, edgeboxes <ref type="bibr" target="#b39">[40]</ref>, randomized Prim <ref type="bibr" target="#b22">[23]</ref>, etc.) and a set of potential neighbors, the unsupervised object and structure discovery problem (OSD) is formalized in <ref type="bibr" target="#b33">[34]</ref> as follows: Let us define the variable e as an element of {0, 1} n×n with a zero diagonal, such that e ij = 1 when images i and j are linked by a (directional) edge, and e ij = 0 otherwise, and the variable x as an element of {0, 1} n×p , with x k i = 1 when region proposal number k corresponds to visual content shared with neighbors of image i in the graph. This leads to the following optimization problem:</p><formula xml:id="formula_0">max x,e S(x, e) = n i=1 j∈N (i) e ij x T i S ij x j , s.t. p k=1 x k i ≤ ν and j =i e ij ≤ τ ∀i,<label>(1)</label></formula><p>where N (i) is the set of potential neighbors of image i, S ij is a p × p matrix whose entry S kl ij measures the similarity between regions k and l of images i and j, and ν and τ are predefined constants corresponding respectively to the maximum number of objects present in an image and to the maximum number of neighbors an image may have. This is however a hard combinatorial optimization problem. As shown in <ref type="bibr" target="#b33">[34]</ref>, an approximate solution can be found by (a) a dual gradient ascent algorithm for a continuous relaxation of Eq. (1) with exact updates obtained by maximizing a supermodular cubic pseudo-Boolean function <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>, (b) a simple greedy scheme, or (c) a combination thereof. Since solving the continuous relaxation of Eq. (1) is computationally expensive and may be less effective for large datasets <ref type="bibr" target="#b33">[34]</ref>, we only consider the version (b) of OSD in our analysis.</p><p>OSD has some limitations: (1) Although the algorithm itself is fully unsupervised, it gives by far its best results with region proposals from randomized Prim <ref type="bibr" target="#b22">[23]</ref>, a region proposal algorithm trained with bounding box supervision.</p><p>(2) Vo et al. use whitened HOG (WHO) <ref type="bibr" target="#b15">[16]</ref> to represent region proposals in their implementation although CNN features work better on the similar image colocalization problem <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. In our experiments, naively switching to CNN features does not give consistent improvement on common benchmarks. OSD with CNN features gives a CorLoc of 82.9, 71.5 and 42.8 compared to 87.1, 71.2 and 39.5 given by OSD with WHO, respectively on OD, VOC 6x2 and VOC all data sets respectively. (3) Finally, due to its high memory cost, the algorithm cannot be applied to large datasets without compromising its final performance. In the next section, we describe our approach to addressing these limitations, as well as extending OSD to solve multi-object discovery.</p><p>3 Proposed Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region Proposals from CNN Features</head><p>We address the limitation of using off-the-shelf region proposals of <ref type="bibr" target="#b33">[34]</ref> with insights gained from the remarkably effective method for image colocalization proposed by Wei et al. <ref type="bibr" target="#b34">[35]</ref>: CNN features pre-trained for an auxiliary task, such as ImageNet classification, give a strong, category-independent signal for unsupervised tasks. In retrospect, this insight is not particularly surprising, and it is implicit in several successful approaches to image retrieval <ref type="bibr" target="#b37">[38]</ref> or co-saliency detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref>. Wei et al. <ref type="bibr" target="#b34">[35]</ref> use it to great effect in the image colocalization task. Feeding an image to a pre-trained convolutional neural network yields a set of feature maps represented as a 3D tensor (e.g., a convolutional layer of VGG16 <ref type="bibr" target="#b28">[29]</ref> or ResNet <ref type="bibr" target="#b16">[17]</ref>). Wei et al. <ref type="bibr" target="#b34">[35]</ref> observe that the "image" obtained by simply adding the feature maps gives hints to the locations of the objects it contains, and identify objects by clustering pixels with high activation. Similar but different from them, we observe that local maxima in the above "images" correspond to salient parts of objects in the original image and propose to exploit this observation for generating region proposals directly from CNN features. As we do not make use of any annotated bounding boxes, our region proposal itself is indeed unsupervised. Our method consists of the following steps. First, we feed the image to a pre-trained convolutional neural network to obtain a 3D tensor of size (H × W × D), noted F . Adding elements of the tensor along its depth dimension yields a (H × W ) 2D saliency map, noted as s g (global saliency map), showing salient locations in the image with each location in s g being represented by the corresponding D-dimensional feature vector from F . Next, we find robust local maxima in the previous saliency map using persistence, a measure used in topological data analysis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref> to find critical points of a function (see Section 4.2 for details). We find regions around each local maximum y using a local saliency map s y of the same size as the global one. The value at any location in s y is the dot product between normalized feature vectors at that location and the local maximum. By construction, the local saliency map highlights locations that are likely to belong to the same object as the corresponding local maximum. Finally, for each local saliency map, we discard all locations with scores below some threshold and the bounding box around the connected component containing the corresponding local maximum is returned as a region proposal. By varying the threshold, we can obtain tens of region proposals per local saliency map. An example illustrating the whole process is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regularized OSD</head><p>Due to the greedy nature of OSD <ref type="bibr" target="#b33">[34]</ref>, its block-coordinate ascent iterations are prone to bad local maxima. Vo et al. <ref type="bibr" target="#b33">[34]</ref> attempt to resolve this problem by using a larger value of ν in the optimization than the actual number of objects they intend to retrieve (which is one in their case) to diversify the set of retained regions in each iteration. The final region in each image is then chosen amongst its retained regions in a post processing step by ranking these using a new score solely based on their similarity to the retained regions in the image's neighbors. Increasing ν in fact gives limited help in diversifying the set of retained regions. Since there is redundancy in object proposals with many highly overlapping regions, the ν retained regions are often nearly identical (see supplementary document for a visual illustration). This phenomenon also prevents OSD from retrieving multiple objects in images. One can use the ranking in OSD's post processing step with non-maximum suppression to return more than one region row shows the original image, the global saliency map sg, local maxima of sg and three local saliency maps sy from three local maxima (marked by red stars). The next three rows illustrate the proposal generation process on the local saliency maps: From left to right, we show in green the connected component formed by pixels with saliency above decreasing thresholds and, in red, the corresponding region proposals.</p><p>from ν retained regions but since ν regions are often highly overlapping, this fails to localize multiple objects.</p><p>By construction, proposals produced by our approach also contain many highly overlapping regions, especially those generated from the same local maximum in the saliency map. However, they come with a nice intrinsic structure: Proposals in an image can be partitioned into groups labelled by the local maximum from which they are generated. Naturally, it makes sense to impose that at most one region in a group is retained in OSD since they are supposed to correspond to the same object. This additional constraint also conveniently helps to diversify the set of proposals returned by the block-coordinate ascent procedure by avoiding to retain highly overlapping regions. Concretely, let G ig be the set of region proposals in image i generated from the g-th local maximum in its global saliency map s g , with 1 ≤ g ≤ L i where L i is the number of local maxima in s g , we propose to add the constraints k∈Gig x k i ≤ 1 ∀i, g to Eq. (1). We coin the new formulation regularized OSD (rOSD). Similar to OSD, a solution to rOSD can be obtained by a greedy block-coordinate ascent algorithm whose iterations are illustrated in the supplementary document. We will demonstrate the effectiveness of rOSD compared to OSD and the state of the art in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Large-Scale Object Discovery</head><p>The optimization algorithm of Vo et al. <ref type="bibr" target="#b33">[34]</ref> requires loading all score matrices S ij into the memory (they can also be computed on-the-fly but at an unacceptable computational cost). The corresponding memory cost is M = ( n i=1 |N (i)|)×K, decided by two main factors: The number of image pairs considered n i=1 |N (i)| and the number of positive entries K in matrices S ij . To reduce the cost on larger datasets, Vo et al. <ref type="bibr" target="#b33">[34]</ref> pre-filter the neighborhood of each image (|N (i)| ≤ 100 for classes with more than 1000 images) and limit K to 1000. This value of K is approximately the average number of proposals in each image, and it is intentionally chosen to make sure that S ij is not too sparse in the sense that approximately every proposal in image i should have a positive match with some proposal in image j. Further reducing the number of positive entries in score matrices is likely to hurt the performance <ref type="table" target="#tab_6">(Table 7)</ref> while a number of 100 potential neighbors is already small and can not be significantly lowered. Effectively scaling up OSD 4 therefore requires lowering considerably the number of proposals it uses. To this end, we propose two different interpretations of the image graph and exploit both to scale up OSD. Two different interpretations of the image graph. The image graph G = (x, e) obtained by solving Eq. (1) can be interpreted as capturing the "true" structure of the input image collection. In this case, ν is typically small (say, 1 to 5) and the discovered "objects" correspond to maximal cliques of G, with instances given by active regions (x k i = 1) associated with nodes in the clique. But it can also be interpreted as a proxy for that structure. In this case, we typically take ν larger (say, 50). The active regions found for each node x i of G are interpreted as the most promising regions in the corresponding image and the active edges e ij link it to other images supporting that choice. We dub this variant proxy OSD.</p><p>For small image collections, it makes sense to run OSD only. For large ones, we propose instead to split the data into random groups with fixed size, run proxy OSD on each group to select the most promising region proposals in the corresponding images, then run OSD using these proposals. Using this two-stage algorithm, we reduce significantly the number of image pairs in each run of the first stage, thus permitting the use of denser score matrices in these runs. In the second stage, since only a very small number of region proposals are considered in each image, we need to keep only a few positive entries in each score matrix and are able to run OSD on the entire image collection. Our approach for large-scale object discovery is summarized the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>Similar to previous works on object discovery <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref> and image colocalization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>, we evaluate object discovery performance with our proposals on four datasets: Object Discovery (OD), VOC 6x2, VOC all and VOC12. OD is a small dataset with three classes airplane, car and horse, and 100 images per class, among which 18, 11 and 7 images are outliers (images not including an object of the corresponding class) respectively. VOC all is a subset of the PASCAL VOC 2007 dataset <ref type="bibr" target="#b10">[11]</ref> obtained by eliminating all images containing only difficult or truncated objects as well as difficult or truncated objects in retained images. It has 3550 images and 6661 objects. VOC 6x2 is a subset of VOC all which contains images of 6 classes aeroplane, bicycle, boat, bus, horse and motorbike divided into 2 views left and right. In total, VOC 6x2 contains 463 images of 12 classes. VOC12 is a subset of the PASCAL VOC 2012 dataset <ref type="bibr" target="#b9">[10]</ref> and obtained in the same way as VOC all. It contains 7838 images and figures 13957 objects. For large-scale experiments, we randomly choose 20000 images from the training set of COCO <ref type="bibr" target="#b21">[22]</ref> and eliminate those containing only crowd bounding boxes as well as bounding boxes marked as crowd in retained images. The resulting dataset, which we call COCO 20k, has 19817 images and 143951 objects.</p><p>As single-object discovery and colocalization performance measure, we use correct localization (CorLoc) defined as the percentage of images correctly localized. In our context, this means the intersection over union (IoU ) between one of the ground-truth regions and one of the predicted regions in the image is greater than 0.5. Since CorLoc does not take into account multiple detections per image, for multi-object discovery, we use instead detection rate at the IoU threshold of 0.5 as measure of performance. Given some threshold ζ, detection rate at IoU = ζ is the percentage of ground-truth bounding boxes that have an IoU with one of the retained proposals greater than ζ. We run the experiments in both the colocalization setting, where the algorithm is run separately on each class of the dataset, and the average CorLoc/detection rate over all classes is computed as the overall performance measure on the dataset, and the true discovery setting where the whole dataset is considered as a single class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Features. We test our methods with the pre-trained CNN features from VGG16 and VGG19 <ref type="bibr" target="#b28">[29]</ref>. For generating region proposals, we apply the algorithm described in Section 3.1 separately to the layers right before the last two max pooling layers of the networks (relu4 3 and relu5 3 in VGG16, relu4 4 and relu5 4 in VGG19), then fuse proposals generated from the two layers as our final set of proposals. Note that using CNN features at multiple layers is important as different layers capture different visual patterns in images <ref type="bibr" target="#b36">[37]</ref>. One could also use more layers from VGG16 (e.g., layers relu3 3, relu4 2 or relu5 2 ) but we only use two for the sake of efficiency. In experiments with OSD, we extract features for the region proposals by applying the RoI pooling operator introduced in Fast-RCNN <ref type="bibr" target="#b12">[13]</ref> to layer relu5 3 of VGG16. Region Proposal Generation Process. For finding robust local maxima of the global saliency maps s g , we rank its locations using persistence <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref>. Concretely, we consider s g as a 2D image and each location in it as a pixel. We associate with each pixel a cluster (the 4-neighborhood connected component of pixels that contains it), together with a "birth" (its own saliency) and "death" time (the highest value for which one of the pixels in its cluster also belongs to the cluster of a pixel with higher saliency, or, if no such location exists, the lowest saliency value in the map). The persistence of a pixel is defined as the difference between its birth and death times. A sorted list of pixels in decreasing persistence order is computed, and the local maxima are chosen as the top pixels in the list. For additional robustness, we also apply non maximum suppression on the list  over a 3×3 neighborhood. Since the saliency map created from CNN feature maps can be very noisy, we eliminate locations with score in s g below α max s g before computing the persistence to obtain only good local maxima. We also eliminate locations with score smaller than the average score in s y and whose score in s g is smaller than β times the average score in s g . We choose the value of the pair (α, β) in {0.3, 0.5} × {0.5, 1} by conducting small-scale object discovery on VOC 6x2. We find that (α, β) = (0.3, 0.5) yields the best performance and gives local saliency maps that are not fragmented while eliminating well irrelevant locations across settings and datasets. We take up to 20 local maxima (after non-maximum suppression) and use 50 linearly spaced thresholds between the lowest and the highest scores in each local saliency map to generate proposals. Object Discovery Experiments. For single-object colocalization and discovery, following <ref type="bibr" target="#b33">[34]</ref>, we use ν = 5, τ = 10 and apply the OSD's post processing to obtain the final localization result. For multi-object setting, we use ν = 50, τ = 10 and apply the post processing with non-maximum suppression at IoU = 0.7 to retain at most 5 regions in the final result. On large classes/datasets, we pre-filter the set of neighbors that are considered in the optimization for each image, using the cosine similarity between features from the fully connected layer fc6 of the pre-trained network, following <ref type="bibr" target="#b1">[2]</ref>. The number of potential neighbors of each image is fixed to 50 in all experiments where the pre-filtering is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Region Proposal Evaluation</head><p>Following other works on region proposals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, we evaluate the quality of our proposals on PASCAL VOC 2007 using the detection rate at various IoU thresholds. But since we intend to later use our proposals for object discovery, unlike other works, we evaluate directly our proposals on VOC all instead of the test set of VOC 2007 to reveal the link between the quality of proposals and the object discovery performance. <ref type="figure" target="#fig_2">Figure 2</ref>(a-c) shows the performance of different proposals on VOC all. It can be seen that our method performs better than others at a very high overlap threshold (0.9) regardless of the number of proposals allowed. At medium threshold (0.7), our proposals are on par (or better for fewer than 500 proposals) with those from selective search <ref type="bibr" target="#b32">[33]</ref> and randomized Prim <ref type="bibr" target="#b22">[23]</ref> and much better than those from edgeboxes <ref type="bibr" target="#b39">[40]</ref>. At a small threshold (0.5), our method is still on par with randomized Prim and edgeboxes, but does not fare as well as selective search. It should be noted that randomized Prim is supervised whereas the others are unsupervised. In OSD, localizing an object in an image means singling out a positive proposal, that is, a proposal having an IoU greater than some threshold with object bounding boxes. It is therefore easier to localize the object if the percentage of positive region proposals is larger. As shown by <ref type="figure" target="#fig_2">Fig. 2(d)</ref>, our method performs very well according to this criterion: Over 8% of our proposals are positive at an IoU threshold of 0.5, and over 3% are still positive for an IoU of 0.7. Also, randomized Prim and our method are by far better than selective search and edgeboxes, which explains the superior object discovery performance of the former over the latter (cf. <ref type="bibr" target="#b33">[34]</ref> and <ref type="table" target="#tab_2">Table 3</ref>). Note that region proposals with a high percentage of positive ones could also be used in other tasks, i.e., weakly supervised object detection, but this is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Object Discovery Performance</head><p>Single-Object Colocalization and Discovery. An important component of OSD is the similarity model used to compute score matrices S ij , which, in <ref type="bibr" target="#b33">[34]</ref>, is the Probabilistic Hough Matching (PHM) algorithm <ref type="bibr" target="#b5">[6]</ref>. Vo et al. <ref type="bibr" target="#b33">[34]</ref> introduce two scores, confidence score and standout score, but use only the latter for it gives better performance. Since our new proposals come with different statistics, we test both scores in our experiments. <ref type="table" target="#tab_0">Table 1</ref> compares colocalization performance on OD, VOC 6x2 and VOC all of OSD using the confidence and standout scores as well as our proposals. It can be seen that on VOC 6x2 and VOC all, the confidence score does better than the standout score, while on OD, the latter does better. This is in fact not particularly surprising since images in OD generally contain bigger objects (relative to image size) than those in the other datasets. In fact, although the standout score is used on all datasets in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b33">[34]</ref>, the authors adjust the parameter γ (see <ref type="bibr" target="#b5">[6]</ref>) used in computing their standout score to favor larger regions when running their models on OD. In all of our experiments from now on, we use the standout score on OD and the confidence score on other datasets (VOC 6x2, VOC all, VOC12 and COCO 20k).</p><p>Our proposal generation process introduces a few hyper-parameters. Apart from α and β, two other important hyper-parameters are the number of local maxima u and the number of thresholds v which together control the number of proposals p per image returned by the process. We study their influence on the colocalization performance by conducting experiments on VOC 6x2 and report the results in <ref type="table" target="#tab_1">Table 2</ref>. It shows that the colocalization performance does not depend much on the values of these parameters. Using (u = 50, v = 100) actually gives the best performance but with twice as many proposals as (u = 20, v = 50). For efficiency, we use u = 20 and v = 50 in all of our experiments.   We report in <ref type="table" target="#tab_2">Table 3</ref> the performance of OSD and rOSD on OD, VOC 6x2 and VOC all with different types of proposals. It can be seen that our proposals give the best results on all datasets among all types of proposals with significant margins: 6.1%, 2.1% and 3.0% in colocalization and 5.3%, 0.5% and 4.7% in discovery, respectively. It is also noticeable that our proposals not only fare much better than the unsupervised ones (selective search and edgeboxes) but outperform those generated by randomized Prim, an algorithm trained with bounding box annotation.</p><p>We compare OSD and rOSD using our region proposals to the state of the art in <ref type="table" target="#tab_10">Table 4</ref> (colocalization) and <ref type="table" target="#tab_4">Table 5</ref> (discovery). In their experiments, Wei et al. <ref type="bibr" target="#b34">[35]</ref> only use features from VGG19. We have conducted experiments with features from both VGG16 and VGG19 but only present experiment results with VGG19 features in comparisons with <ref type="bibr" target="#b34">[35]</ref> due to the space limit. A more comprehensive comparison with features from VGG16 is included in the supplementary material. It can be seen that our use of CNN features (for both creating proposals and representing them in OSD) consistently improves the performance compared to the original OSD <ref type="bibr" target="#b33">[34]</ref>. It is also noticeable that rOSD performs significantly better than OSD on the two large datasets (VOC all and VOC12) while on the two smaller ones (OD and VOC 6x2), their performances are comparable. It is due to the fact that images in OD and VOC 6x2 mostly contain only one well-positioned object thus bad local maxima are not a big problem in the optimization while images in VOC all and VOC12 contain much more complex scenes and the optimization works better with more regularization. In overall, we obtain the best results on the two smaller datasets, fare better than <ref type="bibr" target="#b20">[21]</ref> but are behind <ref type="bibr" target="#b34">[35]</ref> on VOC all and VOC12 in the colocalization setting. It should be noticed that while methods for image colocalization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref> suppose that images in the collection come from the same category and explicitly exploit this <ref type="table" target="#tab_10">Table 4</ref>: Single-object colocalization performance of our approach compared to the state of the art. Note that Wei et al. <ref type="bibr" target="#b34">[35]</ref> outperform our method on VOC all and VOC12 in this case, but the situation is clearly reversed in the much more difficult discovery setting, as demonstrated in <ref type="table" target="#tab_4">Table 5</ref> Method  assumption, rOSD is intended to deal with the much more difficult and general object discovery task. Indeed, in discovery setting, rOSD outperforms <ref type="bibr" target="#b34">[35]</ref> by a large margin, 5.9% and 4.9% respectively on VOC all and VOC12.</p><p>Multi-Object Colocalization and Discovery. We demonstrate the effectiveness of rOSD in multi-object colocalization and discovery on VOC all and VOC12 datasets, which contain images with multiple objects. We compare the performance of OSD and rOSD to Wei et al. <ref type="bibr" target="#b34">[35]</ref> in <ref type="table" target="#tab_5">Table 6</ref>. Although <ref type="bibr" target="#b34">[35]</ref> tackles only the single-object colocalization problem, we modify their method to have a reasonable baseline for the multi-object colocalization and discovery problem. Concretely, we take the bounding boxes around the 5 largest connected components of positive locations in the image's indicator matrix <ref type="bibr" target="#b34">[35]</ref> as the localization results. It can be seen that our method obtains the best performance with significant margins to the closest competitor across all datasets and settings. It is also noticeable that rOSD, again, significantly outperforms OSD in this task. An illustration of multi-object discovery is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. For a fair comparison, we use high values of ν (50) and IoU (0.7) in the multi-object experiments to make sure that both OSD and rOSD return approximately 5 regions per image. Images may of course contain fewer than 5 objects. In such cases, OSD and rOSD usually return overlapping boxes around the actual objects. We can often eliminate these overlapping boxes and obtain better qualitative results by using smaller ν and IoU threshold values. It can be seen in <ref type="figure" target="#fig_3">Fig. 3</ref> that with ν = 25 and IoU = 0.3, rOSD is able to return bounding boxes around objects without many overlapping regions. Note however that the quantitative results may worsen due to the reduced number of regions returned and the fact that many images contain objects that highly overlap, e.g., the last two columns of <ref type="figure" target="#fig_3">Fig. 3</ref>. In such cases, a small IoU threshold prevents discovering all of these objects. See supplementary document for more visualizations and details. Large-Scale Object Discovery. We apply our large-scale algorithm in the discovery setting on VOC all, VOC12 and COCO 20k which are randomly partitioned respectively into 5, 10 and 20 parts of roughly equal sizes. In the first stage of all experiments, we prefilter the initial neighborhood of images and keep only 50 potential neighbors. We choose ν = 50 and keep K 1 (which are 250, 500 and 1000 respectively on VOC all, VOC12 and COCO 20k) positive entries in each score matrix. In the second stage, we run rOSD (OSD) on the entire datasets with ν = 5, limit the number of potential neighbors to 50 and use score matrices with only 50 positive entries. We choose K 1 such that each run in the first stage and the OSD run in the second stage have the same memory cost, hence the values of K chosen above. As baselines, we have applied rOSD (OSD) directly to the datasets, keeping 50 positive entries (baseline 1) and 1000 positive entries (baseline 2) in score matrices. <ref type="table" target="#tab_6">Table 7</ref> shows the object discovery performance on VOC all, VOC12 and COCO 20k for our large-scale algorithm compared to the baselines. It can be seen that our large-scale twostage rOSD algorithm yields significant performance gains over the baseline 1, obtains an improvement of 6.6%, 9.3% and 4.0% in single-object discovery and 2.9%, 4.0% and 0.4% in multi-object discovery, respectively on VOC all, VOC12 and COCO 20k. Interestingly, large-scale rOSD also outperforms the baseline 2, which has a much higher memory cost, on VOC all and VOC12. Execution time. Similar to <ref type="bibr" target="#b33">[34]</ref>, our method requires computing the similarity scores for a large number of image pairs which makes it computationally costly. It takes in total 478 paralellizable CPU hours, 300 unparallelizable CPU seconds and 1 GPU hour to run single-object discovery on VOC all with 3550 images. This is more costly compared to only 812 GPU seconds needed by DDT+ <ref type="bibr" target="#b34">[35]</ref> but is less costly than <ref type="bibr" target="#b33">[34]</ref> using CNN features. The latter requires 546 paralellizable CPU hours, 250 unparalellizable CPU seconds and 4 GPU hours. Note that the unparallelizable computational cost, which comes from the main OSD algorithm, grows very fast (at least linearly in theory, it takes 2.3 hours on COCO 20k in practice) with the data set's size and is the time bottleneck in large scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an unsupervised algorithm for generating region proposals from CNN features trained on an auxiliary and unrelated task. Our proposals come with an intrinsic structure which can be leveraged as an additional regularization in the OSD framework of Vo et al. <ref type="bibr" target="#b33">[34]</ref>. The combination of our proposals and regularized OSD gives comparable results to the current state of the art in image colocalization, sets a new state-of-the-art single-object discovery and has proven effective in the multi-object discovery. We have also successfully extended OSD to the large-scale case and show that our method yields significantly better performance than plain OSD. Future work will be dedicated to investigating other applications of our region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary materials: Toward unsupervised, multi-object discovery in large-scale image collections 1 Regularized OSD (rOSD)</head><p>We have presented in the paper a new version of the OSD formulation <ref type="bibr" target="#b33">[34]</ref> with added constraints based on the structure of our region proposals. Concretely, we propose to solve the optimization problem:</p><formula xml:id="formula_1">max x,e S(x, e) = n i=1 j∈N (i) e ij x T i S ij x j , s.t.∀i            p k=1 x k i ≤ ν, k∈Gig x k i ≤ 1, for all groups g j =i e ij ≤ τ.<label>(2)</label></formula><p>We solve this problem with an iterative block-coordinate ascent algorithm similar to OSD. Its iterations are illustrated in Algorithm 1.</p><p>Algorithm 1: Block coordinate ascent algorithm for rOSD.</p><p>Result: A solution to rOSD. Input: Gi, ν, τ , Sij, number n of images. Initialization: xi = 1p ∀i, eij = 1 ∀i = j. for i = 1 to n do Compute the vector R containing the scores of regions in image i. R ←− n j =i (eijSij + ejiS T ji )xj. I ←− ∅. for g = 1; g ≤ Li do</p><p>Find the region g * with highest score R(g * ) in the group Gig. I ←− I ∪ {g * }. end Choose ν regions in I with highest scores in R, assign their corresponding variables to 1. Assign the variables of other regions to 0. end for i = 1 to n do Compute the indices j1 to jτ of the τ largest scalars</p><formula xml:id="formula_2">x T i Sijxj (1 ≤ j ≤ n). ei ←− 0. for t = 1; t ≤ τ do eij t ←− 1. end end</formula><p>Note that the output of Algorithm 1 depends on the order in which the variables x i are processed in its first for loop. In our implementation, we use a different random permutation of (1, ..., n) in each iteration of the optimization. For each experiment, we run rOSD several times and report the average performance of all runs as the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Large-Scale Object Discovery Algorithm</head><p>We summarize in Algorithm 2 our proposed large-scale algorithm for object discovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results with the Ensemble Method from [34]</head><p>Vo et al. <ref type="bibr" target="#b33">[34]</ref> use an ensemble method (EM) to combine several solutions before post processing to stabilize and improve the final performance of OSD. We investigate the influence of this procedure on the performance of OSD and rOSD with our proposals, and present the result in <ref type="table" target="#tab_0">Tables 1 and 2</ref>. We use VGG16 features in these experiments. It can be seen that the effect of EM is mixed for the tested datasets. It generally harms the performance on VOC all and VOC12 and improves the performance on VOC 6x2 while its effect on OD is unclear. We have therefore chosen to omit EM in the experiments of the main body of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Full Results with both VGG16 and VGG19 Features</head><p>We present in <ref type="table" target="#tab_2">Tables 3, 4</ref> and 5 our full results in colocalization and object discovery with features from both VGG16 and VGG19. It can be seen that, with VGG16 features, rOSD still significantly outperforms OSD on the two large datasets and fares comparably to OSD on the smaller two. It is also noticeable that rOSD significantly outperforms Wei et al. in both colocalization and singleobject discovery on all datasets when VGG16 features are used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Object Experiments</head><p>For a fair comparison to OSD and Wei et al. <ref type="bibr" target="#b34">[35]</ref> in multi-object discovery, we have fixed the number of objects retained in each image by all methods to 5 in the paper. We have also modified the method of Wei et al. such that 5 bounding boxes around the 5 largest clusters of positive pixels in their indicator matrix are returned as objects. For OSD and rOSD, we run the corresponding optimization then apply the following post processing on each image: all ν retained regions are ranked in descending order using the score proposed in <ref type="bibr" target="#b33">[34]</ref> (Eq. 12 in Sec. 2.6 therein), which is solely based on their similarity to the retained regions in the image's neighbors; We then iteratively discard all proposals having an IoU score greater than some threshold with higher-ranked regions; Among remaining regions, we return the 5 highest ranked as retrieved objects. Since this procedure can eliminate all but a few regions if the regions highly overlap, we choose a large value of ν (50) and a large value of IoU threshold (0.7) in our experiments to guarantee that we have exactly 5 objects. This is, however, just a design choice and one can choose to retain fewer or more regions. We have conducted experiments with the number of retrieved objects varied in the interval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> and observed that rOSD always yields better performance than OSD and <ref type="bibr" target="#b34">[35]</ref> regardless of the number of objects retrieved <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Images may of course contain fewer than 5 objects. In such cases, OSD and rOSD usually return overlapping boxes around the actual objects ( <ref type="figure" target="#fig_3">Fig. 3</ref> in the paper). We can eliminate these overlapping boxes and obtain better qualitative results by using smaller ν and IoU threshold. We have conducted preliminary experiments with ν = 25 in the optimization of OSD and rOSD and IoU = 0.3 for suppression threshold in the post processing and show qualitative results in <ref type="figure" target="#fig_2">Fig. 2</ref>. It can be seen that rOSD is now able to return bounding boxes around ob- <ref type="table" target="#tab_2">Table 3</ref>: Single-object colocalization performance of our approach compared to the state of the art. Note that Wei et al. <ref type="bibr" target="#b34">[35]</ref> outperform our method on VOC all and VOC12 with VGG19 features in this case, but the situation is clearly reversed in the much more difficult single-object discovery setting, as demonstrated in <ref type="table" target="#tab_10">Table 4</ref> Method  jects without many overlapping regions. It is also observed that rOSD fares much better than OSD in localizing multiple objects. We also compare the quantitative performance of rOSD, OSD and <ref type="bibr" target="#b34">[35]</ref> in <ref type="table" target="#tab_5">Table 6</ref>. For <ref type="bibr" target="#b34">[35]</ref>, we take as before the bounding boxes around the largest clusters of pixels in the indicator matrix of each image. The number of clusters in this case is chosen to be the number of objects returned by rOSD in the same image. The results show that rOSD again yields by far the best performance. It is also noticeable that while using smaller values of ν and the IoU threshold slightly deteriorates the performance of rOSD, it makes the performance of OSD drop significantly (compare <ref type="table" target="#tab_4">Tables 5 and 6</ref>). This is due to the fact that OSD returns many highly overlapping regions and most of them are eliminated by our procedure. On the other hand, rOSD returns more diverse regions and consequently more regions are retained. In practice, we observe that OSD returns on average 1.47 (respectively 1.52) regions while rOSD returns 3.62 (respectively 3.63) on VOC all (respectively VOC12). Note, however, that rOSD still outperforms OSD and <ref type="bibr" target="#b34">[35]</ref> even when the latter are allowed to retain exactly 5 regions.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluating the Graph Computed by OSD</head><p>Following <ref type="bibr" target="#b5">[6]</ref>, we evaluate the local graph structure obtained by rOSD using the CorRet measure, defined as the average percentage of returned image neighbors that belong to the same (ground-truth) class as the image itself. As a baseline, we consider the local graph induced by the sets of nearest neighbors N (i) computed from the fully connected layer fc6 of the CNN that are used in the same experiment. <ref type="table" target="#tab_6">Table 7</ref> shows the CorRet of local graphs obtained when running rOSD (OSD) on VOC all and VOC12 and large-scale rOSD (OSD) on COCO 20k in the mixed setting. It can be seen that the local image graphs returned by our methods have higher CorRet than the baseline.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on Images of ImageNet Classes not in the Training Set of the Feature Extractors</head><p>Though trained for classifying 1000 object classes of ImageNet, features from convolutional layers of VGGs have shown to be generic: They have been used for various tasks, including unsupervised object discovery. Li et al. <ref type="bibr" target="#b20">[21]</ref> and Wei et al. <ref type="bibr" target="#b34">[35]</ref> have shown that CNN features generalize well beyond the classes in ILSVRC2012 by testing on 6 held-out classes on ImageNet (chipmunk, racoon, rhinoceros, rake, stoat and wheelchair ). We have also tested our method on these classes. Since ImageNet has been under maintenance, we could not download all the official images in the six classes. For preliminary experiments, we have instead downloaded the images using their public URLs (provided on the ImageNet website), eliminated corrupted images, randomly chosen up to 200 images per class and run our experiments on these images. We have compared rOSD, OSD, <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b34">[35]</ref> in this setting <ref type="table" target="#tab_14">(Table 8</ref>). Although rOSD performs significantly better than <ref type="bibr" target="#b20">[21]</ref> in colocalization tasks, it is as before significantly outperformed by <ref type="bibr" target="#b34">[35]</ref> there. In object discovery, rOSD performs slightly better than <ref type="bibr" target="#b34">[35]</ref> for VGG16 features, but significantly worse for VGG19 features. Understanding this discrepancy observed in preliminary experiments is part of our plans for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">More Visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overlapping Regions Returned by OSD and rOSD</head><p>The most important advantage of rOSD over OSD is that the former returns more diverse regions than the former does. We visualize the regions returned by OSD and rOSD in colocalization experiments with ν = 5 in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Persistence</head><p>We use persistence <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref> to find robust local maxima of the global saliency map s g in our work. Considering s g as a 2D image and each location in it as a pixel, we associate with each pixel a cluster (the 4-neighborhood connected component of pixels that contains it), together with both a "birth" (its own saliency) and "death time" (the highest value for which one of the pixels in its cluster also belongs to the cluster of a pixel with higher saliency, or, if no such location exists, the lowest saliency value in the map). The persistence of a pixel is defined as the difference between its birth and death times. <ref type="figure">Figure 4</ref> illustrates persistence for the 1D case. <ref type="figure">Fig. 4</ref>: An illustration of persistence in the 1D case. Left: A 1D function. Right: Its persistence diagram. Points above the diagonal correspond to its local maxima and the vertical distance from these points to the diagonal is their persistence. Local maxima with higher persistence are more robust: B is more robust than A although f (A) &gt; f (B). Given a chosen persistence threshold (shown by dash lines in blue), points with persistence higher than some threshold are selected as robust local maxima. The black horizontal dotted lines show birth and death time of the local maxima of f .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of the unsupervised region proposal generation process. The top</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) IoU = 0.5. (b) IoU = 0.7. (c) IoU = 0.9. (d) positive regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Quality of proposals by different methods. (a-c): Detection rate by number of proposals at different IoU thresholds of randomized Prim (RP) [23], edgeboxes (EB) [40], selective search (SS) [33] and ours; (d): Percentage of positive proposals for the four methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative multi-object discovery results obtained with rOSD. White boxes are ground truth objects and red ones are our predictions. Original images are in the first row. Results with ν = 50 and IoU = 0.7 are in the second row. Results with ν = 25 and IoU = 0.3 are in the third row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Large-scale object discovery algorithm. Input: Dataset D of n images, memory limit M , number of partition k, image neighborhood size N , ν * , τ . Partition D into random k parts D1, ..., D k , each has roughly n/k images. Compute the maximum number of positive entries in the score matrices in each parts: K1 ←− M/(N * n/k ). Compute the maximum number of positive entries in the score matrices in the whole dataset: K2 ←− M/(n * N ). for i = 1 to k do Compute score matrices for image pairs in Di with K1 positive entries. Run proxy OSD on Di with ν = K2. Each image in Di has a new set of region proposals which are those retained by OSD. end Compute score matrices between pairs of images in D with K2 positive entries. Run OSD on the whole dataset D with ν = ν * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 :</head><label>1</label><figDesc>Multi-object discovery performance of rOSD compared to OSD and<ref type="bibr" target="#b34">[35]</ref> when varying the maximum number of returned objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 :</head><label>2</label><figDesc>Multi-object discovery results. In each column, from top to bottom: original image, image with predictions of OSD, image with predictions of rOSD. White boxes are ground truth objects and red ones are our predictions. There are at most 5 predictions per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 :</head><label>3</label><figDesc>Regions returned by OSD and rOSD. In each column, from top to bottom: original image, image with regions returned by OSD, image with regions returned by rOSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Colocalization perfor-</figDesc><table><row><cell cols="3">mance with our proposals in dif-</cell></row><row><cell cols="3">ferent configurations of OSD</cell></row><row><cell>Config.</cell><cell>Confidence</cell><cell>Standout</cell></row><row><cell>OD</cell><cell cols="2">83.7 ± 0.4 89.0 ± 0.6</cell></row><row><cell cols="3">VOC 6x2 73.6 ± 0.6 64.1 ± 0.3</cell></row><row><cell cols="3">VOC all 44.7 ± 0.3 41.4 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Colocalization performance for different</figDesc><table><row><cell cols="3">values of hyper-parameters</cell><cell></cell><cell></cell></row><row><cell>(u, v)</cell><cell>(20,50)</cell><cell>(20,100)</cell><cell>(50,50)</cell><cell>(50,100)</cell></row><row><cell cols="5">CorLoc 73.6 ± 0.8 73.4 ± 0.7 73.3 ± 1.1 74.2 ± 0.8</cell></row><row><cell>p</cell><cell>760</cell><cell>882</cell><cell>1294</cell><cell>1507</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Single-object colocalization and discovery performance of OSD with different types of proposals. We use VGG16 features to represent regions in these experiments</figDesc><table><row><cell>Region proposals</cell><cell>OD</cell><cell>Colocalization VOC 6x2</cell><cell>VOC all</cell><cell>OD</cell><cell>Discovery VOC 6x2</cell><cell>VOC all</cell></row><row><cell>Edgeboxes [40]</cell><cell>81.6 ± 0.3</cell><cell>54.2 ± 0.3</cell><cell>29.7 ± 0.1</cell><cell>81.4 ± 0.3</cell><cell>55.2 ± 0.3</cell><cell>32.6 ± 0.1</cell></row><row><cell>Selective search [33]</cell><cell>82.2 ± 0.2</cell><cell>54.5 ± 0.3</cell><cell>30.9 ± 0.1</cell><cell>81.3 ± 0.3</cell><cell>57.8 ± 0.2</cell><cell>33.0 ± 0.1</cell></row><row><cell cols="2">Randomized Prim [23] 82.9 ± 0.3</cell><cell>71.5 ± 0.3</cell><cell>42.8 ± 0.1</cell><cell>82.5 ± 0.1</cell><cell>70.6 ± 0.4</cell><cell>44.5 ± 0.1</cell></row><row><cell>Ours (OSD)</cell><cell cols="5">89.0 ± 0.6 73.6 ± 0.6 44.7 ± 0.3 87.8 ± 0.4 69.2 ± 0.5</cell><cell>48.7 ± 0.3</cell></row><row><cell>Ours (rOSD)</cell><cell>89.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>0 ± 0.5 73.3 ± 0.5 45.8 ± 0.3 87.6 ± 0.3 71.1 ± 0.8 49.2 ± 0.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Single-object discovery performance on the datasets with our proposals compared to the state of the art</figDesc><table><row><cell>Method</cell><cell>Features</cell><cell>OD</cell><cell>VOC 6x2</cell><cell>VOC all</cell><cell>VOC12</cell></row><row><cell>Cho et al. [6]</cell><cell>WHO</cell><cell>82.2</cell><cell>55.9</cell><cell>37.6</cell><cell>-</cell></row><row><cell>Vo et al. [34]</cell><cell>WHO</cell><cell>82.3 ± 0.3</cell><cell>62.5 ± 0.6</cell><cell>40.7 ± 0.2</cell><cell>-</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG19</cell><cell>75.0</cell><cell>54.0</cell><cell>43.4</cell><cell>46.3</cell></row><row><cell>Ours (OSD)</cell><cell>VGG19</cell><cell>89.1 ± 0.4</cell><cell>71.9 ± 0.7</cell><cell>47.9 ± 0.3</cell><cell>49.2 ± 0.2</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG19</cell><cell>89.2 ± 0.4</cell><cell>72.5 ± 0.5</cell><cell>49.3 ± 0.2</cell><cell>51.2 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Multi-object colocalization and discovery performance of rOSD compared to competitors on VOC all and VOC12 datasets</figDesc><table><row><cell>Method</cell><cell>Features</cell><cell cols="2">Colocalization VOC all VOC12</cell><cell cols="2">Discovery VOC all VOC12</cell></row><row><cell>Vo et al. [34]</cell><cell>WHO</cell><cell>40.7 ± 0.1</cell><cell>-</cell><cell>30.7 ± 0.1</cell><cell>-</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG19</cell><cell>43.3</cell><cell>45.5</cell><cell>28.1</cell><cell>30.3</cell></row><row><cell>Ours (OSD)</cell><cell>VGG19</cell><cell>46.8 ± 0.1</cell><cell>47.9 ± 0.0</cell><cell>34.8 ± 0.0</cell><cell>36.8 ± 0.0</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG19</cell><cell>49.4 ± 0.1</cell><cell>51.5 ± 0.1</cell><cell>37.6 ± 0.1</cell><cell>40.4 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance of our large-scale algorithm compared to the baselines. Our method and baseline 1 have the same memory cost, which is much smaller than the cost of baseline 2 . Also, due to memory limits, we cannot run baseline 2 on COCO 20k</figDesc><table><row><cell>Method</cell><cell>VOC all</cell><cell>Single-object VOC12</cell><cell>COCO 20k</cell><cell>VOC all</cell><cell>Multi-object VOC12</cell><cell>COCO 20k</cell></row><row><cell>Baseline 1 (OSD)</cell><cell>41.1 ± 0.3</cell><cell>40.5 ± 0.2</cell><cell>43.6 ± 0.2</cell><cell>31.4 ± 0.1</cell><cell>32.4 ± 0.0</cell><cell>10.5 ± 0.0</cell></row><row><cell cols="2">Baseline 1 (rOSD) 42.8 ± 0.3</cell><cell>42.6 ± 0.2</cell><cell>44.5 ± 0.1</cell><cell>35.4 ± 0.2</cell><cell>37.2 ± 0.1</cell><cell>11.6 ± 0.0</cell></row><row><cell>Baseline 2 (OSD)</cell><cell>47.9 ± 0.3</cell><cell>49.2 ± 0.2</cell><cell>-</cell><cell>34.8 ± 0.0</cell><cell>36.8 ± 0.0</cell><cell>-</cell></row><row><cell cols="2">Baseline 2 (rOSD) 49.3 ± 0.2</cell><cell>51.2 ± 0.2</cell><cell>-</cell><cell>37.6 ± 0.1</cell><cell>40.4 ± 0.1</cell><cell>-</cell></row><row><cell>Large-scale OSD</cell><cell>45.5 ± 0.3</cell><cell>46.3 ± 0.2</cell><cell>46.9 ± 0.1</cell><cell>34.6 ± 0.0</cell><cell>36.9 ± 0.0</cell><cell>11.1 ± 0.0</cell></row><row><cell cols="7">Large-scale rOSD 49.4 ± 0.1 51.9 ± 0.1 48.5 ± 0.1 38.3 ± 0.0 41.2 ± 0.1 12.0 ± 0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>Influence of the ensemble method of Vo et al. on the colocalization performance of OSD and rOSD with our proposals</figDesc><table><row><cell>Method</cell><cell></cell><cell>OD</cell><cell>VOC 6x2</cell><cell>VOC all</cell><cell>VOC12</cell></row><row><cell>Ours (OSD)</cell><cell>w/o EM</cell><cell>89.0 ± 0.6</cell><cell>73.6 ± 0.6</cell><cell>44.7 ± 0.3</cell><cell>49.0 ± 0.2</cell></row><row><cell>Ours (OSD)</cell><cell>w/ EM</cell><cell>88.2 ± 0.2</cell><cell>75.3 ± 0.2</cell><cell>44.7 ± 0.1</cell><cell>48.7 ± 0.1</cell></row><row><cell>Ours (rOSD)</cell><cell>w/o EM</cell><cell>89.0 ± 0.5</cell><cell>73.3 ± 0.5</cell><cell>45.8 ± 0.3</cell><cell>49.7 ± 0.1</cell></row><row><cell>Ours (rOSD)</cell><cell>w/ EM</cell><cell>89.2 ± 0.3</cell><cell>74.5 ± 0.2</cell><cell>45.5 ± 0.1</cell><cell>49.7 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Influence of the ensemble method of Vo et al. on the single-object discovery performance of OSD and rOSD with our proposals</figDesc><table><row><cell>Method</cell><cell></cell><cell>OD</cell><cell>VOC 6x2</cell><cell>VOC all</cell><cell>VOC12</cell></row><row><cell>Ours (OSD)</cell><cell>w/o EM</cell><cell>87.8 ± 0.4</cell><cell>69.2 ± 0.5</cell><cell>48.7 ± 0.3</cell><cell>51.3 ± 0.2</cell></row><row><cell>Ours (OSD)</cell><cell>w/ EM</cell><cell>87.5 ± 0.3</cell><cell>70.9 ± 0.3</cell><cell>48.6 ± 0.1</cell><cell>50.7 ± 0.1</cell></row><row><cell>Ours (rOSD)</cell><cell>w/o EM</cell><cell>87.6 ± 0.3</cell><cell>71.1 ± 0.8</cell><cell>49.2 ± 0.2</cell><cell>52.1 ± 0.1</cell></row><row><cell>Ours (rOSD)</cell><cell>w/ EM</cell><cell>88.7 ± 0.3</cell><cell>71.9 ± 0.4</cell><cell>48.7 ± 0.1</cell><cell>52.0 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Single-object discovery performance in the mixed setting on the datasets with our proposals compared to the state of the art</figDesc><table><row><cell>Method</cell><cell>Features</cell><cell>OD</cell><cell>VOC 6x2</cell><cell>VOC all</cell><cell>VOC12</cell></row><row><cell>Cho et al. [6]</cell><cell>WHO</cell><cell>82.2</cell><cell>55.9</cell><cell>37.6</cell><cell>-</cell></row><row><cell>Vo et al. [34]</cell><cell>WHO</cell><cell>82.3 ± 0.3</cell><cell>62.5 ± 0.6</cell><cell>40.7 ± 0.2</cell><cell>-</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG16</cell><cell>73.5</cell><cell>66.2</cell><cell>41.9</cell><cell>45.0</cell></row><row><cell>Ours (OSD)</cell><cell>VGG16</cell><cell>87.8 ± 0.4</cell><cell>69.2 ± 0.5</cell><cell>48.7 ± 0.3</cell><cell>51.3 ± 0.2</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG16</cell><cell>87.6 ± 0.3</cell><cell>71.1 ± 0.8</cell><cell>49.2 ± 0.2</cell><cell>52.1 ± 0.1</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG19</cell><cell>75.0</cell><cell>54.0</cell><cell>43.4</cell><cell>46.3</cell></row><row><cell>Ours (OSD)</cell><cell>VGG19</cell><cell>89.1 ± 0.4</cell><cell>71.9 ± 0.7</cell><cell>47.9 ± 0.3</cell><cell>49.2 ± 0.2</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG19</cell><cell>89.2 ± 0.4</cell><cell>72.5 ± 0.5</cell><cell>49.3 ± 0.2</cell><cell>51.2 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Multi-object colocalization and discovery performance of rOSD compared to competitors on VOC all and VOC12 datasets</figDesc><table><row><cell>Method</cell><cell>Features</cell><cell cols="2">Colocalization VOC all VOC12</cell><cell cols="2">Discovery VOC all VOC12</cell></row><row><cell>Vo et al. [34]</cell><cell>WHO</cell><cell>40.7 ± 0.1</cell><cell>-</cell><cell>30.7 ± 0.1</cell><cell>-</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG16</cell><cell>38.3</cell><cell>40.4</cell><cell>25.8</cell><cell>28.2</cell></row><row><cell>Ours (OSD)</cell><cell>VGG16</cell><cell>45.9 ± 0.1</cell><cell>48.1 ± 0.0</cell><cell>34.9 ± 0.1</cell><cell>37.6 ± 0.0</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG16</cell><cell>48.5 ± 0.1</cell><cell>50.7 ± 0.1</cell><cell>37.2 ± 0.1</cell><cell>40.8 ± 0.1</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG19</cell><cell>43.3</cell><cell>45.5</cell><cell>28.1</cell><cell>30.3</cell></row><row><cell>Ours (OSD)</cell><cell>VGG19</cell><cell>46.8 ± 0.1</cell><cell>47.9 ± 0.0</cell><cell>34.8 ± 0.0</cell><cell>36.9 ± 0.0</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG19</cell><cell>49.4 ± 0.1</cell><cell>51.5 ± 0.1</cell><cell>37.6 ± 0.1</cell><cell>40.4 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Multi-object colocalization and discovery performance of rOSD compared to competitors on VOC all and VOC12 datasets when using smaller values of ν (25) and</figDesc><table><row><cell cols="2">IoU (0.3) threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Features</cell><cell cols="2">Colocalization VOC all VOC12</cell><cell cols="2">Discovery VOC all VOC12</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG19</cell><cell>43.1</cell><cell>45.3</cell><cell>27.8</cell><cell>30.0</cell></row><row><cell>Ours (OSD)</cell><cell>VGG19</cell><cell>39.6 ± 0.1</cell><cell>41.6 ± 0.1</cell><cell>29.0 ± 0.1</cell><cell>31.3 ± 0.1</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG19</cell><cell>47.3 ± 0.1</cell><cell>49.3 ± 0.1</cell><cell>36.7 ± 0.1</cell><cell>39.2 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Quality of the returned local image graph as measured by CorRet</figDesc><table><row><cell>Dataset</cell><cell>VOC all</cell><cell>VOC12</cell><cell>COCO 20k</cell></row><row><cell>Baseline</cell><cell>50.7</cell><cell>56.4</cell><cell>36.8</cell></row><row><cell>Ours (OSD)</cell><cell>60.1 ± 0.1</cell><cell>63.2 ± 0.0</cell><cell>39.8 ± 0.0</cell></row><row><cell>Ours (rOSD)</cell><cell>59.8 ± 0.1</cell><cell>63.0 ± 0.0</cell><cell>39.4 ± 0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Colocalization and single-object discovery performance of rOSD compared to OSD, Li et al. [21] and Wei et al. [35] on 6 held-out ImageNet classes</figDesc><table><row><cell>Method</cell><cell>Features</cell><cell>Colocalization</cell><cell>Discovery</cell></row><row><cell>Li et al. 5 [21]</cell><cell>VGG16</cell><cell>48.3</cell><cell>-</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG16</cell><cell>74.3</cell><cell>61.2</cell></row><row><cell>Ours (OSD)</cell><cell>VGG16</cell><cell>61.5 ± 0.3</cell><cell>60.3 ± 0.3</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG16</cell><cell>63.0 ± 0.7</cell><cell>61.6 ± 0.4</cell></row><row><cell>Li et al. [21]</cell><cell>VGG19</cell><cell>51.6</cell><cell>-</cell></row><row><cell>Wei et al. [35]</cell><cell>VGG19</cell><cell>74.8</cell><cell>63.2</cell></row><row><cell>Ours (OSD)</cell><cell>VGG19</cell><cell>61.3 ± 0.5</cell><cell>59.2 ± 0.7</cell></row><row><cell>Ours (rOSD)</cell><cell>VGG19</cell><cell>63.7 ± 0.3</cell><cell>59.4 ± 0.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since the analysis in this section applies to both OSD and rOSD, we refer to both as OSD for ease of notation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Numbers for<ref type="bibr" target="#b20">[21]</ref> are taken from<ref type="bibr" target="#b34">[35]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by the Inria/NYU collaboration, the Louis Vuitton/ENS chair on artificial intelligence and the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). Huy V. Vo was supported in part by a Valeo/Prairie CIFRE PhD Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregating deep convolutional features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning with submodular functions: A convex optimization perspective. Foundations and Trends in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Persistence-based clustering in riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Oudot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skraba</surname></persName>
		</author>
		<idno>41:1-41:38</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) (2015) 3, 7</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR) (2015) 3, 7</meeting>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Computational Topology: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>AMS Press</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topological persistence and simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Letscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete and Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>VOC2007) Results 7</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clustering by composition-unsupervised discovery of image categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of categories from sets of partially matching image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepco 3 : Deep instance co-segmentation by co-peak search and co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised cnn-based co-saliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised detection of regions of interest using iterative link analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems</title>
		<meeting>the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image co-localization by mimicking a good detector&apos;s confidence score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prime Object Proposals with Randomized Prim&apos;s Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV) (2013) 2, 4, 9</title>
		<meeting>the International Conference on Computer Vision (ICCV) (2013) 2, 4, 9</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximate primal solutions and rate analysis for dual subgradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nedić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Persistence Theory: From Quiver Representations to Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMS Surveys and Monographs</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems</title>
		<meeting>the Conference on Neural Information Processing Systems<address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using multiple segmentations to discover objects and their extent in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering object categories in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="176" to="191" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised image matching and object discovery as optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and co-localization by deep descriptor transforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A self-paced multipleinstance learning framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) (2014) 2, 3, 9</title>
		<meeting>the European Conference on Computer Vision (ECCV) (2014) 2, 3, 9</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
		<title level="m">Computing persistent homology. Discrete and Computational Geometry</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
