<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
							<email>huaishaoluo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
							<email>leiji@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the recent success of the pre-training technique for NLP and image-linguistic tasks, some video-linguistic pre-training works are gradually developed to improve video-text related downstream tasks. However, most of the existing multimodal models are pre-trained for understanding tasks, leading to a pretrainfinetune discrepancy for generation tasks. This paper proposes UniVL: a Unified Video and Language pre-training model for both multimodal understanding and generation. It comprises four components, including two singlemodal encoders, a cross encoder, and a decoder with the Transformer backbone. Five objectives, including video-text joint, conditioned masked language model (CMLM), conditioned masked frame model (CMFM), videotext alignment, and language reconstruction, are designed to train each of the components. We further develop two pre-training strategies, stage by stage pre-training (StagedP) and enhanced video representation (EnhancedV), to make the training process of the UniVL more effective. The pre-train is carried out on a sizeable instructional video dataset HowTo100M. Experimental results demonstrate that the UniVL can learn strong video-text representation and achieves state-of-the-art results on five downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent advances of self-supervised learning, pre-training techniques play a vital role in learning visual and language representation. The paradigm is to pre-train the model on a large scale unlabeled data and fine-tune the downstream tasks * This work was done during the first author's internship in MSR Asia  <ref type="figure">Figure 1</ref>: A showcase of video and language pre-train based model for multimodal understanding (e.g., retrieval) and generation (e.g., captioning).</p><p>using task-specific labeled data. Inspired by the BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> model's success for NLP tasks, numerous multimodal image-language pre-training models <ref type="bibr">Li et al., 2019a,b)</ref> have been proposed. Their results have demonstrated the effectiveness of pre-training on various visual and language tasks such as visual question answering. Different from previous text pre-training or image-language pre-training, we focus on video-linguistic pre-training in this paper.</p><p>Videos contain rich visual, acoustic, and language information for people to acquire knowledge or learn how to perform a task. This motivates researchers to investigate whether AI agents can learn task completion from videos like humans with both low-level visual and high-level semantic language signals. Therefore, multimodal videolanguage tasks are of great importance to investigate for both research and applications. In this work, we first propose to pre-train a unified videolanguage model using video and acoustic speech recognition (ASR) transcript in instructional videos to learn a joint representation of both video and language. Then, we fine-tune this model on five typical multimodal tasks, including understanding and generation targets. <ref type="figure">Figure 1</ref> presents a showcase of our pre-training and fine-tuning flow. Take multimodal video captioning as an example. The model inputs video and ASR transcript and predicts a captioning sentence.</p><p>VideoBERT <ref type="bibr" target="#b37">(Sun et al., 2019b)</ref> and CBT <ref type="bibr" target="#b36">(Sun et al., 2019a)</ref> are the first pioneers to investigate video-language pre-training with regard to video representation on instructional videos. They have demonstrated the effectiveness of the BERT based model for capturing video temporal and language sequential features. Besides the above two works, there is some concurrent progress to our model. ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref> leverages global action information to catalyze mutual interactions between linguistic texts and local regional objects. Moreover, a transformer block is introduced to encode global actions, local regional objects, and linguistic descriptions. HERO  hierarchically encodes multimodal inputs. Furthermore, two new pre-training tasks, video-subtitle matching and frame order modeling, are designed to improve the representation learning. VideoAsMT <ref type="bibr" target="#b11">(Korbar et al., 2020)</ref> takes a generative modeling approach that poses the objective as a translation problem between modalities.</p><p>However, most of previous models only pre-train the model on understanding tasks. In this paper, we pre-train on both understanding and generation tasks through an encoder-decoder paradigm. Although the concurrent work VideoAsMT has a similar encoder-decoder as ours, it is not flexible for downstream tasks with only one single unified framework. In this paper, we develop a flexible approach to learn video and language joint representation and adapt downstream multimodal tasks.</p><p>We propose UniVL: a Unified Video and Language pre-training model for multimodal understanding and generation. Our UniVL model adopts Transformer <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> as the backbone and has four components, including two single-modal encoders, a cross encoder, and a decoder. In detail, we first encode the text and visual separately by two single-modal encoders. A videotext joint objective performs on these two encoders, which aims to learn better representation for each modality before fusing them. Such a two-stream de-sign is natural to retrieval tasks due to its scalability to very large datasets. The proposed representation can be indexed and has linear complexity in the number of videos. Then we adopt the Transformer based encoder-decoder model to perform the understanding and generation pre-training by four tasks: conditioned masked language model (CMLM for language corruption), conditioned masked frame model (CMFM for video corruption), video-text alignment, and language reconstruction. Furthermore, we design two pre-training strategies, including stage by stage pre-training strategy (StagedP) and Enhanced video representation (En-hancedV), to promote the UniVL pre-training. The StagedP has two parts in our setting. We only pretrain the text encoder and video encoder by the video-text joint objective for the first stage. Then all modules will be pre-trained under the whole objectives in the second stage. Besides, we adopt an entire masking strategy EnhancedV on text to enhance video representation.</p><p>Our contributions are summarized as follows: 1) We propose a multimodal video-language pretraining model trained on a large-scale instructional video dataset. It is a flexible model for both videolanguage understanding and generation tasks.</p><p>2) The pre-training consists of five objectives, including video-text joint, conditioned masked language model, conditioned masked frame model, video-text alignment, and language reconstruction. Two pre-training strategies are proposed to make these objectives work harmoniously.</p><p>3) We fine-tune our pre-trained model on five typical multimodal video-language tasks: text-based video retrieval, multimodal video captioning, action segmentation, action step localization, and multimodal sentiment analysis. Extensive experiments demonstrate our model's effectiveness on downstream tasks and achieve state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single Modal Pre-Training</head><p>Self-supervised representation learning has been shown to be effective for sequential data, including language and video. Language pre-training models, including BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, GPT <ref type="bibr" target="#b27">(Radford et al., 2018)</ref>, RoBERTa , XLNet , MASS <ref type="bibr" target="#b33">(Song et al., 2019)</ref>, UniLM <ref type="bibr" target="#b4">(Dong et al., 2019)</ref>, and BART   on NLP tasks. BERT <ref type="bibr" target="#b2">(Devlin et al., 2019</ref>) is a denoising auto-encoder network using Transformer with MLM (masked language model) and NSP (next sentence prediction) as pre-training tasks. It has a strong performance for understanding tasks. MASS <ref type="bibr" target="#b33">(Song et al., 2019)</ref> focuses on pre-training for generation tasks. UniLM <ref type="bibr" target="#b4">(Dong et al., 2019)</ref> and BART  continuously study a unified pre-training model for both understanding and generation tasks. Video representation learning mostly focuses on the video sequence reconstruction or future frames prediction as pre-training (pretext) tasks. Early works like <ref type="bibr" target="#b22">(Mathieu et al., 2015;</ref><ref type="bibr" target="#b34">Srivastava et al., 2015;</ref><ref type="bibr" target="#b5">Han et al., 2019)</ref> aim to synthetic video frames through the image patches. Similarly, <ref type="bibr" target="#b44">Wang and Gupta (2015)</ref> adopt a siamese-triplet network to rank continuous patches more similar than patches of different videos. Other works predict the feature vectors in latent space using auto-regressive models with the noise-contrastive estimation (NCE) <ref type="bibr" target="#b20">(Lotter et al., 2016;</ref><ref type="bibr" target="#b25">Oord et al., 2018)</ref>. <ref type="bibr" target="#b36">Sun et al. (2019a)</ref> adopt NCE to predict corrupted (masked) latent space using the auto-encoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Pre-Training</head><p>Recently, numerous visual-linguistic pre-training models are proposed for multimodal tasks. For image and text pre-training, ViLBERT , LXMERT <ref type="bibr" target="#b38">(Tan and Bansal, 2019)</ref> adopt two separate Transformers for image and text encoding independently. Other models like Visualbert <ref type="bibr" target="#b16">(Li et al., 2019b)</ref>, Unicoder-VL <ref type="bibr" target="#b14">(Li et al., 2019a)</ref>, VL-BERT <ref type="bibr" target="#b35">(Su et al., 2020)</ref>, UNITER  use one shared BERT model. These models employ MLM and image-text matching as pretraining tasks which are effective for downstream multimodal tasks. VLP  proposes a unified image-language model for under-standing and generation tasks. Different from these works, we focus on video and text pre-training for universal representation.</p><p>For video and text pre-training, VideoBERT <ref type="bibr" target="#b37">(Sun et al., 2019b)</ref> and CBT <ref type="bibr" target="#b36">(Sun et al., 2019a)</ref> are the first works to explore the capability of pretraining models. Although VideoBERT and CBT pre-train the model on multimodal data, the downstream tasks mainly take video representation for further prediction. ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref> leverages global action information to catalyze mutual interactions between linguistic texts and local regional objects, and introduces a transformer block to encode global actions, local regional objects, and linguistic descriptions. HERO  encodes multimodal inputs in a hierarchical fashion. Besides, two new pre-training tasks, video-subtitle matching and frame order modeling, are designed to improve the representation learning. However, ActBERT and HERO are only pre-train the models on understanding tasks. VideoAsMT <ref type="bibr" target="#b11">(Korbar et al., 2020)</ref> takes a generative modeling approach that poses the objective as a translation problem between modalities. The difference between our work with VideoAsMT is that our model contains two more separate encoders instead of one unified encoder-decoder, while VideoAsMT is inflexible for downstream tasks due to one single unified framework.</p><p>We summarize three pre-training paradigms to cover the previous vision-text pre-training model considering different encoder architecture in literature, as presented in <ref type="figure" target="#fig_0">Figure 2</ref>. Unicoder-VL <ref type="bibr" target="#b14">(Li et al., 2019a)</ref>, VL-BERT <ref type="bibr" target="#b35">(Su et al., 2020)</ref>, UNITER , VLP , VideoBERT <ref type="bibr" target="#b37">(Sun et al., 2019b)</ref>, ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref>, and VideoAsMT <ref type="bibr" target="#b11">(Korbar et al., 2020)</ref> belong to share-type in <ref type="figure" target="#fig_0">Figure 2</ref>(a), where the text and vision sequences are combined as the input of one shared Transformer encoder. ViLBERT  and LXMERT <ref type="bibr" target="#b38">(Tan and Bansal, 2019)</ref> are cross-type shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b). CBT <ref type="bibr" target="#b36">(Sun et al., 2019a)</ref> and HERO  are joint-type shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c). The crosstype and joint-type architectures have two-stream input, and the difference is the interaction across both modalities. Compared with the single-stream input in the share-type, the two-stream input can accommodate each modality's different processing needs and interact at varying representation depths . Besides, the joint-type structure has one cross-modal encoder for full interaction between the two streams comparing with the crosstype. We adopt the joint-type as our encoder in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The problem is defined as: given the input video and the corresponding ASR transcript pairs, pretrain a model to learn joint video and text representation with the self-supervision approach, and fine-tune downstream tasks. In this section, we describe the architecture and pre-training tasks in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Figure 3 presents the UniVL as an encoder-decoder architecture. First, the model extracts representations of the input text tokens and the video frame sequences using various feature extractors. A text encoder then adopts the BERT model to embed the text, and a video encoder utilizes the Transformer encoder to embed the video frames. Next, we employ a Transformer based cross encoder for interacting between the text and the video. Finally, a Transformer decoder is used to reconstruct the input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pre-processing.</head><p>We first pre-process video and language before feeding to the UniVL. For the input text, we tokenize all words by WordPieces <ref type="bibr" target="#b46">(Wu et al., 2016)</ref> following the pre-processing method in BERT to obtain the token sequence t = t i |i ∈ [1, n] , where t i is the i-th token and n is the length of the token sequence. For each video clip, we sample a frame sequence v = v j |j ∈ [1, m] and adopt them to extract features, where v j is the j-th group of video frames and m is the group length of the frame sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Single Modal Encoders.</head><p>We encode the text and video separately. Such a two-stream design has two advantages: module reusing and retrieval orienting. The module reusing means the text module can benefit from the existing text-based pretrained model, e.g., BERT. The retrieval orienting means the two-stream design is natural to retrieval tasks due to its scalability to extensive datasets. The extracted representation can be indexed, and the calculation of similarity has linear complexity in the number of videos. In this paper, we adopt the BERT-Base uncased model to generate the text representation T ∈ R n×d after feeding the token sequence t,</p><formula xml:id="formula_0">T = BERT(t),<label>(1)</label></formula><p>where d is the hidden size of text representation.</p><p>For the video frame sequence v, we adopt the off-the-shelf image feature extractors, e.g., S3D <ref type="bibr" target="#b47">(Xie et al., 2018)</ref></p><formula xml:id="formula_1">, to generate video feature F v ∈ R m×d f v , where d f v is the hidden size.</formula><p>A Transformer encoder is utilized to embed the contextual information of video as follows,</p><formula xml:id="formula_2">V = Transformer(F v ).</formula><p>(2)</p><p>The dimension of V is R m×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Cross Encoder.</head><p>The text encoder and video encoder mainly focus on individual modality. To make the text and video fully interact, we design across encoder, which takes both the text and video modality features as input. Specifically, we first combine the text encoding T and the video encoding V to get the encoding M ∈ R (n+m)×d . Then, a Transformer encoder takes the encoding M as input to generate the attended encoding M ∈ R (n+m)×d ,</p><formula xml:id="formula_3">M = Transformer([T; V]),<label>(3)</label></formula><p>where [; ] denotes the combination operation. It is noted that the combination is operated along with the dimension of sequence, not the dimension of hidden size. One reason is that the text length n and video clip length m are always different. Another reason is that the semantic between text and video are not absolutely aligned. People are likely to describe an event after or before performing it in the video <ref type="bibr" target="#b23">(Miech et al., 2020)</ref>.  <ref type="figure">Figure 3</ref>: The main structure of our UniVL, which comprises four components, including two single-modal encoders, a cross encoder, and a decoder. The model is flexible for many text and video downstream tasks. Four possible tasks are listed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Decoder.</head><p>We empower our pre-trained model to have the capability of learning from and then benefiting for generation tasks by attaching a decoder, which is usually a unidirectional recurrent/attention model to generate tokens one by one. Such a decoder module is proved useful in text-based pre-training tasks, e.g., T5 <ref type="bibr" target="#b28">(Raffel et al., 2019)</ref> and BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref>. It is noted that the decoder has a different target at different phases. The decoder learns to reconstruct the input text (e.g., transcripts) during pre-training because of no available text label. When fine-tuning, the decoder is used to generate results, e.g., video caption, where inputs transcripts and video and outputs caption. The input is the attended encoding M of text and video. We unexceptionally exploit Transformer to get the decoded feature D ∈ R l×d from M,</p><formula xml:id="formula_4">D = Transformer(M),<label>(4)</label></formula><p>where l is the decoder length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training Objectives</head><p>We have five pre-training objectives: 1) videotext joint, 2) conditioned masked language model (for text corruption), 3) conditioned masked frame model (for video corruption), 4) video-text alignment, and 5) language reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Video-Text Joint.</head><p>As our text encoder, the BERT-Base uncased model is a robust extractor of text representation. So, we utilize a video-text joint objective to enhance the capability of the video encoder. It seems a retrieval orienting operation, which is to align the space of representation between text and video. Considering the misalignment between the text and video clip in narrated videos, we adopt MIL-NCE <ref type="bibr" target="#b23">(Miech et al., 2020)</ref> on T and V as our joint objective,</p><formula xml:id="formula_5">L Joint (θ) = −E (t,v)∼B log MIL-NCE (t, v) ,<label>(5)</label></formula><formula xml:id="formula_6">MIL-NCE (t, v) = (v,t)∈P v,t exp(vt ) Z ,<label>(6)</label></formula><formula xml:id="formula_7">Z = (v,t)∈P v,t exp(vt ) + (ṽ,t)∈N v,t exp(ṽt ),<label>(7)</label></formula><p>where P v,t is a set of positive video-transcript pairs. E.g., {(v, t), (v, t −1 ), (v, t +1 )}, where t −1 and t +1 are two closest transcripts in time to t. The negative pairs N v,t take negative transcripts (or video clips) from other instances within the batch B after fixing v (or t).v,ṽ andt,t are generated through mean-pooling on V and T, respectively. θ is the trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">CMLM: Conditioned Masked</head><p>Language Model.</p><p>Following BERT, we also randomly mask 15% tokens with the special token [MASK] in the sentence and re-produce the masked tokens under the condition of video input and known tokens. This loss function is defined on the feature matrix of the text part in M as:</p><formula xml:id="formula_8">L CM LM (θ) = −E tm∼t log P θ (t m | t ¬m , v) ,<label>(8)</label></formula><p>where t ¬m means the contextual tokens surrounding the masked token t m , θ is the trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">CMFM: Conditioned Masked Frame</head><p>Model.</p><p>Similarly, we also propose a masked frame model to predict the correct frames given contextual frames and the input text for semantic constraints. However, it is hard to reconstruct the original RGB frame. We adopt the contrastive learning method to maximize the MI (Mutual information) between the masked output features and the original features. This loss function is NCE <ref type="bibr" target="#b36">(Sun et al., 2019a)</ref>. We randomly mask 15% vectors (also 15% frames) with zeros. The objective is to identify the correct frame compared to negative distractors. The loss is defined as:</p><formula xml:id="formula_9">L CM F M (θ) = −E vm∼v log NCE (v m | v ¬m , t) ,<label>(9)</label></formula><formula xml:id="formula_10">NCE (v m | v ¬m , t) = exp(f vm m vm ) Z ,<label>(10)</label></formula><formula xml:id="formula_11">Z = exp(f vm m vm ) + v j ∈N (vm) exp(f vm m v j ),<label>(11)</label></formula><p>where v ¬m means the surrounding frames except v m , f vm ∈ R 1×d is a linear output of f v vm ∈ F v , F v is the real-valued vectors of video features, m vm ∈ M (v) , and M (v) is the feature matrix of the video part in M. We take other frames in the same batch as negative cases defined as N (v m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Video-Text Alignment.</head><p>We use the fused representation that corresponds to the special token [CLS] to predict scores for the video-text alignment, which is similar to the BERT sentence pair classification task. We adopt the NCE loss to learn to discriminate against the positive from negative video-text pairs. To enhance this capability, we not only randomly sample negative cases but also re-sample video clips from the same video <ref type="bibr" target="#b5">(Han et al., 2019)</ref>. The reason is that the frames inside the same video are more similar than frames of different videos. This loss function is defined as follows,</p><formula xml:id="formula_12">L Align (θ) = −E (t,v)∼B log exp s(t, v) Z ,<label>(12)</label></formula><formula xml:id="formula_13">Z = exp s(t, v) + u∈N (v) exp s(t, u) ,<label>(13)</label></formula><p>where s(·) means two linear layers with a T anh activation function between them, which is performed on the first hidden state of M. We take other video clips in the same batch B as negative cases N (v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Language Reconstruction.</head><p>To reconstruct the input sentence to endow the pretrained model with the generation capability, we employed an auto-regressive decoder with reconstruction objective, and the loss function is,</p><formula xml:id="formula_14">L Decoder (θ) = −Et i ∼t log P θ t i |t &lt;i , t, v .<label>(14)</label></formula><p>It is noted that t is the masked version of groundtruth textt when pre-training. As shown in BART , pre-training decoder benefits generation tasks.</p><p>We jointly optimize our model by a weighted loss:</p><formula xml:id="formula_15">L U niV L =L Joint + L CM LM + L CM F M + L Align + L Decoder .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training Strategies</head><p>We develop two pre-training strategies to train the UniVL model effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">StagedP: Stage by Stage Pre-training.</head><p>The UniVL can benefit from the pre-trained BERT-Base uncased model in the text encoder module. The natural idea is to train a peer to peer video encoder as the BERT-Base. We adopt a two-stage training fashion. For the first stage, we only preserve the text BERT and video Transformer to learn the weights using the Video-Text Joint loss Eq. (5). Next, we decrease the learning rate and continue to further pre-train the UniVL by all five objectives. One advantage is to fasten the pre-training speed, and the other advantage is to make the pre-training progress more smoothing on weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">EnhancedV: Enhanced Video</head><p>Representation.</p><p>To further enhance the video representation, we adopt a masked modality strategy to make the video to generate transcripts without text input. Specifically, we mask the whole text tokens with a 15% possibility. In other words, there are 15% textvideo pairs with entire text tokens masked in each mini-batch, and the model utilizes the video information to complete generation. Such a strategy is a more challenging task for the model to learn a better video representation.</p><p>We first pre-train our model on the large scale dataset. We download videos with ASR transcripts from Howto100M dataset <ref type="bibr" target="#b24">(Miech et al., 2019)</ref> 1 . After filtering the unavailable ones, we get 1.2M videos for pre-training our model. On average, the duration of each video is 6.5 minutes with 110 clip-text pairs. Then, we fine-tune our pre-trained model on five diverse downstream tasks using five datasets, including text-based video retrieval, multimodal video captioning, action segmentation, action step localization, and multimodal sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Youcook2</head><p>Youcook2 <ref type="bibr" target="#b59">(Zhou et al., 2018a)</ref> contains 2,000 cooking videos on 89 recipes with 14K video clips. The overall duration is 176 hours (5.26 minutes on average). Each video clip is annotated with one captioning sentence. We evaluate both text-based video retrieval and multimodal video captioning task on this dataset.</p><p>For the text-based video retrieval task, we follow the same experimental setting in <ref type="bibr" target="#b24">(Miech et al., 2019)</ref>, and use the captions as the input text queries to find the corresponding video clips. For the video captioning task, we use the same setting as in . We filter the data and make sure there is no overlap between pre-training and evaluation data. In all, we have 1,261 training videos and 439 test videos, that is, 9,776 training clip-text pairs and 3,369 test clip-text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">MSR-VTT</head><p>MSR-VTT <ref type="bibr" target="#b48">(Xu et al., 2016)</ref> is the open-domain dataset for video retrieval tasks. It has open domain video clips, and each clip has 20 captioning sentences labeled by human. In all, there are 200K clip-text pairs from 10K videos in 20 categories including sports, music, etc. Following JSFusion <ref type="bibr" target="#b50">(Yu et al., 2018)</ref>, we randomly sampled 1,000 clip-text pairs as test data to evaluate the performance of our model on text-based video retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">COIN</head><p>COIN <ref type="bibr" target="#b39">(Tang et al., 2019)</ref> is to evaluate action segmentation task, which contains 180 different tasks and 11,827 videos. Each video is labeled with 3.91 1 https://www.di.ens.fr/willow/research/howto100m/ step segments. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">CrossTask</head><p>CrossTask  is to evaluate the action step localization task. It contains 83 different tasks and 4.7k videos. For each task, an ordered list of steps with manual descriptions are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">CMU-MOSI</head><p>Multimodal Opinion Sentiment and Emotion Intensity <ref type="bibr" target="#b57">(Zadeh et al., 2016)</ref> is sentence-level sentiment analysis and emotion recognition in online videos. CMU-MOSI contains 2,199 opinion video clips, each annotated with real-valued sentiment intensity annotations in the range <ref type="bibr">[-3, +3]</ref>. We evaluate the performance of our model on multimodal sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Details</head><p>For text encoding, we apply WordPiece embeddings <ref type="bibr" target="#b46">(Wu et al., 2016)</ref> with a 30,000 token vocabulary to input to BERT model. We exploit the BERT-base model <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> with 12 layers of Transformer blocks. Each block has 12 attention heads and the hidden size is 768.</p><p>For video encoding, we first extract the 3D feature from video clips using the S3D model pretrained by <ref type="bibr" target="#b23">Miech et al. (2020)</ref>. The basic visual feature can significantly affect the results from our preliminary experiments. The fps of the 3D feature extractor is 16 and the dimension is 1,024. We then employ Transformer Encoder with 6 layers to capture the sequential information on the 3D feature. Each block has 12 attention heads and the hidden size is 768.</p><p>The model consumes the clip-text pairs. The maximal input tokens of text is 32 and the maximal number of video features is 48. For short sentence and clip, we concatenate contextual tokens and frames. For cross encoder and decoder, we use a 2 layers Transformer Encoder as the encoder and a 3 layer Transformer Decoder as the decoder with 12 heads and 768 hidden size. For generation task during the inference stage, we use the beam search with the size of 5. As previously mentioned, the generated sequence is the groundtruth input transcripts in the pre-training phase. Its target is to sequentially learn full information from the masked transcripts and video features.</p><p>We pre-train our model on 8 NVIDIA Tesla V100 GPUs. There are two sets of hyper-  <ref type="bibr" target="#b10">(Klein et al., 2015)</ref> 4.6 14.3 21.6 75 HowTo100M <ref type="bibr">(Miech et al., 2019) 8.2 24.5 35.3</ref> 24 MIL-NCE <ref type="bibr" target="#b23">(Miech et al., 2020)</ref> 15.1 38.0 51.2 10 ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref> 9.6 26.7 38.0 19 VideoAsMT <ref type="bibr" target="#b11">(Korbar et al., 2020)</ref>   <ref type="bibr" target="#b9">(Kiros et al., 2014)</ref> 3.8 12.7 17.1 66 SNUVL <ref type="bibr" target="#b51">(Yu et al., 2016)</ref> 3.5 15.9 23.8 44 <ref type="bibr" target="#b7">Kaufman et al. (2017)</ref> 4.7 16.6 24.1 41 CT-SAN <ref type="bibr" target="#b52">(Yu et al., 2017)</ref> 4.4 16.6 22.3 35 JSFusion <ref type="bibr" target="#b50">(Yu et al., 2018)</ref> 10.2 31.2 43.2 13 HowTo100M <ref type="bibr" target="#b24">(Miech et al., 2019)</ref> 14.9 40.2 52.8 9 MIL-NCE <ref type="bibr" target="#b23">(Miech et al., 2020)</ref> 9.9 24.0 32.4 29.5 ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref> 8.6 23.4 33.1 36 VideoAsMT <ref type="bibr" target="#b11">(Korbar et al., 2020)</ref>   parameters considering the stage by stage pretraining strategy. In the first stage, the batch size is set to 600 and the model is trained 50 epochs for 1.5 days. In the second stage, the batch size is set to 48 and the model is trained 50 epochs for 12 days. We use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 1e-3 in the first stage and 1e-4 in the second stage, and employ a linear decay learning rate schedule with a warm-up strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Text-based Video Retrieval.</head><p>Text-based video retrieval is defined to retrieve a relevant video/clip given an input text query. As shown in <ref type="figure">Figure 3 (retrieval block)</ref>, the model encodes the input text query and candidate video clips through the text encoder and video encoder respectively. Then calculate the matching scores using two different approaches: one is UniVL (FT-Joint), which calculates the score through dot product as in Eq. (6), and use L Joint as the loss during the finetuning stage; the other is UniVL (FT-Align), which feeds the encodings to both single encoders and the cross encoder to get unified representation and predict the match score through s(·) in Eq. (12) on the first token '[CLS]'. During the fine-tuning stage, the loss is L Align . We use the Adam optimizer with an initial learning rate of 3e-5 and a batch size of 32 video-caption pairs for Youcook2, an initial learning rate of 5e-5 and a batch size of 128 videocaption pairs for MSR-VTT as hyper-parameters to fine-tune for 5 epochs.</p><p>We fine-tune our pre-trained model for textbased video retrieval task on both Youcook2 and MSR-VTT datasets. The evaluation metrics are Re-call@n (R@n) and Median R. Tables 1 and 2 list the retrieval results of all baselines and our model on Youcook2 and MSR-VTT separately. We can see that our model achieves the best performance  over all baselines to a large extent. We present several baseline methods with or without pretraining. Our model outperforms the Howto100M and VideoAsMT models pre-trained on the same dataset on all metrics. Besides, the experimental results present the a large performance gain with pre-training.</p><p>We also notice that UniVL (FT-Align) performs better than UniVL (FT-Joint), which demonstrates that fusion representation generated by the cross encoder is better. Nevertheless, the UniVL (FT-Joint) inference speed is 50 times for Youcook2 and 10 times for MSR-VTT faster than that of the UniVL (FT-Align). Therefore, it is a trade-off between performance and efficiency in practical applications. In the following ablation experiment, we exploit UniVL (FT-Joint) in the retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Multimodal Video Captioning.</head><p>Multimodal video captioning aims to generate a sequence of descriptive sentences. As shown in <ref type="figure">Figure 3</ref> (caption block), the model encodes the input video frames as well as transcripts inside the clips through the video encoder and text encoder respectively, then feeds the encodings to the cross encoder to get unified representation, and finally generates token sequence by the decoder. We use L Decoder as the loss during the fine-tuning stage. The hyper-parameters are an initial learning rate of 3e-5, a batch size of 32 samples, and fine-tune for 5 epochs. <ref type="table" target="#tab_8">Table 3</ref> lists the caption results of all baselines and our models on Youcook2. This generation task adopts the corpus-level generation evaluation met-ric using the pen-source tool 2 , including BLEU (BLEU-3, B-3; BLEU-4, B-4) <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref>, METEOR (M) <ref type="bibr" target="#b1">(Banerjee and Lavie, 2005)</ref>, ROUGE-L (R-L) <ref type="bibr" target="#b18">(Lin and Och, 2004)</ref>, and CIDEr <ref type="bibr" target="#b43">(Vedantam et al., 2015)</ref>. We compare our pretrained model with several baseline methods. We classify the methods with the setting that the input is video-only or video+transcript. <ref type="bibr" target="#b59">Zhou et al. (2018a)</ref> propose an end-to-end model for both procedural segmentation and captioning. <ref type="bibr">Sun et al. (2019b,a)</ref>; <ref type="bibr" target="#b61">Zhu and Yang (2020)</ref>; <ref type="bibr" target="#b11">Korbar et al. (2020)</ref> adopt the pre-training strategy and evaluate the captioning with the only video as input.  and <ref type="bibr" target="#b6">Hessel et al. (2019)</ref> discuss the multimodal input with both video and transcript. Our pre-trained model achieves state-of-the-art results and outperforms the existing pre-trained models, even only considering video as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Action Segmentation.</head><p>We fine-tune our pre-train model on action segmentation task using COIN dataset, which is to predict one pre-defined label for each frame of the given video. As shown in <ref type="figure">Figure 3</ref> (action tasks block), the model encodes the input video frames through the video encoder, followed by a linear classifier upon the output encodings for frame labeling. We do not use the text encoder due to no text description in the dataset. The evaluation metric is frame-wise accuracy (FA). The hyper-parameters are an initial learning rate of 3e-5, a batch size of 32 samples, and fine-tune for 5 epochs. The results are shown in <ref type="table" target="#tab_9">Table 4</ref>. The UniVL significantly outperforms the baselines with more than 14% im-Methods Frame Accuracy (%) NN-Viterbi <ref type="bibr" target="#b30">(Richard et al., 2018)</ref> 21.17 VGG <ref type="bibr" target="#b32">(Simonyan and Zisserman, 2014)</ref> 25.79 TCFPN-ISBA <ref type="bibr" target="#b3">(Ding and Xu, 2018)</ref> 34.30 CBT <ref type="bibr" target="#b36">(Sun et al., 2019a)</ref> 53.90 MIL-NCE <ref type="bibr" target="#b23">(Miech et al., 2020)</ref> 61.00 ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref> 56.95</p><p>UniVL 70.02   22.4 Supervised  31.6 HowTo100M <ref type="bibr" target="#b24">(Miech et al., 2019)</ref> 33.6 MIL-NCE <ref type="bibr" target="#b23">(Miech et al., 2020)</ref> 40.5 ActBERT <ref type="bibr" target="#b61">(Zhu and Yang, 2020)</ref> 41.4</p><p>UniVL 42.0 provements. It shows that the pre-trained UniVL actually learns a good visual representation, even absent of linguistic descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Action</head><p>Step Localization.</p><p>We evaluate the action step localization on CrossTask dataset. As shown in <ref type="figure">Figure 3</ref> (action tasks block), the model encodes the step description (action) and video clip through the text encoder and the video encoder respectively. And then calculate the relevance scores through dot product similar to the retrieval task. To fairly compare to <ref type="bibr" target="#b24">(Miech et al., 2019</ref><ref type="bibr" target="#b23">(Miech et al., , 2020</ref><ref type="bibr" target="#b61">Zhu and Yang, 2020)</ref>, we do not fine-tune on the CrossTask dataset. We perform the evaluation protocol by reporting the average recall (CTR) metric for the localization task 3 . The results are shown in <ref type="table" target="#tab_10">Table 5</ref>. Our results are even better than the supervised baseline, which demonstrates our UniVL model can learn better joint text-video representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Multimodal Sentiment Analysis.</head><p>We evaluate the multimodal sentiment analysis on CMU-MOSI dataset, the goal of which is to identify the sentiment of speaker based on the speakers display of verbal and nonverbal behaviors. We 3 The result is generated following the evaluation process of official project: https://github.com/DmZhukov/CrossTask employ video and corresponding transcripts to accomplish this task. As shown in <ref type="figure">Figure 3</ref> (multimodal classification block), the model encodes the input video frames as well as transcripts inside the clips through the video encoder and text encoder, respectively. Then feeds the encodings to the cross encoder to get unified representation, and finally predicts the sentiment score by a linear on the first token '[CLS]'. The hyper-parameters are an initial learning rate of 1e-5, a batch size 32, and fine-tune for 3 epochs.</p><p>The results are shown in <ref type="table" target="#tab_12">Table 6</ref>. Following , the evaluation metrics are binary accuracy (BA), F1 score, Mean-Absolute Error (MAE), and Pearson Correlation Coefficient (Corr). Compared with the baseline using video, transcript, and audio inputs, our model trained with video and language still achieves the best results without audio information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We analyze the effectiveness of our model design on pre-training objectives and strategies through ablation studies over text-based video retrieval and multimodal video captioning tasks. We also discuss the effectiveness of various visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BA F1 MAE Corr</head><p>MV-LSTM (Rajagopalan et al., 2016) 73.9/-74.0/-1.019 0.601 TFN <ref type="bibr" target="#b53">(Zadeh et al., 2017)</ref> 73.9/ 73.4/-1.040 0.633 MARN <ref type="bibr" target="#b55">(Zadeh et al., 2018b)</ref> 77.1/ 77.0/-0.968 0.625 MFN <ref type="bibr">(Zadeh et al., 2018a)</ref> 77.4/ 77.3/-0.965 0.632 <ref type="bibr">RMFN (Liang et al., 2018)</ref> 78.4/ 78.0/-0.922 0.681 RAVEN  78.0/ -/-0.915 0.691 MulT <ref type="bibr" target="#b41">(Tsai et al., 2019)</ref> /83.0 -/82.8 0.870 0.698 FMT  81       <ref type="table" target="#tab_13">Table 7</ref> shows the effectiveness of each objective or strategy on the retrieval task. The results are reported on both Youcook2 and MSR-VTT datasets. Simultaneously, <ref type="table" target="#tab_14">Table 8</ref> demonstrates the effectiveness of each objective or strategy on the caption task. For the retrieval task, we exploit UniVL (FT-Joint) fine-tuning strategy to study the objectives: Joint loss, Alignment loss, and Decoder loss, and the strategies: StagedP and EnhancedV show consistent improvement. From the result, we can see that the cross encoder and decoder modules can promote the joint representation of video and text. For the caption task, we find that the decoder module shows great advantage and achieves more than 3 points gain on the BLUE-4 metric. Another finding is that the Joint loss decreases the generation task a little, although it performs well in the retrieval task. Excessive emphasis on coarse-grained matching can affect the fine-grained description at the generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Visual Features.</head><p>We compare the S3D video feature pre-trained on Howto100M and ResNet-152 plus ResNeXt-101 pre-trained on labeled ImageNet and Kinetics respectively. The ResNet-152 (RS152) and ResNeXt-101 (RX101) are used to extract 2D and 3D features from video clips respectively similar to Miech et al.</p><p>(2019)'s work. As shown in <ref type="table" target="#tab_15">Table 9</ref> and <ref type="table" target="#tab_16">Table 10</ref>, the visual feature is important in our pre-training model and the downstream tasks. It is worth studying an end to end training from raw videos instead of extracted fixed video features in the future. However, the time-cost and the memory-cost are enormous. The key bottleneck is visual representation, and we propose two possible approaches: designing a lightweight training scheme, e.g., training on keyframes of video, using a small feature dimension size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>This paper proposes UniVL with self-supervised learning for video and language representation on large scale videos. The UniVL is designed with four modules and five objectives for both videolanguage understanding and generation tasks. It is a flexible model for most of the multimodal downstream tasks considering both efficiency and effectiveness. We conduct extensive experiments on evaluating our model for five downstream tasks, e.g., text-based video retrieval and multimodal video captioning. The experimental results demonstrate that our pre-trained model can improve the performance to a large extent over the baseline models and achieve state-of-the-art results on five typical multimodal tasks. Besides, we will investigate our model's performance on more massive datasets and more downstream tasks for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Various paradigms for multimodal pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, have achieved great success</figDesc><table><row><cell></cell><cell>Cross</cell><cell>Cross</cell><cell cols="2">Cross-modal</cell></row><row><cell></cell><cell>Encoder</cell><cell>Encoder</cell><cell>Encoder</cell><cell></cell></row><row><cell>Encoder (Text &amp; Vision)</cell><cell>Encoder (Text)</cell><cell>Encoder (Vision)</cell><cell>Encoder (Text)</cell><cell>Encoder (Vision)</cell></row><row><cell>Text &amp; Vision</cell><cell>Text</cell><cell>Vision</cell><cell>Text</cell><cell>Vision</cell></row><row><cell>(a) Share Type</cell><cell cols="2">(b) Cross Type</cell><cell cols="2">(c) Joint Type</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Video Clip[CLS] toast the[MASK]   [MASK] in the toaster[SEP]    Generation toast the bread slices in the toaster[SEP]    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Alignment</cell><cell></cell><cell>CMLM</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0/1</cell><cell></cell><cell cols="2">bread slices</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Text Encoder (Transformer Encoder)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Transcript</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joint 0/1</cell><cell cols="3">Cross Encoder (Transformer Encoder)</cell><cell cols="2">Decoder (Transformer Decoder)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Video Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(Transformer Encoder)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CMFM</cell><cell>feature masked</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Retrieval</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Caption</cell><cell></cell><cell></cell><cell cols="3">Action Tasks</cell><cell></cell><cell cols="3">Multimodal Classification</cell></row><row><cell>[CLS] peanuts</cell><cell></cell><cell>or</cell><cell></cell><cell>[CLS] you</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">segmentation &amp; step localization</cell><cell>[CLS] you</cell><cell></cell></row><row><cell>Text and … Video Clip</cell><cell>Text Encoder Video Encoder Joint 0.12</cell><cell>Cross Encoder</cell><cell>Aligment 0.12</cell><cell>specially … Transcript Video Clip</cell><cell>Text Encoder Video Encoder</cell><cell>Cross Encoder</cell><cell>Decoder</cell><cell>place the bacon slices …</cell><cell>Video Clip Step Description [CLS] pour juice</cell><cell cols="2">Video Encoder Text Encoder</cell><cell>...</cell><cell>Video Clip specially … Transcript</cell><cell>Text Encoder Video Encoder</cell><cell>Cross Encoder</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Results of text-based video retrieval on MSR-VTT dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>The multimodal video captioning results on Youcook2 dataset. 'V' means video and 'T' means Transcript.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Action segmentation results on COIN.</figDesc><table><row><cell>Methods</cell><cell>Average Recall (%)</cell></row><row><cell>Alayrac et al. (2016)</cell><cell>13.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Action step localization results on CrossTask.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Multimodal sentiment analysis results on CMU-MOSI dataset. BA means binary accuracy, MAE is Meanabsolute Error, and Corr is Pearson Correlation Coefficient. For BA and F1, we report two numbers following: the number on the left side of / is calculated based on the approach from<ref type="bibr" target="#b55">Zadeh et al. (2018b)</ref>, and the right side is by<ref type="bibr" target="#b41">Tsai et al. (2019)</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="2">Dataset R@1 R@5 R@10 Median R</cell></row><row><cell>UniVL</cell><cell>Youcook2 22.2 52.2 66.2</cell><cell>5</cell></row><row><cell cols="2">-w/o Joint Youcook2 19.5 48.0 62.7</cell><cell>6</cell></row><row><cell cols="2">-w/o Alignment Youcook2 16.3 42.3 56.2</cell><cell>8</cell></row><row><cell cols="2">-w/o EnhancedV Youcook2 16.1 41.3 55.8</cell><cell>8</cell></row><row><cell cols="2">-w/o Decoder Youcook2 14.6 40.3 55.5</cell><cell>8</cell></row><row><cell cols="2">-w/o StagedP Youcook2 11.9 35.0 48.9</cell><cell>11</cell></row><row><cell cols="2">-w/o Pre-training Youcook2 7.7 23.9 34.7</cell><cell>21</cell></row><row><cell>UniVL</cell><cell>MSR-VTT 20.6 49.1 62.9</cell><cell>6</cell></row><row><cell cols="2">-w/o Joint MSR-VTT 19.6 45.9 62.6</cell><cell>6</cell></row><row><cell cols="2">-w/o Alignment MSR-VTT 19.3 44.6 60.1</cell><cell>7</cell></row><row><cell cols="2">-w/o EnhancedV MSR-VTT 18.0 45.3 59.3</cell><cell>7</cell></row><row><cell cols="2">-w/o Decoder MSR-VTT 18.9 44.9 57.8</cell><cell>7</cell></row><row><cell cols="2">-w/o StagedP MSR-VTT 18.0 44.3 57.7</cell><cell>8</cell></row><row><cell cols="2">-w/o Pre-training MSR-VTT 16.7 44.0 55.9</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on retrieval task. '-w/o' means reducing the condition above the previous line. StagedP 18.13 12.49 18.78 42.64 1.46 -w/o Pre-training 14.23 9.46 16.27 37.44 1.15</figDesc><table><row><cell>Methods</cell><cell>B-3</cell><cell>B-4</cell><cell>M</cell><cell>R-L CIDEr</cell></row><row><cell>UniVL</cell><cell cols="4">23.87 17.35 22.35 46.52 1.81</cell></row><row><cell cols="5">-w/o Joint 23.96 17.54 22.48 46.77 1.84</cell></row><row><cell cols="5">-w/o Alignment 23.51 17.24 22.02 45.90 1.77</cell></row><row><cell cols="5">-w/o EnhancedV 23.15 17.04 21.83 45.89 1.76</cell></row><row><cell cols="5">-w/o Decoder 19.01 13.22 19.43 43.62 1.53</cell></row><row><cell>-w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on caption task of Youcook2 dataset. '-w/o' means reducing the condition above the previous line.</figDesc><table><row><cell>Method</cell><cell cols="3">Visual Feature R@1 R@5 R@10 Median R</cell></row><row><cell>UniVL</cell><cell cols="2">RS152 + RX101 11.5 29.1 40.1</cell><cell>17</cell></row><row><cell>on Youcook2</cell><cell>S3D</cell><cell>22.2 52.2 66.2</cell><cell>5</cell></row><row><cell>UniVL</cell><cell cols="2">RS152 + RX101 18.7 44.4 58.9</cell><cell>7</cell></row><row><cell>on MSR-VTT</cell><cell>S3D</cell><cell>20.6 49.1 62.9</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of visual features for retrieval task. RS152 denotes ResNet-152, RX101 means ResNeXt-101.</figDesc><table><row><cell cols="2">Method Visual Feature B-3 B-4 M R-L CIDEr</cell></row><row><cell>UniVL</cell><cell>RS152 + RX101 20.42 14.31 19.92 42.35 1.47 S3D 23.87 17.35 22.35 46.52 1.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Ablation study of visual features for multimodal video captioning results on Youcook2 dataset. RS152 denotes ResNet-152, RX101 means ResNeXt-101.</figDesc><table /><note>4.4.1 Modules and Strategies.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/Maluuba/nlg-eval</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4575" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A case study on combining asr and visual features for generating instructional video captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal tessellation: A unified approach for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07203</idno>
		<title level="m">Video understanding as machine translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hero: Hierarchical encoder for video+ language omnirepresentation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal language analysis with recurrent multistage fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyin</forename><surname>Paul Pu Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">605</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>text transformer. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extending long short-term memory for multi-view structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Shyam Sundar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="338" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7386" to="7395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense procedure captioning in narrated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VL-BERT: pretraining of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">COIN: A large-scale dataset for comprehensive instructional video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dajun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6558" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7216" to="7223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Msrvtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="487" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video captioning and retrieval models with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVLSMDC2016 Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3261" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multiview sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Prateek Vij, Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Factorized multimodal transformer for multimodal sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09826</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-task weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
