<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Huber</surname></persName>
							<email>patrikhuber@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
							<email>wuxiaojun@jiangnan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of IoT Engineering</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<postCode>214122</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function.</p><p>To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial landmark localisation, or face alignment, aims at finding the coordinates of a set of pre-defined key points for 2D face images. A facial landmark usually has specific semantic meaning, e.g. nose tip or eye centre, which provides rich geometric information for other face analysis tasks such as face recognition <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b68">69]</ref>, emotion estimation <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b36">37]</ref> and 3D face reconstruction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Thanks to the successive developments in this area of research during the past decades, we are able to perform  very accurate facial landmark localisation in constrained scenarios, even using traditional approaches such as Active Shape Model (ASM) <ref type="bibr" target="#b6">[7]</ref>, Active Appearance Model (AAM) <ref type="bibr" target="#b7">[8]</ref> and Constrained Local Model (CLM) <ref type="bibr" target="#b10">[11]</ref>. The existing challenge is to achieve robust and accurate landmark localisation of unconstrained faces that are impacted by a variety of appearance variations, e.g. in pose, expression, illumination, image blurring and occlusion. To this end, cascaded-regression-based approaches have been widely used, in which a set of weak regressors are cascaded to form a strong regressor <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b19">20]</ref>. However, the capability of cascaded regression is nearly saturated due to its shallow structure. After cascading more than four or five weak regressors, the performance of cascaded regression is hard to improve further <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b16">17]</ref>. More recently, deep neural networks have been put forward as a more powerful alternative in a wide range of computer vision and pattern recognition tasks, including facial landmark localisation <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>To perform robust facial landmark localisation us-ing deep neural networks, different network types have been explored, such as the Convolutional Neural Network (CNN) <ref type="bibr" target="#b54">[55]</ref>, Auto-Encoder Network <ref type="bibr" target="#b72">[73]</ref> and Recurrent Neural Network (RNN) <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b63">64]</ref>. In addition, different network architectures have been extensively studied during the recent years along with the development of deep neural networks in other AI applications. For example, the Fully Convolutional Network (FCN) <ref type="bibr" target="#b37">[38]</ref> and hourglass network with residual blocks have been found very effective <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b11">12]</ref>. One crucial aspect of deep learning is to define a loss function leading to better-learnt representation from underlying data. However, this aspect of the design seems to be little investigated by the facial landmark localisation community. To the best of our knowledge, most existing facial landmark localisation approaches using deep learning are based on the L2 loss. However, the L2 loss function is sensitive to outliers, which has been noted in connection with the bounding box regression problem in the well-known Fast R-CNN algorithm <ref type="bibr" target="#b21">[22]</ref>. Rashid et al. also notice this issue and use the smooth L1 loss instead of L2 <ref type="bibr" target="#b46">[47]</ref>. To further address the issue, we propose a new loss function, namely Wing loss ( <ref type="figure" target="#fig_1">Fig. 1</ref>), for robust facial landmark localisation. The main contributions of our work include:</p><p>• presenting a systematic analysis of different loss functions that could be used for regression-based facial landmark localisation with CNNs, which to our best knowledge is the first such study carried out in connection with the landmark localisation problem. We empirically and theoretically compare L1, L2 and smooth L1 loss functions and find that L1 and smooth L1 perform much better than the widely used L2 loss.</p><p>• a novel loss function, namely the Wing loss, which is designed to improve the deep neural network training capability for small and medium range errors.</p><p>• a data augmentation strategy, i.e. pose-based data balancing, that compensates the low frequency of occurrence of samples with large out-of-plane head rotations in the training set.</p><p>• a two-stage facial landmark localisation framework for performance boosting.</p><p>The paper is organised as follows. Section 2 presents a brief review of the related literature. The regression-based facial landmarking problem with CNNs is formulated in Section 3. The properties of common loss functions (L1 and L2) are discussed in Section 4 which also motivate the introduction of the novel Wing loss function. The pose-based data balancing strategy is the subject of Section 5. The twostage localisation framework is proposed in Section 6. The advocated approach is validated experimentally in Section 7 and the paper is drawn to conclusion in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Network Architectures: Most deep-learning-based facial landmark localisation approaches are regression-based. For such a task, the most straightforward way is to use a CNN model with regression output layers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b46">47]</ref>. The input for a regression CNN is usually an image patch enclosing the whole face region and the output is a vector consisting of the 2D coordinates of facial landmarks. Besides the classical CNN architecture, newly developed CNN systems have also been used for facial landmark localisation and shown promising results, e.g. FCN <ref type="bibr" target="#b37">[38]</ref> and the hourglass network <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Different from traditional CNN-based approaches, FCN and hourglass network output a heat map for each landmark. These heat maps are of the same size as the input image. The value of a pixel in a heat map indicates the probability that its location is the predicted position of the corresponding landmark. To reduce false alarms of a generated 2D sparse heat map, Wu et al. propose a distance-aware softmax function that facilitates the training of their dual-path network <ref type="bibr" target="#b62">[63]</ref>.</p><p>Thanks to the extensive studies of different deep neural networks and their use cases in unconstrained facial landmark localisation, the development of the area has been greatly promoted. However, the current research lacks a systematic analysis on the use of different loss functions. In this paper, we close this gap and design a new loss function for CNN-based facial landmark localisation.</p><p>Dealing with Pose Variations: Extreme pose variations bring many difficulties to unconstrained facial landmark localisation. To mitigate this issue, different strategies have been explored. The first one is to use multiview models. There is a long history of the use of multiview models in landmark localisation, from the earlier studies on ASM <ref type="bibr" target="#b48">[49]</ref> and AAM <ref type="bibr" target="#b9">[10]</ref> to recent work on cascaded-regression-based <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b20">21]</ref> and deep-learningbased approaches <ref type="bibr" target="#b11">[12]</ref>. For example, Feng et al. train multiview cascaded regression models using a fuzzy membership weighting strategy, which, interestingly, outperforms even some deep-learning-based approaches <ref type="bibr" target="#b20">[21]</ref>. The second strategy, which has become very popular in recent years, is to use 3D face models <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31]</ref>. By recovering the 3D shape and estimating the pose of a given input 2D face image, the issue of extreme pose variations can be alleviated to a great extent. In addition, 3D face models have also been widely used to synthesise additional 2D face images with pose variations for the training of a pose-invariant system <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b77">78]</ref>. Last, multi-task learning has been adopted to address the difficulties posed by image degradation, including pose variations. For example, face attribute estimation, pose estimation or 3D face reconstruction can jointly be trained with facial landmark localisation <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b45">46]</ref>. The collaboration of different tasks in a multi-task learning framework can boost the performance of individual sub-tasks.</p><p>Different from these approaches, we treat the challenge as a training data imbalance problem and advocate a posebased data balancing strategy to address this issue.</p><p>Cascaded Networks: In the light of the coarse-to-fine cascaded regression framework, multiple networks can be stacked to form a stronger network to boost the performance. To this end, shape-or landmark-related features should be used to satisfy the training of multiple networks in cascade. However, a CNN using a global face image as input cannot meet this requirement. To address this issue, one solution is to extract CNN features from local patches around facial landmarks. This idea is advocated, for example, by Trigeorgis et al. who use the Recurrent Neural Network (RNN) for end-to-end model training <ref type="bibr" target="#b57">[58]</ref>. As an alternative, we can train a network based on the global image patch for rough facial landmark localisation. Then, for each landmark or a composition of multiple landmarks in a specific region of the face, a network is trained to perform fine-grained landmark prediction <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b66">67]</ref>. For another example, Yu et al. propose to inject local deformations to the estimated facial landmarks of the first network using thin-plate spline transformations <ref type="bibr" target="#b69">[70]</ref>.</p><p>In this paper, we use a two-stage CNN-based landmark localisation framework. The first CNN is a very simple one that can perform rough facial landmark localisation very quickly. The aim of the first network is to mitigate the difficulties posed by inaccurate face detection and in-plane head rotations. Then the second CNN is used to perform finegrained landmark localisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN-based facial landmark localisation</head><p>The target of CNN-based facial landmark localisation is to find a nonlinear mapping:</p><formula xml:id="formula_0">Φ : I → s,<label>(1)</label></formula><p>that outputs a shape vector s ∈ R 2L for a given input colour image I ∈ R H×W ×3 . The input image is usually cropped using the bounding box output by a face detector. The shape vector is in the form of s = [x 1 , ..., x L , y 1 , ..., y L ] T , where L is the number of pre-defined 2D facial landmarks and (x l , y l ) are the coordinates of the lth landmark. To obtain this mapping, first, we have to define the architecture of a multi-layer neural network with randomly initialised parameters. In fact, the mapping </p><formula xml:id="formula_1">Φ = (φ 1 • ... • φ M )(I)</formula><formula xml:id="formula_2">= {I i , s i } N i=1</formula><p>, the target of CNN training is to find a Φ that minimises:</p><formula xml:id="formula_3">N i=1 loss(Φ(I i ), s i ),<label>(2)</label></formula><p>In:64x64x3 32x32x32 16x16x64 8x8x128 4x4x256 2x2x512 FC:1024 Out:2L : 3x3 Convolution, Relu and Max Pooling (/2) <ref type="figure">Figure 2</ref>. Our simple CNN-6 network consisting of 5 convolutional and 1 fully connected layers followed by an output layer.</p><p>where loss() is a pre-defined loss function that measures the difference between a predicted shape vector and its ground truth. In such a case, the CNN is used as a regression model learned in a supervised manner. To optimise the above objective function, optimisation algorithms such as Stochastic Gradient Descent (SGD) can be used.</p><p>To empirically analyse different loss functions, we use a simple CNN architecture, in the following termed CNN-6, for facial landmark localisation, to achieve high speed in model training and testing. The input for this network is a 64×64×3 colour image and the output is a vector of 2L real numbers for the 2D coordinates of L landmarks. As shown in <ref type="figure">Fig. 2</ref>, our CNN-6 has five 3 × 3 convolutional layers, a fully connected layer and an output layer. After each convolutional and fully connected layer, a standard Relu layer is used for nonlinear activation. A Max pooling after each convolutional layer is used to downsize the feature map to half of the size.</p><p>To boost the performance, more powerful network architectures can be used, such as our two-stage landmark localisation framework presented in Section 6 and the recently proposed ResNet architecture <ref type="bibr" target="#b23">[24]</ref>. We will report the results of these advanced network architectures in Section 7. It should be highlighted that, to the best of our knowledge, this is the first time that such a deep residual network, i.e. ResNet-50, is used for facial landmark localisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Wing loss</head><p>The design of a proper loss function is crucial for CNNbased facial landmark localisation. However, mainly the L2 loss has been used in existing deep-neural-network-based facial landmarking systems. In this paper, to the best of our knowledge, we are the first to analyse different loss functions for CNN-based facial landmark localisation and demonstrate that the L1 and smooth L1 loss functions perform much better than the L2 loss. Motivated by our analysis, we propose a new loss function, namely Wing loss, which further improves the accuracy of CNN-based facial landmark localisation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis of different loss functions</head><p>Given a training image I and a network Φ, we can predict the facial landmarks as a vector s = Φ(I). The loss is defined as:</p><formula xml:id="formula_4">-3 -2 -1 0 1 2 3 0 1 2 3 L1 L2 smooth L1</formula><formula xml:id="formula_5">loss(s, s ) = 2L i=1 f (s i − s i ),<label>(3)</label></formula><p>where s is the ground-truth shape vector of the facial landmarks. For f (x) in the above equation, L1 loss uses L1(x) = |x| and L2 loss uses L2(x) = 1 2 x 2 . The smooth L1 loss function is piecewise-defined as:</p><formula xml:id="formula_6">smooth L1 (x) = 1 2 x 2 if |x| &lt; 1 |x| − 1 2 otherwise ,<label>(4)</label></formula><p>which is quadratic for small values of |x| and linear for large values <ref type="bibr" target="#b21">[22]</ref>. More specifically, smooth L1 uses L2(x) for x ∈ (−1, 1) and shifted L1(x) elsewhere. <ref type="figure" target="#fig_3">Fig. 3</ref> depicts the plots of these loss functions. It should be noted that the smooth L1 loss is a special case of the Huber loss <ref type="bibr" target="#b28">[29]</ref>. The loss function that has widely been used in facial landmark localisation is the L2 loss function. However, it is wellknown that the L2 loss is sensitive to outliers. This is the main reason why, e.g., Girshick <ref type="bibr" target="#b21">[22]</ref> and Rashid et al. <ref type="bibr" target="#b46">[47]</ref> use the smooth L1 loss function for their localisation tasks. For evaluation, the AFLW-Full protocol has been used <ref type="bibr" target="#b76">[77]</ref> 1 . This protocol consists of 20k training images and 4386 test images. Each image has 19 facial landmarks. We use three state-of-the-art algorithms <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref> as our baseline for comparison. The first one is the Cascaded Compositional Learning algorithm (CCL) <ref type="bibr" target="#b76">[77]</ref>, which is a multi-view cascaded regression model based on random forests. The second one is the Two-stage Re-initialisation Deep Regression Network (TR-DRN) <ref type="bibr" target="#b40">[41]</ref>. The last baseline algorithm is a multi-view approach based on cascaded shape regression, namely DAC-CSR <ref type="bibr" target="#b20">[21]</ref>.</p><p>We train the CNN-6 network on AFLW using three different loss functions and report the results in <ref type="table" target="#tab_0">Table 1</ref>. The L2 loss function, which has been widely used for facial landmark localisation, performs well. The result is better than CCL in terms accuracy but worse than DAC-CSR and TR-DRN. Surprisingly, when we use L1 or smooth L1 for <ref type="bibr" target="#b0">1</ref> The AFLW dataset is introduced in Section 7.2.1.    <ref type="bibr" target="#b76">[77]</ref> 2.72×10 −2 DAC-CSR (CVPR2017) <ref type="bibr" target="#b20">[21]</ref> 2.27×10 −2 TR-DRN (CVPR2017) <ref type="bibr" target="#b40">[41]</ref> 2</p><formula xml:id="formula_7">.17×10 −2 CNN-6 (L2) 2.41×10 −2 CNN-6 (L1) 2.00×10 −2 CNN-6 (smooth L1) 2.02×10 −2 CNN-6 (Wing loss) 1.88×10 −2</formula><p>the CNN-6 training, the performance in terms of accuracy improves significantly and outperforms all the state-of-theart baseline approaches, despite the CNN network's simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The proposed Wing loss</head><p>We compare the results obtained on the AFLW dataset using the simple CNN-6 network in <ref type="figure" target="#fig_5">Fig. 4</ref> by plotting the Cumulative Error Distribution (CED) curves. We can see that all the loss functions analysed in the last section perform well for large errors. This indicates that the training of a neural network should pay more attention to the samples with small or medium range errors. To achieve this target, we propose a new loss function, namely Wing loss, for CNN-based facial landmark localisation.</p><p>In order to motivate the new loss function, we provide an intuitive analysis of the properties of the L1 and L2 loss functions <ref type="figure" target="#fig_3">(Fig. 3)</ref>. The magnitude of the gradients of these two functions is 1 and |x| respectively, and the magnitude of the corresponding optimal step sizes should be |x| and 1. Finding the minimum in either case is straightforward. However, the situation becomes more complicated when we try to optimise simultaneously the location of multiple points, as in our problem of facial landmark localisation for-mulated in Eq. (3). In both cases the update towards the solution will be dominated by larger errors. In the case of L1, the magnitude of the gradient is the same for all the points, but the step size is disproportionately influenced by larger errors. For L2, the step size is the same but the gradient will be dominated by large errors. Thus in both cases it is hard to correct relatively small displacements.</p><p>The influence of small errors can be enhanced by an alternative loss function, such as ln x. Its gradient, given by 1/x, increases as we approach zero error. The magnitude of the optimal step size is x 2 . When compounding the contributions from multiple points, the gradient will be dominated by small errors, but the step size by larger errors. This restores the balance between the influence of errors of different sizes. However, to prevent making large update steps in a potentially wrong direction, it is important not to overcompensate the influence of small localisation errors. This can be achieved by opting for a log function with a positive offset.</p><p>This type of loss function shape is appropriate for dealing with relatively small localisation errors. However, in facial landmark detection of in-the-wild faces we may be dealing with extreme poses where initially the localisation errors can be very large. In such a regime the loss function should promote a fast recovery from these large errors. This suggests that the loss function should behave more like L1 or L2. As L2 is sensitive to outliers, we favour L1.</p><p>The above intuitive argument points to a loss function which for small errors should behave as a log function with an offset, and for larger errors as L1. Such a composite loss function can be defined as:</p><formula xml:id="formula_8">wing(x) = w ln(1 + |x|/ ) if |x| &lt; w |x| − C otherwise ,<label>(5)</label></formula><p>where the non-negative w sets the range of the nonlinear part to (−w, w), limits the curvature of the nonlinear region and C = w − w ln(1 + w/ ) is a constant that smoothly links the piecewise-defined linear and nonlinear parts. Note that we should not set to a very small value because it makes the training of a network very unstable and causes the exploding gradient problem for very small errors. In fact, the nonlinear part of our Wing loss function just simply takes the curve of ln(x) between [ /w, 1 + /w) and scales it along both the X-axis and Y-axis by a factor of w. Also, we apply translation along the Y-axis to allow wing(0) = 0 and to impose continuity on the loss function.</p><p>From <ref type="figure" target="#fig_5">Fig. 4</ref>, we can see that our Wing loss outperforms L2, L1 and smooth L1 in terms of accuracy. The Wing loss further reduces the average normalised error from 2 × 10 −2 to 1.88 × 10 −2 , which is 6% lower than the best result obtained in the last section <ref type="table" target="#tab_0">(Table 1</ref>) and 13% lower than the best state-of-the-art deep-learning baseline approach, i.e.  TR-DRN. In our experiments, we set the parameters of the Wing loss as w = 10 and = 2. For the results of different parameter settings, please refer to <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Pose-based data balancing</head><p>Extreme pose variations are very challenging for robust facial landmark localisation in the wild. To mitigate this issue, we propose a simple but very effective Pose-based Data Balancing (PDB) strategy. We argue that the difficulty for accurately localising faces with large poses is mainly due to data imbalance, which is a well-known problem in many computer vision applications <ref type="bibr" target="#b52">[53]</ref>. For example, given a training dataset, most samples in it are likely to be nearfrontal faces. The neural network trained on such a dataset is dominated by frontal faces. By over-fitting to the frontal pose it cannot adapt well to faces with large poses. In fact, the difficulty of training and testing on merely frontal faces should be similar to that on profile faces. This is the main reason why a view-based face analysis algorithm usually works well for pose-varying faces. As an evidence, even the classical view-based Active Appearance Model can localise faces with large poses very well (up to 90 • in yaw) <ref type="bibr" target="#b8">[9]</ref>.</p><p>To perform PDB, we first align all the training shapes to a reference shape using Procrustes Analysis, with the mean shape as the reference shape. Then we apply PCA to the aligned training shapes and project the original shapes to the one dimensional space defined by the shape eigenvector (pose space) controlling pose variations. The distribution of projection coefficient of the training samples is represented by a histogram with K bins, plotted in <ref type="figure" target="#fig_6">Figure 5</ref>. With this histogram, we balance the training data by duplicating the <ref type="table">Table 3</ref>. A comparison of different loss functions using our PDB strategy and two-stage landmark localisation framework, measured in terms of the average normalised error (×10 −2 ) on AFLW. The method CNN-6/7 indicates the proposed two-stage localisation framework using CNN-6 as the first network and CNN-7 as the second network (Section 6). For CNN-7, the learning rate is reduced from 1 × 10 −6 to 1 × 10 −8 for L2, and from 1 × 10 samples falling into the bins of lower occupancy. We modify each duplicated sample by performing random image rotation, bounding box perturbation and other data augmentation approaches introduced in Section 7.1. To deal with in-plane rotations, we use a two-stage facial landmark localisation framework that will be introduced in Section 6. The results obtained by the CNN-6 network with PDB are shown in <ref type="table">Table 3</ref>. It should be noted that PDB improves the performance of CNN-6 on the AFLW dataset for all different types of loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Two-stage landmark localisation</head><p>Besides the out-of-plane head rotations, the accuracy of a facial landmark localisation algorithm can be degraded by other factors, such as in-plane head rotations and inaccurate bounding boxes output from a poor face detector. To mitigate this issue, we advocate the use of a two-stage landmark localisation framework.</p><p>In the proposed two-stage localisation framework, we use a very simple network, i.e. the CNN-6 network with 64 × 64 × 3 input images, as the first network. The CNN-6 network is very fast (400 fps on an NVIDIA GeForce GTX Titan X Pascal), hence it will not slow down the speed of our facial landmark localisation algorithm too much. The landmarks output by the CNN-6 network are used to refine the input image for the second network by removing the in-plane head rotation and correcting the bounding box. Also, the input image resolution for the second network is increased for fine-grained landmark localisation from 64 × 64 × 3 to 128 × 128 × 3, with the addition of one set of convolutional, Relu and Max pooling layers. Hence, the term 'CNN-7' is used to denote the second network. The CNN-7 network has a similar architecture to the CNN-6 network in <ref type="figure">Fig. 2</ref>. The difference is that CNN-7 has 6 convolutional layers which resize the feature map from 128×128×3 to 2 × 2 × 512. In addition, for the first convolutional layer in CNN-7, we double the number of 3 × 3 kernels from 32 to 64. We use the term 'CNN-6/7' for our two-stage facial landmark localisation framework and compare it with the CNN-6 network in <ref type="table">Table 3</ref>. As reported in the table, the use of our two-stage landmark localisation framework further improves the accuracy, regardless of the type of loss function used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental results</head><p>In this section, we evaluate our method on the Annotated Facial Landmarks in the Wild (AFLW) dataset <ref type="bibr" target="#b33">[34]</ref> and the 300 Faces in the Wild (300W) dataset <ref type="bibr" target="#b50">[51]</ref>. We first introduce our implementation details and experimental settings. Then we compare our algorithm with state-of-the-art approaches on AFLW and 300W. Last, we analyse the performance of different networks in terms of both accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Implementation details</head><p>In our experiments, we used Matlab 2017a and the Mat-ConvNet toolbox 2 . The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2× Intel Xeon E5-2667 v4 CPU, 256 GB RAM and 4 NVIDIA GeForce GTX Titan X (Pascal) cards. Note that we only use one GPU card for measuring the run time. We set the weight decay to 5 × 10 −4 , momentum to 0.9 and batch size to 8 for network training. Each model was trained for 120k iterations. We did not use any other advanced techniques in our CNN-6 and CNN-7 networks, such as batch normalisation, dropout or residual blocks. The standard ReLu function was used for nonlinear activation, and Max pooling with the stride of 2 was used to downsize feature maps. For the convolutional layer, we used 3 × 3 kernels with the stride of 1. All our networks, except ResNet-50, were trained from scratch without any pre-training on any other dataset. For the proposed PDB strategy, the number of bins K was set to 17 for AFLW and 9 for 300W.</p><p>For CNN-6, the input image size is 64 × 64 × 3. We reduced the learning rate from 3 × 10 −6 to 3 × 10 −8 for the L2 loss, and from 3 × 10 −5 to 3 × 10 −7 for the other loss functions. The parameters of the Wing loss were set to w = 10 and = 2. For CNN-7, the input image size is 128 × 128 × 3. We reduced the learning rate from 1 × 10 −6 to 1 × 10 −8 for the L2 loss, and from 1 × 10 −5 to 1 × 10 −7 for the other loss functions. The parameters of the Wing loss were set to w = 15 and = 3.</p><p>To perform data augmentation, we randomly rotated each training image between [−30, 30] degrees for CNN-6 and between [−10, 10] degrees for CNN-7. In addition, we randomly flipped each training image with the probability of 50%. For bounding box perturbation, we applied random translations to the upper-left and bottom-right corners of the face bounding box within 5% of the bounding  <ref type="figure">Figure 6</ref>. A comparison of the CED curves on the AFLW dataset. We compare our method with a set of state-of-the-art approaches, including SDM <ref type="bibr" target="#b64">[65]</ref>, ERT <ref type="bibr" target="#b31">[32]</ref>, RCPR <ref type="bibr" target="#b4">[5]</ref>, CFSS <ref type="bibr" target="#b75">[76]</ref>, LBF <ref type="bibr" target="#b47">[48]</ref>, GRF <ref type="bibr" target="#b22">[23]</ref>, CCL <ref type="bibr" target="#b76">[77]</ref>, DAC-CSR <ref type="bibr" target="#b20">[21]</ref> and TR-DRN <ref type="bibr" target="#b40">[41]</ref>. box size. Last, we randomly injected Gaussian blur (σ = 1) to each training image with the probability of 50%.</p><p>Evaluation Metric: For evaluation of a facial landmark localisation algorithm, we adopted the widely used Normalised Mean Error (NME). For the AFLW dataset using the AFLW-Full protocol, the given face bounding box of a test sample is a square <ref type="bibr" target="#b76">[77]</ref>. To calculate the NME of a test sample, the AFLW-Full protocol uses the width (or height) of the face bounding box as the normalisation term. For the 300W dataset, we followed the protocol used in <ref type="bibr" target="#b47">[48]</ref>. This protocol uses the inter-pupil distance as the normalisation term, which is different from the standard 300W protocol that uses the outer eye corner distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Comparison with state of the art 7.2.1 AFLW</head><p>We first evaluated our algorithm on the AFLW dataset <ref type="bibr" target="#b33">[34]</ref>, using the AFLW-Full protocol <ref type="bibr" target="#b76">[77]</ref>. AFLW is a very challenging dataset that has been widely used for benchmarking facial landmark localisation algorithms. The images in AFLW consist of a wide range of pose variations in yaw (from −90 • to 90 • ), as shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. The AFLW-Full protocol contains 20,000 training and 4,386 test images, and each image has 19 manually annotated facial landmarks.</p><p>We compare the proposed method with state-of-the-art approaches in terms of accuracy in <ref type="figure">Fig. 6</ref> using the Cumulative Error Distribution (CED) curve. In our experiments, we used our two-stage facial landmark localisation framework by stacking the CNN-6 and CNN-7 networks (denoted by CNN-6/7), as introduced in Section 6. In addition, the proposed Pose-based Data Balancing (PDB) strategy was adopted, as presented in Section 5. We report the results of the proposed approach using four different loss functions. <ref type="table">Table 4</ref>. A comparison of the proposed approach with the stateof-the-art approaches on the 300W dataset in terms of the NME averaged over all the test samples. We follow the protocol used in <ref type="bibr" target="#b47">[48]</ref>. Note that the error is normalised by the inter-pupil distance, rather than the outer eye corner distance. As shown in <ref type="figure">Fig. 6</ref>, our CNN-6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function (magenta solid line). This validates the effectiveness of the proposed two-stage localisation framework and the PDB strategy. Second, by simply switching the loss function from L2 to L1 or smooth L1, the performance of our method has been improved significantly (red solid and black dashed lines). Last, the use of our newly proposed Wing loss function further improves the accuracy (black solid line). The proportion of test samples (Y-axis) associated with a small to medium normalised mean error (X-axis) is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">300W</head><p>The 300W dataset is a collection of multiple face datasets, including LFPW <ref type="bibr" target="#b0">[1]</ref>, HELEN <ref type="bibr" target="#b35">[36]</ref>, AFW <ref type="bibr" target="#b78">[79]</ref> and XM2VTS <ref type="bibr" target="#b43">[44]</ref>. The face images involved in 300W have been semi-automatically annotated by 68 facial landmarks <ref type="bibr" target="#b51">[52]</ref>. To perform the evaluation on 300W, we followed the protocol used in <ref type="bibr" target="#b47">[48]</ref>. The protocol uses the full set of AFW and the training subsets of LFPW and HELEN as the training set, which contains 3148 training samples in total. The test set of the protocol includes the test subsets of LFPW and HELEN, as well as 135 IBUG face images newly collected by the managers of the 300W dataset. The final size of the test set is 689. The test set is further divided into two subsets for evaluation, i.e. the common and challenging subsets. The common subset has 554 face images from the LFPW and HELEN test subsets and the challeng- ing subset constitutes the 135 IBUG face images. Similar to the experiments conducted on the AFLW dataset, we used the two-stage localisation framework with our PDB strategy. The results obtained by our approach with different loss functions are reported in <ref type="table">Table 4</ref>.</p><p>As shown in <ref type="table">Table 4</ref>, our two-stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof-the-art algorithms on the 300W dataset in accuracy. The error has been reduced by almost 20% as compared to the current best result reported by the RAR algorithm <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Run time and network architectures</head><p>Facial landmark localisation has been widely used in many real-time practical applications, hence the speed together with accuracy of an algorithm is crucial for the deployment of the algorithm in commercial use cases.</p><p>To analyse the performance of our Wing loss on more advanced network architectures, we evaluated ResNet <ref type="bibr" target="#b23">[24]</ref> for the task of landmark localisation on AFLW and 300W. We used the ResNet-50 model that was pre-trained on the ImageNet ILSVRC classification problem 3 . We fine-tuned the model on the training sets of AFLW and 300W separately for landmark localisation. The input for ResNet is a 224 × 224 × 3 colour image. It should be highlighted that, to our best knowledge, this is the first time that such a deep network has been used for facial landmark localisation.</p><p>For both AFLW and 300W, by replacing the CNN-6/7 network with ResNet-50, the performance has been further improved by around 10%, as shown in <ref type="table" target="#tab_4">Table 5</ref>. However, this performance boosting comes at the cost of much slower training and inference of ResNet compared to CNN-6/7.</p><p>To validate the effectiveness of our Wing loss for large capacity networks, we also conducted experiments using ResNet-50 with different loss functions on AFLW. The results are reported in <ref type="table">Table.</ref> 6. The results further demonstrate the superiority of the proposed Wing loss over other loss functions for large capacity networks, e.g. ResNet-50.</p><p>Last, we evaluated the speed of different networks on the 300W dataset with 68 landmarks for both GPU and CPU 3 http://www.vlfeat.org/matconvnet/pretrained/ devices. The results are reported in <ref type="table" target="#tab_5">Table 7</ref>. According to the table, our simple CNN-6/7 network is roughly an order of magnitude faster than ResNet-50 at the compromise of 10% performance difference in accuracy. Also, our CNN-6/7 model is much faster than most existing DNNbased facial landmark localisation approaches such as TR-DRN <ref type="bibr" target="#b40">[41]</ref>. The speed of TR-DRN is 83 fps on an NVIDIA GeForce GTX Titan X card. Even with a powerful GPU card, it is hard to achieve video rate (60fps) with ResNet-50. It should be noted that our CNN-6/7 still outperforms the state-of-the-art approaches by a significant margin while running at 170 fps on a GPU card, as shown in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we analysed different loss functions that can be used for the task of regression-based facial landmark localisation. We found that L1 and smooth L1 loss functions perform much better in accuracy than the L2 loss function. Motivated by our analysis of these loss functions, we proposed a new, Wing loss performance measure. The key idea of the Wing loss criterion is to increase the contribution of the samples with small and medium size errors to the training of the regression network. To prove the effectiveness of the proposed Wing loss function, extensive experiments have been conducted using several CNN network architectures. Furthermore, a pose-based data balancing strategy and a two-stage landmark localisation framework were advocated to improve the accuracy of CNN-based facial landmark localisation further. By evaluating our algorithm on multiple well-known benchmarking datasets, we demonstrated the merits of the proposed approach.</p><p>It should be emphasised that the proposed Wing loss is relevant to other regression-based computer vision tasks using convolutional neural networks. However, being constrained by the space limitations, we leave the discussion of its extended use to future reports.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our Wing loss function (Eq. 5) plotted with different parameter settings, where w limits the range of the non-linear part and controls the curvature. By design, we amplify the impact of the samples with small and medium range errors to the network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is a composition of M functions, in which each function stands for a specific layer in the network.Given a set of labelled training samples Ω</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Plots of the L1, L2 and smooth L1 loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>CED curves comparing different loss functions on the AFLW dataset, using the AFLW-Full protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Distribution of the pose coefficients of the AFLW training samples by projecting their shapes to the 1-D pose space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A comparison of different loss functions with the three baseline algorithms in terms of the average error normalised by face</figDesc><table /><note>size. Each training has been performed for 120k iterations. The learning rate is reduced from 3 × 10 −6 to 3 × 10 −8 for L2, and from 3 × 10 −5 to 3 × 10 −7 for the other loss functions.method average normalised error CCL (CVPR2016)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>A comparison of different parameter settings (w and ) for the proposed Wing loss function, measured in terms of the average normalised error (×10 −2 ) on AFLW using our CNN-6 network.</figDesc><table><row><cell>w</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell></row><row><cell>0.5</cell><cell cols="6">1.95 1.92 1.92 1.94 1.97 1.94</cell></row><row><cell>1</cell><cell cols="6">1.95 1.91 1.91 1.90 1.90 1.95</cell></row><row><cell>2</cell><cell cols="6">1.98 1.92 1.91 1.88 1.90 1.98</cell></row><row><cell>3</cell><cell cols="6">2.02 1.96 1.93 1.91 1.89 2.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>−5 to 1 × 10 −7 for the L1, smooth L1 and Wing loss functions.</figDesc><table><row><cell>method</cell><cell>loss</cell><cell>L2</cell><cell>L1</cell><cell cols="2">smooth L1 Wing</cell></row><row><cell>CNN-6</cell><cell></cell><cell cols="2">2.41 2.00</cell><cell>2.02</cell><cell>1.88</cell></row><row><cell cols="2">CNN-6 + PDB</cell><cell cols="2">2.23 1.89</cell><cell>1.91</cell><cell>1.83</cell></row><row><cell>CNN-6/7</cell><cell></cell><cell cols="2">2.06 1.82</cell><cell>1.84</cell><cell>1.71</cell></row><row><cell cols="2">CNN-6/7 + PDB</cell><cell cols="2">1.94 1.73</cell><cell>1.76</cell><cell>1.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>A comparison of our simple network with ResNet-50, in terms of accuracy on AFLW-Full and 300W.</figDesc><table><row><cell></cell><cell></cell><cell>AFLW</cell><cell cols="2">300W Com. Challenge</cell><cell>Full</cell></row><row><cell>CNN-6 + PDB (Wing)</cell><cell></cell><cell>1.83</cell><cell>3.35</cell><cell>7.20</cell><cell>4.10</cell></row><row><cell>CNN-6/7 + PDB (Wing)</cell><cell></cell><cell>1.65</cell><cell>3.27</cell><cell>7.18</cell><cell>4.04</cell></row><row><cell cols="2">ResNet-50 + PDB (Wing)</cell><cell>1.47</cell><cell>3.01</cell><cell>6.01</cell><cell>3.60</cell></row><row><cell cols="6">Table 6. A comparison in accuracy of ResNet-50 using different</cell></row><row><cell cols="4">loss functions, evaluated on AFLW-Full.</cell><cell></cell></row><row><cell>Loss Function</cell><cell>L2</cell><cell>L1</cell><cell cols="2">smooth L1 Wing</cell></row><row><cell>NME (×10 −2 )</cell><cell cols="2">1.68 1.51</cell><cell>1.52</cell><cell>1.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>A comparison of different networks, in the number of model parameters, model size and speed.</figDesc><table><row><cell>network</cell><cell># params</cell><cell>size</cell><cell cols="2">speed (fps) GPU CPU</cell></row><row><cell>CNN-6</cell><cell>3.8 M</cell><cell>14 MB</cell><cell>400</cell><cell>150</cell></row><row><cell>CNN-6/7</cell><cell>12.3 M</cell><cell>46 MB</cell><cell>170</cell><cell>20</cell></row><row><cell>ResNet-50</cell><cell>25 M</cell><cell>99 MB</cell><cell>30</cell><cell>8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.vlfeat.org/matconvnet/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the EPSRC Programme Grant (FACER2VM) EP/N007743/1, EP-SRC/dstl/MURI project EP/R018456/1, the National Natural Science Foundation of China (61373055, 61672265) and the NVIDIA GPU Grant Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Active shape models-their training and application. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View-based active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="227" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">View-based active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="657" to="664" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature Detection and Tracking with Constrained Local Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Mahine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06023</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive cascade deep convolutional neural networks for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Standards &amp; Interfaces</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end 3d face reconstruction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random Cascaded-Regression Copse for Robust Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rätsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05536</idno>
		<title level="m">Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face detection, bounding box aggregation and pose estimation for robust facial landmark localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic Attention-Controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-Set Sample Weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2481" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Growing regression forests by classification: Applications to object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="552" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Efficient 3D Morphable Face Model Fitting. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time 3d face fitting and texture fusion on in-thewild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="441" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment with a single cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D Morphable Face Models and Their Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="185" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotated Facial Landmarks in the Wild: A Large-scale, Realworld Database for Facial Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yin</surname></persName>
		</author>
		<title level="m">Gaussian Mixture 3D Morphable Face Model. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="617" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unconstrained facial landmark localization with backbone-branches fullyconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.03409</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Deep Regression Architecture With Two-Stage Re-Initialization for High Performance Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pose-aware face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Do we really need to collect millions of faces for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="579" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">XM2VTSDB: The extended M2VTS database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio and Video-based Biometric Person Authentication</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page" from="965" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interspecies knowledge transfer for facial keypoint detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face alignment via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A multi-view nonlinear active shape model using kernel PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Psarrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive 3d face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision -Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A semi-automatic methodology for facial landmark annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep Convolutional Network Cascade for Facial Point Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Copula ordinal regression for joint estimation of facial action unit intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Facial landmark detection with tweaked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Constrained joint cascade regression framework for simultaneous facial action unit recognition and facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">GoDP: Globally Optimized Dual Pathway deep network architecture for facial landmark localization in-the-wild. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentiverefinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2664" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint head pose estimation and face alignment framework using global and local cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep deformation network for object landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Occlusion-Free Face Alignment: Deep Regression Networks Coupled With De-Corrupt AutoEncoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unconstrained Face Alignment via Cascaded Compositional Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3409" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Face Alignment Across Large Poses: A 3D Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
