<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><forename type="middle">Soo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Baltimore</orgName>
								<address>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
							<email>areiter@cs.jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent. In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant. However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box. In this work, we propose to use a new class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition. Compared to popular LSTM-based Recurrent Neural Network models, given interpretable input such as 3D skeletons, TCN provides us a way to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition. We provide our strategy in re-designing the TCN with interpretability in mind and how such characteristics of the model is leveraged to construct a powerful 3D activity recognition method. Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret. The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human activity analysis is a crucial yet challenging research area of computer vision. Applications of human activity recognition ranges from video surveillance, humancomputer interaction, robotics and skill evaluation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>. At the core of successful systems for human activity recognition lies an effective representation that can model both the spatial and temporal dynamics of human motion.</p><p>Traditionally, the community has focused on activity recognition in the domain of RGB videos <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref>. For a RGB video, complex human motion in 3D euclidean space is projected on to a series of 2D images and in the pro-cess, loss of valuable 3D spatio-temporal information is inevitable. In recent years, we have witnessed a drastic improvement of cost-effective depth sensors in the form of Microsoft Kinect <ref type="bibr" target="#b8">[9]</ref>. Naturally, computer vision methods leveraging on the 3D structure provided by such 3D sensors, namely RGB+D methods, have been an active area of research <ref type="bibr" target="#b7">[8]</ref>. Applied to human activity recognition, 3D information of how a human body articulates comes in the form of time series sequence of 3D skeletons. Such representations describe human motion as a collection of trajectories in 3D euclidean space of key human joints. Even without the context information and visual cues, early work <ref type="bibr" target="#b15">[16]</ref> in biological perception and more recent methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref> provide strong evidence that encoding humans as a 3D skeleton yields both a discriminative and a robust representation for activity analysis. Given the recent progress of powerful human pose estimators from RGB or RGBD data <ref type="bibr" target="#b24">[25]</ref>, human activity recognition model that builds on top of 3D skeletons is a promising direction.</p><p>Despite this significant progress, the inner workings of such complex temporal models still remain mostly blackboxes. Without the capability to interpret learning based models, we inevitably lack the power to fully support a model's decision regardless of its correctness <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Such short-comings may hinder practical deployment of even the strongest models. The ability to understand and explain precisely how a model came to a wrong prediction is a fundamental first step towards improving the potential of our current methods.</p><p>In this light, we propose Temporal Convolutional Neural Networks (TCN) <ref type="bibr" target="#b18">[19]</ref> applied to 3D Human Action Recognition. Through the lens of TCN, we wish to uncover what exactly learning-based temporal models leverage on especially when trained on interpretable data such as a sequence of 3D skeletons. We re-design the original TCN by factoring out the deeper layers into additive residual terms which yields both interpretable hidden representations and model parameters. Using the resulting architecture, Res-TCN, we validate our approach on currently the largest 3D human activity recognition dataset, NTU-RGBD and obtain stateof-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first provide a literature review on recent developments in learning based 3D human action recognition models. We focus our narrative on models that employ LSTM-based Recurrent Neural Networks. We also extend our review to works focusing on model interpretability and visualization of deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Human Activity Recognition</head><p>Traditional recurrent neural network models suffer from vanishing/exploding gradient problem during optimization and are difficult to train correctly <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref>. By formulation, LSTM neurons begin to address such optimization problems and are capable of modeling long-term dependencies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. Given the temporal recurrent nature of human action analysis, most leading methods in 3D human action recognition adopt LSTM-based RNNs.</p><p>Hierarchical recurrent neural network of <ref type="bibr" target="#b4">[5]</ref> combines the features of different body parts hierarchically. At the initial layer, each sub-network extracts features over a single joint and these representations are fused hierarchically in the deeper layers. A final prediction is made when all joint information is combined. In part-aware LSTM model introduced in <ref type="bibr" target="#b25">[26]</ref>, individual body joints are grouped together in five groups based on their spatial context. The memory units of the LSTM are learned independently per group and the information from different parts is aggregated to produce a final prediction. The work of <ref type="bibr" target="#b35">[36]</ref> leverages on similar intuition that co-occurrence of joints is a strong discriminative feature for human action recognition. A group sparsity constraint on the connection matrix pushes the network to learn the mappings between co-occurring joints and the human activity. Deep spatio-temporal LSTM with Trust Gates is introduced in <ref type="bibr" target="#b21">[22]</ref> to learn features both in the temporal and spatial domains. Similarly, the authors of <ref type="bibr" target="#b28">[29]</ref> propose a spatio-temporal attention model for LSTMbased RNNs. The method comprises of three LSTM networks: a spatial attention sub-LSTM, a temporal attention sub-LSTM and a main LSTM. Both temporal and spatial attention modules are pre-trained separately initially and the entire network is trained end-to-end.</p><p>In the above methods, the key intuition is that a certain subset of joints are more important for recognizing human activities. However, it is difficult to interpret what the model parameters of each LSTM layer represent. In our proposed version of TCN, we show that our model also learns both spatial and temporal attention without the need for initial pre-training stage as in <ref type="bibr" target="#b28">[29]</ref>. Moreover, by model design based on temporal convolutions <ref type="bibr" target="#b18">[19]</ref> and residual connections <ref type="bibr" target="#b10">[11]</ref>, we can begin to directly interpret what our model parameters and features represent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Interpretability and Visualization</head><p>Here, we focus our discussion on interpretability of supervised machine learning models. Post-hoc approaches are often considered to provide interpretation of models. This means that once a model is learned, post-operative experiments are conducted to gain insight into what the model has learned. <ref type="bibr" target="#b5">[6]</ref> proposes a method to find the optimal stimulus for each unit in a deep neural network by performing back propagation with respect to image space to maximally stimulate a neuron. Obtained images give us an insight into the appearance of the input that a neuron is most likely to activate. The work of <ref type="bibr" target="#b26">[27]</ref> sheds light on what spatial context of the image the convolutional neural network (CNN) is leveraging on for image classification through saliency maps. Similarly, the authors of <ref type="bibr" target="#b33">[34]</ref> use a deconvnet <ref type="bibr" target="#b34">[35]</ref> to map the activities of intermediate layers back to the input pixel space so that inputs that maximally activates an intermediate layer can be directly visualized as an image. The methods mentioned above uncover that CNNs learn to decompose the image space into hierarchical modular patterns. However, not all visualized patterns are necessarily interpretable or understandable. In such post-hoc approaches, there is no control over how the model is optimized in the first place. Though it is valuable and interesting to expose what the model has learned after-the-fact, we wish to take a more active approach to the problem. We focus our investigation on how to improve model interpretability by design.</p><p>Another popular direction in post-hoc approaches is the use of examples and prototypes. Example-based explanations and classifiers have shown to offer a condensed view of a dataset, potentially offering a reason why a classifier came to a certain conclusion through other data points in the dataset <ref type="bibr" target="#b30">[31]</ref>. The work of <ref type="bibr" target="#b16">[17]</ref> pushes this idea further by forcing a model to produce both exemplars and criticisms. Even with such examples, the causality between model parameters and the final prediction of the model is still unclear. In our work, we strive to take a more direct approach on model interpretability. We focus on two key questions: 1. How do we interpret the representations learned using TCN and 2. How can we design a deep learning architecture that provides readily interpretable hidden representations and model parameters in the context of 3D human action analysis?  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> and segmentation tasks <ref type="bibr" target="#b2">[3]</ref>. The network is built from stacked units of 1-dimensional convolution followed by a non-linear activation function. The 1-dimensional convolution is across the temporal domain.</p><p>The input to an original TCN is a sequence of video features. A D-dimensional feature vector, whether it is a deep feature from a spatial CNN such as f c7 activation of AlexNet <ref type="bibr" target="#b17">[18]</ref> or a set of kinematic features <ref type="bibr" target="#b18">[19]</ref>, is extracted per each video frame. For a video of T frames, the input X is simply a concatenation of all frame-wise features such that X ∈ R T ×D .</p><p>As with well-known CNN models, repeated blocks of convolutions followed by non-linear activations extract features from the input. More precisely, in a TCN, the l-th convolution layer with a temporal window of d l consists of</p><formula xml:id="formula_0">N l filters {W (i) } N l i=1 where each filter is W (i) ∈ R f l ×N l−1 .</formula><p>Given an output X l−1 of the previous layer, the l-th layer output, X l is simply</p><formula xml:id="formula_1">X l = f (W * X l−1 )<label>(1)</label></formula><p>where f is a non-linear activation function such as ReLU. The whole network is trained with back-propagation. In an attempt to further improve the interpretability of a TCN, we adopt residual connections of <ref type="bibr" target="#b10">[11]</ref>. In the following sections, we discuss how such skip connections and the resulting TCN architecture, namely Res-TCN, leads to improved interpretability of 3D human action recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Interpretability of TCNs with Residual Connections</head><p>In our work, for the purpose of 3D human activity recognition, the input to the model X 0 is a frame-wise skeleton features concatenated temporally across the entire video sequence. An important pre-requisite of the interpretability of TCNs is that each dimension of the frame-wise feature must be interpretable as well. Let x t be a skeleton feature extracted from a video frame t where x t ∈ R D . By construction of the skeleton feature, the d-th dimension of the feature, x t (d), has an interpretable meaning associated with it (for example, Z position of the right elbow joint). The details of feature construction will be covered in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">TCN with Residual Units and Identity Mappings</head><p>The biggest road-block in interpreting current spatiotemporal models such as LSTM-based RNNs for 3D human action analysis stems from the lack of clear connection between the learned model parameters and their hidden representations. However, for TCNs, the formulation of hidden representations from its model parameters is straightforward to comprehend: activation maps are computed by convolving a learnable temporal filter across time and passing the output through a ReLU unit. In a ReLU network, after an iteration of a forward-backward pass, the network parameters are optimized such that convolution of a filter across the characteristic regions of the input more likely produces a positive value in the next iteration. We can exploit such behavior of the model to improve the model interpretability by re-formulating the TCN with residual con- nections <ref type="bibr" target="#b10">[11]</ref>.</p><p>As introduced in <ref type="bibr" target="#b10">[11]</ref>, skip connection with identity mapping introduces beneficial properties for network convergence even for very deep networks. We observe that such design for CNNs improves model interpretability as well given input with semantic meaning. Our Res-TCN model architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Res-TCN stacks building blocks called Residual Units as introduced in <ref type="bibr" target="#b9">[10]</ref> and adapts the pre-activation scheme of <ref type="bibr" target="#b10">[11]</ref>. Each unit in layer l performs the following computation:</p><formula xml:id="formula_2">X l = X l−1 + F (W l , X l−1 ) (2) F (W l , X l−1 ) = W l * σ(X l−1 )<label>(3)</label></formula><p>F denotes the residual unit. For the l-th layer, X l−1 denotes the input, W l is the set of learnable parameters and σ is a ReLU activation function. We can re-write the expression W l * σ(X l−1 ) as W l * max(0, X l−1 ) when σ is ReLU. The only exception in our architecture is the very first convolution layer. The first convolution layer in Res-TCN operates on raw skeleton input and the resulting activation map, X 1 , is passed on the subsequent layers. Given a Res-TCN with N residual units, the hidden representation after N residual units is:</p><formula xml:id="formula_3">X N = X 1 + N i=2 W i * max(0, X i−1 )<label>(4)</label></formula><formula xml:id="formula_4">X 1 = W 1 X 0<label>(5)</label></formula><p>Note that X 1 is a result of convolving a set of filters in layer l = 1 without undergoing any non-linear activation. The set of filters in W 1 and the resulting activation map, X 1 , are directly interpretable given that each dimension of X 0 is directly interpretable as well, such is the case when X 0 is a set of skeleton features. An important observation in our design is that in the l-th residual unit where l ≥ 2, ReLU is performed on X l−1 prior to applying convolution with filters in W l . In other words, the gradient only flows through the positive regions of X l−1 and W l learns to pick up discriminative patterns where X l−1 &gt; 0. The computation W l * max(0, X l−1 ) is then added to the input, X l−1 , and passed on to the next layer. The input to the first residual unit is X 1 and all subsequent residual units in a Res-TCN either adds to or subtracts from X 1 as shown in Equation <ref type="bibr" target="#b3">4</ref>. In this formulation, we are forcing the network to learn discriminative spatio-temporal features in the common language of X 1 . In the experiments section, we visualize hidden representation of an activity from one of the deeper layers and show its connection to X 1 to validate our analysis.</p><p>For prediction, we apply global average pooling after the last merge layer across the entire temporal sequence and attach a softmax layer with number of neurons equal to number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A Closer Look at Model Parameters</head><p>In a Res-TCN architecture, Equation 4 suggests that the representational power of the entire model depends heavily on producing discriminative X 1 through filters in W 1 . In this section, we analyze what each filter in W 1 represents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider a single 1D convolution filter W</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{W</head><formula xml:id="formula_5">(i) 1 } N1 i=1 . W (k) 1</formula><p>computes 1D convolution over X 0 ∈ R T ×D with a defined stride of s and a filter length of f 1 . Examples of converged filters are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Each filter looks at f 1 time steps concurrently across all feature dimensions such that W (k) 1 ∈ R f1×D . An important property of W 1 filters is that each dimension d ∈ 1, ... , D has an explainable meaning associated with it. For example, the d-th dimension of a skeleton feature x t (d) depicts a spatial configuration (euclidean X, Y or Z coordinate wrt. the depth sensor) of a particular joint at time t. For instance, the filter depicted on the left in <ref type="figure" target="#fig_1">Figure 2</ref> has parameters close to zero for all joint location except at indices corresponding to joint numbers 11 and 25 as defined in <ref type="bibr" target="#b25">[26]</ref>. We can take a step further in interpreting this filter: the filter directly encodes how the joints move through time. In a time window defined by f l , for filter weights associated with joint indices 11 and 25, the weights increase sharply towards a peak and then returns back to their starting magnitudes. We know that in order for this particular filter to produce a high positive convolution score, the input X 0 at corresponding dimensions must exhibit a highly correlated sequence structure to that of the filter. We can then provide a clear explanation of what this filter is looking for: "A quick jitter movement of the joints in the right hand". Similarly, consider the filter on the right in <ref type="figure" target="#fig_1">Figure 2</ref>. By design, the bottom half of the parameters correspond to the second actor. Following the same logic described above, we can clearly understand that this filter produces a high positive convolution score with an input where two actors are translating apart from each other. In the following section, we extend our discussion to parameter interpretability in the deeper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">A Deeper Look at Model Parameters</head><p>Let us now extend our analysis to deeper layers in the model. In a Res-TCN formulation, deeper layers are factored out into residual units and an output from a residual unit is simply merged by adding to the input of the residual unit. For example, consider the hidden representation after two convolution layers:</p><formula xml:id="formula_6">X 2 = X 1 + W 2 * max(0, X 1 )<label>(6)</label></formula><p>where X 1 = W 1 * X 0 . Filters in W 2 convolve over the positive regions of the output produced by W 1 * X 0 such that W</p><formula xml:id="formula_7">(k) 2 ∈ R f2×N1 for some k ∈ [0, N 2 )</formula><p>where f 2 defines the filter length in layer l = 2, N 1 is the number of filters in layer l = 1 and likewise for N 2 . Following the formulation of Equation <ref type="bibr" target="#b5">6</ref>, we observe that W 2 acts as a gate that modulates how much information will be transformed and added on to X 1 . Given a filter W to be added to the output. We can recursively trace down such influential filters all the way down to the very first convolution layer where we can directly map filter parameters to interpretable skeleton motion as shown in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We validate that our approach not only leads to an interpretable representation but also to a discriminative one. We evaluate Res-TCN on 3D skeleton based human activity recognition dataset of NTU <ref type="bibr" target="#b25">[26]</ref>. We also provide interpretations on our model predictions based on the concepts discussed in the previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset and Settings</head><p>NTU RGB+D dataset <ref type="bibr" target="#b25">[26]</ref> is currently the largest human activity recognition dataset with full 3D skeleton annotations. It contains 56880 training videos ranging over 60 action classes. The dataset provides two train/test split paradigms: Cross-Subject (CS) and Cross-View (CV) settings. The dataset covers 40 distinct subjects with varying physical traits. In terms of camera viewpoints, three cameras are placed in three different angles:−45 • , 0 • and +45 • .</p><p>Implementation Details: We follow the skeleton feature construction procedure as adapted in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. However, in contrast to their feature extraction stage, we do not perform view normalization prior to feeding the features into our Res-TCN. We take the raw (X,Y,Z) values of each skeleton joint and concatenate all values to form a skeleton feature per frame. Given that there are at most two actors in the scene and there are 25 joints per skeleton, a skeleton feature per frame is a 2 * 25 * 3 = 150 dimensional vector. We use the Keras deep learning framework <ref type="bibr" target="#b3">[4]</ref> with a TensorFlow backend <ref type="bibr" target="#b0">[1]</ref>. We use an initial learning rate of 0.01 and decrease the learning rate by a factor of 10 when the testing loss plateaus for more than 10 epochs. We use stochastic gradient descent with nesterov acceleration with a momentum of 0.9. L-1 regularizer with a weight of 1e −4 is applied to all convolution layers. We use a batch size of 128. Dropout <ref type="bibr" target="#b29">[30]</ref> with rate 0.5 is applied after all activation layers to prevent overfitting. We perform all our experiments on a Nvidia K80 GPU. The implementation and converged model weights will be made publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Why Did My Model Predict This?</head><p>Leveraging on the explainable structure of Res-TCN, we wish to provide an answer to the question: "How/why did my model come to this conclusion?", using only the model parameters and hidden representations as the basis for providing such an explanation.</p><p>Let us choose an arbitrary video clip from NTURGB+D. The particular sequence of skeletons that we visualize in <ref type="figure" target="#fig_4">Figure 4</ref> is of class kicking something and is approximately 70 frames long. The output of the first block (Block-A in <ref type="figure" target="#fig_0">Figure 1</ref>) is displayed above. As discussed in section 4.3, we can trace which of the W 1 filters had the largest influence on any given deeper hidden representation X n where n &gt; 1. For clarity in visualization, for each time step, we only plot the activation values that are within the top 20 1 https://github.com/TaeSoo-Kim/TCNActionRecognition/ percentile. Each column denotes the activation values from all filters in W 4 and each row denotes the corresponding filter's response over time. Consider the dimension in the activation map that is color coded with green in <ref type="figure" target="#fig_4">Figure 4</ref>. By following the logic described in section 4.2, we found that this particular filter produces a high positive response for translational movement of the left ankle and left hip. The yellow filter has high magnitude parameters associated with the right knee joint. And finally, the blue filter picks up signals from the right ankle and the left wrist joint. The activation map of X 4 and the corresponding W 1 filters tell a rather detailed and precisely timed story about the input skeleton sequence: the left ankle and hip joints first translate followed by a sudden movement of the right knee, all the while the left wrist and the right ankle undergo a swinging motion.</p><p>The bit about the swinging motion can be inferred from the relative change in the magnitude of the activation in the dimension corresponding to the blue filter. <ref type="figure" target="#fig_5">Figure 5</ref> zooms into this particular set of filters and shows their activation magnitudes over the entire video sequence. What is very interesting here is that the activation of the filter corresponding to left ankle and hip joints is close to zero at the peak of the kicking motion. At approximately the same time step, the activation magnitude of the dimension corresponding to the right knee joint peaks. The story that the filters are explaining makes sense. The sequence description that we can interpret from the filters and their activations provides insight into why the model arrived to a certain prediction. During a kicking activity, we first step towards the target with our pivot foot, firmly plant the pivot foot (in this case, the left foot), swing the kicking foot around and step back to return to original position. Note that we focused our analysis on selected interesting dimensions of the hidden representation with significant weight magnitudes. It is important to note that all other positive dimensions also factor into the final decision of the classifier but our discussion was focused on the significant and interesting ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to Other State-of-the-Art</head><p>We focused most of our narrative on how a Res-TCN formulation yields explainable spatio-temporal representation compared to state-of-the-art LSTM-based RNN counterparts. We also validate the effectiveness of our model on producing discriminative spatio-temporal features for 3D human action analysis. We compare the performances of published methods on NTURGB+D dataset and show that we improve on the current state-of-the-art on both Cross-Subject and Cross-View settings.  <ref type="bibr" target="#b25">[26]</ref> 60.7 67.3 P-LSTM <ref type="bibr" target="#b25">[26]</ref> 62.9 70.3 Trust Gate <ref type="bibr" target="#b21">[22]</ref> 69.2 77.7 STA-LSTM <ref type="bibr" target="#b28">[29]</ref> 73.4 81.2 Res-TCN 74.3 83.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a new approach to performing 3D human action analysis with a Res-TCN. We discuss how such an architecture enhances the interpretability of model parameters and features compared to other popular RNN based approaches. Given an interpretable input such as sequence of human skeletons positions, we can begin to explain what each of the learned filters in a Res-TCN are leveraging on to make a prediction. We show that the model learns to pay different levels of attention both spatially and temporally. Experimentally, we validate that our model is explainable and produces a discriminative representation for human activity analysis, improving upon the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Res-TCN model architecture. Except the first convolution layer (in gray), the model consists of stacked residual units. action recognition. The properties of a TCN follow those of a modern spatial Convolutional Neural Network (CNN) for recognition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Examples of direct mapping from layer 1 filter parameters to skeleton joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(k) 1 fromFigure 3 .</head><label>13</label><figDesc>An example how we can map convolution filters in deeper layers, such as W k 3 , all the way back to filters in the first layer W1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>large weight value in the d-th dimension of W (k) 2indicates that this particular filter adds to or subtracts from X 1 a weighted version of the incoming signal at the same dimension d where d ∈ [0, N 1 ]. Dimensions with low weight magnitudes contributes less to the final output of the residual unit. Consider example filters from deeper convolution layers shown inFigure 3. Most parameters are close to zero except in certain dimensions. Given the additive nature of residual units, we can directly map such dimensions to filters in the lower layer. If dimension k of W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>An example skeleton sequence, its hidden representation (X4) in Res-TCN and associated filters from W1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The plot depicts the changes in activation magnitudes of three semantically meaningful filters over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison to other learning based methods on NTURGB+D skeleton dataset with Cross-Subject (CS) and Cross-View (CV) settings in accuracy (%).</figDesc><table><row><cell>Methods</cell><cell>CS</cell><cell>CV</cell></row><row><cell cols="3">Dynamic Skeletons [15] 60.2 65.2</cell></row><row><cell>HBRNN [5]</cell><cell cols="2">59.1 64.0</cell></row><row><cell>Deep LSTM</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Overview of Temporal Convolutional Neural NetworksIn this section, we provide a brief overview of the structure of a TCN as provided in the original paper<ref type="bibr" target="#b18">[19]</ref>. Note that the original TCN is designed for temporal action segmentation in video and it follows a convolutional encoderdecoder design. We adapt the encoder portion of the net for</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<title level="m">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno>1341</idno>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional {LSTM} and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Space-time representation of people based on 3D skeletal data: A review. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
		<idno>abs/1605.04988</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2280" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno>abs/1606.03490</idno>
		<title level="m">The mythos of model interpretability. ICML WHI 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>III- 1310-III-1318. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;13</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">3d human pose estimation: A review of the literature and analysis of covariates. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1312.6034</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototype selection for interpretable classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2403" to="2424" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="818" to="833" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3697" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
