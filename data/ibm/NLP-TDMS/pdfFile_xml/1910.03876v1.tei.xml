<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SNIDER: Single Noisy Image Denoising and Rectification for Improving License Plate Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younkwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institute of Science and Technology (GIST)</orgName>
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhyun</forename><surname>Lee</surname></persName>
							<email>leejuhyun@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institute of Science and Technology (GIST)</orgName>
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoyeon</forename><surname>Ahn</surname></persName>
							<email>ajhoyeon@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institute of Science and Technology (GIST)</orgName>
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
							<email>mgjeon@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institute of Science and Technology (GIST)</orgName>
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SNIDER: Single Noisy Image Denoising and Rectification for Improving License Plate Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an algorithm for real-world license plate recognition (LPR) from a low-quality image. Our method is built upon a framework that includes denoising and rectification, and each task is conducted by Convolutional Neural Networks. Existing denoising and rectification have been treated separately as a single network in previous research. In contrast to the previous work, we here propose an end-to-end trainable network for image recovery, Single Noisy Image DEnoising and Rectification (SNIDER), which focuses on solving both the problems jointly. It overcomes those obstacles by designing a novel network to address the denoising and rectification jointly. Moreover, we propose a way to leverage optimization with the auxiliary tasks for multi-task fitting and novel training losses. Extensive experiments on two challenging LPR datasets demonstrate the effectiveness of our proposed method in recovering the high-quality license plate image from the low-quality one and show that the the proposed method outperforms other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>License plate recognition (LPR) from the real-world is one of the fundamental problems in several intelligent transport systems (ITS) applications such as vehicle reidentification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref>, outdoor scene understanding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>, and de-identification for privacy protection <ref type="bibr" target="#b9">[10]</ref>. In the last few years, LPR has been widely studied in theoretical, experimental and numerical ways to provide robust image representation. Many LPR methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref> are capable of capturing the structural properties of images and noise for carefully constrained settings. Despite the recent success, recognizing license plate in the wild is still far from satisfactory due to the variations that suffer from appearance, noise, angle, and illumination.</p><p>Recently, due to the hierarchical feature extraction and learning capability, deep convolutional neural networks (CNNs) have made remarkable advances in many computer vision applications, such as object detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>, semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>, action recognition <ref type="bibr" target="#b36">[37]</ref>, and face recognition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b26">27]</ref>. As a result, CNN-guided LPR methods are also extensively applied to handle the problem of recognizing license plate captured directly real-world camera. For example, Zhuang et al. <ref type="bibr" target="#b40">[41]</ref> transform license plate into a semantic segmentation result with the counting network to handle appearance variations. Although numerous LPR methods have been developed <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>, they are not still capable of learning all types of samples in the wild. For this reasons, their algorithms practically assume a high-quality image as an input. Generally, the typical appearance of the license plate collected in real-world scenes might contain the aforementioned challenges, causing deterioration in LPR performance. Hence, developing and implementing robust LPR framework are highly indispensable, especially for real-world scenes.</p><p>In this paper, we design an end-to-end single noisy image denoising and rectification network (SNIDER) for better LPR based on multiple auxiliary tasks. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the LPR framework in which the proposed SNIDER is combined with a pre-trained LPR network. The SNIDER con-sists of two sub-networks: a denoising network and a rectification network. Motivated by the success of U-Net <ref type="bibr" target="#b30">[31]</ref> in recovering the object details, we employ U-Net structure as an image recovery backbone network, attempting to extract visual content at structural-level details. In the denoising sub-network (DSN), we try to transform a low-quality image to a high-quality image pixel by pixel directly. The DSN can penalize the loss between noisy and noise-free image pairs and thus acquire the output image with the fine textures of the clean component, learning an independent realization of the noise. However, even with such sophisticated DSN, denoising images are unsatisfactory because they still have arbitrary geometric variations. Therefore, the rectification sub-network (RSN) is proposed to correct geometric distortions of denoising license plates and generate more accurate correction image distortion. Furthermore, we propose to leverage the new auxiliary tasks to further optimize the image recovery sub-networks (DSN, RSN) of SNIDER. There are two auxiliary tasks: a text counting module and a segment prediction module. Specifically, we solve each auxiliary module using CNN as a decoder. The counting module is used to predict the number of text in the image as a classification problem. In this module, despite the ambiguous boundary of consecutive text, text counting can distinguish single text, which makes the image quality suitable for text detection. For the segment prediction module, we propose a binary segmentation to emphasize the foreground over the background. The generated segmentation result makes the license plate clean for text recognition. Finally, learning the auxiliary tasks will lead the intermediate features of the recovery main task networks to enhance the difficulties such as geometric variations and low-quality information. More importantly, we introduce a new loss function that trains the SNIDER with auxiliary tasks, which provide significantly higher license plate quality for robust LPR.</p><p>To sum up, we highlight the main contributions of this paper as follows:</p><p>• We propose a novel end-to-end license plate recovery network, where denoising and rectification network are used to generate a clear recovery image for robust LPR performance.</p><p>• We present the auxiliary tasks to leverage the quality of the license plate recovery from low-quality. Mainly a new loss is introduced to provide regularization effects to the backbone SNIDER for robust representation and license plate recovery.</p><p>• Finally, we demonstrate the effectiveness of the proposed method in recovering a high-quality license plate from a low-quality license plate in the real-world and show that the LPR performance outperforms the state-of-the-art methods on two challenging datasets, AOLP-RP <ref type="bibr" target="#b12">[13]</ref> and VTLPs dataset newly collected on the most challenging real-world environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review on low-quality image recovery methods and license plate recognition methods that is most related to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Low-Quality Image Recovery</head><p>To obtain the high-quality image, most of the existing methods depend on the assumption that both signal and noise arise from particular statistical regularities by using hand-crafted algorithms, such as anisotropic diffusion <ref type="bibr" target="#b27">[28]</ref> and total variation <ref type="bibr" target="#b31">[32]</ref>. Besides, non-parametric models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> were developed to model image noise, but they were also not robust to the unconstrained environment in the wild due to priors estimated from limited observations. Recently, due to the advances in deep learning, most denoising algorithms are designed with deep neural network architectures and data-driven approach rather than relying on the priors. Burger et al. <ref type="bibr" target="#b2">[3]</ref> employ multi-layer perceptrons with a data-driven technique based on an extensive image database. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> train the deep CNN by utilizing batch normalization (BN) <ref type="bibr" target="#b13">[14]</ref> and residual learning <ref type="bibr" target="#b11">[12]</ref>.</p><p>Though useful for estimating a clean image, text classifiers are still hard to recognize due to the irregular text geometry. It motivates research for image recovery to extend image rectification. Shi et al. <ref type="bibr" target="#b33">[34]</ref> develop a spatial transformer network (STN) for rectifying text distortion. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> adopt more in-depth representations of images by a residual network. Different from the existing methods, in this paper, we extract deep representations of images using the U-Net-based CNN for denoising as well as rectification. To the best of our knowledge, our research may be first work to apply the two modules mentioned above for LPR at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">License Plate Recognition</head><p>Before the advent of deep learning, most of the traditional LPR methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref> employ two-stage process flow, involving text detection and following text recognition. After the advancement of deep learning, many approaches employ a one-stage process flow without text detection. Li et al. <ref type="bibr" target="#b19">[20]</ref> extract deep feature representations by using RNN with LSTM for acquiring sequential features of the license plate. Bulan et al. <ref type="bibr" target="#b1">[2]</ref> estimate domain shifts between target and multiple source domains for selecting a domain that yields the best recognition performance based on fully convolutional network <ref type="bibr" target="#b22">[23]</ref>. However, these methods only consider high-quality license plate image except for low-quality image, which is easily led to low performance in real-world scenes. Moreover, their methods lack little or no effort to improve image quality, while requiring a lot of computing power. In this work, unlike existing methods, we adopt image recovery for high LPR performance under the low-quality image in real-world scenes. To the best of our knowledge, this is the first time we apply sophisticated image recovery to handle a challenging real-world environment. Besides, our methods are computationally efficient and capable of real-time recognition despite additional recovery modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The proposed approach consists of three parts: 1) main tasks prediction networks G D and G R for denoising and rectification; 2) auxiliary tasks prediction networks D c and D s for count classification and segment prediction; 3) LPR network for text detection and classification. The proposed architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. For training, dataset for main tasks and auxiliary tasks can be inferred from the intentionally transform operation by simple rotation (for rectification) and down-resizing (for denoising), as shown in   and corresponding samples. LPR network LP R then takes G R (G D (I LQ i )) to recognize a recovery image. In the following subsections, we introduce the method to predict the main tasks in Section 3.1. Then, we also address the auxiliary tasks for prediction in Section 3.2. Then, we describe the network training of the proposed architecture in Section 3.3. Finally, we illustrate the testing process in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Denoising and Rectification Network</head><p>Our main task networks include two sub-networks (i.e. denoising sub-network and rectification sub-network), and the first sub-network takes the low-quality image as the input, and the output is the recovered image. In this paper, we design the rectification network to rectify the denoising results from the denoising network.</p><p>The image recovery results <ref type="bibr" target="#b14">[15]</ref> have shown the effectiveness of the U-Net since it can provide high-quality overall details of an image object, without a negative impact on the image generation. Therefore, we adopt a U-Net-based architecture adding skip connections that shuffle low-level information shared between input and output across the network. In contrast to their network, our recovery network includes two sub-networks, which are also the U-Net architecture. As shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_1">Figure 2</ref>.(b,c), our denoising network G D and rectification network G R consist of the encoder and decoder module.</p><p>To achieve the main tasks, we first feed I LQ i into G D to generate denoising results. Given a pair of input image and non-rectified ground-truth denoising image</p><formula xml:id="formula_0">{I LQ i,j , I HQ i,j } N (i,j)</formula><p>, loss function for the G D is the pixel-wise MSE loss, and it is calculated as Eq. <ref type="formula" target="#formula_1">(1)</ref>:</p><formula xml:id="formula_1">L G D (w) = 1 N N (i,j)∈N G Dw (I LQ (i,j) ) − I HQ (i,j) 2 ,<label>(1)</label></formula><p>where w is the parameters of denoising network. Such loss function encourages the G D to not only extract the content information of input image but also generate a high-quality natural image in pixel level. Then, the rectification sub-network G R processes the output from G D , and outputs a rectified high-quality image, which is easier for the LPR network to recognize the identification text. With the training pairs of</p><formula xml:id="formula_2">{G D (I LQ i,j ), I HQ i=0,j } N (i,j)</formula><p>, the G R can be trained using a L1 loss for the predicted result G R (G D (I LQ i,j )):</p><formula xml:id="formula_3">L G R (w) = 1 N N (i,j)∈N G Rw (G Dw (I LQ (i,j) )) − I HQ (i=0,j) 1 ,<label>(2)</label></formula><p>where w is the parameters of the rectification network. Unlike L2 loss, using L1 loss in the pixel level helps to preserve the appearance of an object, such as image color, intensity, and illumination, and leads to denoising result capable of only geometric transformation. Therefore, we can only perform geometric transformations without the appearance damage of the image during the rectification process, which forces the recognizer to be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Auxiliary Tasks Prediction</head><p>Due to the complex real-world environments such as the extremely irregular geometric shape of text as well as the complicated image background, the binary information of the license plate is often noisy. Although we intend G D and G R to capture robust features for image recovery, the results by this structure do not always guarantee a well-enhanced output. Therefore, our work involves an additional learning branch where a richer feature representation is obtained from the backbone network. Motivated by multi-task learning <ref type="bibr" target="#b4">[5]</ref>, we employ the auxiliary tasks, i.e., binary segmentation and count estimation, which will contribute our main task networks produce more discriminative feature representations. Towards this problem, we sum the weights of the last layer of encoders in order to guide auxiliary task networks to help main task networks effectively extract critical information from the low-quality image. For the binary segmentation task, we introduce the segment decoder D s based on U-Net architecture. Detailed architectures of the D s are shown in <ref type="table" target="#tab_0">Table 1</ref>. The D s accepts feature set F summed from the last features of each main task's encoder and outputs a license plate segment with val-ues indicating the probability of pixels belonging to the license plate. Also, ground-truth labels for segmentation can be inferred from the dotted annotations by <ref type="bibr" target="#b3">[4]</ref>'s method as Otsu Thresholding, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Although our segmentation annotations by <ref type="bibr" target="#b3">[4]</ref> do not fully reflect the actual detail appearance of an image, we have shown in the experiments that this auxiliary and straightforward learning strategy leads to effective advances in image recovery. Given a pair of F and the ground-truth segmentation result in I seg , loss function for the D s is the binary cross-entropy loss:</p><formula xml:id="formula_4">L Ds (w) = 1 N N (x,y)∈N I seg (x,y) log(D sw (F ) (x,y) ) + (1 − I seg (x,y) )log(1 − D sw (F ) (x,y) ),<label>(3)</label></formula><p>where I seg (x,y) ∈ {0, 1} is the real classes of pixels in I seg with 1 for the license plate area and 0 for the background, D s (F ) (x,y) denotes the pixel-wise probability by D s .</p><p>Also, we find that the generated recovery samples cannot usually distinguish successive texts due to close to each other. Motivated by the observations, we add a counting decoder D c , which predicts the number of characters in the image. As a result, our D c plays two roles, where the first is to cause separation between adjacent texts more clearly. The other role is to promote the encoders of each main task to generate a higher quality image while backpropagating the penalty. The loss function for the D c is the L2 loss:</p><formula xml:id="formula_5">L Dc = C pred − C G.T 2 ,<label>(4)</label></formula><p>where C pred and C G.T are the predicted value and the ground-truth, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Training</head><p>The full objective function is a weighted sum of all the losses from Eq. (1) to (4):</p><formula xml:id="formula_6">L = λ G D L G D + λ G R L G R + λ Ds L Ds + λ Dc L D C (5)</formula><p>We employ a stage-wise training strategy to optimize main tasks with auxiliary tasks and empirically set the weights of each loss as detailed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Testing</head><p>At the testing phase, the auxiliary tasks are removed. Given a low-quality test image I test , G D and G R output the recovered image via denoising and rectification. Then LPR network LP R based on a YOLO v3 detector <ref type="bibr" target="#b28">[29]</ref> by pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> takes the recovered image and generates the recognition result LP R result of I test , and it is denoted as Eq. <ref type="formula" target="#formula_7">(6)</ref>:</p><formula xml:id="formula_7">LP R result = LP R(G R (G D (I test ))).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setting</head><p>In this section, we describe a list of datasets, metric, and implementation details for the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use LPR datasets AOLP <ref type="bibr" target="#b12">[13]</ref> and newly collected dataset, named VTLP.</p><p>AOLP-RP : AOLP-RP <ref type="bibr" target="#b12">[13]</ref> consists of 611 images collected in Taiwan, including ten numbers and 25 letters (except "O"). This dataset has a challenging factor that the angle of the LP contains oblique samples in terms of distortion. On the other hand, in terms of resolution, all images are relatively easy because they consist of high-resolution samples rather than other datasets.</p><p>VTLP : We introduce a new challenging large-scale dataset collected in South Korea. The dataset contains 10,650 LP images, which are divided into 6,400/4,250 images for training and testing, respectively. All Korean letters are hidden for privacy protection. Images in VTLP consist of text(only 10 digits, not Korean). Compared with the public LPR datasets, our dataset has challenging factors: 1) We apply the manual annotation of large-scale images selected from unconstrained real-world, covering a variety of challenging situations using bounding box coordination; 2) Distance from vehicles to the camera is far from other dataset; 3) Various scene-texts interfere with the detection, low-resolution appearance, and very oblique LP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>We follow the evaluation metric that has been widely used in LPR research <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>. Therefore, if only one of the consecutive characters is misclassified or not detected, it is treated as a failure case. We denote this metric as a recognition accuracy. Also, we address the 36 characters, including 26 letters and 10 digits for text recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>All the reported implementations are based on the Ten-sorFlow framework, and our method has done on one NVIDIA TITAN X GPU and one Intel Core i7-6700K CPU. In all the experiments, we resize all images to 320 × 320. For stable training, we use a gradient clipping trick and the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with high momentum. The proposed network is trained in 1 million iterations with a batch size of 16. The weights in all SNIDER layers are initialized from a zero-mean Gaussian distribution with a standard deviation of 0.01, and the constant 0 as the biases in all layers. All models are trained for the first 100 epochs with a learning rate of 10 −4 despite higher values, and then for the remaining epochs at the learning rate of 10 −5 . Batch normalization <ref type="bibr" target="#b13">[14]</ref> and LeakyReLU <ref type="bibr" target="#b23">[24]</ref> are used in all layers of our networks. Also, for LP R network as baseline, we use the YOLO v3 detector <ref type="bibr" target="#b28">[29]</ref> model pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>.  Two SNIDER models are trained for evaluations and benchmarking with state-of-the-art methods. The first is a backbone model SNIDER, which uses five convolution blocks at encoder and decoder, respectively. In contrast, the other model denoted by SNIDER-Tiny uses a relatively light network thereby too fast for testing. All SNIDER models are trained under the same parameter setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we evaluate the proposed approach on two datasets: AOLP-RP <ref type="bibr" target="#b12">[13]</ref> and VTLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>We first compare our proposed method with the baseline LPR network to prove the effectiveness of image recovery performance. Both LPR results on two datasets are reported for the following five types of our methods where each module is optionally added: a) the only baseline without proposed method; b) adding one main task; c) adding all main tasks; d) adding all main tasks and one auxiliary task; e) adding all of the modules (namely, proposed method).</p><p>We present the LPR accuracy for each type on two datasets in <ref type="table" target="#tab_1">Table 2</ref>, and the visual comparisons are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. From <ref type="table" target="#tab_1">Table 2</ref>, we can find that adding the denoising and the rectification task, respectively, significantly improves the LPR performance (type b, c). In addition, we observe that LPR performance improves more when both tasks are applied at the same time. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>. <ref type="table">Table 3</ref>. Full LPR performance (percentage) comparison of our method with the existing methods on AOLP-RP <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AOLP-RP Full LPR accuracy (%) Baseline (YOLO v3) 91.65 Hsu et al. <ref type="bibr" target="#b12">[13]</ref> 85.76 Li et al. <ref type="bibr" target="#b20">[21]</ref> 88.38 Silva et al. <ref type="bibr" target="#b34">[35]</ref> 98.36 Zhuang et al. <ref type="bibr" target="#b40">[41]</ref> 99.02 SNIDER-Tiny 98.85 SNIDER 99.18 (c), noise and blurring effect are removed from the lowquality image (a), and characters are enhanced well compared to (c). This confirms that performing two tasks at the same time is more helpful to recover high-quality images.</p><p>Despite showing better LPR performance <ref type="table" target="#tab_1">(Table 2</ref>. (c)), we still find that the output image contains elements that interfere with LPR performance. For example, there are still challenges to detect the suitable text region, including a region that is unnecessary for recognition, such as a manufacturer's logo (see in <ref type="figure" target="#fig_5">Figure 4. (d)</ref>), and ambiguity that not well detected between consecutive characters. Therefore, when each auxiliary task is added to main tasks, recovered image quality can be better <ref type="figure" target="#fig_5">(Figure 4</ref>. (e,f)) and we observe some improvements on LPR performance <ref type="table" target="#tab_1">(Table 2</ref>. <ref type="figure">(d)</ref>). Finally, we incorporate all the tasks, perform experiments on it and observe the best performance improvement in LPR <ref type="table" target="#tab_1">(Table 2</ref>. (e)). Furthermore, the recovered image in <ref type="figure" target="#fig_5">Figure  4</ref>. (g) is the most realistic of all results. show the recovery images by using <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref> and SNIDER, respectively. The sample images are from VTLP which suffer from geometric distortions as well as low-quality. The proposed SNIDER performs better in LPR recovery. Best viewed on the computer, in color and zoomed in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-art Methods</head><p>We compare the proposed method with some state-ofthe-art LPR methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>. For the baseline LPR, the SNIDER has been evaluated over the two datasets as described in Section 4.1 that contain low-quality license plate images with a variety of geometric variations.</p><p>As <ref type="table" target="#tab_1">Table 2</ref>, 3 and 4 show, the SNIDER consistently outperforms the SNIDER-Tiny across all datasets due to the use of a more in-depth and broader backbone network. However, SNIDER-Tiny is also evaluated to be more effective than most methods, and if not, it shows a relatively small performance difference. Therefore, it can be explained that SNIDER is more useful for LPR than other methods for the low-quality image.</p><p>AOLP-RP dataset results. For the AOLP-RP, SNIDER demonstrates that our recovery image can significantly improve the performance of LPR on real-world images. This is mainly due to the fact that AOLP dataset which usually have geometrically tilted cases is processed into a well-rectified image. The results are listed in <ref type="table">Table 3</ref>, and our method obtains the highest performance (99.18%), and outperforms the state-of-the-art LPR methods by more than 0.16%. Note that what we want to illustrate in the AOLP-RP evaluation (especially see the difference between Baseline and ours in <ref type="table">Table 3</ref>) is that our method can benefit from the SNIDER, which enhances the image quality despite oblique angle.</p><p>VTLP dataset results. The quantitative results for VTLP dataset are shown in <ref type="table" target="#tab_2">Table 4</ref> and the visual comparisons are illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. Our approach shows superior performance to other LPR algorithms on LPR accuracy and image recovery. Furthermore, we achieve comparable results with state-of-the-art LPR method <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. From <ref type="table" target="#tab_2">Table 4</ref>, our method obtains the highest performance (93.08%), and outperforms the state-of-the-art methods by more than 5.74% (87.34% vs 93.08%). Note that SNIDER achieves robust performance in VTLP that are collected in low-resolution environments rather than other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Parameter Study of the Weights for Tasks</head><p>The set of weights λ in Eq.(5) determines the influence of each task. To choose the optimal selection of λ, we perform various experiments with the SNIDER model on AOLP-RP and VTLP dataset. Since the influence of the main task is larger than that of the auxiliary task, the weight is also set higher. We also need to adjust the weights for fast optimization even within the auxiliary task. <ref type="figure" target="#fig_5">Figure 4</ref> shows the segment decoder D s plays an important role in eliminating unnecessary areas that interfere with LPR. Therefore, we set the weight of the segment decoder higher than the counting decoder. In our experiment, we set the weights for λ G D , λ G R , λ Ds and λ Dc to 0.4, 0.4, 0.15, and 0.05, respectively. <ref type="figure">Figure 6</ref>. Error study on AOLP and VTLP dataset. Best viewed on the computer, in color and zoomed in. <ref type="table">Table 5</ref>. Impact of improving the LPR network and its performance evaluation with SNIDER for the VTLP testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>LPR Accuracy FPS Faster R-CNN <ref type="bibr" target="#b29">[30]</ref> 87.06 2.7 CornerNet-Squeeze <ref type="bibr" target="#b18">[19]</ref> 93. <ref type="bibr" target="#b38">39</ref> 13.1 CenterNet ResNet-18 <ref type="bibr" target="#b38">[39]</ref> 84.68 46 YOLO v3 <ref type="bibr" target="#b28">[29]</ref> (SNIDER-Tiny) 86.66 44 YOLO v3 <ref type="bibr" target="#b28">[29]</ref> (ours) 93.08 37</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Impact of LPR Network</head><p>We evaluate how LPR network choice impact LPR performance on the VTLP testing set. Results are shown in <ref type="table">Table 5</ref>. We mainly adopt a real-time detector for fast processing. Compare with <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref>, SNIDER indicates that the detector plays an important role in LPR performance. Although previous detectors are high-speed processing through lightweight models, they do not guarantee accuracy. Thus, we adopt YOLO v3, which corresponds to the adequate model that includes enough capacity for rich feature representation during real-time processing. <ref type="figure">Figure 6</ref> shows some failure cases, including some false recovery results. These results identify that more progress is needed to improve the rectification performance further. Future work will address this problem by adding the adjacent context to recovering these more challenging license plate images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Weakness Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a new end-to-end trainable image recovery method that is capable of recognizing license plates in the real-world. The proposed recovery network consists of two sub-networks, the denoising sub-network and the rectification network. In particular, two auxiliary tasks are designed to leverage the recovery of license plates, promoting the feature set to be more robust against the geometric variations and blurry data in the real-world scenes. Moreover, a new loss function is introduced to the backbone network to provide regularization effects and a higherquality recovery image. Extensive experiments over various datasets demonstrate superior performance in license plate recovery and recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The proposed system consists of two components: single noisy image denoising and rectification (SNIDER) for recovering a low-quality license plate image and a license plate recognition (LPR) network for recognizing the final recovery image. The SNIDER is an end-to-end trainable network with auxiliary tasks for better image recovery. The LPR network uses a pre-trained DarkNet based on YOLO v3 to detect texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The training and testing process of the proposed approach with the learning of two auxiliary tasks: (a) The input images are fed into SNIDER for the image recovery; (b, c) SNIDER consists of main tasks (i.e. DSN, RSN) and auxiliary tasks, they transform low-quality data into high-quality data for training the DSN, RSN and auxiliary tasks networks; (d, e) LPR network is for testing and outputs a LPR result. The DSN is trained to generate a denoising image from the low-quality input image. Also, the RSN is trained to generate a rectified image from the result of DSN. The auxiliary tasks include text counting and binary segmentation, which are formulated as classification and regression simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>In particular, only one sample of original image I HQ simply can generate four training samples that have been transformed by different angles. Given the training samples I HQ i for G D , I LQ i for G R , I seg i for D s and c for D c , i ∈ {−30 • , −15 • , +15 • , +30 • }, the main tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Label generation for the training of the proposed method. From a high-quality image as ground truth, the rotated images can be obtained using a simple linear transformation, and the lowquality image is processed through downsampling × 1/4 of rotation image. The segmented image is inferred through binarization<ref type="bibr" target="#b3">[4]</ref> of the low-quality image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>G</head><label></label><figDesc>D and G R extract recovery result from input image I LQ i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Ablation Study. (a) : shows noisy input; (b) : only contains denoising net; (c) : only contains rectification net; (d) : adds all main tasks; (e) : adds segment task from (d); (f) : adds counting task from (d); (g) : adds all of tasks, namely our proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison of different license plate recovery methods: For the three sample images in the first column, columns 2-4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details of different proposed network architectures.</figDesc><table><row><cell></cell><cell>SNIDER</cell><cell>SNIDER-Tiny</cell></row><row><cell></cell><cell>conv1(1,2) : (3×3×32 conv)×2, stride 2</cell><cell></cell></row><row><cell></cell><cell>pool1 : (2×2) max pooling, stride 2</cell><cell></cell></row><row><cell></cell><cell>conv2(1,2) : (3×3×64 conv)×2, stride 2</cell><cell>7×7×32 conv,</cell></row><row><cell>Encoder</cell><cell>pool2 : (2×2) max pooling, stride 2</cell><cell>stride 2</cell></row><row><cell>(for G D ,</cell><cell>conv3(1,2) : (3×3×128 conv)×2, stride 2</cell><cell>7×7×64 conv,</cell></row><row><cell>G R )</cell><cell>pool3 : (2×2) max pooling, stride 2</cell><cell>stride 2</cell></row><row><cell></cell><cell>conv4(1,2) : (3×3×256 conv)×2, stride 2</cell><cell>5×5×128 conv</cell></row><row><cell></cell><cell>pool4 : (2×2) max pooling, stride 2</cell><cell></cell></row><row><cell></cell><cell>conv5(1,2) : (3×3×512 conv)×2, stride 2</cell><cell></cell></row><row><cell></cell><cell>x2 upsample : concat(conv5 2, conv4 2)</cell><cell></cell></row><row><cell></cell><cell>conv6(1,2) : (3×3×256), stride 2</cell><cell></cell></row><row><cell>Decoder (for G D , G R , Ds)</cell><cell>x2 upsample : concat(conv6 2, conv3 2) conv7(1,2) : (3×3×128), stride 2 x2 upsample : concat(conv7 2, conv2 2) conv8(1,2) : (3×3×64), stride 2</cell><cell>5×5×64 conv 7×7×32 deconv, dilate 2 7×7×3 deconv,</cell></row><row><cell></cell><cell>x2 upsample : concat(conv8 2, conv2 2)</cell><cell>dilate 2</cell></row><row><cell></cell><cell>conv9(1,2) : (3×3×32), stride 2</cell><cell></cell></row><row><cell></cell><cell>conv10 : (1×1×3)</cell><cell></cell></row><row><cell></cell><cell>N×N×512 conv</cell><cell></cell></row><row><cell>Decoder (for Dc)</cell><cell>1×1×256 conv 1×1×128 conv 1×1×64 conv</cell><cell>N×N×128 conv 1×1×64 conv 1×1×1 conv</cell></row><row><cell></cell><cell>1×1×1 conv</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the effectiveness of different components. DSN, RSN, SD, and CD represent the GD, GR, Ds, and Dc, respectively.</figDesc><table><row><cell cols="2">Type Method</cell><cell>LPR Accuracy AOLP VTLP</cell></row><row><cell>a</cell><cell>Baseline (YOLO v3)</cell><cell>91.65 80.45</cell></row><row><cell>b</cell><cell>Add DSN Add RSN</cell><cell>91.98 84.64 97.05 87.13</cell></row><row><cell>c</cell><cell>Add DSN, RSN</cell><cell>98.53 90.71</cell></row><row><cell>d</cell><cell>Add DSN, RSN, SD Add DSN, RSN, CD</cell><cell>99.02 92.08 98.69 91.08</cell></row><row><cell>e</cell><cell cols="2">Add DSN, RSN, SD, CD (ours) 99.18 93.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Full LPR performance (percentage) comparison of our method with the existing methods on VTLP.</figDesc><table><row><cell>Method</cell><cell>VTLP Full LPR accuracy (%)</cell></row><row><cell>Baseline (YOLO v3)</cell><cell>80.45</cell></row><row><cell>Laroca et al. [18]</cell><cell>87.34</cell></row><row><cell>Silva et al. [35]</cell><cell>84.73</cell></row><row><cell>SNIDER-Tiny</cell><cell>86.66</cell></row><row><cell>SNIDER</cell><cell>93.08</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A license platerecognition algorithm for intelligent transportation system applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loumos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation-and annotation-free license plate recognition with deep localization and failure identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bulan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kozitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shreve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2351" to="2363" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new iterative triclass thresholding technique in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1038" to="1046" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Critical motion detection of nearby moving vehicles in a vision-based driver-assistance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cherng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video denoising by sparse 3d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Signal Processing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="145" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Preservative license plate deidentification for privacy protection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="468" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vehicle license plate recognition based on extremal regions and restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1096" to="1107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Applicationoriented license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="552" to="561" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learningbased approach for license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing X. Proceedings of the 2000 IEEE Signal Processing Society Workshop (Cat. No. 00TH8501)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A robust real-time automatic license plate recognition based on the yolo detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cornernet-lite: Efficient keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05610</idno>
		<title level="m">Reading car license plates using deep convolutional neural networks and lstms</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward end-to-end car license plate detection and recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1126" to="1136" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new framework for background subtraction using multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="493" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning feature representation for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep neural networks for vehicle re-id with visualspatio-temporal path proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">License plate detection and recognition in unconstrained scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Driver drowsiness detection using condition-adaptive representation learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end system of license plate localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Dianat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Mestha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23020</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards human-level license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="306" to="321" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
