<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attentionbased feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is a basic task in computer vision. Despite the recent progress, it remains a challenging task due to factors such as occlusion, deformation, and appearance changes. With the temporal error accumulation, these challenges are further amplified in the online process.</p><p>It is well recognized that the rich temporal information in the video flow is of vital importance for visual tracking. However, most tracking paradigms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49]</ref> handle this task by per-frame object detection, where the tempo- <ref type="figure">Figure 1</ref>. An overview of our transformer-assisted tracking framework. The transformer encoder and decoder are assigned to two parallel branches in a Siamese-like tracking pipeline. Thanks to the encoder-decoder structure, isolated frames are tightly bridged to convey rich temporal information in the video flow. ral relationships among successive frames have been largely overlooked. Take the popular Siamese tracker as an example, only the initial target is considered for template matching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>. The merely used temporal information is the motion prior (e.g., cosine window) by assuming the target moves smoothly, which is widely adopted in visual trackers. In other tracking frameworks with update mechanisms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b2">3]</ref>, previous prediction results are collected to incrementally update the tracking model. Despite the historical frames considered in the above approaches, the video frames are still considered as independent counterparts without mutual reasoning. In real-world videos, some frames inevitably contain noisy contents such as occluded or blurred objects. These imperfect frames will hurt the model update when serving as the templates and will challenge the tracking process when performing as the search frames. Therefore, it is a non-trivial issue to convey rich information across temporal frames to mutually reinforce them. We argue that the video frames should not be treated in isolation and the performance potential is largely restricted due to the overlook of frame-wise relationship.</p><p>To bridge the isolated video frames and convey the rich temporal cues across them, in this work, we introduce the transformer architecture <ref type="bibr" target="#b46">[47]</ref> to the visual tracking community. Different from the traditional usage of the transformer in language modeling and machine translation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b11">12]</ref>, we leverage it to handle the context propagation in the temporal domain. By carefully modifying the classic transformer architecture, we show that its transformation characteristic naturally fits the tracking scenario. Its core component, i.e., attention mechanism <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref>, is ready to establish the pixelwise correspondence across frames and freely convey various signals in the temporal domain.</p><p>Generally, most tracking methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> can be formulated into a Siamese-like framework, where the top branch learns a tracking model using template features, and the bottom branch classifies the current search patch. As shown in <ref type="figure">Figure 1</ref>, we separate the transformer encoder and decoder into two branches within such a general Siameselike structure. In the top branch, a set of template patches are fed to the transformer encoder to generate high-quality encoded features. In the bottom branch, the search feature as well as the previous template contents are fed to the transformer decoder, where the search patch retrieves and aggregates informative target cues (e.g., spatial masks and target features) from history templates to reinforce itself.</p><p>The proposed transformer facilitates visual tracking via:</p><p>• Transformer Encoder. It enables individual template features to mutually reinforce to acquire more compact target representations, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. These encoded high-quality features further benefit the tracking model generation. • Transformer Decoder. It conveys valuable temporal information across frames. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our decoder simultaneously transfers features and spatial masks. Propagating the features from previous frames to the current patch smooths the appearance changes and remedies the context noises while transforming the spatial attentions highlights the potential object location. These manifold target representations and spatial cues make the object search much easier. Finally, we track the target in the decoded search patch. To verify the generalization of our designed transformer, we integrate it into two popular tracking frameworks including a Siamese formulation <ref type="bibr" target="#b0">[1]</ref> and a discriminative correlation filter (DCF) based tracking paradigm <ref type="bibr" target="#b2">[3]</ref>. With our designed transformer, a simple Siamese matching pipeline is able to outperform the current top-performing trackers. By combining with the recent discriminative approach <ref type="bibr" target="#b2">[3]</ref>, our transformer-assisted tracker shows outstanding results on seven prevalent tracking benchmarks including LaSOT <ref type="bibr" target="#b12">[13]</ref>, TrackingNet <ref type="bibr" target="#b38">[39]</ref>, GOT-10k <ref type="bibr" target="#b22">[23]</ref>, UAV123 <ref type="bibr" target="#b36">[37]</ref>, NfS <ref type="bibr" target="#b23">[24]</ref>, OTB-2015 <ref type="bibr" target="#b57">[58]</ref>, and VOT2018 <ref type="bibr" target="#b25">[26]</ref> and sets several new state-of-the-art records.</p><p>In summary, we make three-fold contributions:</p><p>• We present a neat and novel transformer-assisted tracking framework. To our best knowledge, this is the first attempt to involve the transformer in visual tracking. • We simultaneously consider the feature and attention transformations to better explore the potential of the transformer. We also modify the classic transformer to make it better suit the tracking task. • To verify the generalization, we integrate our designed transformer into two popular tracking pipelines. Our trackers exhibit encouraging results on 7 benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Tracking. Given the initial target in the first frame, visual tracking aims to localize it in successive frames. In recent years, the Siamese network has gained significant popularity, which deals with the tracking task by template matching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19]</ref>. By introducing the region proposal network (RPN), Siamese trackers obtain superior efficiency and more accurate target scale estimation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref>. The recent improvements upon Siamese trackers include attention mechanism <ref type="bibr" target="#b54">[55]</ref>, reinforcement learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b51">52]</ref>, target-aware model fine-tuning <ref type="bibr" target="#b30">[31]</ref>, unsupervised training <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref>, sophisticated backbone networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63]</ref>, cascaded frameworks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50]</ref>, and model update mechanisms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62]</ref>. Discriminative correlation filter (DCF) tackles the visual tracking by solving the ridge regression in Fourier domain, which exhibits attractive efficiency <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref>. The recent advances show that the ridge regression can be solved in the deep learning frameworks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref>, which avoids the boundary effect in classic DCF trackers. These methods learn a discriminative CNN kernel to convolve with the search area for response generation. In recent works, the residual terms <ref type="bibr" target="#b42">[43]</ref> and shrinkage loss <ref type="bibr" target="#b32">[33]</ref> are incorporated into the deep DCF formulation. To accelerate the kernel learning process, ATOM <ref type="bibr" target="#b6">[7]</ref> exploits the conjugate gradient algorithm. The recent DiMP tracker <ref type="bibr" target="#b2">[3]</ref> enhances the discriminative capability of the learned CNN kernel in an end-to-end manner, which is further promoted by the probabilistic regression framework <ref type="bibr" target="#b8">[9]</ref>.</p><p>Despite the impressive performance, most existing meth-ods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> generally regard the tracking task as the per-frame object detection problem, failing to adequately exploit the temporal characteristic of the tracking task. Some previous works explore the temporal information using graph neural network <ref type="bibr" target="#b15">[16]</ref>, spatial-temporal regularization <ref type="bibr" target="#b29">[30]</ref>, optical flow <ref type="bibr" target="#b65">[66]</ref>, etc. Differently, we leverage the transformer to model the frame-wise relationship and propagate the temporal cues, which is neat and ready to integrate with the modern deep trackers. Transformer. Transformer is first proposed in <ref type="bibr" target="#b46">[47]</ref> as a new paradigm for machine translation. The basic block in a transformer is the attention module, which aggregates information from the entire input sequence. Due to the parallel computations and unique memory mechanism, transformer architecture is more competitive than RNNs in processing long sequences and has gained increasing popularity in many natural language processing (NLP) tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. Similarly, non-local neural network <ref type="bibr" target="#b56">[57]</ref> also introduces a self-attention block to acquire global representations, which has been adopted in many vision tasks including visual object tracking <ref type="bibr" target="#b60">[61]</ref>. Nevertheless, how to take advantage of the compact transformer encoder-decoder structure for visual tracking has been rarely studied.</p><p>Recently, transformer architecture has been introduced to computer vision such as image generation <ref type="bibr" target="#b40">[41]</ref>. Transformer based object detection approach is proposed in <ref type="bibr" target="#b4">[5]</ref>, which views the object detection task as a direct set prediction problem. However, the above techniques leverage the transformer in the image-level tasks. In this paper, we show that the transformer structure serves as a good fit for video-related scenarios by transferring temporal information across frames. To bridge the domain gap between visual tracking and NLP tasks, we carefully modify the classic transformer to better suit the tracking scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisting Tracking Frameworks</head><p>Before elaborating our transformer for object tracking, we briefly review the recent popular tracking approaches for the sake of completeness. As shown in <ref type="figure">Figure 3</ref>, the mainstream tracking methods such as Siamese network <ref type="bibr" target="#b0">[1]</ref> or discriminative correlation filter (DCF) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> can be formulated into the Siamese-like pipeline, where the top branch learns the tracking model using templates and the bottom branch focuses on the target localization.</p><p>Siamese matching architecture <ref type="bibr" target="#b0">[1]</ref> takes an exemplar patch z and a search patch x as inputs, where z represents the target object while x is a large searching area in subsequent video frames. Both of them are fed to the weightsharing CNN network Ψ(·). Their output feature maps are cross-correlated as follows to generate the response map:</p><formula xml:id="formula_0">r(z, x) = Ψ(z) * Ψ(x) + b · 1,<label>(1)</label></formula><p>where * is the cross-correlation and b · 1 denotes a bias  <ref type="figure">Figure 3</ref>. The simplified pipelines of Siamese <ref type="bibr" target="#b0">[1]</ref> and DCF <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref> based trackers. These tracking approaches can be formulated into a Siamese-like pipeline, where the top branch is responsible for the model generation and the bottom branch localizes the target.</p><p>term. Siamese trackers rely on the target model, i.e., convolutional kernel Ψ(z), for template matching. As another popular framework, deep learning based DCF method optimizes the tracking model f under a ridge regression formulation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> as follows:</p><formula xml:id="formula_1">min f f * Ψ(z ) − y 2 2 + λ f 2 2 ,<label>(2)</label></formula><p>where y is the Gaussian-shaped ground-truth label of template patch z , and λ controls the regularization term to avoid overfitting. Note that z is much larger than the exemplar patch z in Siamese trackers. Therefore, DCF formulation simultaneously considers the target matching and background discrimination. After obtaining the tracking model f , the response is generated via r = f * Ψ(x).</p><p>The traditional DCF methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref> solve ridge regression using circularly generated samples via the closed-form solution in the Fourier domain. In contrast, the recent deep learning based DCF methods solve Eq. 2 using stochastic gradient descent <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b32">33]</ref> or conjugate gradient approach <ref type="bibr" target="#b6">[7]</ref> to avoid the boundary effect. The recent DiMP <ref type="bibr" target="#b2">[3]</ref> optimizes the above ridge regression via a meta-learner in an end-to-end manner, showing state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transformer for Visual Tracking</head><p>As discussed in Section 3, mainstream tracking methods can be formulated into a Siamese-like pipeline. We aim to improve such a general tracking framework by frame-wise relationship modeling and temporal context propagation, without modifying their original tracking manners such as template matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Transformer Overview</head><p>An overview of our transformer is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Similar to the classic transformer architecture <ref type="bibr" target="#b46">[47]</ref>, the encoder leverages self-attention block to mutually reinforce multiple template features. In the decoding process, crossattention block bridges template and search branches to propagate temporal contexts (e.g., feature and attention).  To suit the visual tracking task, we modify the classic transformer in the following aspects: (1) Encoder-decoder Separation. Instead of cascading the encoder and decoder in NLP tasks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b11">12]</ref>, as shown in <ref type="figure">Figure 1</ref>, we separate the encoder and decoder into two branches to fit the Siameselike tracking methods. (2) Block Weight-sharing. The selfattention blocks in the encoder and decoder (yellow boxes in <ref type="figure" target="#fig_2">Figure 4</ref>) share weights, which transform the template and search embeddings in the same feature space to facilitate the further cross-attention computation. (3) Instance Normalization. In NLP tasks <ref type="bibr" target="#b46">[47]</ref>, the word embeddings are individually normalized using the layer normalization. Since our transformer receives image feature embeddings, we jointly normalize these embeddings in the instance (image patch) level to retain the valuable image amplitude information. (4) Slimming Design. Efficiency is crucial for visual tracking scenarios. To achieve a good balance of speed and performance, we slim the classic transformer by omitting the fully-connected feed-forward layers and maintaining the lightweight single-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transformer Encoder</head><p>The basic block in a classic transformer is the attention mechanism, which receives the query Q ∈ R Nq×C , key K ∈ R N k ×C , and value V ∈ R N k ×C as the inputs. In our approach, following <ref type="bibr" target="#b46">[47]</ref>, we also adopt the dot-product to compute the similarity matrix A K→Q ∈ R Nq×N k between the query and key as follows:</p><formula xml:id="formula_2">A K→Q = Atten(Q, K) = Softmax col (QK T /τ ),<label>(3)</label></formula><p>whereQ andK are 2 -normalized features of Q and K across the channel dimension, and τ is a temperature parameter controlling the Softmax distribution, which is inspired by the model distillation <ref type="bibr" target="#b20">[21]</ref> and contrastive learning <ref type="bibr" target="#b5">[6]</ref> techniques. With the propagation matrix A K→Q from key to query, we can transform the value via A K→Q V ∈ R Nq×C . In our framework, the transformer encoder receives a set of template features T i ∈ R C×H×W with a spatial size of H × W and dimensionality C, which are further concatenated to form the template feature ensemble T = Concat(T 1 , · · · , T n ) ∈ R n×C×H×W . To facilitate the attention computation, we reshape T to T ∈ R N T ×C , where N T = n × H × W . As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the main operation in the transformer encoder is self-attention, which aims to mutually reinforce the features from multiple templates. To this end, we first compute the self-attention map</p><formula xml:id="formula_3">A T→T = Atten ϕ(T ), ϕ(T ) ∈ R N T ×N T , where ϕ(·)</formula><p>is a 1 × 1 linear transformation that reduces the embedding channel from C to C/4.</p><p>Based on the self-similarity matrix A T→T , we transform the template feature through A T→T T , which is added to the original feature T as a residual term as follows:</p><formula xml:id="formula_4">T = Ins. Norm A T→T T + T ,<label>(4)</label></formula><p>whereT ∈ R N T ×C is the encoded template feature and Ins. Norm(·) denotes the instance normalization that jointly 2 -normalizes all the embeddings from an image patch, i.e., feature map level (T i ∈ R C×H×W ) normalization.</p><p>Thanks to the self-attention, multiple temporally diverse template features aggregate each other to generate high-qualityT, which is further fed to the decoder block to reinforce the search patch feature. Besides, this encoded template representationT is also reshaped back to T encoded ∈ R n×C×H×W for tracking model generation, e.g., the DCF model in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transformer Decoder</head><p>Transformer decoder takes the search patch feature S ∈ R C×H×W as its input. Similar to the encoder, we first reshape this feature to S ∈ R N S ×C , where N S = H × W . Then, S is fed to the self-attention block as follows:</p><formula xml:id="formula_5">S = Ins. Norm A S→S S + S ,<label>(5)</label></formula><p>where A S→S = Atten ϕ(S ), ϕ(S ) ∈ R N S ×N S is the self-attention matrix of the search feature.</p><p>Mask Transformation. Based on the search featureŜ in Eq. 5 and aforementioned encoded template featureT in Eq. 4, we compute the cross-attention matrix between</p><formula xml:id="formula_6">them via A T→S = Atten φ(Ŝ), φ(T) ∈ R N S ×N T , where φ(·) is a 1 × 1 linear transformation block similar to ϕ(·).</formula><p>This cross-attention map A T→S establishes the pixel-topixel correspondence between frames, which supports the temporal context propagation.</p><p>In visual tracking, we are aware of the target positions in the templates. To propagate the temporal motion priors, we construct the Gaussian-shaped masks of the template features through m(y) = exp − y−c 2 2σ 2 , where c is the ground-truth target position. Similar to the feature ensemble T, we also concatenate these masks m i ∈ R H×W to form the mask ensemble M = Concat(m 1 , · · · , m n ) ∈ R n×H×W , which is further flattened into M ∈ R N T ×1 . Based on the cross attention map A T→S , we can easily propagate previous masks to the search patch via A T→S M ∈ R N S ×1 . The transformed mask is qualified to serve as the attention weight for the search featureŜ as follows:</p><formula xml:id="formula_7">S mask = Ins. Norm A T→S M ⊗Ŝ ,<label>(6)</label></formula><p>where ⊗ is the broadcasting element-wise multiplication. By virtue of the spatial attention, the reinforced search fea-tureŜ mask better highlights the potential target area.</p><p>Feature Transformation. Except for the spatial attention, it is also feasible to propagate the context information from template featureT to the search featureŜ. It is beneficial to convey target representations while the background scenes tend to change drastically in a video, which is unreasonable to temporally propagate. As a consequence, before feature transformation, we first mask the template feature througĥ T ⊗ M to suppress the background area. Then, with the cross-attention matrix A T→S , the transformed feature can be computed via A T→S (T⊗M ) ∈ R N S ×C , which is added toŜ as a residual term:</p><formula xml:id="formula_8">S feat = Ins. Norm A T→S (T ⊗ M ) +Ŝ .<label>(7)</label></formula><p>Compared with originalŜ, feature-level enhancedŜ feat aggregates temporally diverse target representations from a series of template featuresT to promote itself. Finally, we equally combine the aforementioned spatially masked featureŜ mask and feature-level enhanced fea-tureŜ feat , and further normalize them as follows:</p><formula xml:id="formula_9">S final = Ins. Norm Ŝ feat +Ŝ mask .<label>(8)</label></formula><p>The final output featureŜ final ∈ R N S ×C is reshaped back to the original size for visual tracking. We denote the reshaped version ofŜ final as S decoded ∈ R C×H×W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Tracking with Transformer-enhanced Features</head><p>Transformer structure facilitates the tracking process by generating high-quality template feature T encoded and search feature S decoded . We learn the tracking model using T encoded following two popular paradigms: • Siamese Pipeline. In this setting, we simply crop the target feature in T encoded as the template CNN kernel to convolve with S decoded for response generation, which is identical to the cross-correlation in SiamFC <ref type="bibr" target="#b0">[1]</ref>. • DCF Pipeline. Following the end-to-end DCF optimization in DiMP approach <ref type="bibr" target="#b2">[3]</ref>, we generate a discriminative CNN kernel using T encoded to convolve with S decoded for response generation. After obtaining the tracking response, we utilize the classification loss proposed in DiMP <ref type="bibr" target="#b2">[3]</ref> to jointly train the backbone network, our transformer, and the tracking model in an end-to-end manner. Please refer to <ref type="bibr" target="#b2">[3]</ref> for more details.</p><p>In the online tracking process, to better exploit the temporal cues and adapt to the target appearance changes, we dynamically update the template ensemble T. To be specific, we drop the oldest template in T and add the current collected template feature to T every 5 frames. The feature ensemble maintains a maximal size of 20 templates. Once the template ensemble T is updated, we compute the new encoded feature T encoded via our transformer encoder. While the transformer encoder is sparsely utilized (i.e., every 5 frames), the transformer decoder is leveraged in each frame, which generates per-frame S decoded by propagating the representations and attention cues from previous templates to the current search patch.</p><p>It is widely recognized that DCF formulation in DiMP <ref type="bibr" target="#b2">[3]</ref> is superior to the simple cross-correlation in Siamese trackers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. Nevertheless, in the experiments, we show that with the help of our transformer architecture, a classic Siamese pipeline is able to perform against the recent DiMP. Meanwhile, with our transformer, the DiMP tracker acquires further performance improvements. As shown in <ref type="figure">Figure 10</ref>, even though the strong baseline DiMP <ref type="bibr" target="#b2">[3]</ref> already shows impressive distractor discrimination capability, our designed transformer further assists it to restrain the background confidence for robust tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Based on the Siamese matching and DiMP based tracking frameworks, in the following experiments, we denote our Transformer-assisted trackers as TrSiam and TrDiMP, respectively. In these two versions, the backbone model is ResNet-50 <ref type="bibr" target="#b17">[18]</ref> for feature extraction. Before the encoder and decoder, we additionally add one convolutional layer (3×3 Conv + BN) to reduce the backbone feature channel from 1024 to 512. The input template and search patches are 6 times of the target size and further resized to 352×352. The temperature τ in Eq. 3 is set to 1/30. The parameter sigma σ in the feature mask is set to 0.1. Similar to the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>, we utilize the training splits of LaSOT <ref type="bibr" target="#b12">[13]</ref>, TrackingNet <ref type="bibr" target="#b38">[39]</ref>, GOT-10k <ref type="bibr" target="#b22">[23]</ref>, and COCO <ref type="bibr" target="#b31">[32]</ref> for offline training. The proposed transformer network is jointly trained with the original tracking parts (e.g., tracking optimization model <ref type="bibr" target="#b2">[3]</ref> and IoUNet <ref type="bibr" target="#b8">[9]</ref>) in an end-toend manner. Our framework is trained for 50 epochs with 1500 iterations per epoch and 36 image pairs per batch. The ADAM optimizer <ref type="bibr" target="#b24">[25]</ref> is employed with an initial learning rate of 0.01 and a decay factor of 0.2 for every 15 epochs.</p><p>In the online tracking stage, the main difference between TrSiam and TrDiMP lies in the tracking model generation manner. After predicting the response map for target localization, they all adopt the recent probabilistic IoUNet <ref type="bibr" target="#b8">[9]</ref> for target scale estimation. Our trackers are implemented in Python using PyTorch. TrSiam and TrDiMP operate about 35 and 26 frames per second (FPS) on a single Nvidia GTX 1080Ti GPU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>To verify the effectiveness of our designed transformer structure, we choose the GOT-10k test set <ref type="bibr" target="#b22">[23]</ref> with 180 videos to validate our TrSiam and TrDiMP methods 1 . GOT-10k hides the ground-truth labels of the test set to avoid the overly hyper-parameter fine-tuning. It is worth mentioning that there is no overlap in object classes between the train and test sets of GOT-10k, which also verifies the generalization of our trackers to unseen object classes.</p><p>In <ref type="table" target="#tab_2">Table 1</ref>, based on the Siamese and DiMP baselines, we validate each component in our transformer: Transformer Encoder. First, without any decoder block, we merely utilize encoder to promote the feature fusion of <ref type="bibr" target="#b0">1</ref> With the probabilistic IoUNet <ref type="bibr" target="#b8">[9]</ref> and a larger search area, our baseline performance is better than the standard DiMP <ref type="bibr" target="#b2">[3]</ref>. Note that all the experiments ( <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="table" target="#tab_2">Table 1</ref>) are based on the same baseline for fair.    multiple templates, which slightly improves two baselines. Transformer Decoder. Our decoder consists of feature and mask transformations, and we independently verify them:</p><p>(1) Feature Propagation. With the feature transformation, as shown in <ref type="table" target="#tab_2">Table 1</ref>, the Siamese pipeline obtains a notable performance gain of 4.3% in AO and the strong DiMP baseline still acquires an improvement of 1.4% in AO on the GOT-10k test set. From the training perspective, we can observe that this block effectively reduces the losses of two baselines as shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>(2) Mask Propagation. This mechanism propagates temporally collected spatial attentions to highlight the target area. Similar to the feature transformation, our mask transformation alone also steadily improves the tracking performance <ref type="table" target="#tab_2">(Table 1</ref>) and consistently reduces the training errors of both two pipelines ( <ref type="figure" target="#fig_4">Figure 6</ref>). Complete Transformer. With the complete transformer, as shown in <ref type="table" target="#tab_2">Table 1</ref>, the Siamese and DiMP baselines obtain notable performance gains of 5.3% and 2.1% in AO, respectively. The transformer also significantly reduces their training losses ( <ref type="figure" target="#fig_4">Figure 6</ref>). It is worth mentioning that DiMP already achieves outstanding results while our approach consistently improves such a strong baseline. With our transformer, the performance gap between Siamese and DiMP  baselines has been largely narrowed (from 4.7% to 1.5% in AO), which reveals the strong tracking potential of a simple pipeline by adequately exploring the temporal information. Structure Modifications. Finally, we discuss some architecture details of our transformer: (1) Shared-weight Selfattention. Since our transformer is separated into two parallel Siamese tracking braches, the performance obviously drops without the weight-sharing mechanism as shown in <ref type="table" target="#tab_3">Table 2</ref>. Due to this weight-sharing design, we also do not stack multiple encoder/decoder layers like the classic transformer <ref type="bibr" target="#b46">[47]</ref>, which will divide the template and search representations into different feature subspaces. (2) Feedforward Network. Feed-forward network is a basic block in the classic transformer <ref type="bibr" target="#b46">[47]</ref>, which consists of two heavyweight fully-connected layers. In the tracking scenario, we observe that this block potentially causes the overfitting issue due to its overmany parameters, which does not bring performance gains and hurts the efficiency. (3) Head Number. Classic transformer adopts multi-head attentions (e.g., 8 heads) to learn diverse representations <ref type="bibr" target="#b46">[47]</ref>. In the experiments, we observe that increasing the head number slightly improves the accuracy but hinders the tracking efficiency from real-time. We thus choose the single-head attention to achieve a good balance of performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">State-of-the-art Comparisons</head><p>We compare our proposed TrSiam and TrDiMP trackers with the recent state-of-the-art trackers on seven tracking benchmarks including TrackingNet <ref type="bibr" target="#b38">[39]</ref>, GOT-10k <ref type="bibr" target="#b22">[23]</ref>, LaSOT <ref type="bibr" target="#b12">[13]</ref>, VOT2018 <ref type="bibr" target="#b25">[26]</ref>, Need for Speed <ref type="bibr" target="#b23">[24]</ref>, UAV123 <ref type="bibr" target="#b36">[37]</ref>, and OTB-2015 <ref type="bibr" target="#b57">[58]</ref>. TrackingNet <ref type="bibr" target="#b38">[39]</ref>. TrackingNet is a recently released largescale benchmark. We evaluate our methods on the test set of TrackingNet, which consists of 511 videos. In this benchmark, we compare our approaches with the state-of-the-art trackers such as DiMP-50 <ref type="bibr" target="#b2">[3]</ref>, D3S <ref type="bibr" target="#b33">[34]</ref>, SiamFC++ <ref type="bibr" target="#b58">[59]</ref>, Retain-MAML <ref type="bibr" target="#b48">[49]</ref>, DCFST <ref type="bibr" target="#b63">[64]</ref>, PrDiMP-50 <ref type="bibr" target="#b8">[9]</ref>, KYS <ref type="bibr" target="#b1">[2]</ref>, and Siam-RCNN <ref type="bibr" target="#b47">[48]</ref>. As shown in <ref type="table" target="#tab_4">Table 3</ref>, the proposed TrDiMP achieves a normalized precision score of  83.3% and a success score of 78.4%, surpassing previous state-of-the-art trackers such as PrDiMP-50 and KYS. Note that PrDiMP and KYS improve the DiMP tracker via probabilistic regression and tracking scene exploration, representing the current leading algorithms on several datasets. With our designed transformer, the simple Siamese matching baseline (i.e., TrSiam) also shows outstanding performance with a normalized precision score of 82.9% and a success score of 78.1%. GOT-10k <ref type="bibr" target="#b22">[23]</ref>. GOT-10k is a large-scale dataset including more than 10,000 videos. We test our methods on the test set of GOT-10k with 180 sequences. The main characteristic of GOT-10k is that the test set does not have overlap in object classes with the train set, which is designed to assess the generalization of the visual tracker. Following the test protocol of GOT-10k, we further train our trackers with only the GOT-10k training set. As shown in <ref type="table" target="#tab_5">Table 4</ref>, in a fair comparison scenario (i.e., without additional training data), both our TrDiMP and TrSiam still outperform other top-performing trackers such as SiamR-CNN <ref type="bibr" target="#b47">[48]</ref>, DCFST <ref type="bibr" target="#b63">[64]</ref>, and KYS <ref type="bibr" target="#b1">[2]</ref>, verifying the strong generalization of our methods to unseen objects.</p><p>LaSOT <ref type="bibr" target="#b12">[13]</ref>. LaSOT is a recent large-scale tracking benchmark consisting of 1200 videos. The average video length of this benchmark is about 2500 frames, which is more challenging than the previous short-term tracking datasets. Therefore, how to cope with the drastic target appearance  changes using temporal context is vital in this dataset. We evaluate our approaches on the LaSOT test set with 280 videos. The precision and success plots of the state-of-theart methods are shown in <ref type="figure" target="#fig_5">Figure 7</ref>, where the recently proposed C-RPN <ref type="bibr" target="#b13">[14]</ref>, SiamRPN++ <ref type="bibr" target="#b27">[28]</ref>, ATOM <ref type="bibr" target="#b6">[7]</ref>, DiMP-50 <ref type="bibr" target="#b2">[3]</ref>, and PrDiMP-50 <ref type="bibr" target="#b8">[9]</ref> are included for comparison. Our TrSiam and TrDiMP outperform aforementioned methods by a considerable margin. To the best of our knowledge, SiamR-CNN <ref type="bibr" target="#b47">[48]</ref> achieves the current best result on the La-SOT. Overall, our TrDiMP (63.9% AUC and 26 FPS) exhibits very competitive performance and efficiency in comparison with SiamR-CNN (64.8% AUC and 4.7 FPS). VOT2018 <ref type="bibr" target="#b25">[26]</ref>. VOT2018 benchmark contains 60 challenging videos. The performance on this dataset is evaluated using the expected average overlap (EAO), which takes both accuracy (average overlap over successful frames) and robustness (failure rate) into account. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, our TrSiam and TrDiMP clearly outperform all the participant trackers on the VOT2018.</p><p>In <ref type="table" target="#tab_2">Table 10</ref>, we further show the accuracy, robustness, and EAO scores of the recent top-performing trackers including SiamRPN++ <ref type="bibr" target="#b27">[28]</ref>, DiMP-50 <ref type="bibr" target="#b2">[3]</ref>, PrDiMP-50 <ref type="bibr" target="#b8">[9]</ref>, Retain-MAML <ref type="bibr" target="#b48">[49]</ref>, KYS <ref type="bibr" target="#b1">[2]</ref>, and D3S <ref type="bibr" target="#b33">[34]</ref>. Compared with these recently proposed approaches, our TrDiMP approach still exhibits satisfactory results. Among all the compared trackers, only D3S slightly outperforms our TrDiMP, which is trained using additional data with segmentation annotations for accurate mask prediction. NfS <ref type="bibr" target="#b23">[24]</ref>. NfS dataset contains 100 challenging videos with fast-moving objects. We evaluate our TrSiam and TrDiMP on the 30 FPS version of NfS. The AUC scores of comparison approaches are shown in <ref type="table" target="#tab_7">Table 5</ref>. Our approaches set new state-of-the-art records on this benchmark. The proposed TrDiMP surpasses previous top-performing trackers such as DCFST <ref type="bibr" target="#b63">[64]</ref> and SiamR-CNN <ref type="bibr" target="#b47">[48]</ref>. Note that the recent SimR-CNN utilizes a powerful ResNet-101 for object re-detection. Our simple TrSiam, without sophisticated models or online optimization techniques, still outperforms existing methods and operates in real-time. UAV123 <ref type="bibr" target="#b36">[37]</ref>. This benchmark includes 123 aerial videos collected by the low-attitude UAV platform. The proposed trackers also achieve promising results in comparison to the recent remarkable approaches in <ref type="table" target="#tab_7">Table 5</ref>. Specifically, our TrDiMP performs on par with PrDiMP-50 <ref type="bibr" target="#b8">[9]</ref>, which represents the current best algorithm on this benchmark. OTB-2015 <ref type="bibr" target="#b57">[58]</ref>. OTB-2015 is a popular tracking benchmark with 100 challenging videos. As shown in <ref type="table" target="#tab_7">Table 5</ref>, on this dataset, our TrDiMP achieves an AUC score of 71.1%, surpassing the recently proposed SiamRPN++ <ref type="bibr" target="#b27">[28]</ref>, PrDiMP-50 <ref type="bibr" target="#b8">[9]</ref>, SiamR-CNN <ref type="bibr" target="#b47">[48]</ref>, and KYS <ref type="bibr" target="#b1">[2]</ref>. With the proposed transformer, our Siamese matching based TrSiam also performs favorably against existing state-of-the-art approaches with an AUC score of 70.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we introduce the transformer structure to the tracking frameworks, which bridges the isolated frames in the video flow and conveys the rich temporal cues across frames. We show that by carefully modifying the classic transformer architecture, it favorably suits the tracking scenario. With the proposed transformer, two popular trackers gain consistent performance improvements and set several new state-of-the-art records on prevalent tracking datasets. To our best knowledge, this is the first attempt to exploit the transformer in the tracking community, which preliminarily unveils the tracking potential hidden in the frame-wise relationship. In the future, we intend to further explore the rich temporal information among individual video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Hyper-parameters</head><p>In the online tracking stage, the only involved hyperparameters are the template sampling interval as well as the template ensemble size. As shown in <ref type="table">Table 7</ref>, we observe that sampling the template every 5 frames shows promising results. This sparse update mechanism is also widely adopted in many previous trackers such as ECO <ref type="bibr" target="#b7">[8]</ref> and ATOM <ref type="bibr" target="#b6">[7]</ref>. Besides, increasing the memory size (i.e., the total sample number in the template ensemble T) also steadily improves the performance. To achieve a good balance of performance and efficiency, we choose the maximum ensemble size of 20.</p><p>As for other tracking-related hyper-parameters, we follow our baseline approach DiMP <ref type="bibr" target="#b2">[3]</ref> without modification. More details can be found in the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Improvements upon Baselines</head><p>In <ref type="table" target="#tab_10">Table 8</ref> and 9, we compare our TrSiam and TrDiMP with their corresponding baselines on seven tracking benchmarks. As shown in <ref type="table" target="#tab_10">Table 8</ref>, our designed transformer consistently improves the Siamese baseline on seven tracking datasets. For example, our TrSiam approach outperforms its baseline by 5.3%, 4.7%, 3.3%, and 3.0% in terms of AUC score on the challenging GOT-10k, NfS, LaSOT, and Track-ingNet datasets, respectively. On the OTB-2015 dataset, our approach still improves the baseline by 1.6%. The OTB-2015 dataset is known to be highly saturated over recent years. Note that our Siamese baseline already achieves a high performance level of 69.2% AUC on the OTB-2015. Thus, it is relatively harder to obtain a significant performance gain on this benchmark.</p><p>In <ref type="table" target="#tab_11">Table 9</ref>, we further exhibit the comparison results between our transformer-assisted TrDiMP and its baseline DiMP <ref type="bibr" target="#b2">[3]</ref>. It is worth mentioning that the DiMP approach already introduces a memory mechanism to incrementally update the tracking model and explores the temporal information to some extent. Besides, our baseline includes the recent probabilistic IoUNet <ref type="bibr" target="#b8">[9]</ref> for accurate target scale estimation and adopts a larger search area (6 times of the target object) for tracking (i.e., the superDiMP setting 2 ), which significantly outperforms the standard DiMP approach presented in <ref type="bibr" target="#b2">[3]</ref> . It is well recognized that improving a strong baseline is much more challenging. Although our baseline achieves outstanding results on various tracking benchmarks, our proposed transformer consistently improves it on all datasets. <ref type="table">Table 7</ref>. Ablation experiments on the template sampling interval and template ensemble size. The testing approach is our TrSiam. The performance is evaluated on the GOT-10k test set <ref type="bibr" target="#b22">[23]</ref> and NfS <ref type="bibr" target="#b23">[24]</ref> in terms of AUC score.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Attention Visualization</head><p>As shown in <ref type="figure" target="#fig_7">Figure 9</ref> (a), after self-attention, the pixels get some minor weights from their neighboring pixels to reinforce themselves. In the decoding process, as shown in <ref type="figure" target="#fig_7">Figure 9</ref> (b), the cross-attention matrice between two different patches is sparse, which means the query seeks several most correlated keys to propagate the context. After Softmax, the attention weights are not averaged by the similar athletes in Bolt2 sequence, which illustrates our attention block can discriminate the distractors to some extent. Benefiting such (feature/mask) propagations, the tracking responses are accurate, as shown in <ref type="figure">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Response Visualization</head><p>In <ref type="figure">Figure 10</ref>, we exhibit more detailed visualization results of our tracking framework. From <ref type="figure">Figure 10</ref> (second column), we can observe that our baseline (i.e., DiMP <ref type="bibr" target="#b2">[3]</ref>) tends to be misled by distracting objects in the challenging scenarios. By adopting the feature transformation mecha-  <ref type="figure">Figure 10</ref>. Visualization of the tracking response maps of DiMP baseline <ref type="bibr" target="#b2">[3]</ref>. The "w/o Transformer" denotes the baseline approach DiMP <ref type="bibr" target="#b2">[3]</ref>. The "w/ Feature" denotes the baseline with a feature propagation based transformer. The "w/ Mask" represents the baseline with a mask propagation based transformer. Finally, the "w/ Transformer" is our complete transformer-assisted tracker, i.e., TrDiMP. Our proposed components (feature and mask transformations) effectively suppress the background responses.</p><p>nism (third column in <ref type="figure">Figure 10</ref>), the target representations in the search region are effectively reinforced, which facilitates the object searching process. Therefore, the response values of the background regions are largely restrained. The mask transformation block propagates the spatial attentions from previous templates to the current search region, which also effectively suppresses the background objects (fourth column in <ref type="figure">Figure 10</ref>). Finally, our complete transformer architecture combines both feature and mask transformations, and the final response maps (last column in <ref type="figure">Figure 10</ref>) are more robust for object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on VOT2019</head><p>VOT2019 <ref type="bibr" target="#b26">[27]</ref> is a recently released challenging benchmark, which replaces 12 easy videos in VOT2018 <ref type="bibr" target="#b25">[26]</ref> by 12 more difficult videos. We compare our approach with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure Case</head><p>When the target object is occluded or invisible, the cross attention maps between the current frame and historic templates are inaccurate. Therefore, our framework struggles to handle the heavy occlusion (e.g., <ref type="figure">Figure 11</ref>) or out-of-view. Another potential limitation of our work is the high computational memory of the attention matrix, which is also a common issue in the transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#105</head><p>#140 #150 Target <ref type="figure">Figure 11</ref>. Failure case. TrDiMP fails to track the occluded target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Attribute Analysis</head><p>Finally, in <ref type="figure" target="#fig_0">Figure 12</ref>, we provide the attribute evaluation on the LaSOT <ref type="bibr" target="#b12">[13]</ref> benchmark. On the LaSOT, our approaches show good results in various scenarios such as motion blur, background clutter, low resolution, and viewpoint change. As shown in <ref type="table" target="#tab_10">Table 8</ref>, with the proposed transformer, our TrSiam outperforms its baseline by 3.3% AUC. It should be noted that our simple TrSiam does not adopt complex online model optimization techniques, which is more efficient than the recent approaches such as DiMP <ref type="bibr" target="#b2">[3]</ref> and PrDiMP <ref type="bibr" target="#b8">[9]</ref>.  <ref type="figure" target="#fig_0">Figure 12</ref>. Attribute-based evaluation on the LaSOT benchmark <ref type="bibr" target="#b12">[13]</ref>. The legend shows the AUC scores of the success plots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Top: the transformer encoder receives multiple template features to mutually aggregate representations. Bottom: the transformer decoder propagates the template features and their assigned masks to the search patch feature for representation enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Production M Template Feature Mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>An overview of the proposed transformer architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Tracking response maps of the DiMP baseline<ref type="bibr" target="#b2">[3]</ref> without (second column) and with (third column) our designed transformer architecture. With the proposed transformer, the confidences of the distracting objects are effectively suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Training loss plots of the Siamese pipeline (left) and DCF pipeline (right). By combining both feature and mask transformations, our approach significantly reduces the training losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Precision and success plots on the LaSOT test set<ref type="bibr" target="#b12">[13]</ref>. In the legend, the distance precision (DP) and area-under-curve (AUC) are reported in the left and right figures, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Expected average overlap (EAO) graph with trackers ranked from right to left. Our TrDiMP and TrSiam trackers outperform all the participant trackers on the VOT2018<ref type="bibr" target="#b25">[26]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of the attention maps of the encoder (selfattention block) and decoder (cross-attention block).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>CNN Template Patch Search Patch Weight Sharing CNN Crop DCF Tracking Model Tracking Model Conv Conv Result Result Feature Extraction Siamese Pipeline DCF Pipeline Different Tracking Options</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Ablative experiments of our transformer for the Siamese and DiMP pipelines i.e., TrSiam and TrDiMP trackers. The performance is evaluated on the GOT-10k test set<ref type="bibr" target="#b22">[23]</ref> in terms of average overlap (AO).</figDesc><table><row><cell>Different Tracking Variations</cell><cell>Siamese (AO)</cell><cell>DiMP (AO)</cell></row><row><cell>Baseline Performance</cell><cell>62.0</cell><cell>66.7</cell></row><row><cell>Only Encoder (w/o Any Decoder)</cell><cell>63.8 1.8%↑</cell><cell>67.3 0.6%↑</cell></row><row><cell>Encoder + Decoder (Only Feature Transf.)</cell><cell>66.3 4.3%↑</cell><cell>68.1 1.4%↑</cell></row><row><cell>Encoder + Decoder (Only Mask Transf.)</cell><cell>67.1 5.1%↑</cell><cell>67.8 1.1%↑</cell></row><row><cell>Encoder + Decoder (Feature &amp; Mask Transf.)</cell><cell>67.3 5.3%↑</cell><cell>68.8 2.1%↑</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Baseline</cell><cell cols="2">Weight-sharing</cell><cell cols="2">Feed-forward</cell><cell cols="3">Head Number</cell></row><row><cell></cell><cell></cell><cell>w/o</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell><cell>1</cell><cell>2</cell><cell>4</cell></row><row><cell>AO (%)</cell><cell>62.0</cell><cell>63.4</cell><cell>67.3</cell><cell>67.3</cell><cell>67.0</cell><cell cols="3">67.3 67.2 67.6</cell></row><row><cell>Speed (FPS)</cell><cell>40</cell><cell>35</cell><cell>35</cell><cell>35</cell><cell>22</cell><cell>35</cell><cell>31</cell><cell>25</cell></row></table><note>Ablative study of our transformer architecture. The base- line tracker is TrSiam. The evaluation metric is average overlap (AO) score on the GOT-10k test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art trackers on the TrackingNet test set<ref type="bibr" target="#b38">[39]</ref> in terms of precision (Prec.), normalized precision (N. Prec.), and success (AUC score). Our TrDiMP and TrSiam exhibit promising results.</figDesc><table><row><cell></cell><cell>[1]</cell><cell>[40] [50] [14]</cell><cell>[28]</cell><cell>[7]</cell><cell>[3]</cell><cell>[59]</cell><cell>[34]</cell><cell>[49]</cell><cell>[9]</cell><cell>[64]</cell><cell>[2]</cell><cell>[48]</cell><cell></cell></row><row><cell>Prec. (%)</cell><cell>53.3</cell><cell>56.5 66.1 61.9</cell><cell>69.4</cell><cell>64.8</cell><cell>68.7</cell><cell>70.5</cell><cell>66.4</cell><cell>-</cell><cell>70.4</cell><cell cols="2">70.0 68.8</cell><cell>80.0</cell><cell>72.7</cell><cell>73.1</cell></row><row><cell cols="2">N. Prec. (%) 66.3</cell><cell>70.5 77.8 74.6</cell><cell>80.0</cell><cell>77.1</cell><cell>80.1</cell><cell>80.0</cell><cell>76.8</cell><cell>82.2</cell><cell>81.6</cell><cell cols="2">80.9 80.0</cell><cell>85.4</cell><cell>82.9</cell><cell>83.3</cell></row><row><cell cols="2">Success (%) 57.1</cell><cell>60.6 71.2 66.9</cell><cell>73.3</cell><cell>70.3</cell><cell>74.0</cell><cell>75.4</cell><cell>72.8</cell><cell>75.7</cell><cell>75.8</cell><cell cols="2">75.2 74.0</cell><cell>81.2</cell><cell>78.1</cell><cell>78.4</cell></row></table><note>SiamFC MDNet SPM C-RPN SiamRPN++ ATOM DiMP-50 SiamFC++ D3S Retain-MAML PrDiMP-50 DCFST KYS Siam-RCNN TrSiam TrDiMP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison results on the GOT-10k test set<ref type="bibr" target="#b22">[23]</ref> in terms of average overlap (AO), and success rates (SR) at overlap thresholds 0.5 and 0.75. We show the tracking results without (w/o) and with (w/) additional training data (LTC: LaSOT, TrackingNet, and COCO).</figDesc><table><row><cell></cell><cell cols="12">SiamFC SiamFCv2 SiamRPN SPM ATOM DiMP-50 SiamFC++ D3S PrDiMP-50 DCFST KYS Siam-RCNN</cell><cell cols="2">TrSiam</cell><cell cols="2">TrDiMP</cell></row><row><cell></cell><cell>[1]</cell><cell>[46]</cell><cell>[29]</cell><cell>[50]</cell><cell>[7]</cell><cell>[3]</cell><cell>[59]</cell><cell>[34]</cell><cell>[9]</cell><cell>[64]</cell><cell>[2]</cell><cell>[48]</cell><cell cols="4">w/o LTC w/ LTC w/o LTC w/ LTC</cell></row><row><cell>SR0.5(%)</cell><cell>35.3</cell><cell>40.4</cell><cell>54.9</cell><cell cols="2">59.3 63.4</cell><cell>71.7</cell><cell>69.5</cell><cell>67.6</cell><cell>73.8</cell><cell>75.3</cell><cell>75.1</cell><cell>-</cell><cell>76.6</cell><cell>78.7</cell><cell>77.7</cell><cell>80.5</cell></row><row><cell>SR0.75(%)</cell><cell>9.8</cell><cell>14.4</cell><cell>25.3</cell><cell cols="2">35.9 40.2</cell><cell>49.2</cell><cell>47.9</cell><cell>46.2</cell><cell>54.3</cell><cell>49.8</cell><cell>51.5</cell><cell>-</cell><cell>57.1</cell><cell>58.6</cell><cell>58.3</cell><cell>59.7</cell></row><row><cell>AO (%)</cell><cell>34.8</cell><cell>37.4</cell><cell>46.3</cell><cell cols="2">51.3 55.6</cell><cell>61.1</cell><cell>59.5</cell><cell>59.7</cell><cell>63.4</cell><cell>63.8</cell><cell>63.6</cell><cell>64.9</cell><cell>66.0</cell><cell>67.3</cell><cell>67.1</cell><cell>68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>State-of-the-art comparison on the NfS<ref type="bibr" target="#b23">[24]</ref>, UAV123<ref type="bibr" target="#b36">[37]</ref>, and OTB-2015<ref type="bibr" target="#b57">[58]</ref> datasets in terms of AUC score. Both our TrDiMP and TrSiam exhibit outstanding results on all benchmarks with competitive efficiency.KCF SiamFC CFNet MDNet C-COT ECO ATOM UPDT SiamRPN++ DiMP-50 SiamR-CNN PrDiMP-50 DCFST KYS TrSiam TrDiMP</figDesc><table><row><cell></cell><cell></cell><cell cols="2">[20]</cell><cell></cell><cell cols="2">[1]</cell><cell></cell><cell cols="2">[46]</cell><cell></cell><cell cols="2">[40]</cell><cell>[11]</cell><cell>[8]</cell><cell>[7]</cell><cell>[4]</cell><cell>[28]</cell><cell>[3]</cell><cell>[48]</cell><cell>[9]</cell><cell>[64]</cell><cell>[2]</cell></row><row><cell>NfS [24]</cell><cell></cell><cell cols="3">21.7</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell cols="2">42.9</cell><cell cols="3">48.8 46.6 58.4</cell><cell>53.7</cell><cell>50.2</cell><cell>62.0</cell><cell>63.9</cell><cell>63.5</cell><cell>64.1 63.5</cell><cell>65.8</cell><cell>66.5</cell></row><row><cell cols="2">UAV123 [37]</cell><cell cols="3">33.1</cell><cell cols="2">49.8</cell><cell></cell><cell cols="2">43.6</cell><cell></cell><cell cols="2">52.8</cell><cell cols="3">51.3 52.2 64.2</cell><cell>54.5</cell><cell>61.3</cell><cell>65.3</cell><cell>64.9</cell><cell>68.0</cell><cell>-</cell><cell>-</cell><cell>67.4</cell><cell>67.5</cell></row><row><cell cols="5">OTB-2015 [58] 47.5</cell><cell cols="2">58.2</cell><cell></cell><cell cols="2">56.8</cell><cell></cell><cell cols="2">67.8</cell><cell cols="3">68.2 69.1 66.9</cell><cell>70.2</cell><cell>69.6</cell><cell>68.4</cell><cell>70.1</cell><cell>69.6</cell><cell>70.9</cell><cell>69.5</cell><cell>70.8</cell><cell>71.1</cell></row><row><cell>Speed (FPS)</cell><cell></cell><cell></cell><cell>270</cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell>1</cell><cell>0.3</cell><cell>8</cell><cell>35</cell><cell>&lt;1</cell><cell>30</cell><cell>35</cell><cell>4.7</cell><cell>30</cell><cell>25</cell><cell>20</cell><cell>35</cell><cell>26</cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>66</cell><cell>61</cell><cell>56</cell><cell>51</cell><cell>46</cell><cell>41</cell><cell>36</cell><cell>31</cell><cell>26</cell><cell>21</cell><cell>16</cell><cell>11</cell><cell>6</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparison with recent state-of-the-art trackers on the VOT2018<ref type="bibr" target="#b25">[26]</ref> in terms of accuracy (A), robustness (R), and expected average overlap (EAO).</figDesc><table><row><cell></cell><cell cols="3">SiamRPN DiMP-50 PrDiMP-50</cell><cell>Retain-</cell><cell>KYS D3S TrDiMP</cell></row><row><cell></cell><cell>++ [28]</cell><cell>[3]</cell><cell>[9]</cell><cell cols="2">MAML [49] [2] [34]</cell></row><row><cell>A (↑)</cell><cell>0.600</cell><cell>0.597</cell><cell>0.618</cell><cell>0.604</cell><cell>0.609 0.640 0.600</cell></row><row><cell>R (↓)</cell><cell>0.234</cell><cell>0.153</cell><cell>0.165</cell><cell>0.159</cell><cell>0.143 0.150 0.141</cell></row><row><cell>EAO (↑)</cell><cell>0.414</cell><cell>0.440</cell><cell>0.442</cell><cell>0.452</cell><cell>0.462 0.489 0.462</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Comparison results of the Siamese pipeline between without and with our transformer on 7 tracking benchmarks. We compute the relative gain in the VOT2018, while in the rest datasets, we exhibit the absolute gain.</figDesc><table><row><cell>Dataset</cell><cell>Siamese Baseline</cell><cell>TrSiam (Ours)</cell><cell>∆</cell></row><row><cell>Need for Speed [24] (AUC)</cell><cell>61.1</cell><cell>65.8</cell><cell>4.7%↑</cell></row><row><cell>OTB-2015 [58] (AUC)</cell><cell>69.2</cell><cell>70.8</cell><cell>1.6%↑</cell></row><row><cell>UAV123 [37] (AUC)</cell><cell>65.6</cell><cell>67.4</cell><cell>1.8%↑</cell></row><row><cell>LaSOT [13] (AUC)</cell><cell>59.1</cell><cell>62.4</cell><cell>3.3%↑</cell></row><row><cell>GOT-10k [23] (AO)</cell><cell>62.0</cell><cell>67.3</cell><cell>5.3%↑</cell></row><row><cell>TrackingNet [39] (Success)</cell><cell>75.1</cell><cell>78.1</cell><cell>3.0%↑</cell></row><row><cell>VOT2018 [26] (EAO)</cell><cell>0.389</cell><cell>0.417</cell><cell>7.2%↑</cell></row><row><cell>Tracking Speed (FPS)</cell><cell>40</cell><cell>35</cell><cell>5 FPS ↓</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Comparison results of the DiMP pipeline between without and with our transformer on 7 tracking benchmarks. We compute the relative gain in the VOT2018, while in the rest datasets, we exhibit the absolute gain.</figDesc><table><row><cell>Dataset</cell><cell>DiMP Baseline</cell><cell>TrDiMP (Ours)</cell><cell>∆</cell></row><row><cell>Need for Speed [24] (AUC)</cell><cell>64.7</cell><cell>66.5</cell><cell>1.8%↑</cell></row><row><cell>OTB-2015 [58] (AUC)</cell><cell>70.1</cell><cell>71.1</cell><cell>1.0%↑</cell></row><row><cell>UAV123 [37] (AUC)</cell><cell>67.2</cell><cell>67.5</cell><cell>0.3%↑</cell></row><row><cell>LaSOT [13] (AUC)</cell><cell>63.0</cell><cell>63.9</cell><cell>0.9%↑</cell></row><row><cell>GOT-10k [23] (AO)</cell><cell>66.7</cell><cell>68.8</cell><cell>2.1%↑</cell></row><row><cell>TrackingNet [39] (Success)</cell><cell>78.1</cell><cell>78.4</cell><cell>0.3%↑</cell></row><row><cell>VOT2018 [26] (EAO)</cell><cell>0.446</cell><cell>0.462</cell><cell>3.6%↑</cell></row><row><cell>Tracking Speed (FPS)</cell><cell>30</cell><cell>26</cell><cell>4 FPS ↓</cell></row><row><cell>B. Visualization</cell><cell></cell><cell></cell><cell></cell></row></table><note>B.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>The accuracy (A), robustness (R), and expected average overlap (EAO) of state-of-the-art methods on the VOT-2019<ref type="bibr" target="#b26">[27]</ref>.</figDesc><table><row><cell></cell><cell cols="7">SPM SiamRPN++ SiamMask ATOM SiamDW DiMP-50 TrDiMP</cell></row><row><cell></cell><cell>[50]</cell><cell>[28]</cell><cell>[56]</cell><cell>[7]</cell><cell>[63]</cell><cell>[3]</cell><cell>Ours</cell></row><row><cell>A (↑)</cell><cell>0.577</cell><cell>0.599</cell><cell>0.594</cell><cell>0.603</cell><cell>0.600</cell><cell>0.594</cell><cell>0.598</cell></row><row><cell>R (↓)</cell><cell>0.507</cell><cell>0.482</cell><cell>0.461</cell><cell>0.411</cell><cell>0.467</cell><cell>0.278</cell><cell>0.231</cell></row><row><cell cols="2">EAO (↑) 0.275</cell><cell>0.285</cell><cell>0.287</cell><cell>0.292</cell><cell>0.299</cell><cell>0.379</cell><cell>0.397</cell></row><row><cell cols="8">some top-performing approaches on VOT2019. Table 10</cell></row><row><cell cols="8">shows the accuracy, robustness, and EAO scores of differ-</cell></row><row><cell cols="8">ent trackers. Compared with DiMP-50, our TrDiMP shows</cell></row><row><cell cols="8">similar tracking accuracy but exhibits a much lower failure</cell></row><row><cell cols="8">rate (i.e., robustness score). Compared with other recent</cell></row><row><cell cols="8">deep trackers with the ResNet-50 backbone, our TrDiMP</cell></row><row><cell cols="8">significantly surpasses them such as SiamRPN++, SiamDW</cell></row><row><cell cols="8">[63] and SiamMask [56] by a considerable margin. The</cell></row><row><cell cols="8">VOT2019 challenge winner (i.e., DRNet) shows an EAO</cell></row><row><cell cols="8">score of 0.395 [27]. Overall, the proposed TrDiMP outper-</cell></row><row><cell cols="8">forms the current top-performing trackers with a promising</cell></row><row><cell cols="3">EAO score of 0.397.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/visionml/pytracking/tree/ master/ltr</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by the National Natural Science Foundation of China under Contract 61836011, 61822208, and 61836006, and in part by the Youth Innovation Promotion Association CAS under Grant 2018497.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11981</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abdelrahman Eldesokey, et al. The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukezic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zhenyu He, and Ming-Hsuan Yang. Target-aware deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep regression tracking with shrinkage loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">D3s-a discriminative single shot segmentation tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context-aware correlation filter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Crest: Convolutional residual learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">End-to-end asr: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Siam r-cnn: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spm-tracker: Series-parallel matching for real-time visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Post: Policy-based switch tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised deep representation learning for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="418" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-cue correlation filters for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning feature embeddings for discriminant model based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">TrDiMP (Ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">TrSiam (Ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ATOM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siamrpn++</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Rpn</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mdnet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">TrDiMP (Ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">TrSiam (Ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ATOM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siamrpn++</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Rpn</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mdnet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
