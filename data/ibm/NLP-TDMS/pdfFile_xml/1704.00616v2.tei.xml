<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
							<email>zolfagha@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg im Breisgau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
							<email>oliveira@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg im Breisgau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Sedaghat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg im Breisgau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg im Breisgau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition is a complex task in computer vision, due to the variety of possible actions is large and there are multiple visual cues that play an important role. In contrast to object recognition, action recognition involves not only the detection of one or multiple persons, but also the awareness of other objects, potentially involved in the action, such as the pose of the person, and their motion. Actions can span various time intervals, making good use of videos and their temporal context is a prerequisite for solving the task to its full extent <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>The success of convolutional networks in recognition has also influenced action recognition. Due to the importance of multiple visual cues, as shown by Jhuang et al. <ref type="bibr" target="#b11">[12]</ref>, multistream architectures have been most popular. This trend was initiated by Simonyan and Zisserman <ref type="bibr" target="#b32">[33]</ref>, who proposed a simple fusion of the action class scores obtained with two separate convolutional networks, where one was trained on raw images and the other on optical flow. The relative success of this strategy shows that deep networks for action recognition cannot directly infer the relevant motion cues from the raw images, although, in principle, the network could learn to compute such cues.</p><p>In this paper, we propose a three-stream architecture that also includes pose, see <ref type="figure" target="#fig_0">Figure 1</ref>. Existing approaches model the temporal dynamics of human postures with hand-crafted features. We rather propose to compute the position of human body parts with a fast convolutional network. Moreover, we use a network architecture with spatio-temporal convolutions <ref type="bibr" target="#b36">[37]</ref>. This combination can capture temporal dynamics of body parts over time, which is valuable to improve action recognition performance, as we show in dedicated experiments. The pose network also yields the spatial localization of the persons, which allows us to apply the approach to spatial action localization in a straightforward manner.</p><p>The second contribution is on the combination of the multiple streams, as also illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The combination is typically done by summation of scores, by a linear classifier, or by early or late concatenation of features within the network. In this paper, we propose the integration of different modalities via a Markov chain, which leads to a sequential refinement of action labels. We show that such sequential refinement is beneficial over independent training of streams. At the same time, the sequential chain imposes an implicit regularization. This makes the architecture more robust to over-fitting -a major concern when jointly training very large networks. Experiments on multiple benchmarks consistently show the benefit of the sequential refinement approach over alternative fusion strategies.</p><p>Since actions may span different temporal resolutions, we analyze videos at multiple temporal scales. We demonstrate that combining multiple temporal granularity levels improves the capability of recognizing different actions. In contrast to some other state-of-the-art strategies to analyze videos over longer time spans, e.g., temporal segmentation networks <ref type="bibr" target="#b42">[43]</ref>, the architecture still allows the temporal localization of actions by providing actionness scores of frames using a sliding window over video. We demonstrate this flexibility by applying the approach also to temporal and spatio-temporal action detection. Compared to previous spatio-temporal action localization methods, which are typically based on region proposals and action tubes, the pose network in our approach directly provides an accurate person localization at no additional computational costs. Therefore, it consistently outperforms the previous methods in terms of speed and mean average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Feature based approaches. Many traditional works in the field of action recognition focused on designing features to discriminate action classes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref>. These features were encoded with high order encodings, e.g., bag of words (BoW) <ref type="bibr" target="#b34">[35]</ref> or Fisher vector based encodings <ref type="bibr" target="#b30">[31]</ref>, to produce a global representation for video and to train a classifier on the action labels. Recent research showed that most of these approaches are not only computationally expensive, but they also fail on capturing context and high-level information.</p><p>CNN based approaches. Deep learning has enabled the replacement of hand-crafted features by learned features, and the learning of whole tasks end-to-end. Several works employed deep architectures for video classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Thanks to their hiearchical feature representation, deep networks learn to capture localized features as well as context cues and can exploit high-level information from large scale video datasets. Baccouche et al. <ref type="bibr" target="#b1">[2]</ref> firstly used a 3D CNN to learn spatio-temporal features from video and in the next step they employed an LSTM to classify video sequences. More recently, several CNN based works presented efficient deep models for action recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>. Tran et al. <ref type="bibr" target="#b36">[37]</ref> employed a 3D architecture to learn spatio-temporal features from videos.</p><p>Fusion of multiple modalities. Zisserman et al. <ref type="bibr" target="#b32">[33]</ref> proposed a two-stream CNN to capture the complementary information from appearance and motion, each modality in an independent stream. Feichtenhofer et al. <ref type="bibr" target="#b7">[8]</ref> investigated the optimal position within a convolution network in detail to combine the separate streams. Park et al. <ref type="bibr" target="#b27">[28]</ref> proposed a gated fusion approach. In a similar spirit, Wang et al. <ref type="bibr" target="#b45">[46]</ref> presented an adaptive fusion approach, which uses two regularization terms to learn fusion weights. In addition to optical flow, some works made use of other modalities like audio <ref type="bibr" target="#b45">[46]</ref>, warped flow <ref type="bibr" target="#b42">[43]</ref>, and object information <ref type="bibr" target="#b10">[11]</ref> to capture complementary information for video classification. In the present work, we introduce a new, flexible fusion technique for early or late fusion via a Markov chain and show that it outperforms previous fusion methods.</p><p>Pose feature based methods. Temporal dynamics of body parts over time provides strong information on the performing action. Thus, this information has been employed for action recognition in several works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39]</ref>. Cheron et al. <ref type="bibr" target="#b3">[4]</ref> used pose information to extract high-level features from appearance and optical flow. They showed that using pose information for video classification is highly effective. Wang et al. <ref type="bibr" target="#b38">[39]</ref> used data mining techniques to obtain a representation for each video and finally, by using a bag-of-words model to classify videos. In the present work, we compute the human body layout efficiently with a deep network and learn the relevant spatio-temporal pose features within one of the streams of our action classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Inputs to the Network</head><p>We rely on three input cues: the raw RGB images, optical flow, and human pose in the form of human body part segmentation. All inputs are provided as spatio-temporal inputs covering multiple frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Optical Flow</head><p>We compute the optical flow with the method from Zach et al. <ref type="bibr" target="#b47">[48]</ref>, which is a reliable variational method that runs sufficiently fast. We convert the x-component and ycomponent of the optical flow to a 3 channel RGB image by stacking components and magnitude of them <ref type="bibr" target="#b28">[29]</ref>. The flow and magnitude values in the image are multiplied by 16 and quantized into the [0,255] interval <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Body Part Segmentation</head><p>Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27</ref>], depth estimation <ref type="bibr" target="#b19">[20]</ref> and optical flow estimation <ref type="bibr" target="#b6">[7]</ref>. For this work, we make use of Fast-Net <ref type="bibr" target="#b26">[27]</ref>, a network for human body part segmentation, which will provide our action recognition network with body pose information. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the architecture of Fast-Net. The encoder part of the network is initialized with the VGG network <ref type="bibr" target="#b33">[34]</ref>. Skip connections from the encoder to the decoder part ensure the reconstruction of details in the output up to the original input resolution.</p><p>We trained the Fast-Net architecture on the J-HMDB <ref type="bibr" target="#b11">[12]</ref> and the MPII [1] action recognition datasets. J-HMDB provides body part segmentation masks and joint locations, while MPII provides only joint locations. To make body part masks compatible across datasets, we apply the following methodology, which only requires annotation for the joint locations. First, we derive a polygon for the torso from the joint locations around that area. Secondly, we approximate the other parts by ellipses scaled consistently based on the torso area and the distance between the respective joints; see second column of <ref type="figure" target="#fig_2">Fig. 3</ref>. We convert the body part segmentation into a 3 channel RGB image, mapping each label to a correspondent pre-defined RGB value.</p><p>To the best of our knowledge, we are the first who trained a convolutional network on body part segmentation for the purpose of action recognition. <ref type="figure" target="#fig_2">Figure 3</ref> shows exemplary results of the body part segmentation technique on J-HMDB and MPII datasets. Clearly, the network provides good accuracy on part segmentation and is capable of handling images with multiple instances. The pose estimation network has a resolution of 150×150 and runs at 33 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Action Recognition Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-stream Fusion with a Markov Chain</head><p>To integrate information from the different inputs we rely on the model of a multi-stream architecture <ref type="bibr" target="#b32">[33]</ref>, i.e., each input cue is fed to a separate convolutional network stream that is trained on action classification. The innovation in our approach is the way we combine these streams. In contrast to the previous works, we combine features from the different streams sequentially. Starting with the human body part stream, we refine the evidence for an action class with the optical flow stream, and finally apply a refinement by the RGB stream.</p><p>We use the assumption that the class predictions are conditionally independent due to the different input modalities. Consequently, the joint probability over all input streams factorizes into the conditional probabilities over the separate input streams.</p><p>In a Markov chain, given a sequence of inputs X = {X 1 , X 2 , ..., X S }, we wish to predict the output sequence to the Markov property, P (Y |X) can be decomposed:</p><formula xml:id="formula_0">Y = {Y 1 , Y 2 , ..., Y S } such that P (Y |X) is maximized. Due OF Pose RGB 3DCNN 3DCNN 3DCNN FC1 Y pred Concatenated Features FC2 FC FC FC OF Pose RGB 3DCNN 3DCNN 3DCNN Y OF Y RGB Y Pose YPose YOF Y Pose FC FC FC FC FC FC FC FC FC h1 h2 h3</formula><formula xml:id="formula_1">P (Y |X) = P (Y 1 |X) S s=2 P (Y s |X, Y 1 , . . . , Y s−1 ) (1)</formula><p>For the state s ∈ {1, . . . , S}, we denote by h s the hidden state of that stream. We use deep networks to model the likelihood in <ref type="formula">(1)</ref>:</p><formula xml:id="formula_2">h s = f ([h s−1 , 3DCNN(X s ), (Y 1 , . . . , Y s−1 )]) P (Y s |X, Y &lt;s ) = softmax(Net s (h s )),<label>(2)</label></formula><p>where f is a non-linearity unit (ReLU), h s−1 denotes the hidden state from the previous stream, and y s is the prediction of stream s. For the 3DCNN(·), we use the convolutional part of the network presented in <ref type="figure" target="#fig_4">Figure 5</ref> to encapsulate the information in the input modality, and Net s is the fully connected part in <ref type="figure" target="#fig_4">Figure 5</ref>. At each fusion stage, we concatenate the output of the function 3DCNN(·) with the hidden state and the outputs from the previous stream and apply the non-linearity f before feeding them to Net s . Finally, at the output part, we use Net s to predict action labels from h s . With the softmax(·) function we convert these scores into (pseudo-)probabilities.</p><p>Using the above notation, we consider input modalities as X = {X pose , X OF , X RGB }, and X s = {x t } T t=1 , where x t is the t-th frame in X s , and T is the total number of frames in X s . At the stage s = 1, by considering X 1 = X pose we start with an initial hidden state and obtain an initial prediction (see <ref type="figure" target="#fig_3">Figure 4</ref>-right):</p><formula xml:id="formula_3">h 1 = 3DCNN(X pose ) P (Y 1 |X) = softmax(Net 1 (h 1 ))<label>(3)</label></formula><p>At each subsequent stage s 2, we obtain a refined prediction y s by combining the hidden state and the predictions from the previous stage.</p><formula xml:id="formula_4">h 2 = f ([h 1 , 3DCNN(X OF ), (Y 1 )]) P (Y 2 |X, Y &lt;2 ) = softmax(Net 2 (h 2 )) h 3 = f ([h 2 , 3DCNN(X RGB ), (Y 1 , Y 2 )]) P (Y 3 |X, Y &lt;3 ) = softmax(Net 3 (h 3 ))<label>(4)</label></formula><p>In the proposed model, at each stage, the next prediction is made conditioned on all previous predictions and the new input. Therefore, when training the network, the prediction of the output class label does not only depend on the input, but also on the previous state. Thus, the network in that stream will learn complementary features to refine the class labels from the previous streams. With this chaining and joint training, the information at the previous stages serve as the present belief for the predictions at the current stage, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>-right. This sequential improvement of the class label enables the combination of multiple cues within a large network, while keeping the risk of over-fitting low. This is in contrast to the fusion approaches that combine features from different, independently trained streams. In such a case, the different streams are not enforced to learn complementary features. In the other extreme, approaches that train all streams jointly but not sequentially, are more prone to over-fitting, because the network is very large, and, in such case, lacks the regularization via the separate streams and their additional losses.</p><p>It should be expected that the ordering of the sequence plays a role for the final performance. We compared different ordering options in our experiments and report them in the following section. The ordering that starts with the pose as input and ends with the RGB image yielded the best results.</p><p>It is worth noting that the concept of sequential fusion could be applied to any layer of the network. Here we placed the fusion after the first fully-connected layer, but the fusion could also be applied to the earlier convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Configuration</head><p>In all streams, we use the C3D architecture <ref type="bibr" target="#b36">[37]</ref> as the base architecture, which has 17.5M parameters. The network has 8 three-dimensional convolution layers with kernel size of 3×3×3 and stride 1, 5 three-dimensional pooling layers with kernel size of 2×2×2 and stride 2 and two fully connected layers followed by a softmax; see <ref type="figure" target="#fig_4">Figure 5</ref>. Each stream is connected with the next stream via layer FC6; see <ref type="figure" target="#fig_3">Figure 4</ref>-right. Each stream takes 16 frames as input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>The network weights are learned using mini-batch stochastic gradient descent (SGD) with a momentum of 0.9 and weight decay of 5e −4 . We jointly optimize the whole network without truncating gradients and update the weights of each stream based on the full gradient including the contribution from the following stream. We initialize the learning rate with 1e −4 and decrease it by a factor of 10 every 2k for J-HMDB, 20k for UCF101 and NTU, and at multiple steps for HMDB51. The maximum number of iterations was 20k for J-HMDB, 40k for HMDB51 and 60k for the UCF101 and NTU datasets. We initialize the weights of all streams with an RGB network pre-trained on the large-scale Sports-1M dataset <ref type="bibr" target="#b13">[14]</ref>.</p><p>We split each video into clips of 16 frames with an overlap of 8 frames and feed each clip individually into the network stream with size of 16 × 112 × 112. We apply corner cropping as a form of data augmentation to the training data. Corner cropping extracts regions from the corners and the center of the image. It helps to prevent the network from bias towards the center area of the input. Finally, we resize these cropped regions to the size of 112 × 112. In each iteration, all streams take the same clip from the video with the same augmentation but with different modalities as input.</p><p>We used Caffe <ref type="bibr" target="#b12">[13]</ref> and an NVIDIA Titan X GPU to run our experiments. The training time for the J-HMDB dataset was ∼ 10 hours for the full network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Temporal Processing of the Whole Video</head><p>At test time, we feed the architecture with a temporal window of 16 frames. The stride over the video is 8. Each set of inputs is randomly selected for cropping operations, which are 4 corners and 1 center crop for the original image and their horizontal flipping counterpart. We extract scores before the softmax normalization in the last stream (Y RGB).</p><p>In case of action classification, the final score of a video is calculated by taking the average of scores over all tem-poral windows across a video and 10 crop scores per clip. Apart from averaging, we also tested a multi-resolution approach, which we call multi-granular (MG), where we trained separate networks for three different temporal resolutions. These are assembled as (1) 16 consecutive frames, (2) 16 frames from a temporal window of 32 frames by a sample rate of 2, and (3) 16 frames sampled randomly from the entire video. For the final score, we take the average over the scores produced by these temporal resolution networks. This approach extends the temporal context that the network can see, which can be useful for more complex actions with longer duration.</p><p>In case of temporal action detection, we localize the action in time by thresholding the score provided for each frame. Clearly, the MG approach is not applicable here. In addition to the action score, also the human body part network helps in temporal localization: we do not detect an action as long as no human is detected. More details on the spatio-temporal action detection are provided in the experimental section and in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>UCF-101 <ref type="bibr" target="#b35">[36]</ref> contains more than 2 million frames in more than 13, 000 videos, which are divided into 101 human action classes. The dataset is split into three folds and each split contains about 8000 videos for training. The UCF101 dataset also comes with a subset for spatiotemporal action detection.</p><p>HMDB51 <ref type="bibr" target="#b14">[15]</ref> contains 6766 videos divided into 51 action classes, each with at least 101 samples. The evaluation follows the same protocol used for UCF-101.</p><p>J-HMDB contains a subset of videos from the HMDB dataset, for which it provides additional annotation, in particular optical flow and joint localization <ref type="bibr" target="#b11">[12]</ref>. Thus, it is well-suited for evaluating the contribution of optical flow, body part segmentation, and the fusion of all cues via a  <ref type="table">Table 1</ref>: The value of different cues and their integration for action recognition on the UCF101, HMDB51, and J-HMDB datasets (split 1). Adding optical flow and pose is always beneficial. Integration via the proposed Markov chain clearly outperforms the baseline fusion approach. In all cases, the accuracy achieved with estimated optical flow and body parts almost reaches the upper bound performance when providing ground truth values for those inputs.</p><p>Markov chain. The dataset comprises 21 human actions.</p><p>The complete dataset has 928 clips and 31838 frames. There are 3 folds for training and testing for this dataset. The videos in J-HMDB are trimmed and come with bounding boxes. Thus, it can be used also as a benchmark for spatial action localization. NTU RGB+D is a recent action recognition dataset that is quite large and provides depth and pose ground truth <ref type="bibr" target="#b31">[32]</ref>. It contains more than 56000 sequences and 4 million frames. NTU provides 60 action classes and 3D coordinates for 25 joints. Additionally, the high intra-class variations make NTU one of the most challenging datasets. <ref type="table">Table 1</ref> shows that fusion with the sequential Markov chain model outperforms the baseline fusion consistently across all datasets. The baseline fusion is shown in <ref type="figure" target="#fig_3">Figure 4</ref> and can be considered a strong baseline. It consists of fusing the multiple modalities through feature concatenation followed by a set of fully connected layers. The network is trained jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Action Classification</head><p>Adding pose leads to a substantial improvement over the two-stream version. This confirms that pose plays an important role as complementary modality for action recognition tasks. Again, the Markov chain fusion is advantageous with a large margin.</p><p>For the J-HMDB dataset, ground truth for optical flow and pose is available and can be provided to the method. While not being relevant in practice, running the recognition with this ground truth shows on how much performance Datasets Methods UCF101 HMDB51 J-HMDB TS Fusion <ref type="bibr" target="#b7">[8]</ref> 92.5% 65.4% -LTC <ref type="bibr" target="#b37">[38]</ref> 91.7% 64.8% -Two-stream <ref type="bibr" target="#b32">[33]</ref> 88.0% 59.4% -TSN <ref type="bibr" target="#b42">[43]</ref> 94.2% 69.4% -CPD <ref type="bibr" target="#b25">[26]</ref> 92.3% 66.2% -Multi-Granular <ref type="bibr" target="#b17">[18]</ref> 90.8% 63.6% -M-fusion <ref type="bibr" target="#b27">[28]</ref> 89.1% 54.9% -KVMF <ref type="bibr" target="#b48">[49]</ref> 93.1% 63.3% -P-CNN <ref type="bibr" target="#b3">[4]</ref> --61.1% Action tubes <ref type="bibr" target="#b8">[9]</ref> --62.5% TS R-CNN <ref type="bibr" target="#b28">[29]</ref> --70.5% MR-TS R-CNN <ref type="bibr" target="#b28">[29]</ref> --71.1% Ours (chained) 91.1% 69.7% 76.1% <ref type="table">Table 2</ref>: Comparison to the state of the art on UCF101, HMDB51, and J-HMDB datasets (over all three splits).</p><p>is lost due to erroneous optical flow and pose estimates. Surprisingly, the difference between the results is rather small, showing that the network does not suffer much from imperfect estimates. This conclusion can be drawn independently of the fusion method. Finally, the temporal multi-granularity fusion (MG) further improves results. Especially on HMDB51, there is a large benefit. <ref type="table" target="#tab_1">Table 3</ref> compares the proposed network to the state of the art in action classificaation. In contrast to <ref type="table">Table 1</ref>, the comparison does not show the direct influence of single contributions anymore, since this table compares whole systems that are based on quite different components. Many of these systems also use other features extraction approaches, such as improved dense trajectories (IDT), which generally have a positive influence on the results, but also make the system more complicated and harder to control. Our network outperforms the state of the art on J-HMDB, NTU, and HMDB51. Also, on UCF101 dataset our approach is on par with the current state of the art while it does not rely on any additional hand-crafted features. In two stream case (RGB+OF), if we replace the 3DCNN network by the TSN approach <ref type="bibr" target="#b42">[43]</ref>, we obtain a classification accuracy of 94.05% on UCF101 (over 3 splits), which is the state of the art also on this dataset. However, the TSN approach does not allow for action detection anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Comparison with the state-of-the-art</head><p>Finally, we ran the network on the recent NTU RGB+D dataset, which is larger and more challenging than the previous datasets. The dataset is popular for the evaluation of methods that are based on human body pose. Clearly, the result of our network, shown in <ref type="table">Table ?</ref>?, compares favorably Methods Cross Subject % Deep LSTM <ref type="bibr" target="#b31">[32]</ref> 60.7% P-LSTM <ref type="bibr" target="#b31">[32]</ref> 62.93% HOGˆ2 <ref type="bibr" target="#b24">[25]</ref> 32.2% FTP DS <ref type="bibr" target="#b9">[10]</ref> 60.23% ST-LSTM <ref type="bibr" target="#b20">[21]</ref> 69.2% Ours (Pose) 67.8% Ours (RGB+OF+Pose -Baseline) 76.9% Ours (RGB+OF+Pose -Chained) 80.8%  <ref type="table">Table 4</ref>: Impact of chain order on the performance (clip accuracy) on UCF101 and HMDB51 datasets (split1). "O" = Optical flow, "P" = Pose and "R" = RGB.  <ref type="table">Table 5</ref>: Sequential improvement of classification accuracy on UCF101, HMDB51 and J-HMDB datasets (Split1) by adding modalities to the chained network.</p><p>to the existing methods. As a result, the used pose estimation network is competitive with pose estimates using depth images and that our way to integrate this information with the raw images and optical flow is advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2</head><p>Ordering of modalities in the Markov chain. <ref type="table">Table 4</ref> shows an analysis on how the order of the modalities affects the final classification accuracy. Clearly, the ordering has an effect. The proposed ordering starting with the pose and then adding the optical flow and the RGB images performed best, but there are alternative orders that do not perform much worse. <ref type="table">Table 5</ref> quantifies the improvement in accuracy when adding a modality. Clearly, each additional modality improves the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Fusion location</head><p>In principle the chained fusion can be applied to any layer in the network. We studied the effect of this choice. In contrast to the large scale evaluation in Feichtenhofer et al. <ref type="bibr" target="#b7">[8]</ref>, we tested only two locations: FC6 and FC7. <ref type="table" target="#tab_4">Table 6</ref> shows a clear difference only on the J-HMDB dataset. There it seems that an earlier fusion, at a level where the features are not too abstract yet, is advantageous. This is similar to   <ref type="table">Table 7</ref>: Effect of the temporal window size. Using more frames as input to the network consistently increases classification performance.</p><p>the outcome of the study by Feichtenhofer et al. <ref type="bibr" target="#b7">[8]</ref>, where the last convolutional layer worked best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Effect of clip length</head><p>We analyzed the effect of the size of the temporal window on the action recognition performance. Larger windows clearly improve the accuracy on all datasets; see <ref type="table">Table 7</ref>.</p><p>For the J-HMDB dataset (RGB modality) we use a temporal window ranging from 4 to 16 frames every 4 frames. The highest accuracy is obtained with a 16 frames clip size. Based on the J-HMDB minimum video size, 16 is the highest possible time frame to be explored. We also tested multiple temporal resolutions for the NTU dataset (pose modality). Again, we obtained the best results for the network with the larger clip length as input.</p><p>The conducted experiments confirm that increasing the length of the clip, we decrease the chance of getting unrelated parts of an action in a video. In addition, with longer sequences, 3D convolutions can better exploit their ability to capture abstract spatio-temporal features for recognizing actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Action Detection</head><p>To demonstrate the generality of our approach, we show also results on action detection on UCF101 and J-HMDB. Many of the top performing methods for action classification are not applicable to action detection, because they integrate information over time in a complex manner, are too slow, or are unable to spatially localize the action. This is different for our approach, which is efficient and can be run in a sliding window manner over time and provides good spatial localization via the human body part segmentation. In order to create temporally consistent spatial detections, we link action bounding boxes over time to pro-  <ref type="figure">Figure 6</ref>: Scheme for spatio-temporal action detection. The chained network provides action class scores and body part segmentations per frame. From these we compute action tubes and their actionness scores; see the supplemental material for details. duce action tube <ref type="bibr" target="#b8">[9]</ref>; see the supplemental material for details. We use the frame level action classification scores to make predictions at the tube level. <ref type="figure">Figure 6</ref> schematically outlines the detection procedure.</p><p>We also present a set of qualitative action detection experiments for the UCF and J-HMDB datasets. <ref type="figure">Figure 7</ref> shows several examples where we can robustly localize the action, even when unusual pose, illumination, viewpoints and motion blur are presented. Additional results exploring failure cases are provided in supplementary material.</p><p>Following recent works on action detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29]</ref>, we report video-AP. A detection is considered correct if the intersection over union (IoU) with the ground-truth is above a threshold δ and the action label is predicted correctly. The IoU between two tubes is defined as the IoU over the temporal domain, multiplied by the average of the IoU between boxes averaged over all overlapping frames. Video-AP measures the area under the precision-recall curve of the action tube predictions. <ref type="table" target="#tab_6">Table 8</ref> and <ref type="table" target="#tab_7">Table 9</ref> show the video mAP results on spatial and spatio-temporal action detection with different IoU thresholds on J-HMDB and UCF101 (split1) datasets respectively. Although we did not optimize our approach for action detection, we obtain state-of-the-art results on both datasets. Moreover, the approach is fast: spatial detection runs at a rate of 31 fps and spatio-temporal detection with 10 fps. Compared to the recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>, our detection framework has two desirable properties: (1) the pose network directly provides a single detection box per person, which causes a large speed-up; (2) the classification <ref type="figure">Figure 7</ref>: Qualitative results on the action detection task. The first two rows correspond to detections on UCF101, the last ones on J-HMDB. Ground truth bounding boxes are shown in green and detections in red. Our spatial localization is accurate and robust to unusual pose.   takes advantage of three modalities and the chained fusion, which yields highly accurate per-frame scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a network architecture that integrates multiple cues sequentially via a Markov chain model. We have shown that this sequential fusion clearly outperforms other ways of fusion, because it can consider the mutual dependencies of cues during training while avoiding overfitting due to very large network models. Our approach provides state-of-the-art performance on all four challenging action classification datasets UCF101, HMDB51, J-HMDB and NTU RGB+D while not using any additional handcrafted features. Moreover, we have demonstrated the value of a reliable pose representation estimated via a fast convolutional network. Finally, we have shown that the approach generalizes also to spatial and spatio-temporal action detection, where we obtained state-of-the-art results as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The chained multi-stream 3D-CNN sequentially refines action class labels by analyzing motion and pose cues. Pose is represented by human body parts detected by a deep network. The spatio-temporal CNN can capture the temporal dynamics of pose. Additional losses on Y P ose and Y OF are used for training. The final output of the network Y RGB is provided at the end of the chain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Human body part segmentation architecture. Convolutions are shown in green, pooling in blue, feature map dropout in brown, up-convolutional layers in red and softmax in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on J-HMDB and MPII datasets (task with 15 body parts). First column: Input image. Second column: Ground truth. Third column: Result predicted with Fast-Net. First two rows correspond to results on J-HMDB and the last ones on MPII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Baseline fusion architecture (left) and the proposed approach (right). In the chained architecture, there is a separate loss function for each stream. The final class label is obtained at the end of the chain (rightmost prediction).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Base architecture used in each stream of the action recognition network. The convolutional part is a 3DCNN architecture. We define the remaining fully connected layers as N et s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Comparison to literature on the NTU RGB+D benchmark.</figDesc><table><row><cell>Dataset</cell><cell>OPR</cell><cell>ORP</cell><cell>RPO</cell><cell>ROP</cell><cell>PRO</cell><cell>POR</cell></row><row><cell>HMDB51</cell><cell>59.8%</cell><cell>57.3%</cell><cell>54.8%</cell><cell>54.1%</cell><cell>56.4%</cell><cell>60.0%</cell></row><row><cell>UCF101</cell><cell>86.8%</cell><cell>86.2%</cell><cell>84.3%</cell><cell>84.7%</cell><cell>85.1%</cell><cell>87.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Classification performance for different fusion locations on UCF101, HMDB51 and J-HMDB datasets (split1).</figDesc><table><row><cell>Dataset</cell><cell cols="2">Clip length Accuracy</cell></row><row><cell></cell><cell>4</cell><cell>44.8%</cell></row><row><cell>J-HMDB (RGB)</cell><cell>8 12</cell><cell>49.6% 58.7%</cell></row><row><cell></cell><cell>16</cell><cell>60.8%</cell></row><row><cell>NTU RGB+D (Pose)</cell><cell>16 32</cell><cell>61.6% 67.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Spatial action detection results (Video mAP) on the J-HMDB dataset. Across all IoU thresholds, our model outperforms the state of the art. Weinzaepfel et al. [44] 54.28 51.68 46.77 37.82 Yu et al.</figDesc><table><row><cell>UCF101</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Spatio-temporal action detection results (Video mAP) on UCF101 dataset (split1). Across all IoU thresholds, our model outperforms the state of the art.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We acknowledge funding by the ERC Starting Grant VideoLearn and the Freiburg Graduate School of Robotics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">2d human pose estimation: New benchmark and state of the art analysis. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Behavior Unterstanding, HBU&apos;11</title>
		<meeting>the Second International Conference on Human Behavior Unterstanding, HBU&apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pose-based CNN Features for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR &apos;05</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient twostream motion and appearance 3d cnns for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1608.08851</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazrba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action recognition by learning deep multi-granular spatio-temporal video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval, ICMR &apos;16</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval, ICMR &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<idno>abs/1606.04992</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
		<imprint>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularizing long short term memory with 3d human-skeleton sequences for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW &apos;13</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved dense trajectory with cross streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference, MM &apos;16</title>
		<meeting>the 2016 ACM on Multimedia Conference, MM &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient deep models for monocular road segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining multiple sources of knowledge in deep cnns for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-region two-stream R-CNN for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 -European Conference on Computer Vision</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E J</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth IEEE International Conference on Computer Vision</title>
		<meeting>the Ninth IEEE International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
	<note>ICCV &apos;03</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1604.04494</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision, ICCV &apos;13</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision, ICCV &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2708" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015 -IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="3164" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards weaklysupervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1605.05197</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Fusing multi-stream deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1509.06086</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1302" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th DAGM Conference on Pattern Recognition</title>
		<meeting>the 29th DAGM Conference on Pattern Recognition<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
