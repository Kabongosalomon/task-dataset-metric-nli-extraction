<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Digital Media</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Digital Media</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Google Cloud cGANs MSGANs Input</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent code z Image</head><p>This bird has feathers that are black and has a red belly Text <ref type="figure">Figure 1</ref>: Mode seeking generative adversarial networks (MSGANs). (Left) Existing conditional generative adversarial networks tend to ignore the input latent code z and generate images of similar modes. (Right) We propose a simple yet effective mode seeking regularization term that can be applied to arbitrary conditional generative adversarial networks in different tasks to alleviate the mode collapse issue and improve the diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-toimage translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-toimage translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality. * Equal contribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref> have been shown to capture complex and high-dimensional image data with numerous applications effectively. Built upon GANs, conditional GANs (cGANs) <ref type="bibr" target="#b20">[21]</ref> take external information as additional inputs. For image synthesis, cGANs can be applied to various tasks with different conditional contexts. With class labels, cGANs can be applied to categorical image generation. With text sentences, cGANs can be applied to text-to-image synthesis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. With images, cGANs have been used in tasks including image-to-image translation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, semantic manipulation <ref type="bibr" target="#b29">[30]</ref> and style transfer <ref type="bibr" target="#b15">[16]</ref>.</p><p>For most conditional generation tasks, the mappings are in nature multimodal, i.e., a single input context corresponds to multiple plausible outputs. A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations. For instance, in the dog-to-cat image-to-image translation task <ref type="bibr" target="#b14">[15]</ref>, the input dog images decide contents like orientations of heads and positions of facial landmarks, while the noise vectors help the generation of different species. However, cGANs usually suffer from the mode collapse <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> problem, where generators only produce samples from a single or few modes of the distribution and ignore other modes. The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts.</p><p>There are two main approaches to address the mode collapse problem in GANs. A number of methods focus on discriminators by introducing different divergence metrics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref> and optimization process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>. The other methods use auxiliary networks such as multiple generators <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> and additional encoders <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>. However, mode collapse is relatively less studied in cGANs. Some recent efforts have been made in the image-to-image translation task to improve diversity <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. Similar to the second category with the unconditional setting, these approaches introduce additional encoders and loss functions to encourage the one-to-one relationship between the output and the latent code. These methods either entail heavy computational overheads on training or require auxiliary networks that are often task-specific that cannot be easily extended to other frameworks.</p><p>In this work, we propose a mode seeking regularization method that can be applied to cGANs for various tasks to alleviate the mode collapse problem. Given two latent vectors and the corresponding output images, we propose to maximize the ratio of the distance between images with respect to the distance between latent vectors. In other words, this regularization term encourages generators to generate dissimilar images during training. As a result, generators can explore the target distribution, and enhance the chances of generating samples from different modes. On the other hand, we can train the discriminators with dissimilar generated samples to provide gradients from minor modes that are likely to be ignored otherwise. This mode seeking regularization method incurs marginal computational overheads and can be easily embedded in different cGAN frameworks to improve the diversity of synthesized images.</p><p>We validate the proposed regularization algorithm through an extensive evaluation of three conditional image synthesis tasks with different baseline models. First, for categorical image generation, we apply the proposed method on DCGAN <ref type="bibr" target="#b21">[22]</ref> using the CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> dataset. Second, for image-to-image translation, we embed the proposed regularization scheme in Pix2Pix <ref type="bibr" target="#b10">[11]</ref> and DRIT <ref type="bibr" target="#b14">[15]</ref> using the facades <ref type="bibr" target="#b2">[3]</ref>, maps <ref type="bibr" target="#b10">[11]</ref>, Yosemite <ref type="bibr" target="#b33">[34]</ref>, and cat dog <ref type="bibr" target="#b14">[15]</ref> datasets. Third, for text-to-image synthesis, we incorporate StackGAN++ <ref type="bibr" target="#b31">[32]</ref> with the proposed regularization term using the CUB-200-2011 <ref type="bibr" target="#b28">[29]</ref> dataset. We evaluate the diversity of synthesized images using perceptual distance metrics <ref type="bibr" target="#b32">[33]</ref>.</p><p>However, the diversity metric alone cannot guarantee the similarity between the distribution of generated images and the distribution of real data. Therefore, we adopt two recently proposed bin-based metrics <ref type="bibr" target="#b23">[24]</ref>, the Number of Statistically-Different Bins (NDB) metric which determines the relative proportions of samples fallen into clusters predetermined by real data, and the Jensen-Shannon Divergence (JSD) distance which measures the similarity between bin distributions. Furthermore, to verify that we do not achieve diversity at the expense of realism, we evaluate our method with the Fréchet Inception Distance (FID) <ref type="bibr" target="#b8">[9]</ref> as the metric for quality. Experimental results demonstrate that the proposed regularization method can facilitate existing models from various applications achieving better diversity without loss of image quality. <ref type="figure">Figure 1</ref> shows the effectiveness of the proposed regularization method for existing models.</p><p>The main contributions of this work are:</p><p>• We propose a simple yet effective mode seeking regularization method to address the mode collapse problem in cGANs. This regularization scheme can be readily extended into existing frameworks with marginal training overheads and modifications. • We demonstrate the generalizability of the proposed regularization method on three different conditional generation tasks: categorical generation, image-toimage translation, and text-to-image synthesis.</p><p>• Extensive experiments show that the proposed method can facilitate existing models from different tasks achieving better diversity without sacrificing visual quality of the generated images. Our code and pre-trained models are available at https://github.com/HelenMao/MSGAN/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conditional generative adversarial networks. Generative adversarial networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> have been widely used for image synthesis. With adversarial training, generators are encouraged to capture the distribution of real images. On the basis of GANs, conditional GANs synthesize images based on various contexts. For instances, cGANs can generate high-resolution images conditioned on lowresolution images <ref type="bibr" target="#b13">[14]</ref>, translate images between different visual domains <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, generate images with desired style <ref type="bibr" target="#b15">[16]</ref>, and synthesize images according to sentences <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. Although cGANs have achieved success in various applications, existing approaches suffer from the mode collapse problem. Since the conditional contexts provide strong structural prior information for the output images and have higher dimensions than the input noise vectors, generators tend to ignore the input noise vectors, which are responsible for the variation of generated images. As a result, the generators are prone to produce images with similar appearances. In this work, we aim to address the mode collapse problem for cGANs.  <ref type="figure">Figure 2</ref>: Illustration of motivation. Real data distribution contains numerous modes. However, when mode collapse occurs, generators only produce samples from a few modes. From the data distribution when mode collapse occurs, we observe that for latent vectors z 1 and z 2 , the distance between their mapped images I 1 and I 2 will become shorter in a disproportionate rate when the distance between two latent vectors is decreasing. We present on the right the ratio of the distance between images with respect to the distance of the corresponding latent vectors, where we can spot an anomalous case (colored in red) where mode collapse occurs. The observation motivates us to leverage the ratio as the training objective explicitly.</p><p>Reducing mode collapse. Some methods focus on the discriminator with different optimization process <ref type="bibr" target="#b19">[20]</ref> and divergence metrics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref> to stabilize the training process. The minibatch discrimination scheme <ref type="bibr" target="#b25">[26]</ref> allows the discriminator to discriminate between whole mini-batches of samples instead of between individual samples. In <ref type="bibr" target="#b5">[6]</ref>, Durugkar et al. use multiple discriminators to address this issue. The other methods use auxiliary networks to alleviate the mode collapse issue. ModeGAN <ref type="bibr" target="#b1">[2]</ref> and VEE-GAN <ref type="bibr" target="#b26">[27]</ref> enforce the bijection mapping between the input noise vectors and generated images with additional encoder networks. Multiple generators <ref type="bibr" target="#b6">[7]</ref> and weight-sharing generators <ref type="bibr" target="#b17">[18]</ref> are developed to capture more modes of the distribution. However, these approaches either entail heavy computational overheads or require modifications of the network structure, and may not be easily applicable to cGANs.</p><p>In the field of cGANs, some efforts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> have been recently made to address the mode collapse issue on the image-to-image translation task. Similar to ModeGAN and VEEGAN, additional encoders are introduced to provide a bijection constraint between the generated images and input noise vectors. However, these approaches require other task-specific networks and objective functions. The additional components make the methods less generalizable and incur extra computational loads on training. In contrast, we propose a simple regularization term that imposes no training overheads and requires no modifications of the network structure. Therefore, the proposed method can be readily applied to various conditional generation tasks. Recently, the concurrent work <ref type="bibr" target="#b30">[31]</ref> also adopts a loss term similar to our work for reducing mode collapse for cGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Diverse Conditional Image Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>The training process of GANs can be formulated as a mini-max problem: a discriminator D learns to be a classifier by assigning higher discriminative values to the real data samples and lower ones to the generated ones. Meanwhile, a generator G aims to fool D by synthesizing realistic examples. Through adversarial training, the gradients from D will guide G toward generating samples with the distribution similar to the real data one.</p><p>The mode collapse problem with GANs is well known in the literature. Several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> attribute the missing mode to the lack of penalty when this issue occurs. Since all modes usually have similar discriminative values, larger modes are likely to be favored through the training process based on gradient descent. On the other hand, it is difficult to generate samples from minor modes.</p><p>The mode missing problem becomes worse in cGANs. Generally, conditional contexts are high-dimensional and structured (e.g., images and sentences) as opposed to the noise vectors. As such, the generators are likely to focus on the contexts and ignore the noise vectors, which account for diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mode Seeking GANs</head><p>In this work, we propose to alleviate the missing mode problem from the generator perspective. <ref type="figure">Figure 2</ref> illustrates the main ideas of our approach. Let a latent vector z from the latent code space Z be mapped to the image space I. When mode collapse occurs, the mapped images are collapsed into a few modes. Furthermore, when two latent codes z 1 and z 2 are closer, the mapped images I 1 = G(c, z 1 ) and I 2 = G(c, z 2 ) are more likely to be collapsed into the same mode. To address this issue, we propose a mode seeking regularization term to directly maximize the ratio of the distance between G(c, z 1 ) and G(c, z 2 ) with respect to the distance between z 1 and z 2 ,</p><formula xml:id="formula_0">L ms = max G ( d I (G(c, z 1 ), G(c, z 2 )) d z (z 1 , z 2 ) ),<label>(1)</label></formula><p>where d * (·) denotes the distance metric.</p><p>The regularization term offers a virtuous circle for training cGANs. It encourages the generator to explore the image space and enhances the chances for generating samples of minor modes. On the other hand, the discriminator is forced to pay attention to generated samples from minor modes. <ref type="figure">Figure 2</ref> shows a mode collapse situation where two close samples, z 1 and z 2 , are mapped onto the same mode M 2 . However, with the proposed regularization term, z 1 is mapped to I 1 , which belongs to an unexplored mode M 1 . With the adversarial mechanism, the generator will thus have better chances to generate samples of M 1 in the following training steps.</p><p>As shown in <ref type="figure">Figure 3</ref>, the proposed regularization term can be easily integrated with existing cGANs by appending it to the original objective function.</p><formula xml:id="formula_1">L new = L ori + λ ms L ms ,<label>(2)</label></formula><p>where L ori denotes the original objective function and λ ms the weights to control the importance of the regularization.</p><p>Here, L ori can be as a simple loss function. For example, in categorical generation task,</p><formula xml:id="formula_2">L ori = E c,y [log D(c, y)] + E c,z [log (1 − D(c, G(c, z)))],<label>(3)</label></formula><p>where c, y, z denote class labels, real images, and noise vectors, respectively. In image-to-image translation task <ref type="bibr" target="#b10">[11]</ref>,</p><formula xml:id="formula_3">L ori = L GAN + E x,y,z [ y − G(x, z) 1 ],<label>(4)</label></formula><p>where x denotes input images and L GAN is the typical GAN loss. L ori can be arbitrary complex objective function from any task, as shown in <ref type="figure">Figure 3</ref> (b). We name the proposed method as Mode Seeking GANs (MSGANs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed regularization method through extensive quantitative and qualitative evaluation. We ap-</p><formula xml:id="formula_4">(a) Proposed regularization Latent code ! " ! # $ " $ # StackGAN++ max ( ) (G(c, z " ), G(c, z # )) ( 0 (z " , z # ) 1 (b) Applying proposed regularization on StackGAN++ Figure 3: Proposed regularization. (a)</formula><p>We propose a regularization term that maximizes the ratio of the distance between generated images with respect to the distance between their corresponding input latent codes. (b) The proposed regularization method can be applied to arbitrary cGANs. Take StackGAN++ <ref type="bibr" target="#b31">[32]</ref>, a model for text-to-image synthesis, as an example, we easily apply the regularization term regardless of the complex tree-like structure of the original model.</p><p>ply MSGANs to the baseline models from three representative conditional image synthesis tasks: categorical generation, image-to-image translation, and text-to-image synthesis. Note that we augment the original objective functions with the proposed regularization term while maintaining original network architectures and hyper-parameters. We employ L 1 norm distance as our distance metrics for both d I and d z and set the hyper-parameter λ ms = 1 in all experiments. More implementation and evaluation details, please refer to the appendixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metrics</head><p>We conduct evaluations using the following metrics. FID. To evaluate the quality of the generated images, we use FID <ref type="bibr" target="#b8">[9]</ref> to measure the distance between the generated distribution and the real one through features extracted by Inception Network <ref type="bibr" target="#b27">[28]</ref>. Lower FID values indicate better quality of the generated images. LPIPS. To evaluate diversity, we employ LPIPS <ref type="bibr" target="#b32">[33]</ref> following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. LIPIS measures the average feature distances between generated samples. Higher LPIPS score indicates better diversity among the generated images. JS ↓ DCGAN 0.033 ± 0.001 0.034 ± 0.002 0.035 ± 0.001 0.029 ± 0.003 0.032 ± 0.001 MSGAN 0.024 ± 0.001 0.030 ± 0.002 0.033 ± 0.003 0.027 ± 0.001 0.029 ± 0.003 NDB and JSD. To measure the similarity between the distribution between real images and generated one, we adopt two bin-based metrics, NDB and JSD, proposed in <ref type="bibr" target="#b23">[24]</ref>. These metrics evaluate the extent of mode missing of generative models. Following <ref type="bibr" target="#b23">[24]</ref>, the training samples are first clustered using K-means into different bins which can be viewed as modes of the real data distribution. Then each generated sample is assigned to the bin of its nearest neighbor. We calculate the bin-proportions of the training samples and the synthesized samples to evaluate the difference between the generated distribution and the real data distribution. NDB score and JSD of the bin-proportion are then computed to measure the mode collapse. Lower NDB score and JSD mean the generated data distribution approaches the real data distribution better by fitting more modes. Please refer to <ref type="bibr" target="#b23">[24]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Conditioned on Class Label</head><p>We first validate the proposed method on categorical generation. In categorical generation, networks take class labels as conditional contexts to synthesize images of different categories. We apply the regularization term to the baseline framework DCGAN <ref type="bibr" target="#b21">[22]</ref>.</p><p>We conduct experiments on the CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> dataset which includes images of ten categories. Since images in the CIFAR-10 dataset are of size 32 × 32 and upsampling degrades the image quality, we do not compute LPIPS in this task. <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> present the results of NDB, JS, and FID. MSGAN mitigates the mode collapse issue in most classes while maintaining image quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Conditioned on Image</head><p>Image-to-image translation aims to learn the mapping between two visual domains. Conditioned on images from the source domain, models attempt to synthesize corresponding images in the target domain. Despite the multimodal nature of the image-to-image translation task, early work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref> abandons noise vectors and performs one-toone mapping since the latent codes are easily ignored during training as shown in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>. To achieve multimodality, several recent attempts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> introduce additional encoder networks and objective functions to impose a bijection constraint between the latent code space and the image space. To demonstrate the generalizability, we apply the proposed method to a unimodal model Pix2Pix <ref type="bibr" target="#b10">[11]</ref> using paired training data and a multimodal model DRIT <ref type="bibr" target="#b14">[15]</ref> using unpaired images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Conditioned on Paired Images</head><p>We take Pix2Pix as the baseline model. We also compare MSGAN to BicycleGAN <ref type="bibr" target="#b34">[35]</ref> which generates diverse im- ages with paired training images. For fair comparisons, architectures of the generator and the discriminator in all methods follow the ones in BicycleGAN <ref type="bibr" target="#b34">[35]</ref>.</p><p>We conduct experiments on the facades and maps datasets. MSGAN obtains consistent improvements on all   <ref type="figure" target="#fig_0">Figure. 4</ref> and <ref type="table">Table.</ref> 3 demonstrate the qualitative and quantitative results, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Conditioned on Unpaired Images</head><p>We choose DRIT <ref type="bibr" target="#b14">[15]</ref>, one of the state-of-the-art frameworks to generate diverse images with unpaired training data, as the baseline framework. Though DRIT synthesizes diverse images in most cases, mode collapse occurs in some challenging shape-variation cases (e.g., translation between cats and dogs). To demonstrate the robustness of the proposed method, we evaluate on the shape-preserving Yosemite (summer winter) <ref type="bibr" target="#b33">[34]</ref> dataset and the cat dog <ref type="bibr" target="#b14">[15]</ref> dataset that requires shape variations.</p><p>As the quantitative results exhibited in <ref type="table">Table.</ref> 4, MSGAN performs favorably against DRIT in all metrics on both datasets. Especially in the challenging cat dog dataset, MSGAN obtains substantial diversity gains. From the statistical point of view, we visualize the bin proportions of the dog-to-cat translation in <ref type="figure">Figure.</ref> 6. The graph shows the severe mode collapse issue of DRIT and the substantial improvement with the proposed regularization term. Qualitatively, <ref type="figure" target="#fig_1">Figure. 5</ref> shows that MSGAN discovers more modes without the loss of visual quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Conditioned on Text</head><p>Text-to-image synthesis targets at generating images conditioned on text descriptions. We integrate the proposed regularization term on StackGAN++ <ref type="bibr" target="#b31">[32]</ref> using the CUB-200-2011 <ref type="bibr" target="#b28">[29]</ref> dataset. To improve diversity, StackGAN++ introduces a Conditioning Augmentation (CA) module that re-parameterizes text descriptions into text codes of the Gaussian distribution. Instead of applying the regularization term on the semantically meaningful text codes, we focus on exploiting the latent codes randomly sampled from the prior distribution. However, for a fair comparison, we evaluation MSGAN against StackGAN++ in two settings: 1) Perform generation without fixing text codes for text descriptions. In this case, text codes also provide variations for output images. 2) Perform generation with fixed text codes. In this setting, the effects of text codes are excluded. <ref type="table">Table.</ref> 5 presents quantitative comparisons between MS-GAN and StackGAN++. MSGAN improves the diversity of StackGAN++ and maintains visual quality. To better illustrate the role that latent codes play for the diversity, we show qualitative comparisons with the text codes fixed. In this setting, we do not consider the diversity resulting from CA. <ref type="figure" target="#fig_3">Figure. 7</ref> illustrates that latent codes of StackGAN++ have minor effects on the variations of the image. On the contrary, latent codes of MSGAN contribute to various appearances and poses of birds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Interpolation of Latent Space in MSGANs</head><p>We perform linear interpolation between two given latent codes and generate corresponding images to have a better understanding of how well MSGANs exploit the latent space. <ref type="figure" target="#fig_4">Figure. 8</ref> shows the interpolation results on the dogto-cat translation and the text-to-image synthesis task. In the dog-to-cat translation, we can see the coat colors and patterns varies smoothly along with the latent vectors. In the text-to-image synthesis, both orientations of birds and the appearances of footholds change gradually with the variations of the latent codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we present a simple but effective mode seeking regularization term on the generator to address the model collapse in cGANs. By maximizing the distance between generated images with respect to that between the corresponding latent codes, the regularization term forces the generators to explore more minor modes. The proposed regularization method can be readily integrated with existing cGANs framework without imposing training overheads and modifications of network structures. We demonstrate the generalizability of the proposed method on three different conditional generation tasks including categorical generation, image-to-image translation, and text-to-image synthesis. Both qualitative and quantitative results show that the proposed regularization term facilitates the baseline frameworks improving the diversity without sacrificing visual quality of the generated images.</p><p>Appendix A. Implementation Details <ref type="table">Table 9</ref> summarizes the datasets and baseline models used on various tasks. For all of the baseline methods, we incorporate the original objective functions with the proposed regularization term. Note that we remain the original network architecture design and use the default setting of hyper-parameters for the training.</p><p>DCGAN. Since the images in the CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> dataset are of size 32 × 32, we modify the structure of the generator and discriminator in DCGAN <ref type="bibr" target="#b21">[22]</ref>, as shown in <ref type="table" target="#tab_1">Table 10</ref>. We use the batch size of 32, learning rate of 0.0002 and Adam <ref type="bibr" target="#b11">[12]</ref> optimizer with β 1 = 0.5 and β 2 = 0.999 to train both the baseline and MSGAN network.</p><p>Pix2Pix. We adopt the generator and discriminator in Bi-cycleGAN <ref type="bibr" target="#b33">[34]</ref> to build the Pix2Pix <ref type="bibr" target="#b10">[11]</ref> model. Same as BicycleGAN, we use a U-Net network <ref type="bibr" target="#b24">[25]</ref> for the generator, and inject the latent codes z into every layer of the generator. The architecture of the discriminator is a two-scale PatchGAN network <ref type="bibr" target="#b10">[11]</ref>. For the training, both Pix2Pix and MSGAN framework use the same hyper-parameters as the officially released version 1 .</p><p>DRIT. DRIT <ref type="bibr" target="#b14">[15]</ref> involves two stages of image-to-image translations in the training process. We only apply the mode seeking regularization term to generators in the first stage, which is modified on the officially released code 2 . <ref type="bibr" target="#b31">[32]</ref> is a tree-like structure with multiple generators and discriminators. We use the output images from the last generator and input latent codes to calculate the mode seeking regularization term. The implementation is based on the officially released code 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StackGAN++. StackGAN++</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Evaluation Details</head><p>We employ the official implementation of FID 4 , NDB and JSD 5 , and LPIPS 6 . For NDB and JSD, we use the K-means method on training samples to obtain the clusters. Then the generated samples are assigned to the nearest cluster to compute the bin proportions. As suggested by the author of <ref type="bibr" target="#b23">[24]</ref>, there are at least 10 training samples for each cluster. Therefore, we cluster the number of bins K ≈ N train /20 in all tasks, where N train denotes the number of training samples for computing the clusters. We have verified that the performance is consistent within a large range of K. For evaluation, we randomly generate N images for a given conditional context on various tasks. We conduct five independent trials and report the mean and standard derivation based on the result of each trial. More evaluation details of one trial are presented as follows.</p><p>Conditioned on Class Label. We randomly generate N = 5000 images for each class label. We use all the training samples and the generated samples to compute FID. For NDB and JSD, we employ the training samples in each class to calculate K = 250 clusters.</p><p>Conditioned on Image. We randomly generate N = 50 images for each input image in the test set. For LPIPS, we randomly select 50 pairs of the 50 images of each context in the test set to compute LPIPS and average all the values for this trial. Then, we randomly choose 100 input images and their corresponding generated images to form 5000 generated samples. We use the 5000 generated samples and all samples in training set to compute FID. For NDB and JSD, we employ all the training samples for clustering and choose K = 20 bins for facades, and K = 50 bins for other datasets.</p><p>Conditioned on Text. We randomly select 200 sentences and generate N = 10 images for each sentence, which forms 2000 generated samples. Then, we randomly select N train = 2000 samples for computing FID, and clustering them into K = 100 bins for NDB and JSD. For LPIPS, we randomly choose 10 pairs for each sentence and average the values of all the pairs for this trial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Ablation Study on the Regularization Term</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. The Weighting Parameter λ ms</head><p>To analyze the influence of the regularization term, we conduct an ablation study by varying the weighting parameter λ ms on image-to-image translation task using the facades dataset. <ref type="table">Table.</ref> 6 presents the quantitative results with diverse λ ms . It can be observed that increasing λ ms improves the diversity of the generated images. Nevertheless, as the weighting parameter λ ms becomes larger than a threshold value (1.0), the training becomes unstable, which yields low quality, and even low diversity synthesized images. As a result, we empirically set the weighting parameter λ ms = 1.0 for all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. The Design Choice of the Distance Metric</head><p>We have explored other design choice of the distance metric. We conduct experiments using discriminator feature distance in our regularization term in a way similar to feature matching loss <ref type="bibr" target="#b29">[30]</ref>,</p><formula xml:id="formula_5">L ms = 1 L L l=1 D l (G(c, z 2 )) − D l (G(c, z 1 )) 1 z 2 − z 1 1 ,<label>(5)</label></formula><p>where D l denotes the l th layer of the discriminator. We apply it to Pix2Pix on the facades dataset. <ref type="table" target="#tab_7">Table. 7</ref> shows that MSGAN using feature distance also obtains improvement over Pix2Pix. However, MSGAN using L 1 distance has higher diversity. Therefore, we employ MSGAN using L 1 distance for all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Computational Overheads</head><p>We compare MSGAN with Pix2Pix, BicycleGAN in terms of training time, memory consumption, and model parameters on an NVIDIA TITAN X GPU. <ref type="table" target="#tab_8">Table. 8</ref> shows that our method incurs marginal overheads. However, Bicy-cleGAN requires longer time per iteration and larger memory with an additional encoder and another discriminator network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Additional Results</head><p>We present more results of categorical generation, image-to-image translation, and text-to-image synthesis in <ref type="figure">Figure 10</ref>, <ref type="figure">Figure 11</ref>, <ref type="figure">Figure 12</ref>, <ref type="figure">Figure 13</ref>, <ref type="figure" target="#fig_0">Figure 14</ref>, and <ref type="figure" target="#fig_1">Figure 15</ref>, respectively. <ref type="table">Table 9</ref>: Statistics of different generation tasks. We summarize the number of training and testing images in each generation task. The baseline model used for each task is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Class Label Paired Images Unpaired Images Text Dataset CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> Facades <ref type="bibr" target="#b2">[3]</ref> Maps <ref type="bibr" target="#b10">[11]</ref> Yosemite <ref type="bibr" target="#b33">[34]</ref> Cat Dog <ref type="bibr" target="#b14">[15]</ref> CUB-200-2011 <ref type="bibr">[</ref>  <ref type="table" target="#tab_1">Table 10</ref>: The architecture of the generator and discriminator of DCGAN. We employ the following abbreviation: N= Number of filters, K= Kernel size, S= Stride size, P= Padding size. "Conv", "Dconv","BN" denote the convolutional layer, transposed convolutional layer and batch normalization, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Generator Discriminator</head><p>1 Dconv(N512-K4-S1-P0), BN, Relu Conv(N128-K4-S2-P1), Leaky-Relu 2</p><p>Dconv(N256-K4-S2-P1), BN, Relu Conv(N256-K4-S2-P1), BN, Leaky-Relu 3</p><p>Dconv(N128-K4-S2-P1), BN, Relu Conv(N512-K4-S2-P1), BN, Leaky-Relu 4</p><p>Dconv(N3-K4-S2-P1), Tanh Conv(N1-K4-S1-P0), Sigmoid airplane automobile dog horse ship truck Input Generated images <ref type="figure">Figure 9</ref>: More categorical generation results of CIFAR-10. We show the results of DCGAN <ref type="bibr" target="#b21">[22]</ref> with the proposed mode seeking regularization term on categorical generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Generated images <ref type="figure">Figure 10</ref>: More image-to-image translation results of facades and maps. Top three rows: facades, bottom three rows: maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated images</head><p>This chubby bird has a small fat bill and a red belly.</p><p>A small bird with a blue nape, white breast and body with a short bill.</p><p>This beautiful bird has round black eyes and a striking yellow underbelly and brown plumage. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Diversity comparison. The proposed regularization term helps Pix2Pix learn more diverse results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Diversity comparison. We compare MSGAN with DRIT on the dog-to-cat, cat-to-dog, and winter-to-summer translation tasks. Our model produces more diverse samples over DRIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the bins on dog→cat translation. The translated results of DRIT collapse into few modes, while the generated image of MSGAN fit the real data distribution better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Diversity comparison. We show examples of StackGAN++<ref type="bibr" target="#b31">[32]</ref> and MSGAN on the CUB-200-2011 dataset of text-to-image synthesis. When the text code is fixed, the latent codes in MSGAN help to generate more diverse appearances and poses of birds as well as different backgrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Linear interpolation between two latent codes in MSGAN. Image synthesis results with linear-interpolation between two latent codes in the dog-to-cat translation and text-to-image synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 15 :</head><label>15</label><figDesc>More text-to-image synthesis results of CUB-200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>NDB and JSD results on the CIFAR-10 dataset. DCGAN 49.60 ± 3.43 53.00 ± 7.28 34.40 ± 6.11 46.00 ± 1.41 44.80 ± 3.90 MSGAN 46.60 ± 7.40 51.80 ± 2.28 39.40 ± 1.95 41.80 ± 3.70 46.80 ± 4.92</figDesc><table><row><cell>Metrics</cell><cell>Models</cell><cell>airplane</cell><cell>automobile</cell><cell>bird</cell><cell>cat</cell><cell>deer</cell></row><row><cell>NDB ↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JS ↓</cell><cell cols="5">DCGAN MSGAN 0.031 ± 0.001 0.033 ± 0.001 0.027 ± 0.001 0.034 ± 0.001 0.035 ± 0.002 0.025 ± 0.002 0.030 ± 0.002 0.027 ± 0.001</cell><cell>0.033 ± 0.001 0.035 ± 0.003</cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell>frog</cell><cell>horse</cell><cell>ship</cell><cell>truck</cell></row><row><cell>NDB ↓</cell><cell cols="2">DCGAN MSGAN 33.80 ± 3.27 50.40 ± 4.62</cell><cell>52.00 ± 3.81 42.00 ± 2.92</cell><cell>54.40 ± 4.04 47.60 ± 5.03</cell><cell>42.80 ± 5.45 41.00 ± 2.92</cell><cell>47.80 ± 4.55 43.80 ± 6.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID results on the CIFAR-10 dataset.</figDesc><table><row><cell>Model</cell><cell>DCGAN</cell><cell>MSGAN</cell></row></table><note>FID↓ 29.65 ± 0.06 28.73 ± 0.06</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on the facades and maps dataset.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>Facades</cell><cell></cell></row><row><cell></cell><cell>Pix2Pix [11]</cell><cell>MSGAN</cell><cell>BicycleGAN [35]</cell></row><row><cell>FID ↓</cell><cell>139.19 ± 2.94</cell><cell>92.84 ± 1.00</cell><cell>98.85 ± 1.21</cell></row><row><cell>NDB↓</cell><cell>14.40 ± 1.82</cell><cell>12.40 ± 0.55</cell><cell>13.80 ± 0.45</cell></row><row><cell>JSD↓</cell><cell>0.074 ± 0.012</cell><cell>0.038 ± 0.004</cell><cell>0.058 ± 0.004</cell></row><row><cell cols="4">LPIPS↑ 0.0003 ± 0.0000 0.1894 ± 0.0011 0.1413 ± 0.0005</cell></row><row><cell>Datasets</cell><cell></cell><cell>Maps</cell><cell></cell></row><row><cell></cell><cell>Pix2Pix [11]</cell><cell>MSGAN</cell><cell>BicycleGAN [35]</cell></row><row><cell>FID ↓</cell><cell>168.99 ± 2.58</cell><cell>152.43 ± 2.52</cell><cell>145.78 ± 3.90</cell></row><row><cell>NDB↓</cell><cell>49.00 ± 1.00</cell><cell>41.60 ± 0.55</cell><cell>46.60 ± 1.34</cell></row><row><cell>JSD↓</cell><cell>0.088 ± 0.018</cell><cell>0.031 ± 0.003</cell><cell>0.023 ± 0.002</cell></row><row><cell cols="4">LPIPS↑ 0.0016 ± 0.0003 0.2189 ± 0.0004 0.1150 ± 0.0007</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results of the Yosemite (Summer Winter) and the Cat Dog dataset. LPIPS↑ 0.1150 ± 0.0003 0.1468 ± 0.0005 0.0965 ± 0.0004 0.1183 ± 0.0007</figDesc><table><row><cell>Datasets</cell><cell cols="2">Summer2Winter</cell><cell cols="2">Winter2Summer</cell></row><row><cell></cell><cell>DRIT [15]</cell><cell>MSGAN</cell><cell>DRIT [15]</cell><cell>MSGAN</cell></row><row><cell>FID ↓</cell><cell>57.24 ± 2.03</cell><cell>51.85 ± 1.16</cell><cell>47.37 ± 3.25</cell><cell>46.23 ± 2.45</cell></row><row><cell>NDB↓</cell><cell>25.60 ± 1.14</cell><cell>22.80 ± 2.96</cell><cell>30.60 ± 2.97</cell><cell>27.80 ± 3.03</cell></row><row><cell>JSD↓</cell><cell>0.066 ± 0.005</cell><cell>0.046 ± 0.006</cell><cell>0.049 ± 0.009</cell><cell>0.038 ± 0.004</cell></row><row><cell>Datasets</cell><cell cols="2">Cat2Dog</cell><cell cols="2">Dog2Cat</cell></row><row><cell></cell><cell>DRIT [15]</cell><cell>MSGAN</cell><cell>DRIT [15]</cell><cell>MSGAN</cell></row><row><cell>FID↓</cell><cell>22.74 ± 0.28</cell><cell>16.02 ± 0.30</cell><cell>62.85 ± 0.21</cell><cell>29.57 ± 0.23</cell></row><row><cell>NDB↓</cell><cell>42.00 ± 2.12</cell><cell>27.20 ± 0.84</cell><cell>41.00 ± 0.71</cell><cell>31.00 ± 0.71</cell></row><row><cell>JSD↓</cell><cell>0.127 ± 0.003</cell><cell>0.084 ± 0.002</cell><cell>0.272 ± 0.002</cell><cell>0.068 ± 0.001</cell></row><row><cell>LPIPS↑</cell><cell>0.245 ± 0.002</cell><cell>0.280 ± 0.002</cell><cell>0.102 ± 0.001</cell><cell>0.214 ± 0.001</cell></row><row><cell cols="3">metrics over Pix2Pix. Moreover, MSGAN demonstrates</cell><cell></cell><cell></cell></row><row><cell cols="3">comparable diversity to BicycleGAN, which applies an ad-</cell><cell></cell><cell></cell></row><row><cell>ditional encoder network.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on the CUB-200-2011 dataset. We conduct experiments in two settings: 1) Conditioned on text descriptions, where every description can be mapped to different text codes. 2) Conditioned on text codes, where the text codes are fixed so that their effects are excluded.</figDesc><table><row><cell></cell><cell cols="2">Conditioned on text descriptions</cell><cell cols="2">Conditioned on text codes</cell></row><row><cell></cell><cell>StackGAN++ [32]</cell><cell>MSGAN</cell><cell>StackGAN++ [32]</cell><cell>MSGAN</cell></row><row><cell>FID ↓</cell><cell>25.99 ± 4.26</cell><cell>25.53 ± 1.83</cell><cell>27.12 ± 1.15</cell><cell>27.94 ± 3.10</cell></row><row><cell>NDB↓</cell><cell>38.20 ± 2.39</cell><cell>30.60 ± 2.51</cell><cell>39.00 ± 0.71</cell><cell>30.60 ± 2.41</cell></row><row><cell>JSD↓</cell><cell>0.092 ± 0.005</cell><cell>0.073 ± 0.003</cell><cell>0.102 ± 0.016</cell><cell>0.095 ± 0.016</cell></row><row><cell>LPIPS↑</cell><cell>0.362 ± 0.004</cell><cell>0.373 ± 0.007</cell><cell>0.156 ± 0.004</cell><cell>0.207 ± 0.005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results with different λ ms on the facades dataset.</figDesc><table><row><cell>λ ms</cell><cell>0</cell><cell>0.1</cell><cell>0.5</cell><cell>1</cell><cell>3</cell></row><row><cell cols="6">FID ↓ 134.95 126.31 88.41 92.45 338.45</cell></row><row><cell cols="6">LPIPS↑ 0.0003 0.0155 0.0929 0.1888 0.1393</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results on the facades dataset. 139.19 ± 2.94 92.84 ± 1.00 100.16 ± 3.14 NDB↓ 14.40 ± 1.82 12.40 ± 0.55 11.80 ± 1.48 JSD↓ 0.074 ± 0.012 0.038 ± 0.004 0.072 ± 0.014 LPIPS↑ 0.0003 ± 0.0000 0.1894 ± 0.0011 0.0565 ± 0.0003</figDesc><table><row><cell>Pix2Pix [11]</cell><cell>MSGAN-L1</cell><cell>MSGAN-FD</cell></row><row><cell>FID ↓</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of computational overheads on the facades dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">Time (s) Memory (MB) Parameters (M)</cell></row><row><cell>Pix2Pix [11]</cell><cell>0.122</cell><cell>1738</cell><cell>58.254</cell></row><row><cell>MSGAN</cell><cell>0.122</cell><cell>1739</cell><cell>58.254</cell></row><row><cell>BicycleGAN [35]</cell><cell>0.192</cell><cell>2083</cell><cell>64.303</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/junyanz/BicycleGAN/ 2 https://github.com/HsinYingLee/DRIT 3 https://github.com/hanzhanggit/StackGAN-v2 4 https://github.com/bioinf-jku/TTUR 5 https://github.com/eitanrich/gans-n-gmms 6 https://github.com/richzhang/ PerceptualSimilarity</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative multiadversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-agent diverse generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smolley. Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On GANs and GMMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VEEGAN: Reducing mode collapse in GANs using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Diversitysensitive conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan++</surname></persName>
		</author>
		<title level="m">Realistic image synthesis with stacked generative adversarial networks. TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Toward multimodal imageto-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
