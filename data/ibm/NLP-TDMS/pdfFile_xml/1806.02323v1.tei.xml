<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Accurate Online Video Object Segmentation via Tracking Parts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Accurate Online Video Object Segmentation via Tracking Parts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first utilize a partbased tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background. Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks. Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame. Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation aims at separating target objects from the background and other instances on the pixel level. Segmenting objects in videos is a fundamental task in computer vision because of its wide applications such as video surveillance, video editing, and autonomous driving. However, it is a challenging task due to camera motion, object deformation, occlusion between instances and cluttered background. Particularly for online applications, significant different issues arise when the methods are required to be robust and fast without given access to future frames. In this paper, we focus on solving the problem of online video object segmentation. Given the object in the first frame, our goal is to immediately perform online segmentation on this target object without knowing future frames. For real application usages, the difficulties lie in the requirement of efficient runtime performance while maintaining accurate seg- <ref type="bibr">Figure 1</ref>. Accuracy versus runtime comparisons on the DAVIS 2016 dataset. We evaluate the state-of-the-art methods and demonstrate that our approach is significantly faster, while maintaining high accuracy. Note that the runtime includes the pre-processing steps averaged on all frames for fair comparisons. mentation. <ref type="figure">Figure 1</ref> illustrates comparisons of the state-ofthe-art methods in terms of speed and performance, where we show that the proposed algorithm is fast, accurate and applicable to online tasks.</p><p>Existing video object segmentation algorithms can be broadly classified into unsupervised and semi-supervised settings. Unsupervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref> mainly segment moving objects from the background without any prior knowledge of the target, e.g., initial object masks. However, these methods cannot handle multiple object segmentation as they are not capable of identifying a specific instance. In addition, several methods require batch model processing (i.e., all the frames are available) before segmenting the object <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>, which cannot be applied to online applications. On the other hand, semi-supervised methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">44]</ref> are given with an initial object mask which provides critical visual cues of the target. Thus, these methods can handle multi-instance cases and usually perform better than the unsupervised approaches. However, many state-of-the-art semi-supervised methods heavily rely on the segmentation mask in the first frame. For instance, before making predictions on the test video, the state-of-the-art methods need to finetune the networks for each video <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>, or the model for each instance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>. This finetuning step on the video or instance level is computationally expensive, where it usually takes more than ten minutes to update a model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. In ad- <ref type="figure">Figure 2</ref>. Proposed framework for online video object segmentation. Our algorithm first generates parts of the target object in the first frame. These parts are then tracked in the next frame to obtain tracking boxes. With our ROI segmentation network and a similarity-based scoring function, final segmentation outputs are generated through the entire video. dition, data preparation (e.g., optical flow generation <ref type="bibr" target="#b41">[42]</ref>) and training data augmentation <ref type="bibr" target="#b18">[19]</ref> require additional processing time. As such, these methods cannot be used for time-sensitive online applications that require fast and accurate segmentation results of a specific target object (see <ref type="figure">Figure 1</ref>).</p><p>In this paper, we propose a video object segmentation algorithm that can immediately start to segment a specific object through the entire video fast and accurately. To this end, we utilize a part-based tracking method and exploit a convolutional neural network (CNN) for representations but does not need the time-consuming finetuning stage on the target video. The proposed method mainly consists of three parts: part-based tracking, region-of-interest segmentation, and similarity-based aggregation.</p><p>Part-based Tracking. Naturally, object tracking is an effective way to localize the target in the next frame. However, non-rigid objects often have large deformation with fast movement, thereby making it difficult to accurately localize the target <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. To better utilize the tracking cues, we adopt a part-based tracking scheme to resolve challenging issues such as occlusions and appearance changes <ref type="bibr" target="#b26">[27]</ref>. We first randomly generate object proposals around the target in the first frame, and select representative parts based on the overlapping scores with the initial mask. We then apply the tracker for each part to provide temporally consistent region of interests (ROIs) for subsequent frames.</p><p>ROI Segmentation. Once each part is localized in the next frame, we construct a CNN-based ROI SegNet to predict the segmentation mask that belongs to the target object. Different from conventional foreground segmentation networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref> that focus on segmenting the entire object, our ROI SegNet learns to segment partial objects given the bounding box of part.</p><p>Similarity-based Aggregation. With part tracking and ROI segmentation, the object location and segmentation mask can be roughly identified. However, there could be false positives due to incorrect tracking results. To reduce noisy segmentation parts, we design a similarity-based method to aggregate parts by computing the feature distance between tracked parts and the initial object mask. <ref type="figure">Figure 2</ref> shows the main steps of the proposed algorithm.</p><p>To validate the proposed algorithm, we conduct extensive experiments with comparisons and ablation study on the DAVIS benchmark datasets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>. We show that the proposed method performs favorably against state-ofthe-art approaches in accuracy, while achieving much better runtime performance. The contributions of this work are as the following. First, we propose a fast and accurate video object segmentation method that is applicable to online tasks. Second, we develop the part-based tracking and similarity-based aggregation methods that effectively utilize the information contained in the first frame, without adding much computational load. Third, we design an ROI SegNet that takes bounding boxes of parts as the input, and outputs the segmentation mask for each part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Video Object Segmentation. Unsupervised video object segmentation methods aim to automatically discover and separate prominent objects from the background. These methods are based on probabilistic models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>, motions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35]</ref>, and object proposals <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>. Existing approaches often rely on visual cues such as superpixels, saliency maps or optical flow to obtain initial object regions, and need to process the entire video in batch mode for refining object segmentation. In addition, generating and processing thousands of candidate regions in each frame is usually time-consuming. Recently, CNN-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> exploit learning rich hierarchical features (e.g., ImageNet pre-training) and large augmented data to achieve the state-of-the-art segmentation results. However, these unsupervised methods are not able to segment a specific object due to motion confusions between different instances and dynamic background.</p><p>Semi-supervised Video Object Segmentation. Semisupervised methods aim to segment a specific object with an initial mask. Numerous algorithms have been proposed based on tracking <ref type="bibr" target="#b9">[10]</ref>, object proposals <ref type="bibr" target="#b36">[37]</ref>, graphical model <ref type="bibr" target="#b31">[32]</ref>, and optical flow <ref type="bibr" target="#b41">[42]</ref>. Similar to the unsupervised approaches, CNN-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> have achieved significant improvement for video object segmentation. However, these methods usually heavily rely on finetuning models through the first frame <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>, data augmentation <ref type="bibr" target="#b18">[19]</ref>, online model adaptation <ref type="bibr" target="#b43">[44]</ref> and joint training with optical flow <ref type="bibr" target="#b5">[6]</ref>. These steps are computationally expensive (e.g., it takes more than 10 minutes for finetuning on the first frame in each video) and are not suitable for online vision applications.</p><p>To alleviate the issue of computational loads, a few methods are developed by propagating the object mask in the first frame through the entire video <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Without exploiting much information in the first frame, these approaches suffer from the error accumulation after propagating a long period of time and thus do not perform as well as other methods. In contrast, the proposed algorithm incorporates partbased tracking and always keeps eyes on the first frame by a similarity-based part aggregation strategy.</p><p>Object Tracking. Tracking has been widely used to localize objects in videos as an additional cue for performing object segmentation <ref type="bibr" target="#b42">[43]</ref>. Conventional methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> adopt correlation filter-based trackers to account for appearance changes. Recently, numerous methods have been developed based on deep neural networks and classifiers. The CF2 method <ref type="bibr" target="#b29">[30]</ref> learns correlation filters adaptively based on CNN features, thereby enhancing the ability to handle challenging factors such as deformation and occlusion. In addition, the SINT scheme <ref type="bibr" target="#b38">[39]</ref> utilizes a Siamese network to learn feature similarities between proposals and the initial observation of target object. The SiaFC algorithm <ref type="bibr" target="#b1">[2]</ref> develops an end-to-end Siamese tracking network with fullyconvolutional layers, which allows the tracker to compute similarity scores for all the proposals in one forward pass. In this work, we adopt the Siamese network for tracking object parts, where each part is locally representative and endures less deformation through the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>In this section, we describe each component of the proposed method. First, we present the part-based tracker, where the goal is to localize object parts through the entire video. Second, we construct the ROI SegNet, a general and robust network to predict segmentation results for object parts. Third, we introduce our part aggregation method to generate final segmentation results by computing similarity scores in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Part-based Tracking</head><p>Object tracking is a difficult task due to challenging factors such as object deformation, fast movement, occlusion, and background noise. To deal with these issues, part-based methods <ref type="bibr" target="#b26">[27]</ref> have been developed to track local regions instead of the entire object with larger appearance changes. Since our goal is to localize most object regions in the next frame for further segmentation, utilizing a part-based method matches our need and can effectively maintain a high recall rate.</p><p>Part Generation. In order to track parts, one critical problem is how to generate these parts in the first place. Conventional object parts are discovered from a large amount of intra-class data via discriminability and consistency. However, this assumption does not hold for online video segmentation, as only one object mask is provided in the first frame of the target video. To resolve this issue, we propose a simple yet effective way to generate representative parts guided by the object mask. First, we randomly generate part proposals with various sizes and locations around the object, and remove the ones with low overlapping ratio to the object mask. We compute the intersection-over-union (IoU) score between the proposal and the object, and keep the ones with scores larger than a threshold (i.e., 0.3 in this work). To ensure that each part contains mostly pixels from the object, we further measure the score: S p = bbox∩gtbox bbox , where bbox is the bounding box of a proposal and gtbox is the known object box in the first frame. Part proposals with S p &gt; 0.7 are used as candidates for a non-maximum suppression (NMS) step. Based on the proposed selection process, we reduce thousands of proposals to only 50 ∼ 300 representative parts depending on the object size. Note that, we also transform the bounding box for each part to be tight within the object mask, reducing background noise for more effective tracking and segmentation. Some example results are shown in <ref type="figure">Figure 3</ref> for generated parts (with high scores) in the first frame. <ref type="figure">Figure 3</ref>. Sample results for part tracking. We show some high-scored parts and their tracking results. Green and yellow boxes are the results by applying object tracker <ref type="bibr" target="#b1">[2]</ref> and by our method via aggregating parts, respectively. It shows that our result (yellow boxes) are robust to object deformation and occlusion, due to the stability of tracking parts. <ref type="figure">Figure 4</ref>. Illustration of the proposed ROI SegNet. Given an image and their parts, we resize and align each part as the input to the network. We use the ResNet-101 architecture containing 5 convolution modules. We up-sample and concatenate feature maps from the last three modules. An additional convolution layer is utilized for the binary prediction of parts.</p><p>where T is a function to compute similarity scores between the part P i t and the image I t+1 . We use the SiaFC method <ref type="bibr" target="#b1">[2]</ref> as our baseline tracker T to compute the score map S t . Due to its fully-convolutional architecture, we compute score maps for multiple parts in one forward pass. Once obtaining the score map, we select the bounding box with the largest response as the tracking result. Some tracking results are shown in <ref type="figure">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ROI SegNet</head><p>Based on the tracking results of object parts, the next task is to segment partial object within the bounding box. Recent instance-level segmentation methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref> have demonstrated the state-of-the-art results by training networks for certain categories and output their segmentations. Our part segmentation problem is similar to the instance-level segmentation task but for the partial object. In addition, training such a network would require an alignment step for different parts as they may vary significantly in size, shape, and appearance for different instances or object categories. Hence, we utilize an ROI data layer by cropping image patches from parts as inputs to the network, in which these patches are aligned through resizing. Similar to semantic segmentation, our objective is to minimize the weighted cross-entropy loss for a binary (foreground/background) task:</p><formula xml:id="formula_0">L(P ) = −(1 − w) i,j∈f g log E(y ij = 1; θ) −w i,j∈bg log E(y ij = 0; θ),<label>(2)</label></formula><p>where θ denotes CNN parameters, y ij denotes the network prediction for the input part P at pixel (i, j) and w is the foreground-background pixel-number ratio used to balance the weights <ref type="bibr" target="#b44">[45]</ref>.</p><p>Network Architecture. We utilize the ResNet-101 architecture <ref type="bibr" target="#b11">[12]</ref> as the base network for segmentation and transform it to fully-convolutional layers <ref type="bibr" target="#b28">[29]</ref>. To enhance feature representations, we up-sample feature maps from the last three convolution modules and concatenate them together. The concatenated features are then followed by a convolution layer for the binary prediction. <ref type="figure">Figure 4</ref> shows the architecture of our ROI SegNet.</p><p>Network Training. To train the proposed network, we first augment images from the training set of the DAVIS dataset <ref type="bibr" target="#b35">[36]</ref> via random scaling and affine transformations (i.e., flipping, ±10% shifting, ±10% scaling, ±30 • rotation). Then, parts are extracted for each instance as the same method as introduced in part-based tracking. We use the Stochastic Gradient Descent (SGD) optimizer with the patch size 80 × 80 and the batch size of 100 for training. The initial learning rate starts from 10 −6 and decreases by half for every 50,000 iterations. We train the network for 200,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Similarity-based Part Aggregation</head><p>After obtaining all the segmentation results from parts, one simple way to generate the final segmentation is to compute an averaging score map from each part. However, parts may be tracked off the object or include background noise, resulting in inaccurate part segments. To avoid adding these false positives, we develop a scoring function by looking back to the initial object mask. That is, we seek to know if the current part is similar to any of the parts in the first frame. Although objects may appear quite differently from the first frame, we find that local parts are actually more robust to such appearance changes.</p><p>Specifically, we first compute the similarity score between each part in P t at frame t and initial parts P 0 in the feature space. Then we select part P n 0 with the highest similarity for the current part P m t by:</p><formula xml:id="formula_1">n = argmin i∈N f (P m t ) − f (P i 0 )) 2 2 ,<label>(3)</label></formula><p>where f is the feature vector representing each part, extracted from the last layer in our ROI SegNet with an average pooling on the part mask. Overall, our scoring function consists of three components:</p><p>S seg (P t ) = S ave (P t ) · S sim (P t , P n 0 ) · S con (P n 0 ), <ref type="bibr" target="#b3">(4)</ref> where P n 0 is a set of initial parts selected based on Equation (3) and · is the element-wise multiplication operation. The first function S ave is the simple averaging score of part segments in the current frame t:</p><formula xml:id="formula_2">S ave (P t ) = i∈Pt S i /|P t |,<label>(5)</label></formula><p>where P t is the set of parts at frame t and S i is the segmentation score for each part i. Second, S sim is the similarity score between current and initial parts in the feature space based on (3). Since the selected initial part segment may have poor quality, we add S con by forwarding P n 0 to the ROI SegNet and measuring its segmentation overlapping ratio to the initial mask as the confidence score:</p><formula xml:id="formula_3">S con (P n 0 ) = J(G(P n 0 ), gt),<label>(6)</label></formula><p>where J is the IoU measurement, G is the ROI SegNet and gt is the object mask in the first frame. With the guidance <ref type="figure">Figure 5</ref>. Part aggregation results. We compare score maps via the functions of Save and Sseg. Without computing the similarity score to the first frame, the result of Save contains noisy segments, while our aggregation algorithm performs segmentation more precisely. of the initial object mask and parts without using expensive model finetuning step, our part aggregation method can effectively remove false positives. <ref type="figure">Figure 5</ref> shows some examples of score maps with different scoring functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metrics</head><p>We conduct experiments on the DAVIS benchmark datasets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref> which contain high-quality videos with dense pixel-level object segmentation annotations. The DAVIS 2016 dataset consists of 50 sequences (30 for training and 20 for validation), with 3,455 annotated frames of real-world moving objects. Each video in the DAVIS 2016 dataset contains a single annotated foreground object, so both semi-supervised and unsupervised methods can be evaluated. The DAVIS 2017 dataset contains 150 videos with 10,459 annotated frames and 376 object instances. It is a challenging dataset as there are multiple instances in each video, where objects could occlude each other. In this setting, it is difficult for unsupervised methods to separate different instances. For performance evaluation, we use the mean region similarity (J mean), contour accuracy (F mean) and temporal stability (T mean) as in the benchmark setting <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref>. The source code <ref type="table">Table 1</ref>. Ablation study on DAVIS 2016. "+ Sseg" and "+ Sseg + Tracker + CRF" denote results for Ours-part and Ours-ref in <ref type="figure">Figure 1</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tracker Evaluation</head><p>Our part-based tracker focuses on tracking local regions and cannot directly output the object location in the next frame. However, we can roughly find the object center based on the aggregated part segments. Motivated by the tracking-by-detection algorithms <ref type="bibr" target="#b0">[1]</ref>, we utilize detection proposals <ref type="bibr" target="#b27">[28]</ref> as candidates of object bounding boxes, and select the one closest to the object center as the tracking result. We then validate this part-based tracker on the DAVIS 2016 dataset with comparisons to our baseline SiaFC method <ref type="bibr" target="#b1">[2]</ref> and other tracking algorithms including CF2 <ref type="bibr" target="#b12">[13]</ref>, ECO <ref type="bibr" target="#b7">[8]</ref>, and MDNet <ref type="bibr" target="#b32">[33]</ref>. Experimental results are presented in <ref type="figure">Figure 3</ref> and 6, where we show that our part-based trackers consistently maintain better IoU-recall curves for localizing objects.</p><p>Although our ultimate goal is for video object segmentation, this evaluation is useful for understanding the challenges on the DAVIS dataset. One interesting fact is that if there is a good tracker, it should be able to help the segmentation task. Thus, a high recall rate under a high IoU is required as once partial object is missing, it is not possible to recover the corresponding segment. As shown in <ref type="figure" target="#fig_1">Figure  6</ref>, most trackers achieve around 60% recall rate under a 0.5 IoU while ours is 80%, which enables potential usages of applying our tracker to improve segmentation results. We will present our results by integrating this tracker in the ablation study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study on Segmentation</head><p>We present ablation study in <ref type="table">Table 1</ref> on the DAVIS 2016 validation set to evaluate the effectiveness of each component in the proposed video object segmentation framework. We start with the unsupervised version of SFL <ref type="bibr" target="#b5">[6]</ref> as our baseline due to its balance between speed and accuracy. To demonstrate the usefulness of using part, we first conduct an experiment by combining the baseline result and the score map from <ref type="bibr" target="#b1">[2]</ref> via tracking an entire object. Specifically, we average the foreground probability from <ref type="bibr" target="#b5">[6]</ref> and the segmentation map of <ref type="bibr" target="#b1">[2]</ref> through the ROI SegNet. However, we find that the tracking accuracy is highly unstable, which usually loses objects and even results in a worse performance than the baseline segmentation (1.1% drop in J Mean). It shows that combining tracking and segmentation is not a trivial task, and we use part-based model to achieve a better combination.</p><p>After adopting our part-based tracker and ROI SegNet to obtain part segments, we compare results with or without part aggregation. The one that utilizes part aggregation via the function S seg in Equation (4) performs better (4% improvement in J Mean) than only computing the score function S ave . It shows that with the consideration of initial object mask, false part segmentations can be largely reduced as they are not similar to any of the object parts in the first frame. In addition, we take advantage of our tracker combined with detection proposals as mentioned in Section 4.2 and use it to further improve our results, denoted as "+Tracker" in <ref type="table">Table 1</ref>. To further improve the boundary accuracy, we add a refinement step using dense CRF <ref type="bibr" target="#b21">[22]</ref>. In <ref type="figure">Figure 1</ref>, we denote the result of using S seg as Ourspart, and the one combined with our tracker and CRF with refinement as Ours-ref.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Segmentation Results</head><p>DAVIS 2016. We evaluate our proposed method on the validation set of DAVIS 2016 <ref type="bibr" target="#b35">[36]</ref> with comparisons to stateof-the-art algorithms, including semi-supervised and unsupervised settings. In <ref type="table" target="#tab_1">Table 2</ref>, we show results with different settings, including the need of initial object mask, future frames and pre-processing steps. Based on these requirements and their runtime speed, we then analyze the capability for online applications.  For unsupervised methods that do not need the initial mask, they usually need to compute optical flow as the motion cue (FSEG <ref type="bibr" target="#b13">[14]</ref> and LMP <ref type="bibr" target="#b39">[40]</ref>) or foresee the entire video (LVO <ref type="bibr" target="#b40">[41]</ref> and ARP <ref type="bibr" target="#b20">[21]</ref>) to improve the performance, which is not applicable to online usages. In addition, these methods cannot distinguish different instances and perform segmentation on a specific object.</p><p>In the semi-supervised setting, recent methods require various pre-processing steps before starting to segment the object in the video, which weaken the ability for online applications. These pre-processing steps include model finetuning (OnAVOS <ref type="bibr" target="#b43">[44]</ref>, Lucid <ref type="bibr" target="#b18">[19]</ref>, OSVOS <ref type="bibr" target="#b3">[4]</ref>, MSK <ref type="bibr" target="#b19">[20]</ref>, SFL <ref type="bibr" target="#b5">[6]</ref>), data synthesis (Lucid <ref type="bibr" target="#b18">[19]</ref>) and flow computing (MSK <ref type="bibr" target="#b19">[20]</ref>, CTN <ref type="bibr" target="#b15">[16]</ref>, and OFL <ref type="bibr" target="#b41">[42]</ref>). For fair comparisons in the online setting, these pre-processing steps are included in the runtime by averaging on all the frames.</p><p>The most closest setting to our method is VPN <ref type="bibr" target="#b14">[15]</ref> and BVS <ref type="bibr" target="#b31">[32]</ref> that do not have heavy pre-processing steps. However, these approaches may propagate segmentation errors after tracking for a long period of time. In contrast, our algorithm always constantly refers to the initial object mask via parts and can reduce such errors in the long run, improving more than 12% in J Mean against VPN <ref type="bibr" target="#b14">[15]</ref>. Overall, the proposed video object segmentation framework <ref type="table">Table 3</ref>. Segmentation results on DAVIS 2017 validation set. We show our baseline results with different modules, including foreground/background regularization (FG), Spatial Propagation Network (SPN) and a refinement procedure. runs at the fastest speed, and can achieve J Mean in the 3rd place with further refinement, while still maintaining a fast runtime speed compared to state-of-the-art methods. Some qualitative comparisons are presented in <ref type="figure" target="#fig_3">Figure 8</ref>.</p><p>DAVIS 2017. To evaluate how our method deals with multiple instances in videos, we conduct experiments on the DAVIS 2017 validation set <ref type="bibr" target="#b37">[38]</ref> which consists of 30 challenging videos and each one has two instances on average. Existing methods all rely on sophisticated processing steps <ref type="bibr" target="#b24">[25]</ref> to achieve better performance, and hence we compare our method with SPN <ref type="bibr" target="#b4">[5]</ref> that only involves the finetuning step in <ref type="table">Table 3</ref>. For the baseline algorithm, we start with our part-based aggregation method via part-based tracker and ROI SegNet, while <ref type="bibr" target="#b4">[5]</ref> finetunes a CNN-based model for each instance. The baseline results show that, without the need of the computationally expensive finetuning process, our method even outperforms the existing method. One reason is that as the video becomes more complicated, finetuning-based methods may suffer from confusions between instances. In contrast, our method employs a partbased tracker that can effectively capture local cues for further segmentation. Following <ref type="bibr" target="#b4">[5]</ref>, we then sequentially add different components, including foreground/background regularization, a Spatial Propagation Network and a region-based refinement step. In addition, we integrate the object tracker proposed in Section 4.2 to further refine the segmentation. Overall, without the need of finetuning on each instance, our approach achieves a similar performance or outperforms the one that requires finetuning. We also note that finetuning is expensive not only in speed but also in stored size, as hundreds of objects would result in a huge number of stored models, which is not practical in real-world applications. In <ref type="figure" target="#fig_4">Figure 9</ref>, we present some example results on the DAVIS 2017 dataset.</p><p>Runtime Analysis. In the proposed framework, our method runs at 0.60 seconds on average per instance per frame without the refinement step, including part-based tracking (0.2s), ROI segmentation (0.3s), and part aggregation (0.1s). With CRF (1s) and tracker (0.2s) refinements, our method runs at 1.8 seconds per instance per frame with better performance. We note that for tracking and segmenting parts, we parallelly use Titan X GPUs to handle hundreds of parts for faster inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>In this paper, we propose a fast and accurate video object segmentation method that is applicable to online applications. Different from existing algorithms that heavily rely on pre-processing the object mask in the first frame, our method exploits the initial mask via a part-based tracker and an effective part aggregation strategy. The part-based tracker provides good localization for local regions surrounding the object, ensuring that most portion of the object is retained for further segmentation purpose. We then design an ROI segmentation network to accurately output partial object segmentations. Finally, a similarity-based scoring function is developed to aggregate parts and generate the final result. Our algorithm exploits the strength of CNN-based frameworks for tracking and segmentation to achieve fast runtime speed, while closely monitoring the information contained in the first frame for the state-of-theart performance. The proposed algorithm can be applied to other video analytic tasks that require fast and accurate online video object segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>IoU-Recall curve for trackers on the DAVIS 2016 dataset. Dashed lines (-agg) denote results by utilizing the proposed part-based tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Sample results of using different components in the proposed method. We show gradual improvement over baseline with part aggregation, CRF refinement and an object tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Example results of comparisons between state-of-the-art methods on DAVIS 2016. Approaches with no, weak, strong online applicability are marked in yellow, blue and green, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Example results for multiple instances on DAVIS 2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Overall segmentation results on DAVIS 2016. We analyze various settings for different algorithms as well as provide online applicability based on their runtime speed (with different colors).</figDesc><table><row><cell>Method</cell><cell cols="8">Initial mask Future frames Pre-processing Online Speed J mean F mean T mean</cell></row><row><cell>OnAVOS [44] Lucid [19] Ours-ref OSVOS [4] MSK [20] Ours-part ARP [21] SFL [6] LVO [41] CTN [16]</cell><cell>! ! ! ! ! ! ! !</cell><cell>! !</cell><cell>finetuning data, finetuning no finetuning flow, finetuning no data finetuning flow flow</cell><cell>weak weak strong weak weak strong no weak no weak</cell><cell>13s 40s 1.8s 10s 12s 0.60s -7.9s -29.95s</cell><cell>0.861 0.848 0.824 0.798 0.797 0.779 0.762 0.761 0.759 0.735</cell><cell>0.849 0.823 0.795 0.806 0.754 0.760 0.706 0.760 0.721 0.693</cell><cell>0.190 0.158 0.263 0.378 0.218 0.229 0.393 0.189 0.265 0.220</cell></row><row><cell>FSEG [14] VPN [15]</cell><cell>!</cell><cell></cell><cell>flow no</cell><cell>weak strong</cell><cell>7s 0.63s</cell><cell>0.707 0.702</cell><cell>0.653 0.655</cell><cell>0.328 0.324</cell></row><row><cell>LMP [40] OFL [42]</cell><cell>!</cell><cell></cell><cell>flow flow</cell><cell>weak weak</cell><cell>18s 60s</cell><cell>0.700 0.680</cell><cell>0.659 0.634</cell><cell>0.572 0.222</cell></row><row><cell>BVS [32]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Part Tracking. Given a set of parts P t = {P 1 t , P 2 t , ..., P i t } in frame I t , our goal is to output a score map S t that measures the location likelihood of part P i t appearing in the next frame I t+1 :S t = T (P i t , I t+1 ),(1)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This project is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to segment instances in videos with spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video object segmentation with re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time part-based visual tracking via adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation with iterative online fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic cosegmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
