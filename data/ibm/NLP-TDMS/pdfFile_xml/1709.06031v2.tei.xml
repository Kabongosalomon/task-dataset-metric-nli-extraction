<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Object Segmentation Without Temporal Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
						</author>
						<title level="a" type="main">Video Object Segmentation Without Temporal Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Object Segmentation</term>
					<term>Convolutional Neural Networks</term>
					<term>Semantic Segmentation</term>
					<term>Instance Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames. When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly. This paper explores the orthogonal approach of processing each frame independently, i.e. disregarding the temporal information. In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame. We present Semantic One-Shot Video Object Segmentation (OSVOS S ), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot). We show that instance-level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS. We perform experiments on two recent single-object video segmentation databases, which show that OSVOS S is both the fastest and most accurate method in the state of the art. Experiments on multi-object video segmentation show that OSVOS S obtains competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A video is a temporal sequence of static images that give the impression of continuous motion when played consecutively and rapidly. The illusion of motion pictures is due to the persistence of human vision <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b71">[71]</ref>: the fact that it cannot perceive very high frequency changes <ref type="bibr" target="#b71">[71]</ref> because of the temporal integration of incoming light into the retina <ref type="bibr" target="#b65">[65]</ref>. This property has been exploited since the appearance of the phenakistoscope <ref type="bibr" target="#b63">[63]</ref> or the zoetrope <ref type="bibr" target="#b22">[23]</ref>, which displayed a sequence of drawings creating the illusion of continuous movement.</p><p>In order to achieve the high frequency to produce the video illusion, consecutive images vary very smoothly and slowly: the information in a video is very redundant and neighboring frames carry very similar information. In video coding, for instance, this is the key idea behind video compression algorithms such as motion-compensated coding <ref type="bibr" target="#b65">[65]</ref>, where instead of storing each frame independently, one picks a certain image and only codes the modifications to be done to it to generate the next frame.</p><p>Video processing in general, and video segmentation in particular, is also dominated by this idea, where motion estimation has emerged as a key ingredient for some of the state-of-the-art video segmentation algorithms <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b67">[67]</ref>. Exploiting it is not a trivial task however, as one has to compute temporal matches in the form of optical flow or dense trajectories <ref type="bibr" target="#b3">[4]</ref>, which can be an even harder problem to solve.</p><p>On the other hand, processing each frame independently would allow us to easily parallelize the computation, and to not be affected by sequence interruptions, to process the frames at any desire rate, etc. This paper explores how to segment objects in videos when processing each frame independently, that is, by ignoring the temporal information and redundancy. In other words, we cast video object segmentation as a per-frame segmentation problem given the model of the object from one (or various) manually-segmented frames.</p><p>This stands in contrast to the dominant approach where temporal consistency plays the central role, assuming that objects do not change too much between one frame and the next. Such methods adapt their single-frame models smoothly throughout the video, looking for targets whose shape and appearance vary gradually in consecutive frames, but fail when those constraints do not apply, unable to recover from relatively common situations such as occlusions and abrupt motion.</p><p>We argue that temporal consistency was needed in the past, as one had to overcome major drawbacks of the then inaccurate shape or appearance models. On the other hand, in this paper deep learning will be shown to provide a sufficiently accurate model of the target object to produce very accurate results even when processing each frame independently. This has some natural advantages: OSVOS S is able to segment objects throughout occlusions, it is not limited to certain ranges of motion, it does not need to process frames sequentially, and errors are not temporally propagated. In practice, this allows OSVOS S to handle e.g. interlaced videos of surveillance scenarios, where cameras can go blind for a while before coming back on again.</p><p>Given the first frame, we create an appearance model of the object of interest and then look for the pixels that better match this model in the rest of the frames. To do so, we will make use of Convolutional Neural Networks (CNNs), which are revolutionizing many fields of computer vision. For instance, they have dramatically boosted the performance for problems like image classification <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b62">[62]</ref> and object detection <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Image segmentation has also been taken over by CNNs arXiv:1709.06031v2 [cs.CV] 16 May 2018 <ref type="figure">Fig. 1</ref>. Example result of our technique: The segmentation of the first frame (red) is used to learn the model of the specific object to track, which is segmented in the rest of the frames independently (green). One every 10 frames shown of 90 in total.</p><p>recently <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b72">[72]</ref>, with deep architectures pretrained on the weakly related task of image classification on ImageNet <ref type="bibr" target="#b59">[60]</ref>. One of the major downsides of deep network approaches, however, is their hunger for training data. Yet, with various pre-trained network architectures one may ask how much training data do we really need for the specific problem at hand? This paper investigates segmenting an object along an entire video, when we only have one single labeled training example, e.g. the first frame. <ref type="figure">Figure 1</ref> shows an example result of OSVOS S , where the input is the segmentation of the first frame (in red), and the output is the mask of the object in the 90 frames of the sequence (in green).</p><p>The first contribution of the paper is to adapt the CNN to a particular object instance given a single annotated image. To do so, we gradually adapt a CNN pre-trained on image recognition <ref type="bibr" target="#b59">[60]</ref> to video object segmentation. This is achieved by training it on a set of videos with manually segmented objects. Finally, it is finetuned at test time on a specific object that is manually segmented in a single frame. <ref type="figure">Figure 2</ref> shows the overview of the method. Our proposal tallies with the observation that leveraging these different levels of information to perform object segmentation would stand to reason: from generic information of a large amount of categories, passing through the knowledge of the usual shapes of objects in videos, down to the specific properties of a particular object we are interested in segmenting.</p><p>Our second contribution is to extend the model of the object with explicit semantic information. In the example of <ref type="figure">Figure 1</ref>, for instance, we would like to leverage the fact that we are segmenting an object of the category person and that there is a single instance of it.</p><p>In particular, we will use an instance-aware semantic segmentation algorithm <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b35">[36]</ref> to extract a list of proposal of object masks in each frame, along with their categories. Given the first annotated frame, we will infer the categories of the objects of interest by finding the best-overlapping masks. We refer to this step as "semantic selection."</p><p>Our method uses the extracted semantic information from the first frame to segment the rest of the video. It enforces the resulting masks to align well with the same categories selected in the first frame. If we were segmenting a person on a motorbike, then this information should be kept throughout the video. In particular, we find instances extracted from the semantic instance segmentation algorithm that best match the model of the object, and we effectively combine them with the appearance model of the object, using a conditional classifier. We call this step "semantic propagation."</p><p>Our third contribution is that OSVOS S can work at various points of the trade-off between speed and accuracy. In this sense, given one annotated frame, the user can choose the level of fine-tuning performed on it, giving them the freedom between a faster method or more accurate results. Experimentally, we show that OSVOS S can run at 300 miliseconds per frame and 75.1% accuracy, and up to 86.5% when processing each frame in 4.5 seconds, for an image of 480×854 pixels.</p><p>Technically, we adopt the architecture of Fully Convolutional Networks (FCN) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b39">[40]</ref>, suitable for dense predictions. FCNs have recently become popular due to their performance both in terms of accuracy and computational efficiency <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Arguably, the Achilles' heel of FCNs when it comes to segmentation is the coarse scale of the deeper layers, which leads to inaccurately localized predictions. To overcome this, a large variety of works from different fields use skip connections of larger feature maps <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b72">[72]</ref>, or learnable filters to improve upscaling <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b74">[74]</ref>.</p><p>We perform experiments on two video object segmentation datasets (DAVIS 2016 <ref type="bibr" target="#b49">[50]</ref> and Youtube-Objects <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b54">[55]</ref>) and show that OSVOS S significantly improves the state of the art in them, both in terms of accuracy and speed. We perform additional experiments for multi-object video segmentation on DAVIS 2017 <ref type="bibr" target="#b53">[54]</ref>, where we obtain competitive results by directly applying our method without adaptation to the new problem.</p><p>All resources of this paper, including training and testing code, pre-computed results, and pre-trained models will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Semi-supervised Video Object Segmentation: Most of the current literature on semi-supervised video object segmentation enforces temporal consistency in video sequences to propagate the initial mask into the following frames. The most recent works heavily rely on optical flow, and make use of CNNs to learn to refine the mask of the object at frame n to frame n + 1 <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b48">[49]</ref> or combine the training of a CNN with ideas of bilateral filtering between consecutive frames <ref type="bibr" target="#b25">[26]</ref>. Also, <ref type="bibr" target="#b70">[70]</ref> follows up with the idea introduced in OSVOS and uses the result on the the predicted frames on the whole sequence to further train the network at test time. Previously, and in order to reduce the computational complexity, some works make use of superpixels <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, patches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b55">[56]</ref>, object proposals <ref type="bibr" target="#b50">[51]</ref>, or the bilateral space <ref type="bibr" target="#b45">[46]</ref>. After that, an optimization using one of the previous aggregations of pixels is usually performed; which can consider the full video sequence <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b50">[51]</ref>, a subset of frames <ref type="bibr" target="#b16">[17]</ref>, or only the results in frame n to obtain the mask in n+1 <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b55">[56]</ref>. As part of their pipeline, some of the methods include the computation of optical flow <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b55">[56]</ref>, or/and Conditional Random Fields (CRFs) <ref type="bibr" target="#b48">[49]</ref> which can considerably reduce their speed. Different from those approaches, OSVOS S is a simpler pipeline which segments each frame independently, and produces more accurate results, while also being significantly faster.</p><p>FCNs for Segmentation: Segmentation research has closely followed the innovative ideas of CNNs in the last few years. The advances observed in image recognition <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b62">[62]</ref> have been beneficial to segmentation in many forms (semantic <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b46">[47]</ref>, instance-level <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b51">[52]</ref>, biomedical <ref type="bibr" target="#b58">[59]</ref>, generic <ref type="bibr" target="#b40">[41]</ref>, etc.). Many of the current best performing methods are based on a deep CNN architecture, usually pre-trained on ImageNet <ref type="bibr" target="#b59">[60]</ref>, trainable end-to-end. The idea of dense predictions with CNNs was pioneered by <ref type="bibr" target="#b13">[14]</ref> and formulated by <ref type="bibr" target="#b39">[40]</ref> in the form of Fully Convolutional Networks (FCNs) for semantic segmentation. The authors noticed that by changing the last fully connected layers to 1 × 1 convolutions it is possible to train on images of arbitrary size, by predicting correspondinglysized outputs. Their approach boosts efficiency over patch-based approaches where one needs to perform redundant computations in overlapping patches. More importantly, by removing the parameter-intensive fully connected layers, the number of trainable parameters drops significantly, facilitating training with relatively fewer labeled data.</p><p>In most CNN architectures <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b62">[62]</ref>, activations of the intermediate layers gradually decrease in size, because of spatial pooling operations or convolutions with a stride. Making dense predictions from downsampled activations results in coarsely localized outputs <ref type="bibr" target="#b39">[40]</ref>. Deconvolutional layers that learn how to upsample are used in <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b74">[74]</ref> to recover accurately localized predictions. In <ref type="bibr" target="#b51">[52]</ref>, activations from shallow layers are gradually injected into the prediction to favor localization. However, these architectures come with many more trainable parameters and their use is limited to cases with sufficient data.</p><p>Following the ideas of FCNs, Xie and Tu <ref type="bibr" target="#b73">[73]</ref> separately supervised the intermediate layers of a deep network for contour detection. The duality between multiscale contours and hierarchical segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b52">[53]</ref> was further studied by Maninis et al. <ref type="bibr" target="#b41">[42]</ref> by bringing CNNs to the field of generic image segmentation. In this work we explore how to train an FCN for accurately localized dense prediction based on very limited annotation: a single segmented frame.</p><p>Semantic Instance Segmentation: Semantic instance segmentation is a relatively new computer vision task which has recently gained increasing attention. In contrast to semantic segmentation or object detection, the goal of instance segmentation is to provide a segmentation mask for each individual instance. The task was first introduced in <ref type="bibr" target="#b18">[19]</ref>, where they extract both region and foreground features using the R-CNN <ref type="bibr" target="#b15">[16]</ref> framework and region proposals. Then, the features are concatenated and classified by an SVM. Several works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b75">[75]</ref> following that path have been proposed in recent years. There also exist some approaches based on iteration <ref type="bibr" target="#b33">[34]</ref>, and recurrent neural networks <ref type="bibr" target="#b57">[58]</ref>. The recent best-performing methods use fully convolutional position sensitive architectures <ref type="bibr" target="#b35">[36]</ref>, or a modified Faster-RCNN <ref type="bibr" target="#b56">[57]</ref> pipeline, extended to instance segmentation <ref type="bibr" target="#b20">[21]</ref>. In contrast to such class-sensitive methods, in which unseen classes are treated as background, our method is class agnostic, and is able to segment generic objects, given only one annotated example.</p><p>Using Semantic Information to Aid Other Computer Vision Tasks: Semantic information is a very relevant cue for the human vision system, and some computer vision algorithms leverage it to aid various tasks. <ref type="bibr" target="#b17">[18]</ref> improves reconstruction quality by jointly reasoning about class segmentation and 3D reconstruction. Using a similar philosophy, <ref type="bibr" target="#b37">[38]</ref> estimates the depth of each pixel in a scene from a single monocular image guided by semantic segmentation, and improves the results significantly. To the best of our knowledge, we are the first ones to apply instance semantic information to the task of object segmentation in videos.</p><p>Conditional Models: Conditional models prove to be a very powerful tool when the feature statistics are complex. In this way, prior knowledge can be introduced by incorporating a dependency to it. <ref type="bibr" target="#b10">[11]</ref> builds a conditional random forest to estimate face landmarks whose classifier is dependent on the pose of head. Similarly, <ref type="bibr" target="#b64">[64]</ref> proposes to estimate human pose dependent on torso orientation, or human height, which can be a useful cue for the task of pose estimation. The same also applies to boundary detection, <ref type="bibr" target="#b68">[68]</ref> proposes to train a series of conditional boundary detectors, and the detectors are weighted differently during test based on the global context of the test image. In this work, we argue that the feature distribution of foreground and background pixels are essentially different, and so a monolithic classifier for the whole image is bound to be suboptimal. Thus, we utilize the conditional classifier to better model the different distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ONE-SHOT VIDEO OBJECT SEGMENTATION (OSVOS)</head><p>This section describes our algorithm to gradually fine-tune the CNN in order to build a strong appearance model for video object segmentation given the first annotated frame. This was presented in our conference contribution <ref type="bibr" target="#b4">[5]</ref>. We will refer to the method as OSVOS, to differentiate it from OSVOS S (Section 4), in which we use semantic instance segmentation as further guiding signal.</p><p>Let us assume that one would like to segment an object in a video, for which the only available piece of information is its foreground/background segmentation in one frame. Intuitively, one could analyze the entity, create a model, and search for it in the rest of the frames. For humans, this very limited amount of information is more than enough, and changes in appearance, shape, occlusions, etc. do not pose a significant challenge, because we leverage strong priors: first "It is an object," and then "It is this particular object." Our method is inspired by this gradual refinement.</p><p>We train a Fully Convolutional Neural Network (FCN) for the binary classification task of separating the foreground object from the background. We use two successive training steps: First we train on a large variety of objects, offline, to construct a model that is able to discriminate the general notion of a foreground object, i.e. , "It is an object." Then, at test time, we fine-tune the network for a small number of iterations on the particular instance that we aim to segment, i.e. , "It is this particular object." The overview of our method is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">End-to-end trainable foreground FCN</head><p>Ideally, we would like our CNN architecture to satisfy the following criteria: (i) Accurately localized segmentation output, as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on frame N of test sequence</head><p>Base Network  <ref type="figure">Fig. 2</ref>. Overview of OSVOS: <ref type="bibr" target="#b0">(1)</ref> We start with a pre-trained base CNN for image labeling on ImageNet; its results in terms of segmentation, although conform with some image features, are not useful. <ref type="formula">(2)</ref> We then train a parent network on the training set of DAVIS 2016; the segmentation results improve but are not focused on an specific object yet. (3) By fine-tuning on a segmentation example for the specific target object in a single frame, the network rapidly focuses on that target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary Snapping</head><p>Snap the foreground mask to accurate contours discussed in Section 2, (ii) relatively small number of parameters to train from a limited amount of annotated data, and (iii) relatively fast testing times.</p><p>We draw inspiration from the CNN architecture of <ref type="bibr" target="#b42">[43]</ref>, originally used for biomedical image segmentation. It is based on the VGG <ref type="bibr" target="#b62">[62]</ref> network, modified for accurately localized dense prediction (Point i). The fully-connected layers needed for classification are removed (Point ii), and efficient image-toimage inference is performed (Point iii). The VGG architecture consists of groups of convolutional plus Rectified Linear Units (ReLU) <ref type="bibr" target="#b44">[45]</ref> layers grouped into 5 stages. Between the stages, pooling operations downscale the feature maps as we go deeper into the network. We connect convolutional layers to form separate skip paths from the last layer of each stage (before pooling). Upscaling operations take place wherever necessary, and feature maps from the separate paths are concatenated to construct a volume with information from different levels of detail. We linearly fuse the feature maps to a single output which has the same dimensions as the image, and we assign a loss function to it. The proposed architecture is shown in <ref type="figure">Figure 3</ref> (1), foreground branch.</p><p>The pixel-wise cross-entropy loss for binary classification (we keep the notation of Xie and Tu <ref type="bibr" target="#b72">[72]</ref>) is in this case defined as:</p><formula xml:id="formula_0">L (W) = − j y j logP (y j =1|X;W)+(1−y j )log (1−P (y j =1|X;W)) = − j∈Y + logP (y j =1|X;W) − j∈Y − logP (y j =0|X; W)</formula><p>where W are the standard trainable parameters of a CNN, X is the input image, y j ∈ {0, 1}, j = 1, . . . , |X| is the pixel-wise binary label of X, and Y + and Y − are the positive and negative labeled pixels. P (·) is obtained by applying a sigmoid to the activation of the final layer.</p><p>In order to handle the imbalance between the two binary classes, Xie and Tu <ref type="bibr" target="#b72">[72]</ref> proposed a modified version of the cost function, originally used for contour detection (we drop W for the sake of readability): <ref type="bibr" target="#b0">(1)</ref> where β = |Y − |/|Y |. Equation 1 allows training for imbalanced binary tasks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b72">[72]</ref>.</p><formula xml:id="formula_1">L mod = −β j∈Y + logP (y j =1|X) − (1−β) j∈Y − logP (y j =0|X)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training details</head><p>Offline training: The base CNN of our architecture <ref type="bibr" target="#b62">[62]</ref> is pre-trained on ImageNet <ref type="bibr" target="#b59">[60]</ref> for image labeling, which has proven to be a very good initialization to other tasks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b74">[74]</ref>. Without further training, the network is not capable of performing segmentation, as illustrated in <ref type="figure">Figure 2</ref> (1). We refer to this network as the "base network."</p><p>We therefore further train the network on the binary masks of the training set of DAVIS 2016, to learn a generic notion of how to segment objects from their background, their usual shapes, etc. We use Stochastic Gradient Descent (SGD) with momentum 0.9 for 50000 iterations. We augment the data by mirroring and zooming in. The learning rate is set to 10 −8 , and is gradually decreased. After offline training, the network learns to segment foreground objects from the background, as illustrated in <ref type="figure">Figure 2</ref> (2). We refer to this network as the "parent network."</p><p>Online training/testing: With the parent network available, we can proceed to our main task ("test network" in <ref type="figure">Figure 2</ref>): Segmenting a particular entity in a video, given the image and the segmentation of the first frame. We proceed by further training (fine-tuning) the parent network for the particular image/groundtruth pair, and then testing on the entire sequence, using the new weights. The timing of our method is therefore affected by two times: the fine-tuning time (once per annotated mask) and the segmentation of all frames (once per frame). In the former we have a trade-off between quality and time: the more iterations we allow the technique to learn, the better results but the longer the user will have to wait for results. The latter does not depend on the training time: OSVOS is able to segment each 480p frame (480 × 854) in 130 ms.</p><p>Regarding the fine-tuning time, we present two different modes: One can either need to fine-tune online, by segmenting a frame and waiting for the results in the entire sequence, or offline, having access to the object to segment beforehand. Especially in the former mode, there is the need to control the amount of time dedicated to training: the more time allocated for finetuning, the more the user waits and the better the results are. In order to explore this trade-off, in our experiments we train for a period between 10 seconds and 10 minutes per sequence. <ref type="figure" target="#fig_2">Figure 4</ref> shows a qualitative example of the evolution of the results quality depending on the time allowed for fine-tuning. In the experimental evaluation, <ref type="figure" target="#fig_11">Figure 12</ref> quantifies this evolution.</p><p>Ablation analysis shows that both offline and online training are crucial for good performance: If we perform our online training directly from the base network (ImageNet model), the performance drops significantly. Only dropping the online training for a specific object (using the parent network directly) also yields a significantly worse performance, as already transpired from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contour snapping</head><p>In the field of image classification <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b62">[62]</ref>, where our base network was designed and trained, spatial invariance is a design choice: no matter where an object appears in the image, the classification result should be the same. This is in contrast to the accurate localization of the object contours that we expect in (video) object segmentation. Despite the use of skip connections <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b73">[73]</ref> to minimize the loss of spatial accuracy, we observe that OSVOS' segmentations have some room for improvement in terms of contour localization.</p><p>To overcome this limitation, we propose a complementary CNN in a second branch that is trained to detect object contours. <ref type="figure">Figure 3</ref> shows the global architecture: (1) shows the main foreground branch, where the foreground pixels are estimated; <ref type="bibr" target="#b1">(2)</ref> shows the contour branch, which detects all contours in the scene (not only those of the foreground object). This allows us to train offline, without the need to fine-tune on a specific example. We used the exact same architecture in the two branches, but training for different losses. We noticed that jointly training a network with shared layers for both tasks rather degrades the results thus we kept the computations for the two objectives uncorrelated. This allows us to train the contour branch only offline and thus it does not affect the online timing. Since there is need for high recall in the contours, we train on the PASCAL-Context <ref type="bibr" target="#b43">[44]</ref> database, which provides contour annotations for the full scene of an image.</p><p>Once we have the estimated object contours, the boundary snapping step <ref type="figure">(Figure 3</ref> (3)), consists of two different steps: a) Superpixel snapping: It computes superpixels that align to the computed contours (branch 2) by means of an Ultrametric Contour Map (UCM) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b52">[53]</ref>, which we threshold at a low strength. We then take a foreground mask (branch 1) and we select superpixels via majority voting (those that overlap with the foreground mask over 50%) to form the final foreground segmentation. b) Contour recovery: It recovers the very thin structures that are lost when snapping to superpixels. It enumerates the connected components of the foreground mask (branch 1), and then matches their contours to the detected contours in branch <ref type="bibr" target="#b1">(2)</ref>. The connected components whose contour matches the generic contours (branch 2) above a certain tolerance are added to the final result mask.</p><p>This refinement process results in a further boost in performance, and is fully modular, meaning that depending on the timing requirements one can choose not to use them, sacrificing accuracy for execution time; since the module comes with a small, yet avoidable computational overhead. Please refer to the timing experiments ( <ref type="figure" target="#fig_11">Figure 12</ref>) for a quantitative evaluation of this trade off: at which range of desired speeds one can afford to use contour snapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEMANTIC GUIDANCE (OSVOS S )</head><p>The motivation behind semantic guidance is to improve the model we construct from the first frame with information about the category of the object and the number of instances, e.g. we track two people and a motorbike. We extract the semantic instance information from instance-aware semantic segmentation algorithms. We experiment with three top-performing methods: MNC <ref type="bibr" target="#b8">[9]</ref>, FCIS <ref type="bibr" target="#b35">[36]</ref> and the most recent MaskRCNN <ref type="bibr" target="#b20">[21]</ref>. We modify the algorithm and the network architecture to select and propagate the specific instances we are interested in (Section 4.2), and then we adapt the network architecture to include these instance inside the CNN (Section 4.3). The global network overview is first presented in Section 4.1. <ref type="figure">Figure 5</ref> illustrates the structure and workflow of the proposed semantic-aware network. Sharing the common base network (VGG) as the feature extractor, three pixel-wise classifiers are jointly learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Overview</head><p>The first classifier, First-Round Foreground Estimation, is the original OSVOS head, which is purely appearance based, with no knowledge about the semantic segmentation source and produces the first foreground estimation. The result of that classifier and the information from an external semantic instance segmentation system are combined in the semantic selection and propagation steps (Section 4.2) to produce the top matching instances that we refer to as the semantic prior.</p><p>The two other classifiers inside the conditional classifier operate on both the features of the common base network and the semantic prior, and are dependent on each other: one is responsible for the pixels with a foreground prior, whereas the other for the background ones. Finally, the two sets of predictions are fused into the final prediction. See Section 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Selection and Semantic Propagation</head><p>We leverage a semantic instance segmentation algorithm as an input to estimate the semantics of the object to be segmented. Specifically, we choose MNC <ref type="bibr" target="#b8">[9]</ref>, FCIS <ref type="bibr" target="#b35">[36]</ref>, or MaskRCNN <ref type="bibr" target="#b20">[21]</ref> as our input instance segmentation algorithms, and we use their publicly available implementations. We show that each of the improvements in instance segmentation is translated in a boost for the task of video object segmentation, which suggests that our method will be able to incorporate future improvements in the field. The three instance semantic segmentation methods (MNC, FCIS, and MaskRCNN) are multi-stage networks that consist of three major components: shared convolutional layers, region proposal network (RPN), and region-of-interest(ROI)-wise classifiers. We use the available models which are pre-trained on PASCAL for the first one and on COCO for the other two. We note that our method is category agnostic, and the objects to segment do not necessarily need to be part of the PASCAL or COCO category vocabulary, as it will be shown in the experiments.</p><p>The output of the instance segmentation algorithm is given as a set of binary masks, together with their category, and their confidence of being a true object. We search for the object of interest inside the pool of most confident masks: our objective is to find a subset of masks with consistent semantics throughout the video as our semantic prior.</p><p>The process can be divided into two stages, namely semantic selection and semantic propagation. Semantic selection happens in the first frame, where we select the masks that match the object according to the given ground-truth mask (please note that we are in a semi-supervised framework where the true mask of the first frame is given as input). The number of instances and their categories are what we enforce to be consistent throughout the entire video. <ref type="figure">Figure 6</ref> depicts an example of both steps. Semantic selection, on the left in green, finds that we are interested in a motorbike plus a person (bottom), by overlapping the ground truth (middle) to the instance segmentation proposals (top). There are two cases where semantic selection may fail: a) the objects of interest are not part of the semantic vocabulary of the instance segmenter, and b) the wrong instances are selected by this step. Results show that our classifiers are robust to such failures, preserving high quality outputs in both cases. Thus, a fast greedy search for selecting the instances is sufficient to preserve high performance.</p><p>The semantic propagation stage (in orange) occurs at the following frames, where we propagate the semantic prior we estimated in the first frame to the following ones. No information from future frames is used in this stage. The instance segmentation masks (first row), are filtered using the first-round foreground estimation from the OSVOS head (middle row), and the top matching person and motorbike from the pool are selected (bottom row). In cases that an instance of the selected classes does not overlap with the output of OSVOS, as in cases of occlusions and moving camera, we exclude the particular instance from the semantic prior, for the specific frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conditional Classifier</head><p>Dense labeling using fully convolutional networks is commonly formulated as a per-pixel classification problem. It can be therefore understood as a global classifier sliding over the whole image, and assigning either the foreground or background label to each pixel according to a monolithic appearance model. In this work, we want to incorporate the semantic prior to the final classification, which will be given as a mask of the most promising instance (or set of instances) in the current frame.</p><p>If semantic instance segmentation worked perfectly, we could directly select the best-matching instance to the appearance model, but in reality the results are far from perfect (as we will show in the experiments). We can only, therefore, use the instance segmentation mask as a guidance, or a guess, of what the limits of that instance are, but we still need to perform a refinement step. Our proposed solution to incorporate this mask but still keep the per-pixel classification is to train two classifiers and weigh them according to the confidence we have in that pixel being part of the instance or not. We argue that using a single set of parameters for the whole image is suboptimal.</p><p>Formally, for each pixel i, we estimate its probability of being a foreground pixel given the image: p(i|I). The probability can be decomposed into the sum of k conditional probabilities weighted by the prior:</p><formula xml:id="formula_2">p(i|I) = K k=1 p(i|I, k) p(k|I).</formula><p>In our experiments, we use K = 2, and we build two conditional classifiers, one focusing on the instance foreground pixels, and the other focusing on the instance background pixels. The prior term p(k|I) is estimated based on the instance segmentation output. Specifically, a pixel relies more on the instance foreground classifier if it is located within the instance segmentation mask; and more importance is given to the instance background classifier if it falls out of the instance segmentation mask. In our experiments, we apply a Gaussian filter to spatially smooth the selected masks as our semantic prior.</p><p>The conditional classifier is implemented as a layer which can be easily integrated in the network in a end-to-end trainable manner. The layer takes two prediction maps f 1 and f 2 and the weight maps p(k|I) which come from the semantic selection. Without loss of generality, we will assume that k = 1 corresponds to the foreground of the semantic prior. For convenience, we set w = p(k = 1|I), and in our case 1−w = p(k = 2|I) (background prior). The inference process is illustrated in <ref type="figure">Figure 7</ref>, where each input element is multiplied by its corresponding weight from the weight map, then summed with the corresponding element in the other map:  <ref type="figure">Fig. 7</ref>. Forward pass of the conditional classifier layer: Red denotes foreground probability, and blue background probability. The output is the weighted sum of the two conditional classifier.</p><formula xml:id="formula_3">f out (x, y) = w(x, y) f 1 (x, y) + 1−w(x, y) f 2 (x, y). (2)</formula><p>In Equation 2, x and y represent the horizontal and vertical pixel location on a frame. This equation suggest that the decision for the pixels near the selected instances are made by the instance foreground classifier (f 1 (x, y)), whereas the instance background classifier (f 2 (x, y)) decides for the rest of the pixels.</p><p>Similarly, in the back-propagation step, the gradient from the top g top is propagated to the two parts according to the weight map:</p><formula xml:id="formula_4">g 1 (x, y) = w(x, y) g top (x, y) g 2 (x, y) = 1 − w(x, y) g top (x, y).</formula><p>The conditional classifier is necessary to incorporate the semantic prior information, in order to make softer decisions. Techniques that can be used as alternatives incorporating only a single classifier, such as masking of the features by the semantic prior, lead to hard decisions guided by the semantics, unable to recover in regions where they are wrong. For example, in <ref type="figure">Figure 7</ref>, the left hand of the dancer is not detected by the semantic prior, and it will be immediately classified as background in the case of feature masking. The background classifier of our proposed method, however, is able to recover the region, correctly classifying it as a foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training and Inference</head><p>We follow the same ideas as OSVOS to train and test the network, every step enriched with the semantic selection and propagation steps. The parent network is trained using semantic instances that overlap with the ground-truth masks of the DAVIS 2016 training set. Similarly, during online fine-tuning we use the label of the first frame, as well as the outputs of the OSVOS head for the next ones. As was done before, each frame is processed independently of the others. As shown in the experiments, the plug-in of the instance segmentation module dramatically improves the quality of the final segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL VALIDATION</head><p>Experimental Setup: We will mainly work on the DAVIS 2016 database <ref type="bibr" target="#b49">[50]</ref>, using their proposed metrics: region similarity (intersection over union J ), contour accuracy (F measure), and temporal instability (T ). The dataset contains 50 full-HD annotated video sequences, 30 in the training set and 20 in the validation set. All our results will be trained on the former, evaluated on the latter. As a global comparison metric we will  <ref type="figure">Fig. 8</ref>. Semantic selection evaluation: Semantic instances selected by the semantic selection step, with its category overlaid. We observe that in some cases either the semantic labels (h-i) or the number of instances (j) is incorrect. The final results, however, are robust to such mistakes.  use the mean between J and F, as proposed in the DAVIS 2017 challenge <ref type="bibr" target="#b53">[54]</ref>. We compare against a large body of very recent semisupervised state-of-the-art techniques (OnAVOS <ref type="bibr" target="#b70">[70]</ref>, MSK <ref type="bibr" target="#b48">[49]</ref>, CTN <ref type="bibr" target="#b27">[28]</ref>, VPN <ref type="bibr" target="#b25">[26]</ref>, OFL <ref type="bibr" target="#b67">[67]</ref>, BVS <ref type="bibr" target="#b45">[46]</ref>, and FCP <ref type="bibr" target="#b50">[51]</ref>) using the pre-computed results provided by the respective authors. For context, we also add the results of the latest unsupervised techniques (ARP <ref type="bibr" target="#b29">[30]</ref>, FSEG <ref type="bibr" target="#b23">[24]</ref>, LMP <ref type="bibr" target="#b66">[66]</ref>, FST <ref type="bibr" target="#b47">[48]</ref>, NLC <ref type="bibr" target="#b11">[12]</ref>, MSG <ref type="bibr" target="#b3">[4]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head><p>Moreover, we perform experiments on DAVIS 2017 which contains videos with multiple objects. We compute the results on the test-dev set using the submission website provided by the organizers of the challenge. We compare against OnAVOS <ref type="bibr" target="#b70">[70]</ref>, its submission to the DAVIS 2017 challenge which achieves the fifth place <ref type="bibr" target="#b69">[69]</ref> and the other top-performing methods of the challenge <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b61">[61]</ref>.</p><p>For completeness, we also experiment on the Youtube-objects dataset <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b54">[55]</ref> against those techniques with public segmentation results (OnAVOS <ref type="bibr" target="#b70">[70]</ref>, OSVOS <ref type="bibr" target="#b4">[5]</ref>, MSK <ref type="bibr" target="#b48">[49]</ref>, OFL <ref type="bibr" target="#b67">[67]</ref>, BVS <ref type="bibr" target="#b45">[46]</ref>). We do not take pre-computed evaluation results directly from the paper tables because the benchmarking algorithm is not consistent among the different authors.</p><p>Ablation Study: <ref type="table" target="#tab_3">Table 1</ref> shows how much each of the improvements presented builds up to the final result. We start by evaluating the network using only ImageNet pre-trained weights, before including any further training to the pipeline. The results in terms of segmentation (J &amp;F = 18.9%) are completely random (as visually shown in <ref type="figure">Figure 2</ref>). Fine-tuning on the mask of the first frame already boosts the results to competitive levels (+OneShot). By pre-training the parent model, we allow finetuning to start from a much more meaningful set of weights, from a problem closer to the final one, so performance increases by  12% (+Parent). Adding semantics and the conditional classifier (+Semantics) plays an important role both in terms of regions and contours (J &amp;F), but especially on temporal stability (T ). Snapping to superpixels (+Superpixels) and recovering the contours (+Contours) improve the results around half a point overall, the former especially in terms of J , the latter in terms of F, as it stands to reason. Semantic Selection and Propagation: <ref type="figure">Figure 8</ref> qualitatively evaluates the semantic-selection algorithm: it displays the selected semantic instances on the first frame of eight videos. Examples (a) and (b) show correct detections in terms of category when a single instance is present. Results (c) to (f) show that the algorithm works also in terms of the quantity of instances when more than one of them is needed. Images (g) to (i) display cases where the category of the object is not present in MS COCO <ref type="bibr" target="#b36">[37]</ref> (on which the instance segmentation algorithm was trained), so the closest semantic match is used instead. Please note that the precise category is not needed for our algorithm to work, as long as that category is consistent throughout the video (e.g. as long   <ref type="figure">-and un-supervised, and a practical bound)</ref>. For the number of images, we count those datasets that have some form of segmentation (instance or semantic), and we mark the models pre-trained on Imagenet with § . k stands for thousands. The number of images in italics is not directly used to train for the task of video object segmentation, but to train the auxiliary semantic instance segmentation network used by OSVOS S .   as the camel is always detected as a cow). Last image (j) shows a failure case where two persons are detected when just a single one (albeit upside down) is present, but the algorithm is afterwards robust to this mistake. Once the semantic selection is done on the first frame, the information is propagated throughout the video. <ref type="table" target="#tab_5">Table 2</ref> quantitatively evaluates this step by comparing our automatic selection of instances against an oracle that selects the best instance in each frame independently. We use three different instance segmentation algorithms (MNC <ref type="bibr" target="#b8">[9]</ref>, FCIS <ref type="bibr" target="#b6">[7]</ref> and MaskRCNN <ref type="bibr" target="#b20">[21]</ref>). The results show that in all cases our automatic selection gets very close to the oracle selection (best possible instance), so we are not losing much quality in this step; and this is so in all instance segmentation algorithms, showing that we are robust to the particular algorithm used and so we will be able to incorporate future improvements in this front. The last column shows our final result, which significantly improves the oracle selection, so instance segmentation alone is not enough, as already pointed out in previous sections. For the rest of the paper, we refer to OSVOS S as the OSVOS S -MaskRCNN variant of our method.</p><p>Comparison to State of the Art in DAVIS 2016: <ref type="table" target="#tab_7">Table 3</ref> shows the comparison of OSVOS S and OSVOS against a large set of very recent video segmentation algorithms, semi-supervised (using the first segmented frame as input) and unsupervised (only the raw video as input). Apart from the standard metrics of DAVIS 2016 <ref type="bibr" target="#b49">[50]</ref>, we also add the most recent mean between J and F, as used in the 2017 DAVIS Challenge <ref type="bibr" target="#b53">[54]</ref>.</p><p>OSVOS S is the best performing technique overall, one point above the second semi-supervised technique and 12.6 points above the best unsupervised one. Last column shows the best result one could obtain from picking superpixels from COB <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, a state-of-the-art generic image segmentation algorithm, at a very fine scale. We select the superpixels by snapping the ground-truth masks to them, thus creating a very strong bound. OSVOS S is only </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Attribute-based performance (J &amp;F ): Impact of the attributes of the sequences on the results. For each attribute, results on the sequences with that particular feature and in italics the gain with respect to those on the set of sequences without the attribute. LR stands for low resolution, SV for scale variation, FM for fast motion, CS for camera shake, DB for dynamic background, MB for motion blur, OCC for occlusions, and OV for object out of view.</p><p>0.3 points below the value of this oracle, further highlighting the outstanding quality of our results. Next, we break down the performance on DAVIS 2016 per sequence. <ref type="figure" target="#fig_7">Figure 9</ref> shows the previous state-of-the-art techniques in bars, and OSVOS S using a line; sorted by the difficulty of the sequence for our technique. We see that we outperform the majority of algorithms in the majority of sequences, especially so in the more challenging ones (e.g. Kite-Surf, Bmx-Trees). Please also note that OSVOS S results are above 70% in all but one sequence and above 80% in all but three, which highlights the robustness of the approach. <ref type="table">Table 4</ref> shows the per-attribute comparison in DAVIS 2016, that is, the mean results on a subset of sequences where a certain challenging attribute is present (e.g. camera shake or occlusions). The increase/decrease of performance when each attribute is not present (small positive/negative numbers in italics) is significantly low, which shows that OSVOS S is also very robust to the different challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of training images (parent network):</head><p>To evaluate how many annotated data are needed to retrain a parent network, <ref type="table" target="#tab_11">Table 5</ref> shows the performance of OSVOS S when using a subset of the DAVIS 2016 training set. We directly used the output of the CNN, without snapping, for efficiency. We randomly selected a fixed percentage of the annotated frames over all videos of the training set, and evaluated using the Region Similarity (J ) metric. We conclude that by using only~200 annotated frames,  we are able to reach almost the same performance than when using the full DAVIS 2016 training split. Thus, we therefore do not require full video annotations for the training procedure, that are often expensive to acquire. Even more, since our method is by definition disregarding temporal information, it is natural that the training data do not require to be temporally coherent.</p><p>Misclassified-Pixels Analysis: <ref type="figure" target="#fig_8">Figure 10</ref> shows the error analysis of our method. We divide the misclassified pixels in three categories: Close False Positives (FP-Close), Far False Positives (FP-Far) and False Negatives (FN): (i) FP-Close are those near the contour of the object of interest, so contour inaccuracies, (ii) FP-Far reveal if the method detects other objects or blobs apart from the object of interest, and (iii) FN tell us if we miss a part of the object during the sequence. The measure in the plot is the percentage of pixels in a sequence that fall in a certain category.</p><p>The main strength of OSVOS S compared to OSVOS and MSK is considerably reducing the number of false negatives. We believe this is due to OSVOS S 's ability to complete the object of interest when parts that were occluded in the first frame become visible, thanks to the semantic concept of instance. On the other hand, the output of the instance segmentation network that we are currently using, FCIS <ref type="bibr" target="#b6">[7]</ref>, is not very precise on the boundaries of the objects, and even though our conditional classifier is able to recover in part, FP-Close is slightly worse than that of the competition. On the plus side, since the instance segmentation is an independent input to our algorithm, we will probably directly benefit from better instance segmentation algorithms.</p><p>Performance Decay: As indicated by the J -Decay and F-Decay values in <ref type="table" target="#tab_7">Table 3</ref>, OSVOS S exhibits a better ability than OSVOS and MSK to maintain performance as frames evolve, and we interpret that this is so thanks to the injected semantic prior. The performance decay is similar to that of OnAVOS, even though it performs a costly iterative algorithm which fine-tunes the result to various frames of the sequence. Our method, on the other hand, uses the information of the first frame only, and keeps the quality throughout the sequence.</p><p>To further highlight this result and analyze it more in detail, <ref type="figure" target="#fig_9">Figure 11</ref> shows the evolution of J as the sequence advances, to examine how the performance drops over time. Since the videos in DAVIS 2016 are of different length, we normalize them to [0, 100] as a percentage of the sequence length. We then compute the mean J curve among all video sequences. As it can be seen from <ref type="figure" target="#fig_9">Figure 11</ref>, our method is significantly more stable in terms of performance drop compared to OSVOS and MSK, and has a similar curve than OnAVOS.</p><p>We also report the lowest point of the curve which indicates the worst performance across the video. Based on this metrics, OSVOS S is at 82.0, while for semantic-blind methods, the numbers are 81.0, 73.7, and 69.8.</p><p>The results therefore confirm that the semantic prior we introduce can mitigate the performance drop caused by appearance change, while maintaining high fidelity in details. The semantic information is particularly helpful in the later stage of videos where dramatic appearance changes with respect to the first frame are more probable.</p><p>Speed: The computational efficiency of video object segmentation is crucial for the algorithms to be usable in practice. OSVOS S can adapt to different timing requirements, providing progressively better results the more time we can afford, by letting the fine-tuning algorithm at test time do more or fewer iterations. As introduced before, OSVOS S 's time can be divided into the finetuning time plus the time to process each frame independently.</p><p>To compare to other techniques, we will evaluate the mean computing time per frame: fine-tuning time (done once per sequence) averaged over the length of that sequence, plus the forward pass on each frame. <ref type="figure" target="#fig_11">Figure 12</ref> shows the quality of the result with respect to the time it takes to process each 480p frame. The computation time for our method has been obtained using an NVidia Titan X GPU and for other methods the timing reported in their publications has been used. Our techniques are represented by curves: OSVOS S   <ref type="bibr" target="#b70">[70]</ref> MSK <ref type="bibr" target="#b48">[49]</ref> VPN <ref type="bibr" target="#b25">[26]</ref> CTN <ref type="bibr" target="#b27">[28]</ref> OFL <ref type="bibr" target="#b67">[67]</ref> BVS <ref type="bibr" target="#b45">[46]</ref>  ( ), without boundary snapping ( ), and without semantics ( ), which show the gain in quality with respect to the finetuning time. The best results come at the price of adding the semantics or the snapping cost, so depending on the needed speed, one of the three modes can be chosen. Dashed lines represent the regimes of each technique that are not in the Pareto front, i.e. where it is better to choose another mode within our techniques (faster for the same quality or best quality for the same speed).</p><p>Since OSVOS S processes frames independently, one could also perform the fine-tuning offline, by training on a picture of the object to be segmented beforehand (e.g. take a picture of a sports player before a match). In this scenario, OSVOS S can process each frame by one forward pass of the CNN ( | | ), and so be considerably fast.</p><p>Compared to other techniques, our techniques are faster and/or more accurate at all regimes, from fast modes: 75.1 versus 59.4 of BVS ( ) at 300 miliseconds, to high-quality regimes: same performance than OnAVOS ( ) but an order of magnitude faster (2.5 versus 12 seconds). The trade-off between performance and speed in video object segmentation has been largely ignored (or purposely hidden) in the literature although we believe it is of critical importance, and so we encourage future research to evaluate their methods in this performance-vs-speed plane.</p><p>Comparison to State of the Art in Youtube-Objects: For completeness, we also do experiments on Youtube-objects <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b54">[55]</ref>, without changing any parameter from our algorithm nor retraining the parent network. <ref type="table" target="#tab_14">Table 6</ref> shows the results of the quantitative evaluation against the rest of techniques. OSVOS S obtains the best results overall, being two points better than the runner up; and having the best results in eight out of ten categories. These experiments show the robustness and generality of our approach even to domain (dataset) shifts.</p><p>Multi-object video segmentation in DAVIS 2017: We test OSVOS S in the more challenging DAVIS 2017 dataset where multiple objects have to be segmented in the same video sequence. We apply our method as is, precessing every object in a sequence independently. <ref type="table" target="#tab_15">Table 7</ref> illustrates the results obtained in the testdev set of the dataset, compared to the top-performing methods of the DAVIS challenge, and to our direct competitor (OnAVOS). Even though our method is not specifically designed to handle multiple object instances, we achieve competitive results (comparable to the third entry), and we outperform OnAVOS. Our method falls behind the two first entries as it is not optimized to segment multiple objects, and is uses a single model, without the bells and whistles that naturally come with challenge submissions.   Instance segmentation quality: In this section we analyze the influence of the quality of the instance segmentation method in our final result. To this end, we use three different methods, i.e. MNC <ref type="bibr" target="#b8">[9]</ref>, FCIS <ref type="bibr" target="#b35">[36]</ref>, and Mask-RCNN <ref type="bibr" target="#b20">[21]</ref>. Developments to the field over the last two years have lead to competitive results on COCO <ref type="bibr" target="#b36">[37]</ref> test-dev, with resulting Average Precision (AP) varying from 24.6% for MNC, to 33.6% and 37.1% for FCIS and Mask-RCNN, respectively. <ref type="table" target="#tab_17">Table 8</ref> shows the performance gains obtained by using a different instance segmentation method within the OSVOS S pipeline in three different datasets. Results suggest that our method is able to incorporate improved instance segmentation results, and directly translates them into more accurate results for video object segmentation. The improvements are particularly large for DAVIS 2017, where there is still room for improvement.</p><p>Qualitative Results: <ref type="figure">Figure 13</ref> and <ref type="figure" target="#fig_2">Figure 14</ref> show some qualitative results of OSVOS S in DAVIS 2016 and Youtube-Objects, respectively. The first column shows the ground-truth mask used as input to our algorithm (in red). The rest of the columns show our segmented results in the following frames. These visual results qualitatively corroborate the robustness of our approach to occlusions, dynamic background, change of appearance, etc.</p><p>Limitations of OSVOS S : Both OSVOS and OSVOS S are very practical for applications due to their accuracy, and their frame-independent design which comes with increased speed compared to competing methods. Limitations of OSVOS mainly regard appearance of objects, such as similar objects, dynamic changes in appearance and viewpoint, and are successfully tackled by introducing the coarse instance segmentation input in OSVOS S . False positives can be successfully tackled by introducing optical   flow models <ref type="bibr" target="#b28">[29]</ref>, whereas <ref type="bibr" target="#b34">[35]</ref> handle false negatives by introducing a re-identification module, with the cost of extra processing time. Limitations regarding out-of-vocabulary instances are handled well by our method, however, that may not transfer to other domains with uncommon objects or parts of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>This paper presents Semantic One-Shot Video Object Segmentation (OSVOS S ), a semi-supervised video object segmentation technique that processes each frame independently and thus ignores the temporal information and redundancy of a video sequence. This has the inherent advantage of being robust to object occlusions, lost frames, etc, while keeping execution speed low.</p><p>OSVOS S shows state-of-the-art results in both DAVIS 2016 and Youtube-Objects at the whole range of operating speeds. It is significantly faster and/or better performing than the competition: 75.1 versus 59.4 at 300 miliseconds per frame, or 4.5 versus 12 seconds at the best performance (86.5 vs 85.5).</p><p>To do so, we build a powerful appearance model of the object from a single segmented frame. In contrast to most deep learning approaches, that often require a huge amount of training data in order to solve a specific problem, and in line with humans, that can solve similar challenges with only a single training example; we demonstrate that OSVOS S can reproduce this capacity of oneshot learning in a machine: Based on a parent network architecture pre-trained on a generic video segmentation dataset, we fine-tune it on merely one training sample. OSVOS S also leverages an instance segmentation algorithm that provides a semantic prior to guide the appearance model computed on the first frame. This adds robustness to appearance changes of the object and in practice helps in keeping the quality throughout a longer period of the video.</p><p>The appearance model is combined with the semantic prior by means of a new conditional classifier as a trainable module in a CNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, and L. Van Gool are with the ETHZ, Zürich. First two authors contributed equally. • L. Leal-Taixé and D. Cremers are with the TUM, München. • Contacts in http://www.vision.ee.ethz.ch/˜cvlsegmentation/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Fig. 3 .</head><label>23</label><figDesc>Two-stream FCN architecture: The main foreground branch (1) is complemented by a contour branch (2) which improves the localization of the boundaries (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative evolution of the fine tuning: Results at 10 seconds and 1 minute per sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Network architecture overview: Our network is composed of three major components: a base network as the feature extractor, and three classifiers built on top with shared features: a first-round foreground estimator to produce the semantic prior, and two conditional classifiers to model the appearance likelihood. Semantic selection and propagation: Illustrative example of the estimation of the semantics of the object from the first frame (semantic selection) and its propagation to the following frames (semantic propagation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>C</head><label></label><figDesc>ar -S ha do w C ar -R ou nd ab ou t B la ck sw an C am el C ow s D og D ri ft -S tr ai gh t Pa rk ou r G oa t H or se ju m p-H ig h L ib by D ri ft -C hi ca ne Sc oo te r-B la ck B re ak da nc e D an ce -T w ir l So ap bo x K ite -S ur f B m x-T re es M ot oc ro ss -J um p Pa ra gl id in g-L au nc h 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>DAVIS 2016 Validation: Per-sequence results of mean region similarity and contour accuracy (J &amp;F ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Error analysis of our method: Errors divided into False Positives (FP-Close and FP-Far) and False Negatives (FN). Values are percentage (%) of FP-Close, FP-Far or FN pixels in a sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Decay of the quality with time: Performance of various methods with respect to the time axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Quality versus timing: J &amp;F with respect to the processing time per frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Qualitative results on DAVIS 2016: OSVOS S results on a variety of representatives sequences. The input to our algorithm is the ground truth of the first frame (red). Outputs of all frames (green) are produced independent of each other. Qualitative results on Youtube-Objects: OSVOS S results on a variety of representatives sequences. The input to our algorithm is the ground truth of the first frame (red). Outputs of all frames (green) are produced independent of each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 Ablation study on DAVIS 2016:</head><label>1</label><figDesc>From a network pretrained on ImageNet, all improvement steps to the proposed OSVOS S (right-most column).</figDesc><table /><note>Numbers in italics show how much the results improve (in blue) or worsen (in red) in that metric with respect to the previous column.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 Semantic propagation: Comparing</head><label>2</label><figDesc>the automatic selection of instances against an oracle and our final result.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 DAVIS 2016 Validation:</head><label>3</label><figDesc>OSVOS S versus the state of the art (both semi</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>−3.6 89.5 −5.3 80.1 0.1 78.9 −1.8 69.0 3.2 57.7 13.5 45.7 26.7 SV 82.9 6.2 82.3 5.4 74.8 9.1 71.8 9.6 62.5 14.8 58.6 15.3 51.0 24.5 FM 85.2 2.1 84.2 1.9 77.7 3.9 75.0 4.0 65.8 8.7 57.4 16.1 48.7 26.1 CS 89.8 −5.0 88.3 −4.3 80.8 −0.9 76.8 1.1 71.8 −0.6 68.8 −1.5 64.2 2.3 DB 82.8 4.4 75.0 12.4 75.3 5.8 72.5 5.9 60.4 13.0 42.0 30.4 42.8 27.0 MB 82.8 6.8 80.8 8.5 74.7 9.9 72.1 9.9 66.1 9.5 62.1 10.4 53.6 22.0 OCC 86.8 −0.4 84.0 2.1 79.8 0.6 75.8 2.5 70.8 0.8 73.2 −7.7 66.2 −0.7 OV 82.4 5.2 80.8 5.9 71.1 11.4 68.3 11.6 63.9 9.3 53.8 17.5 48.5 21.5</figDesc><table><row><cell cols="2">Attr OSVOS S OnAVOS OSVOS</cell><cell>MSK</cell><cell>CTN</cell><cell>VPN</cell><cell>OFL</cell></row><row><cell>LR</cell><cell>89.3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5 Amount of training data:</head><label>5</label><figDesc>Region similarity (J ) as a function of the number of training images for the parent network of OSVOS S . Full DAVIS 2016 training set is 2079 training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 6 Youtube-Objects evaluation:</head><label>6</label><figDesc>Per-category and overall mean intersection over union (J ).</figDesc><table><row><cell>Method</cell><cell>Test-Dev J &amp;F</cell></row><row><cell>Apata [29]</cell><cell>66.6</cell></row><row><cell>Lixx [35]</cell><cell>66.1</cell></row><row><cell>Wangzhe [33]</cell><cell>57.7</cell></row><row><cell>Lalafine123 [61]</cell><cell>57.4</cell></row><row><cell>Voiglaender [69]</cell><cell>56.5</cell></row><row><cell>OnAVOS [70]</cell><cell>52.8</cell></row><row><cell>OSVOS S</cell><cell>57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 7 DAVIS 2017 evaluation:</head><label>7</label><figDesc>Performance of OSVOS S compared to the DAVIS 2017 challenge winners, on the test-dev set. Our single model achieves competitive results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 8 Performance vs. instance segmentation quality:</head><label>8</label><figDesc>Evaluation with respect to the instance segmentation algorithm.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Research funded by the EU Framework Programme for Research and Innovation Horizon 2020 (Grant No. 645331, EurEyeCase), and by the Swiss Commission for Technology and Innovation (CTI, Grant No. 19015.1 PFES-ES, NeGeVA). The authors gratefully acknowledge support by armasuisse and thank NVidia Corporation for donating the GPUs used in this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-for-low and low-forhigh: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jumpcut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint 3d scene reconstruction and class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Living Pictures: Their History, Photo-Production and Practical Working</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hopwood</surname></persName>
		</author>
		<idno>1899. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Optician &amp; Photographic Trades Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Instance re-identification flow for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Nguyen-Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-V</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><forename type="middle">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; X.-S</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-D</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video object segmentation with re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep retinal image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Nicolas</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multiple-instance video segmentation with sequence-specific object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Firl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Die stroboscopischen Scheiben; oder, Optischen Zauberscheiben: Deren Theorie und wissenschaftliche Anwendung</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stampfer</surname></persName>
		</author>
		<idno>1833. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Conditional regression forests for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Digital video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Prentice Hall Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Situational object boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Video Processing and Communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Prentice Hall PTR</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02135</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">He received the Diploma degree in Electrical and Computer Engineering from National Technical University of Athens (NTUA) in 2014. He worked as undergraduate research assistant in the Signal Processing and Computer Vision group of NTUA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevis-Kokitsi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Maninis is a PhD candidate at ETHZ, Switzerland, in Prof</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">He received the degree in Electrical Engineering and the M.Sc. in Telecommunications Engineering from the Universitat Politècnica de Catalunya, BarcelonaTech (UPC)</title>
	</analytic>
	<monogr>
		<title level="m">2014. His research interest include computer vision with special focus on video object segmentation and deep learning</title>
		<meeting><address><addrLine>New Jersey (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Sergi Caelles is a Ph.D. candidate at ETHZ, Switzerland, in Prof. Luc Van Gool&apos;s Computer Vision Lab</orgName>
		</respStmt>
	</monogr>
	<note>He worked at Bell Laboratories</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Luc Van Gool&apos;s Computer Vision Lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Switzerland</forename><surname>Ethz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sc in Physics from the University of Science and Technology of China (USTC) in 2013, and M.Sc in Electrical Engineering and Information Technology from ETH Zürich in 2015. His research interests lie in deep learning for semantic segmentation and object detection</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>He received a B</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Luc Van Gool&apos;s Computer Vision Lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Switzerland</forename><surname>Ethz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He received the degree in Mathematics in 2008, the degree in Electrical Engineering in 2008, the M.Sc. in Research on Information and Communication Technologies in 2010, and the Ph.D with honors in 2014; all from the Universitat Politècnica de Catalunya, BarcelonaTech (UPC). He worked at Disney Research</title>
		<meeting><address><addrLine>Zürich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Jordi Pont-Tuset is a post-doctoral researcher at</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
