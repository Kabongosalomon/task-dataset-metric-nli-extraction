<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning by Cross-Modal Audio-Video Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
							<email>humam.alwassel@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country>KAUST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
							<email>dhruvm@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
							<email>bkorbar@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
							<email>torresani@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country>KAUST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning by Cross-Modal Audio-Video Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual and audio modalities are highly correlated, yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose Cross-Modal Deep Clustering (XDC), a novel selfsupervised method that leverages unsupervised clustering in one modality (e.g., audio) as a supervisory signal for the other modality (e.g., video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC outperforms single-modality clustering and other multi-modal variants. XDC achieves state-of-the-art accuracy among self-supervised methods on multiple video and audio benchmarks. Most importantly, our video model pretrained on large-scale unlabeled data significantly outperforms the same model pretrained with full-supervision on ImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first self-supervised learning method that outperforms large-scale fully-supervised pretraining for action recognition on the same architecture. * Work done during an internship at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Do we need to explicitly name the actions of "laughing" or "sneezing" in order to recognize them? Or can we learn to visually classify them without labels by associating characteristic sounds with these actions? Indeed, a wide literature in perceptual studies provides evidence that we rely heavily on hearing sounds to make sense of actions and dynamic events in the visual world. For example, objects moving together are perceived as bouncing off each other when the visual stimulus is accompanied by a brief sound <ref type="bibr" target="#b57">[58]</ref>, and the location and timing of sounds are leveraged as important cues to direct our spatiotemporal visual attention <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43]</ref>. The influence of hearing sounds in visual perception is also suggested by perceptual studies showing that individuals affected by profound deafness exhibit poorer visual perceptual performance compared to age-matched hearing controls <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>In this work, we investigate the hypothesis that spatiotemporal models for action recognition can be reliably pretrained from unlabeled videos by capturing cross-modal information from audio and video. The motivation for our study stems from two fundamental challenges facing a fully-supervised line of attack to learning video models. The first challenge is the exorbitant cost of scaling up the size of manually-labeled video datasets. The recent creation of large-scale action recognition datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> has undoubtedly enabled a major leap forward in video models accuracies. However, it may be argued that additional significant gains by dataset growth would require scaling up existing labeled datasets by several orders of magnitude. The second challenge is posed by the unclear definition of suitable label spaces for action recognition. Recent video datasets differ substantially in their label spaces, which range from sports actions <ref type="bibr" target="#b25">[26]</ref> to verb-noun pairs for kitchen activities <ref type="bibr" target="#b6">[7]</ref>. This suggests that the definition of the "right" label space for action recognition, and more generally for video understanding, is still very much up for debate. It also implies that finetuning models pretrained on large-scale labeled datasets is a suboptimal proxy for learning models for small-or medium-size datasets due to the label-space gap often encountered between source and target datasets. In this paper, we present three approaches for training video models from self-supervised audio-visual information. At a high-level, the idea behind all three frameworks is to leverage one modality (say, audio) as a supervisory signal for the other (say, video). We posit that this is a promising avenue because of the simultaneous synergy and complementarity of audio and video: correlations between these two modalities make it possible to perform prediction from one to the other, while their intrinsic differences make cross-modal prediction an enriching self-supervised task compared to within-modality learning. Specifically, we adapt the single-modality DeepCluster work of Caron et al. <ref type="bibr">[6]</ref> to our multi-modal setting. DeepCluster was introduced as a self-supervised procedure for learning image representation. It alternates between unsupervised clustering of image features and using these cluster assignments as pseudo-labels to revise the image representation. In our work, the clusters learned from one modality are used as pseudo-labels to refine the representation for the other modality. In two of our approaches-Multi-Head Deep Clustering (MDC) and Concatenation Deep Clustering (CDC)-the pseudo-labels from the second modality are supplementary, i.e., they complement the pseudo-labels generated in the first modality. The third approach-Cross-Modal Deep Clustering (XDC)-instead uses the pseudo-labels from the other modality as an exclusive supervisory signal. This means that in XDC, the audio clusters drive the learning of the video representation and vice versa. Our experiments support several interesting conclusions: • All three of our cross-modal methods yield representations that generalize better to the downstream tasks of action recognition and audio classification, compared to their within-modality counterparts. • XDC (i.e., the cross-modal deep clustering relying on the other modality as an exclusive supervisory signal) outperforms all the other approaches. This underscores the complementarity of audio and video and the benefits of learning label-spaces across modalities. • Self-supervised cross-modal learning with XDC on a large-scale video dataset yields an action recognition model that achieves higher accuracy when finetuned on HMDB51 or UCF101, compared to that produced by fully-supervised pretraining on Kinetics. To the best of our knowledge, this is the first method to demonstrate that self-supervised video representation learning outperforms large-scale fully-supervised pretraining for action recognition. Moreover, unlike previous self-supervised methods that are only pretrained on curated data (e.g., Kinetics <ref type="bibr" target="#b26">[27]</ref> without action labels), we also report results of XDC pretrained on a large-scale uncurated video dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Early unsupervised representation learning. Pioneering works include deep belief networks <ref type="bibr" target="#b20">[21]</ref>, autoencoders <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b66">67]</ref>, shift-invariant decoders <ref type="bibr" target="#b52">[53]</ref>, sparse coding algorithms <ref type="bibr" target="#b32">[33]</ref>, and stacked ISAs <ref type="bibr" target="#b31">[32]</ref>. While these approaches learn by reconstructing the input, our approach learns from a self-supervised pretext task by generating pseudo-labels for supervised learning from unlabeled data. Self-supervised representation learning from images and videos. Several pretext tasks exploit image spatial context, e.g., by predicting the relative position of patches <ref type="bibr" target="#b7">[8]</ref> or solving jigsaw puzzles <ref type="bibr" target="#b40">[41]</ref>. Others include creating image classification pseudo-labels (e.g., through artificial rotations <ref type="bibr" target="#b12">[13]</ref> or clustering features <ref type="bibr">[6]</ref>), colorization <ref type="bibr" target="#b80">[81]</ref>, inpainting <ref type="bibr" target="#b46">[47]</ref>, motion segmentation <ref type="bibr" target="#b45">[46]</ref>, and instance counting <ref type="bibr" target="#b41">[42]</ref>. Some works have extended image pretext tasks to video <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b78">79]</ref>. Other video pretext tasks include frame ordering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b77">78]</ref>, predicting flow or colors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b69">70]</ref>, exploiting region correspondences across frames <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>, future frame prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>, and tracking <ref type="bibr" target="#b76">[77]</ref>. Unlike this prior work, our model uses two modalities: video and audio. Cross-modal learning and distillation. Several works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref> train a fully-supervised encoder on one modality and distill its discriminative knowledge to an encoder of a different modality. Other works learn from unlabeled data for a specific target task <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b54">55]</ref>. Unlike these methods, our work is purely self-supervised and aims at learning representations that transfer well to a wide range</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Modality Deep Clustering (SDC)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Deep Clustering (MDC) Cross-Modal Deep Clustering (XDC) Concatenation Deep Clustering (CDC)</head><p>! " # "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Frames</head><p>Audio Signal  These features, or their concatenations, are clustered using k-means. The cluster assignments are then used as pseudo-labels to train the encoders. We start with randomlyinitialized encoders, then alternates between clustering to generate pseudo-labels and training to improve the encoders. The four models employ different ways to cluster features and generate self-supervision signals. Illustration video is from <ref type="bibr" target="#b62">[63]</ref>.</p><p>of downstream tasks. Previous cross-modal self-supervised methods most relevant to our work include audio-visual correspondence <ref type="bibr" target="#b0">[1]</ref>, deep aligned representations <ref type="bibr" target="#b2">[3]</ref>, audio-visual temporal synchronization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref>, contrastive multiview coding <ref type="bibr" target="#b64">[65]</ref>, and learning image representations using ambient sound <ref type="bibr" target="#b44">[45]</ref>. While <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b64">65]</ref> use only a single frame, we use a video clip. Unlike our method, <ref type="bibr" target="#b44">[45]</ref> clusters handcrafted audio features and does not iterate on the pseudo-labels. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref> require constructing positive/negative examples for in-sync and out-of-sync video-audio pairs. This sampling strategy makes these approaches more difficult to scale compared to ours, as many potential out-of-sync pairs can be generated, yielding largely different results depending on the sampling choice <ref type="bibr" target="#b28">[29]</ref>. Recent works, such as MIL-NCE <ref type="bibr" target="#b37">[38]</ref> and CBT <ref type="bibr" target="#b63">[64]</ref>, learn from unlabeled instructional videos using text from ASR, while our approach makes use of the audio signal instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Technical approach</head><p>Here, we briefly discuss previous work on single-modality deep clustering in images <ref type="bibr">[6]</ref>. Then, we introduce our three multi-modal deep clustering frameworks for representation learning <ref type="figure" target="#fig_1">(Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-modality deep clustering</head><p>Caron et al. <ref type="bibr">[6]</ref> proposed DeepCluster for self-supervised representation learning from images. DeepCluster iteratively clusters deep features from a single-modality encoder, and then uses the cluster assignments to train the same encoder to improve its representation. Inspired by the simplicity of this work, our paper studies deep clustering in the large-scale multi-modal setting. For completeness, we summarize DeepCluster details. Let X be the set of unlabeled inputs (e.g., images), E be an encoder that maps an input x ∈ X to a deep feature vector f ∈ R d . DeepCluster iterates between clustering the features F = {f = E(x) | x ∈ X} and discriminative training to improve E using the clustering assignments as pseudo-labels. The process starts with a randomly-initialized E, and only the weights of the classification fc-layer are reset between clustering iterations when the supervision-taxonomy is switched. DeepCluster uses a 2D CNN (e.g. ResNet-50) for E and clusters the features after each epoch using k-means. We refer to DeepCluster as Single-Modality Deep Clustering (SDC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-modal deep clustering</head><p>Contrary to the single-modality case, there exist multiple encoders in a multi-modal setting, each of which encodes a different modality of the input. In our paper, we consider two modalities, the visual and the audio modalities from the unlabeled training video clips. In particular, let X be the set of unlabeled video clips, and E v and E a be the visual and audio encoders, respectively. Let</p><formula xml:id="formula_0">F v = {f v = E v (x) ∈ R dv | x ∈ X} and F a = {f a = E a (x) ∈ R da | x ∈</formula><p>X} be the set of visual and audio deep features produced by the two encoders, respectively. There are different ways we can adapt the deep clustering framework to a multi-modal input. We describe three approaches (MDC, CDC, and XDC) by detailing the steps taken at each deep clustering iteration. Refer to the supplementary material for the implementation differences between SDC and our three approaches.</p><p>Multi-Head Deep Clustering (MDC). This model builds on SDC by adding a second classification head supervised by the other modality. Thus, in this model, each encoder has two classification heads. At each deep clustering iteration, MDC uses the cluster assignments of F v as pseudo-labels for one head and that of F a as pseudo-labels for the other head. Thus, each encoder needs to predict the cluster assignments of its own modality (as in SDC), but also those generated by the other modality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Pretraining datasets. We use four datasets: Kinetics <ref type="bibr" target="#b26">[27]</ref>, AudioSet <ref type="bibr" target="#b9">[10]</ref>, IG-Kinetics <ref type="bibr" target="#b11">[12]</ref>, and IG-Random, which have 240K, 2M, 65M, and 65M training videos, respectively. As our approach is self-supervised, thus the labels from the first three datasets are not used during pretraining. While Kinetics and AudioSet are supervised benchmarks for action recognition and audio classification, IG-Kinetics is a weakly-supervised dataset collected from a social media website using tags related to Kinetics actions. IG-Random is an uncurated dataset of random videos from the same website. Videos are 10-second long in Kinetics and AudioSet and 10-to-60-second long in IG-Kinetics and IG-Random. We filter out around 7K Kinetics videos that have no audio. Furthermore, we randomly sample 240K videos from AudioSet and denote this subset as AudioSet-240K. We generate this subset to have AudioSet data of the same size as Kinetics, in order to study the effects of pretraining with the same data size but on a different data distribution and domain.</p><p>Downstream datasets. We evaluate our pretraining performance on three downstream benchmarks: UCF101 <ref type="bibr" target="#b58">[59]</ref>, HMBD51 <ref type="bibr" target="#b29">[30]</ref>, and ESC50 <ref type="bibr" target="#b48">[49]</ref>, which have 13K, 7K, and 2K examples from 101, 51, and 50 classes, respectively. UCF101 and HMDB51 are action recognition datasets, while ESC50 is a sound classification dataset. UCF101 and HMDB51 have 3 official train/test splits, while ESC50 has 5 splits. We conduct our ablation study (Subsection 4.2) using split-1 of each dataset. We also report our average performance over all splits when we compare with state-of-the-art methods in Section 6.</p><p>Baselines. We consider two baselines: Scratch and Supervised Pretraining (Superv). The first is a randomly-initialized model trained from scratch directly on the downstream task, while the second is a model pretrained in a supervised fashion on a large labeled dataset (e.g., Kinetics) and then finetuned on the downstream task. We note that these two baselines are commonly regarded as the lower and upper bounds to gauge the quality of self-supervised representation learning methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Backbone architectures. We employ R(2+1)D <ref type="bibr" target="#b65">[66]</ref> and ResNet <ref type="bibr" target="#b18">[19]</ref> as E v and E a , respectively. E v 's input is a 3×L×H×W clip, where 3 refers to the RGB channels, L is the number of frames, and H and W are the frame height and width. E a 's input is a Q×P spectrogram image extracted from the audio signal, where Q is the number of MEL filters and P is the number of audio frames.</p><p>Pretraining and evaluation details. We choose the 18-layer variants of R(2+1)D and ResNet encoders. We use clips of L=8 frames for pretraining and finetuning our visual encoder E v . We scale frames such that the smallest dimension is 256 pixels and then random crop images of size 224×224.</p><p>We extract video clips at 30 fps and employ temporal jittering during training. For the audio input, we sample 2 seconds and use Q=40 MEL filters and P =100 audio frames. For inference on the downstream tasks, we uniformly sample 10 clips per testing example and average their predictions to make a video-level prediction. We use only one crop per clip: the center 8×224×224 crop for video and the full 40×100 crop for audio. We provide more details in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>Study 1: Single-modality vs. multi-modal deep clustering. This experiment compares the four models presented in Section 3. We pretrain SDC, MDC, CDC, and XDC on Kinetics and report their performance on the downstream tasks in <ref type="table" target="#tab_1">Table 1</ref>. To better understand XDC, we also conduct a new set of baselines, called same-modality-XDC, where XDC is trained with two encoders defined on the same modality (either visual or audio). Note that all models in this ablation study use the same visual and audio encoders and only differ in the way they use self-supervision. It takes on average 5 to 6 deep clustering iterations for these models to converge. Observations: (I) The four selfsupervised deep clustering models outperform the Scratch baseline on all downstream benchmarks. This shows that our self-supervised pretraining is effective and generalizes well to multiple tasks.</p><p>(II) All multi-modal models (MDC, CDC, and XDC) significantly outperform SDC by up to 12.4%, 7.6%, and 11.5% on UCF101, HMDB51, and ESC50, respectively. This validates the importance of multi-modal modeling compared to single-modality. (III) XDC achieves the best performance across all tasks. What distinguishes XDC from the other models is that each modality encoder in XDC is self-supervised purely by the signal from the other modality. The encoders in CDC, MDC, and SDC all employ a self-supervision signal coming from the same modality. Thus, this suggests that encoders learn better when purely supervised by a different modality. We provide the following intuition on why XDC is better than CDC and MDC. XDC groups samples together when they are similar in one of the two modalities (video to supervise the audio encoder, audio to supervise the visual encoder). Instead, CDC groups samples together only if they are similar according to both the audio and the video modality (to supervise both encoders). Thus, XDC visual and audio clusters allow for more diversity than those of CDC. We hypothesize that this diversity allows XDC to learn richer representations, which translates into better performance on the downstream tasks. Also, recent work <ref type="bibr" target="#b73">[74]</ref> has shown that models trained on different modalities learn and generalize at different speeds, and that training them jointly (as done in MDC which uses two-modality heads) is sub-optimal. We believe that this could contribute to MDC performing worse than XDC, which optimizes for each modality independently. (IV) The same-modality-XDC baselines perform similarly to SDC and are 8-12% worse than multi-modal-XDC. This suggests that cross-modality provides a superior supervisory signal for self-supervised learning and that multi-modal-XDC is the best model not because of its optimization strategy but rather because of the use of the other modality for pseudo-labeling. Given the results of this study, we opt to use only XDC in the rest of the experiments. Finally, to show that XDC works for different backbones, we re-do Study 1 with ResNet3D in the supplementary material.</p><p>Study 2: The number of k-means clusters. This study explores the effects of changing the hyperparameter k in k-means clustering. We pretrain XDC on three datasets, Kinetics, AudioSet-240K, and AudioSet, using k=64, 128, 256, 512, and 1024 clusters ( <ref type="table" target="#tab_2">Table 2)</ref>. Observations: (I) The best k value is not sensitive to the number of semantic labels in the downstream datasets. For example, HMDB51 and ESC50 have about the same number of labels but different best k value. (II) Similarly, the best k value seems uncorrelated with the number of original semantic labels of the pretraining dataset, e.g. 400 in Kinetics. We reiterate here that our approach is self-supervised and does not use the labels of the pretraining dataset. (III) The best k value tends to get larger as the pretraining data size increases. For example, the best k for HMDB51 shifts from 128 to 256 when moving from pretraining on AudioSet-240K to the full AudioSet. We hypothesize that there is a more diverse sample set to cluster when the pretraining data size increases. Thus, we can have more fine-grained clusters (higher k) and make our self-supervised classification problem harder. This aligns with previous self-supervised works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref> that showed benefits from making the pretext task harder.   In addition to video datasets, we also experiment with ImageNet to understand how much action recognition benefits from supervised pretraining on object classification. For ImageNet, we inflate the images into static video clips (repeating the same frame) and pretrain our video model on this dataset. <ref type="table" target="#tab_3">Table 3</ref> presents the results of this study. Observations: (I) XDC improves across all three downstream tasks as the pretraining data size increases. For example, XDC on HMDB51 improves by 9.8%, 22.2%, and 24.1% when pretrained on AudioSet, IG-Random, and IG-Kinetics, respectively, compared to the results when pretrained on Kinetics. (II) XDC outperforms Kinetics fully-supervised pretraining by 5.1% on HMDB51 and by 0.6% on UCF101. To the best of our knowledge, XDC is the first method to demonstrate that self-supervision can outperform large-scale full-supervision in representation learning for action recognition. (III) The performance of the fully-supervised pretrained model is influenced by the taxonomy of the pretraining data more than the size. For example, supervised-pretraining on Kinetics gives better performance on both UCF101 and HMDB51 compared to supervised-pretraining on AudioSet (which is 8 times larger than Kinetics) and ImageNet. One the other hand, XDC performance is less sensitive to the data type, as it implicitly learns the label space rather than depend on a space manually defined by annotators.</p><p>Study 4: Curated vs. uncurated pretraining data. The overarching goal of self-supervised representation learning is to learn from the massive amounts of unlabeled data. Previous selfsupervised methods have pretrained on videos from supervised (curated) datasets (e.g., Kinetics) without using the labels. However, even without using labels, those videos are still biased due to the sampling distribution (e.g., taxonomy of the curated dataset). To this end, we study the effects of self-supervised representation learning from uncurated data. <ref type="table" target="#tab_4">Table 4</ref> compares XDC pretrained on IG-Kinetics (curated, as videos were tag-retrieved) vs. IG-Random (uncurated) using 1M, 16M, and 65M videos. Observations: (I) Curated pretraining gives better results on UCF101 and HMDB51, while uncurated pretraining is better on ESC50 at large scale. We hypothesize that the bias of IG-Kinetics towards semantics of human actions is the reason behind the positive effect of curation on  UCF101 and HMDB51. However, such bias negatively impacts the performance on ESC50. (II) The performance gap between the curated and uncurated pretraining shrinks significantly as we increase the data size. For example, the performance gap on HMDB51 drops from 5.2% to 2.1% and 1.9% when the pretraining size increases from 1M to 16M and 65M videos, respectively. This implies that XDC can learn meaningful representations from truly uncurated data. To the best of our knowledge, XDC is the first self-supervised method to study pretraining on large-scale uncurated video data.</p><p>Study 5: Full finetuning vs. learning fc-only. Here, we study two approaches for transferring XDC to downstream tasks. Full finetuning: we finetune all parameters of the pretrained encoder on the downstream task. Learning fc-only: we fix the pretrained encoder and learn a linear classifier for the downstream task, i.e., a fully-connected (fc) layer on top of the frozen features. <ref type="table" target="#tab_5">Table 5</ref> compares XDC with the supervised pretrained approaches under these two transfer-learning schemes. Observations: (I) The accuracy of most pretrained models (fully-supervised or self-supervised) degrades, when used as a fixed feature extractor compared to when they are fully-finetuned on the downstream tasks. Nonetheless, the relative performance of XDC compared to supervised pretrained models stays generally the same when fully vs. fc-only finetuned on the downstream task. This suggests that XDC pretraining is useful both as a fixed feature extractor and as a pretraining initialization. (II) XDC as a fixed feature extractor outperforms many fully-finetuned supervised models. For example, fc-only XDC outperforms, by significant margins, the fully-finetuned supervised AudioSet-and ImageNetpretrained models on both UCF101 and HMDB51. (III) We observe that fully-supervised pretraining, followed by fc-only finetuning performs well when the pretraining taxonomy is well aligned with that of the downstream task. For example, pretraining on Kinetics by learning fc-only on HMDB51 and UCF101 gives the best performance. This is expected as the label spaces of HMBD51 and UCF101 overlap largely with that of Kinetics. This suggests that fully-supervised pretraining is more taxonomy/downstream-task dependent, while our self-supervised XDC is taxonomy-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Understanding XDC</head><p>What does XDC actually learn? What semantic signals does the algorithm use to train its encoders? Here, we try to answer these questions by inspecting the k-means clustering results produced by the last iteration of XDC. <ref type="figure">Figure 2</ref> visualizes some audio and video clusters learned by XDC when trained on Kinetics. These clusters are the top 2 audio clusters (left) and the top 2 video clusters (right) ranked by purity w.r.t. Kinetics action labels. More clusters are presented in <ref type="table" target="#tab_6">Table 6</ref>. We observe that the top-purity clusters learned from both modalities exhibit strong semantic coherence. For example, the audio 1st and 8th ranked clusters include concepts related to playing musical instruments that have similar sounds, while the 1st ranked video cluster also groups playing-instrument concepts, but mainly because of their appearance, as the cluster is all about guitars. Other interesting clusters include: grouping by motor-engine sounds (audio #10), by different swimming strokes (video #4), by audio cluster #125, purity: 0.70 audio cluster #105, purity: 0.33 video cluster #27, purity: 0.36 video cluster #48, purity: 0.37 <ref type="figure">Figure 2</ref>: Visualization of XDC clusters on Kinetics videos. The top-2 audio clusters (left) and video clusters (right) in terms of purity w.r.t. the Kinetics labels. Clusters are represented by the 10 closest videos (shown as frames) to their centroid. Interestingly, XDC learned to group "scuba diving" with "snorkeling" (second left, cluster #105) based on audio features and "scuba diving" with "feeding fish" (rightmost, cluster #27) based on visual features. Please refer to our project website for an interactive visualization of all XDC clusters.  different golf shots (video #5), and different cooking activities (video #10). In the bottom-ranked clusters, although the purity w.r.t. Kinetics concepts is low, we still find some coherence, mostly at the scene level: a farm setting in audio #127 ("grooming horse", "milking cow") and gym activities in video #63 ("pull ups", "punching bag"). Many other bottom-ranked clusters appear to lack semantic coherence when viewed through the lens of Kinetics labels. However, one of the motivations behind the design of self-supervised methods is precisely to bypass the hand-design of label spaces, which may not be the optimal ones for general representation learning. Our experiments suggest that the label space learned by XDC yields strong and general audio and video features even though it does not align perfectly with the taxonomies of existing datasets.</p><p>6 State-of-the-art self-supervised learning comparison Experimental setup. Here, training is similar to our ablations except that we re-train our video encoder on the last clustering assignment using 32-frame clips. Then following <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b65">66]</ref>, we finetune on UCF101 and HMDB51 using 32-frame clips for both XDC and the fully-supervised baselines. Inference is similar to our ablations except for using 32-frame clips. For the audio event classification dataset DCASE <ref type="bibr" target="#b61">[62]</ref>, we follow <ref type="bibr" target="#b28">[29]</ref> and extract conv_5 features for 60 uniformly-sampled clips per audio sample and learn a linear SVM. We report the average top-1 accuracy over all splits.</p><p>Video action recognition. <ref type="table">Table 7</ref>(a) compares XDC pretrained on four large-scale datasets against state-of-the-art self-supervised methods, after finetuning on the UCF101 and HMDB51 benchmarks 2 . We also compare against two fully-supervised methods pretrained on ImageNet and Kinetics. Results: (I) XDC pretrained on IG-Kinetics sets new state-of-the-art performance for self-supervised methods on both benchmarks, outperforming Elo [50] by 1.7% on UCF101 and 1.5% on HMDB51. Moreover, XDC significantly outperforms fully-supervised pretraining on Kinetics: by 1.3% on UCF101 and by 3.8% on HMDB51. (II) When directly compared on the same R(2+1)D-18 architecture, XDC pretrained on Kinetics slightly outperforms AVTS <ref type="bibr" target="#b28">[29]</ref> by 0.6% on UCF101 and 0.3% on HMDB51. However, when both methods are pretrained on AudioSet, XDC outperforms AVTS with larger margins: by 3.9% on UCF101 and 5.6% on HMDB51. This shows that XDC scales better than AVTS. To further verify that XDC scales better, we pretrained AVTS on AudioSet-240K using R(2+1)D-18 and got 76.9% and 40.7% for UCF101 and HMDB51 on split-1, showing a smaller margin between XDC and AVTS than when both are pretrained on the full AudioSet (cf. <ref type="table" target="#tab_3">Table 3</ref>).</p><p>Audio event classification. <ref type="table">Table 7</ref>(b) compares XDC pretrained on AudioSet and IG-Random against the state-of-the-art self-supervised methods for audio classification. XDC achieves state-ofthe-art performance on DCASE and competitive results on ESC50 with only a 1.1% gap with <ref type="bibr" target="#b55">[56]</ref>. <ref type="table">Table 7</ref>: State-of-the-art comparison. We report the average top-1 accuracy over the official splits for all benchmarks. (a) Video action recognition: Comparison between XDC with self-supervised and fullysupervised methods on UCF101 and HMDB51. Not only does XDC set new state-of-the-art performance for self-supervised methods, it also outperforms fully-supervised Kinetics and ImageNet pretraining. * For fair comparison with XDC, we report AVTS performance without dense prediction, i.e., we average the predictions of 10 uniformly-sampled clips at inference. † For direct comparison with XDC, we evaluate AVTS using R(2+1)D-18 and 10 uniformly-sampled clips at inference. (b) Audio event classification: We compare XDC with self-supervised methods on ESC50 and DCASE. XDC achieves state-of-the-art performance on DCASE.</p><p>(a) Video action recognition. 78 SoundNet <ref type="bibr" target="#b1">[2]</ref> 88 L 3 -Net <ref type="bibr" target="#b0">[1]</ref> 93 AVTS <ref type="bibr" target="#b28">[29]</ref> 94 XDC (AudioSet) 95 XDC (IG-Random) 95</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">XDC for temporal action localization</head><p>In this section, we further demonstrate that XDC can be useful beyond video and audio classification.</p><p>In particular, we employ the recent G-TAD <ref type="bibr" target="#b79">[80]</ref> action localization algorithm, where we replace the clip features (originally extracted from a TSN <ref type="bibr" target="#b72">[73]</ref> model pretrained on Kinetics) with our XDC features from the R(2+1)D-18 model pretrained on IG-Kinetics or IG-Random. We compare against the features from the R(2+1)D-18 model fully-supervised pretrained on Kinetics. We emphasize that we do not finetune any of the feature extractors used in this experiment. We follow the default hyperparameters setting of G-TAD. <ref type="table" target="#tab_8">Table 8</ref> shows temporal action localization results of G-TAD with different features on THUMOS14 <ref type="bibr" target="#b24">[25]</ref> dataset. It reports the mean Average Precision (mAP) results at different temporal Intersection over Union (tIoU) thresholds. Both XDC variants outperform the fully-supervised features across all tIoU thresholds. This confirms the same trend observed in tasks presented in Section 6 and suggests that XDC can be used for other tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented Cross-Modal Deep Clustering (XDC), a novel self-supervised model for video and audio. XDC outperforms not only existing self-supervised methods but also fully-supervised ImageNet-and Kinetics-pretraining for action recognition. To the best of our knowledge, XDC is the first to show self-supervision outperforming large-scale full-supervision pretraining for action recognition when pretrained on the same architecture and a larger number of uncurated videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact Statement</head><p>Video has become a commonplace in society. Its uses range from entertainment, to communication and teaching. Thus, the learning of semantic representations of video has broad and far-reaching potential applications. The authors do not foresee major ethical issues associated to this work. However, as the proposed approach is self-supervised, it will learn the inherent properties and structure of the training data. Thus, the learned model may exhibit biases intrinsically present in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Optimization challenges</head><p>In this section, we give the details of the full optimization cycle and discuss differences between the single-modality baseline and our multi-modal models.</p><p>Trivial solutions. As discussed in <ref type="bibr">[6]</ref>, SDC may converge to trivial solutions, corresponding to empty clusters or encoder parameterizations, where the classifier predicts the same label regardless of the input. DeepCluster proposes workarounds to tackle these issues, involving reassigning empty cluster centers and sampling training images uniformly over the cluster assignments. While these strategies mitigate the issues, they do not fix the main cause of the problem: SDC learns a discriminative classifier on the same input from which it learns the labels. On the other hand, our multi-modal deep clustering models are less prone to trivial solutions because they learn the discriminative classifier on one modality and obtain the labels from a different modality. In our training, we never encountered the issue of empty clusters or few-class predictions for any of our multi-modal clustering approaches.</p><p>Initialization and convergence. Our initial pseudo-labels come from clustering features of randomlyinitialized encoders. Such pseudo-labels are "good enough" to capture some weak similarities between the input samples as features from randomly-weighted networks have shown decent results on image and audio classification <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b56">57]</ref>. Another potential option involves generating the initial pseudo-labels by clustering hand-crafted features, e.g. iDT <ref type="bibr" target="#b70">[71]</ref> and audio spectrograms. Hand-crafted features capture low-level semantics that may help the encoders learn better or faster. Indeed, in small-scale experiments, we observed that clustering handcrafted features in the initial iteration reduces the number of clustering iterations needed to learn a well-performing encoder. However, we decided to not pursue this further, since these features are computationally expensive to extract and thus are not suitable for large-scale training on millions of examples. Furthermore, handcrafted features may bias the learning to reflect the design choices behind these manually-engineered descriptors.</p><p>Clustering and optimization schedule. Following previous work <ref type="bibr">[6]</ref>, we cluster the deep features using the k-means algorithm primarily for its desirable properties of efficiency and scalability. The number of k-means clusters is a key hyperparameter in our framework. Intuitively, using more clusters makes the pretext task harder, as it increases the number of pseudo-classes the classifier must recognize. On the other hand, the diversity of samples to cluster effectively dictates the maximum k, for which the grouping is still sensible. Taking into account these factors, we explore the effects of k in our ablation study in Subsection 4.2 of the main manuscript. Another important hyperparameter of our framework is the number of training epochs for the encoders, before re-clustering the learned features. DeepCluster re-clusters after each epoch, which is an expensive design choice when scaling to millions of training samples. Thus, we choose to fix the pseudo-labels and train the encoders until the validation loss for predicting the pseudo-labels saturates. Then, we re-cluster the newly learned features, reassign pseudo-labels, reset the classification layer, and repeat the same process. We find this strategy to be more efficient, as it reduces the number of times we need to invoke k-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Learning using audio rather than text from ASR</head><p>We note that while our approach was demonstrated by leveraging audio, the method is general and is easy to adapt to other modalities, including text. While video and text are semantically correlated, audio and video are temporally correlated. Thus, these two form of correlations are likely to provide different forms of self-supervision, potentially leading to further gains when used in combination. A disadvantage of text from ASR is that it is only available for videos with speech. Audio provides information about environmental sounds beyond speech (e.g. walking steps, playing guitar, and dog barking) and allows us to train on uncurated datasets of arbitrary Web videos, as we demonstrated with IG-Random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters and training details</head><p>Training. We train our models using caffe2 with distributed SGD on a GPU cluster, and employ the warmup scheme proposed in <ref type="bibr" target="#b13">[14]</ref>. The main training parameters are presented in <ref type="table" target="#tab_9">Table 9</ref>. We note that the epoch size can be different from the actual number of videos. This is because the total  Pretraining parameters. We pretrain XDC and other baselines using the parameters described in <ref type="table" target="#tab_1">Table 10</ref>. Early stopping is used for pretraining on small datasets such as Kinetics <ref type="bibr" target="#b26">[27]</ref> and AudioSet <ref type="bibr" target="#b9">[10]</ref> to stop before the model starts overfitting on the pretext task. For IG-Kinetics <ref type="bibr" target="#b11">[12]</ref> and IG-Random, we do not observe overfitting. We pretrain XDC on IG-Kinetics and IG-Random longer in the last deep clustering iteration (denoted as IG-Kinetics* and IG-Random* in <ref type="table" target="#tab_1">Table 10</ref>). When pretraining our R(2+1)D on longer clips (e.g. 32 frames), due to the GPU memory limit, we reduce the mini-batch size to 8 (instead of 32) and the base learning rate to 0.0025 (instead of 0.01).</p><p>Finetuning parameters. We provide finetuning hyperparameters in <ref type="table" target="#tab_1">Table 11</ref>. Different pretraining methods may have different optimal base learning rate when finetuned on downstream tasks. Thus to make a fair comparison, we cross-validate the finetuning using the same set of base learning rates (presented in <ref type="table" target="#tab_1">Table 12</ref>) and report the best result for each pretraining method. As we observed that higher learning rates tend to be beneficial when learning FC-only, we use a wider set of learning rates to cross-validate FC-only models. As done during pretraining, when finetuning R(2+1)D on longer clips (i.e. 32 frames), we reduce the mini-batch size to 8 and reduce the base learning rate to 1/4 of its original rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D XDC using a different backbone architecture</head><p>We pretrain XDC on Kinetics with ResNet3D-18 as the visual backbone and keep the same audio encoder (ResNet-18). The results are compared with those of baselines in <ref type="table" target="#tab_1">Table 13</ref>. XDC with the ResNet3D-18 backbone outperforms the training from scratch baseline by good margins on three downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional qualitative results</head><p>XDC clusters. <ref type="table" target="#tab_1">Tables 14 and 15</ref> present the top and bottom 10 audio and video clusters learned with XDC on Kinetics, ranked by their purity with respect to Kinetics labels. We list the 5 most frequent concepts of each cluster.  <ref type="table" target="#tab_1">Table 12</ref>: Finetuning base learning rates. For a fair comparison, we cross-validate all pretraining methods with the same set of base learning rates. We report the best finetuning result for each method. Learning FC-only benefits from cross-validation with a wider range of base learning rates. Setup Base learning rates Full 0.001, 0.002, 0.004, 0.006, 0.008, 0.01 FC only 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04 <ref type="table" target="#tab_1">Table 13</ref>: XDC using a different backbone. We present the results of XDC on a different backbone, ResNet3D-18, for the visual encoder. We compare XDC pretrained on Kinetics vs. the two baselines: Scratch and fully-supervised Kinetics-pretraining (Superv) for the same backbone. We report the top-1 accuracy on split- XDC filters. <ref type="figure" target="#fig_2">Figure 3</ref> visualizes and compares conv_1 spatial and temporal filters of R(2+1)D learned by self-supervised XDC pretraining on IG-Kinetics versus fully-supervised pretraining on Kinetics. We observe some differences in both spatial and temporal filters between XDC and fully-supervised pretraining. In particular, XDC learns a more diverse set of motion filters.   For each, we list the 5 concepts with the highest purity (given in parentheses).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our framework. We present Single-Modality Deep Clustering (SDC) baseline vs. our three multi-modal deep clustering models: Multi-Head Deep Clustering (MDC), Concatenation Deep Clustering (CDC), and Cross-Modal Deep Clustering (XDC). The video and audio encoders (Ev and Ea) map unlabeled videos to visual and audio features (fv and fa).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Study 3 :</head><label>3</label><figDesc>Pretraining data type and size. Here, we investigate the effects of two pretraining characteristics: data size and type. To this end, we pretrain XDC on Kinetics (240K examples), AudioSet-240K (240K examples), AudioSet (2M examples), IG-Kinetics (65M examples), and IG-Random (65M examples). Kinetics and IG-Kinetics videos are collected originally for activity recognition, while AudioSet videos are aimed for audio event classification. IG-Random is an uncurated/unsupervised dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>#</head><label></label><figDesc>Kinetics concepts 1 play bagpipes(0.70), play harmonica(0.04), play violin(0.03) 2 scuba diving(0.33), snorkeling(0.27), feeding fish(0.11) 8 play cello(0.15), play trombone(0.11), play accordion(0.09) 10 mowing lawn(0.14), driving tractor(0.09), motorcycling(0.06) 127 abseiling(0.01), grooming horse(0.01), milking cow(0.01) 128 washing feet(0.01), motorcycling(0.01), headbanging(0.01) # Kinetics concepts 1 play bass guitar(0.37), play guitar(0.16), tap guitar(0.15) 4 swim backstroke(0.21), breast stroke(0.16), butterfly stroke(0.1) 5 golf putting(0.18), golf chipping(0.11), golf driving(0.05) 10 cook chicken(0.11), barbeque(0.07), fry vegetables(0.06) 63 pull ups(0.01), gymnastics tumbling(0.01), punching bag(0.01) 64 capoeira(0.01), riding elephant(0.01), feeding goats(0.01)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b69">70)</ref>, playing 2harmonica(0.04), playing violin(0.03), playing accordion(0.02), marching(0.01)<ref type="bibr" target="#b1">2</ref> scuba diving(0.33), snorkeling(0.<ref type="bibr" target="#b26">27</ref>), feeding fish(0.11), canoeing or kayaking(0.02), jumping into pool(0.02) 3 playing cymbals(0.21), playing drums(0.17), marching(0.03), air drumming(0.02), drumming fingers(0.02) 4 passing American football(0.17), play kickball(0.06), catching or throwing softball(0.05), kick field goal(0.02), sled dog racing(0.02) 5 presenting weather forecast(0.17), playing poker(0.05), testifying(0.03), tying knot (not on a tie)(0.02), golf putting(0.02) 6 hurling (sport)(0.17), swimming backstroke(0.05), skiing slalom(0.04), vault(0.03), ski jumping(0.02) 7 presenting weather forecast(0.15), news anchoring(0.05), filling eyebrows(0.02), braiding hair(0.02), tossing salad(0.02) 8 playing cello(0.15), playing trombone(0.11), playing accordion(0.09), playing harp(0.07), playing clarinet(0.06) 9 playing recorder(0.14), playing violin(0.12), playing trumpet(0.08), playing harmonica(0.07), tapping guitar(0.06) 10 mowing lawn(0.14), driving tractor(0.09), motorcycling(0.06), blowing leaves(0.04), water skiing(0.04) 119 side kick(0.02), front raises(0.01), dunking basketball(0.01), smoking(0.01), high kick(0.01) 120 clay pottery making(0.02), crawling baby(0.02), brushing teeth(0.01), playing harmonica(0.01), eating spaghetti(0.01) 121 pushing cart(0.01), hula hooping(0.01), high kick(0.01), blowing out candles(0.01), bench pressing(0.01) 122 shot put(0.01), feeding birds(0.01), squat(0.01), push up(0.01), high jump(0.01) 123 opening present(0.01), petting cat(0.01), pushing cart(0.01), washing dishes(0.01), punching bag(0.01) 124 trimming or shaving beard(0.01), petting cat(0.01), front raises(0.01), massaging back(0.01), tai chi(0.01) 125 feeding birds(0.01), tobogganing(0.01), riding elephant(0.01), feeding goats(0.01), jumping into pool(0.01) 126 climbing tree(0.01), writing(0.01), archery(0.01), brushing hair(0.01), shining shoes(0.01) 127 abseiling(0.01), grooming horse(0.01), milking cow(0.01), feeding goats(0.01), juggling balls(0.01) 128 washing feet(0.01), motorcycling(0.01), headbanging(0.01), cheerleading(0.01), krumping(0.01)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>spatial and temproal ltes learned by Kinetics fully supervision. b) conv1 spatial and temporal lters learned by IG65M self-supervised XDC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>R(2+1)D filters learned with self-supervised XDC vs. fully-supervised training. (a) R(2+1)D conv_1 filters learned by fully-supervised training on Kinetics. (b) The same filters learned by self-supervised XDC pretraining on IG-Kinetics. XDC learns a more diverse set of temporal filters compared to fully-supervised pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Single-modality vs. multi-modal deep clustering. We compare the four self-supervised deep clustering models (Section 3) and the three baselines: Scratch, Supervised Pretraining (Superv), and samemodality-XDC (XDC with two encoders defined on the same modality). Models are pretrained via selfsupervision on Kinetics and fully finetuned on each downstream dataset. We report the top-1 accuracy on split-1 of each dataset. All multi-modal models significantly outperform the single-modality deep clustering model. We mark in bold the best and underline the second-best models.</figDesc><table><row><cell>same-modality-XDC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The number of clusters (k). We show the effect of the number of k-means clusters on XDC performance. XDC is pretrained on three large datasets, and then fully finetuned on three downstream tasks. We report the top-1 accuracy on split-1. The best k value increases as the size of the pretraining dataset increases.</figDesc><table><row><cell>Pretraining</cell><cell>Downstream</cell><cell></cell><cell></cell><cell>k</cell><cell></cell></row><row><cell>Dataset</cell><cell>Dataset</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512 1024</cell></row><row><cell>Kinetics (240K videos)</cell><cell>UCF101 HMDB51 ESC50</cell><cell cols="4">73.8 73.1 74.2 74.0 72.6 36.5 39.0 38.3 37.7 37.7 78.0 76.3 75.0 74.5 71.5</cell></row><row><cell>AudioSet-240K (240K videos)</cell><cell>UCF101 HMDB51 ESC50</cell><cell cols="4">77.4 77.2 76.7 77.1 75.3 41.3 42.6 41.6 40.6 40.7 78.5 77.8 77.3 76.8 73.5</cell></row><row><cell>AudioSet (2M videos)</cell><cell>UCF101 HMDB51 ESC50</cell><cell cols="4">84.1 84.3 84.9 84.4 84.2 47.4 47.6 48.8 48.5 48.4 84.8 85.8 85.0 84.5 83.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pretraining data type and size. We compare XDC pretrained on five datasets vs. fully-supervised pretrained baselines (Superv). XDC significantly outperforms fully-supervised pretraining on HMDB51.</figDesc><table><row><cell></cell><cell>Pretraining</cell><cell></cell><cell cols="2">Downstream Dataset</cell><cell></cell></row><row><cell cols="2">Method Dataset</cell><cell>Size</cell><cell cols="3">UCF101 HMDB51 ESC50</cell></row><row><cell cols="2">Scratch None</cell><cell>0</cell><cell>54.5</cell><cell>24.1</cell><cell>54.3</cell></row><row><cell>Superv</cell><cell>ImageNet</cell><cell>1.2M</cell><cell>79.9</cell><cell>44.5</cell><cell>NA</cell></row><row><cell>Superv</cell><cell>Kinetics</cell><cell>240K</cell><cell>90.9</cell><cell>58.0</cell><cell>82.3</cell></row><row><cell>Superv</cell><cell cols="2">AudioSet-240K 240K</cell><cell>76.6</cell><cell>40.8</cell><cell>78.3</cell></row><row><cell>Superv</cell><cell>AudioSet</cell><cell>2M</cell><cell>84.0</cell><cell>53.5</cell><cell>90.3</cell></row><row><cell>XDC</cell><cell>Kinetics</cell><cell>240K</cell><cell>74.2</cell><cell>39.0</cell><cell>78.0</cell></row><row><cell>XDC</cell><cell cols="2">AudioSet-240K 240K</cell><cell>77.4</cell><cell>42.6</cell><cell>78.5</cell></row><row><cell>XDC</cell><cell>AudioSet</cell><cell>2M</cell><cell>84.9</cell><cell>48.8</cell><cell>85.8</cell></row><row><cell>XDC</cell><cell>IG-Random</cell><cell>65M</cell><cell>88.8</cell><cell>61.2</cell><cell>86.3</cell></row><row><cell>XDC</cell><cell>IG-Kinetics</cell><cell>65M</cell><cell>91.5</cell><cell>63.1</cell><cell>84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Curated vs. uncurated pretraining data. XDC pretrained on IG-Kinetics (curated) vs. IG-Random (uncurated) using different training set sizes. Uncurated pretraining has better results on ESC at large scale. On UCF and HMDB, the accuracy gap between curated and uncurated pretraining decreases as data size increases. Random IG-Kinetics ∆ IG-Random IG-Kinetics ∆ IG-Random IG-Kinetics ∆</figDesc><table><row><cell cols="2">Downstream Dataset IG-UCF101 79.6</cell><cell>1M 84.2</cell><cell>-4.6</cell><cell cols="2">Pretraining Size 16M 84.1 87.6</cell><cell>-3.5</cell><cell>88.8</cell><cell>65M 91.5</cell><cell>-2.7</cell></row><row><cell>HMDB51</cell><cell>45.1</cell><cell>50.3</cell><cell>-5.2</cell><cell>55.2</cell><cell>57.3</cell><cell>-2.1</cell><cell>61.2</cell><cell>63.1</cell><cell>-1.9</cell></row><row><cell>ESC50</cell><cell>77.8</cell><cell>79.5</cell><cell>-1.7</cell><cell>84.3</cell><cell>82.5</cell><cell>+1.8</cell><cell>86.3</cell><cell>84.8</cell><cell>+1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Full finetuning vs. learning fc-only. We compare XDC against the supervised pretrained models (Superv) under two transfer-learning schemes: when models are used as features extractor ('fc' column) or as a finetuning initialization ('all' column). XDC fixed features outperform several fully-finetuned Superv models.</figDesc><table><row><cell>Method</cell><cell>Pretraining Dataset</cell><cell cols="2">UCF101 fc all</cell><cell cols="2">HMDB51 fc all</cell><cell cols="2">ESC50 fc</cell><cell>all</cell></row><row><cell>Random</cell><cell>None</cell><cell>6.0±1.0</cell><cell cols="6">54.5 7.5±0.6 24.1 61.3±2.5 54.3</cell></row><row><cell>Superv</cell><cell>ImageNet</cell><cell>74.5</cell><cell>79.9</cell><cell>42.8</cell><cell>44.5</cell><cell>NA</cell><cell></cell><cell>NA</cell></row><row><cell>Superv</cell><cell>Kinetics</cell><cell>89.7</cell><cell>90.9</cell><cell>61.5</cell><cell>58.0</cell><cell>79.5</cell><cell cols="2">82.3</cell></row><row><cell>Superv</cell><cell>AudioSet</cell><cell>80.2</cell><cell>84.0</cell><cell>51.6</cell><cell>53.5</cell><cell>88.5</cell><cell cols="2">90.3</cell></row><row><cell>XDC</cell><cell>IG-Random</cell><cell>80.7</cell><cell>88.8</cell><cell>49.9</cell><cell>61.2</cell><cell>84.5</cell><cell cols="2">86.3</cell></row><row><cell>XDC</cell><cell>IG-Kinetics</cell><cell>85.3</cell><cell>91.5</cell><cell>56.0</cell><cell>63.1</cell><cell>84.3</cell><cell cols="2">84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>XDC clusters. Top and bottom audio (left) and video (right) XDC clusters ranked by clustering purity w.r.t. Kinetics labels. For each cluster, we list the three concepts with the highest purity (given in parentheses).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Temporal action localization on THUMOS14. We compare G-TAD algorithm using our XDC features vs. using the fully-supervised Kinetics-pretrained (Superv) features. We report the mean Average Precision (mAP) results at different temporal Intersection over Union (tIoU) thresholds. Both XDC variants outperform the fully-supervised features across all tIoU thresholds.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">mAP @ tIoU</cell><cell></cell></row><row><cell>Features Type</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>Superv (Kinetics)</cell><cell cols="5">50.9 44.4 36.6 28.4 19.8</cell></row><row><cell cols="6">XDC (IG-Random) 51.5 44.8 36.9 28.6 20.0</cell></row><row><cell cols="6">XDC (IG-Kinetics) 51.5 44.9 37.2 28.7 20.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Training parameter definitions. The abbreviations and descriptions of each training parameters.</figDesc><table><row><cell>Abv.</cell><cell>Name</cell><cell>Description</cell></row><row><cell>es</cell><cell>epoch size</cell><cell>The total number of examples the</cell></row><row><cell></cell><cell></cell><cell>model trains on in one epoch.</cell></row><row><cell>bs</cell><cell>batch size</cell><cell>The size of a mini-batch.</cell></row><row><cell>lr</cell><cell>base lr</cell><cell>The initial learning rate.</cell></row><row><cell>we</cell><cell cols="2">warmup epoch The number of epochs used for</cell></row><row><cell></cell><cell></cell><cell>warmup [14].</cell></row><row><cell>se</cell><cell>step epoch</cell><cell>Every se epochs, the learning rate</cell></row><row><cell>γ</cell><cell>lr decay</cell><cell>is decayed by multiplying with γ.</cell></row><row><cell>te</cell><cell>total epoch</cell><cell>The training lasts for te epochs.</cell></row><row><cell>wd</cell><cell>weight decay</cell><cell>The weight decay used in SGD.</cell></row><row><cell cols="2">e-stop early stop</cell><cell>Stop training when validation loss</cell></row><row><cell></cell><cell></cell><cell>is increased in 3 consecutive epochs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Pretraining parameters. We use early-stopping for Kinetics and AudioSet since we observe some overfiting on the pretext tasks. For the last iteration of XDC on IG-Kinetics and IG-Random, we pretrain XDC 3x longer (iteration denoted as IG-Kinetics* and IG-Random* in this table). γ is set to 0.01 for all settings.</figDesc><table><row><cell>method</cell><cell>dataset</cell><cell>es</cell><cell>bs lr</cell><cell>we/se/te</cell><cell>wd</cell><cell>e-stop</cell></row><row><cell>Superv</cell><cell>Kinetics</cell><cell>1M</cell><cell cols="4">32 0.01 10/10/45 10 −4 no</cell></row><row><cell>Superv</cell><cell>AudioSet</cell><cell>2M</cell><cell cols="4">32 0.04 10/20/45 10 −5 no</cell></row><row><cell cols="2">All DCs Kinetics</cell><cell>1M</cell><cell cols="4">32 0.01 10/10/30 10 −4 yes</cell></row><row><cell cols="2">All DCs AudioSet</cell><cell>2M</cell><cell cols="4">32 0.01 10/10/45 10 −4 yes</cell></row><row><cell cols="2">All DCs IG-Kinetics &amp; IG-Random</cell><cell cols="3">10M 32 0.01 1/3/10</cell><cell cols="2">10 −4 no</cell></row><row><cell cols="5">All DCs IG-Kinetics* &amp; IG-Random* 10M 32 0.01 0/9/30</cell><cell cols="2">10 −4 no</cell></row><row><cell cols="7">number of clips the model sees during training (with temporal jittering) can be larger than the number</cell></row><row><cell>of videos.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Finetuning parameters. Different pretraining methods have different ranges of optimal base learning rate when finetuning on downstream tasks. Thus, we cross-validate all methods with the same set of base learning rates and report the best result for each method. γ is set to 0.01 for all settings.</figDesc><table><row><cell>dataset</cell><cell>es</cell><cell cols="2">bs we/se/te wd</cell><cell>e-stop</cell></row><row><cell cols="2">HMDB51 40K</cell><cell>32 2/2/8</cell><cell cols="2">0.005 no</cell></row><row><cell>UCF101</cell><cell cols="2">106K 32 2/2/8</cell><cell cols="2">0.005 no</cell></row><row><cell>ESC50</cell><cell>20K</cell><cell>32 2/2/8</cell><cell cols="2">0.005 no</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>XDC audio clusters. Top and bottom 10 XDC audio clusters ranked by clustering purity w.r.t.Kinetics labels. For each, we list the 5 concepts with the highest purity (given in parentheses).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>XDC video clusters. Top and bottom 10 XDC video clusters ranked by clustering purity w.r.t. Kinetics labels.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All XDC pretrained models are publicly released on our project website.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Mengmeng Xu for his valuable help with the THUMOS14 experiments. The authors appreciate the anonymous NeurIPS reviewers for their constructive feedback. Humam Alwassel was partially supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2017-3405.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Torralba</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Read</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00932</idno>
		<title level="m">Deep aligned representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Academic achievement test performance of hearing-impaired students. united states, spring, 1969.(series d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Difrancesca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gallaudet university. Center for Assessment and Demographic Studies</title>
		<meeting><address><addrLine>washington, dc</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evolution of Sound Localization in Mammals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rickye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">E</forename><surname>Heffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="691" to="715" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Slow and steady feature analysis: higher order temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Auditory scene classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Toub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AASP Challenge</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The psychology of deafness: Sensory deprivation, learning, and adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Helmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Myklebust</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>Grune &amp; Stratton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Attention and Brain Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Risto Näätänen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Environmental sound classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLSP</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Randomly weighted cnns for (music) audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Histogram of gradients of time-frequency representations for audio scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Gasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recurrence quantification analysis features for environmental sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waldo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASPAA</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised audio-visual co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hardik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dharmesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemant A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bipin</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Crossmodal influences on visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladan</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Life Reviews</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01, 2012. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Detection and classification of acoustic scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1733" to="1746" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">This violinist plays beautiful music St</title>
		<ptr target="https://creativecommons.org/licenses/by/3.0.3" />
		<imprint>
			<pubPlace>Petersburg, Russia</pubPlace>
		</imprint>
	</monogr>
	<note>Street Performers</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Selfsupervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">What makes training multi-modal classification networks hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">playing ukulele(0.09) 2 scuba diving(0.36), snorkeling(0.32), feeding fish(0.10), diving cliff(0.02), jumping into pool(0.02) 3 presenting weather forecast(0.26), playing poker(0.10), news anchoring(0.05), testifying(0.03), giving or receiving award(0.02) 4 swimming backstroke(0.21), swimming breast stroke(0.16), swimming butterfly stroke(0.10)</title>
	</analytic>
	<monogr>
		<title level="m"># Kinetics concepts 1 playing bass guitar(0.37), playing guitar(0.16), tapping guitar(0.15), strumming guitar(0.09)</title>
		<imprint/>
	</monogr>
	<note>play ice hockey(0.04), jump into pool(0.04) 5 golf putting(0.18), golf chipping(0.11), golf driving(0.05), hitting baseball(0.03), archery(0.03</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">playing trombone(0.06), playing piano(0.06), playing accordion(0.05) 9 windsurfing(0.12), jetskiing(0.10), water skiing(0.09), surfing water(0.08), kitesurfing(0.06) 10 cooking chicken(0.11), barbequing(0.07), frying vegetables(0.06), cooking sausages(0.04), making pizza(0.04) 55 yoga(0.02), folding napkins(0.02), doing nails(0.02), cutting watermelon(0.01), writing(0.01) 56 eating spaghetti(0.02), making pizza(0.02), brushing teeth(0.02), blowing out candles(0.02), reading book(0.02) 57 answering questions(0.02), tai chi(0.02), dancing ballet(0.02), dunking basketball(0.02), sign language interpreting(0.01) 58 trimming or shaving</title>
		<idno>finger snapping(0.02) 61 air drumming(0.02</idno>
	</analytic>
	<monogr>
		<title level="m">passing American football</title>
		<imprint/>
	</monogr>
	<note>in game)(0.06), skiing slalom(0.04), playing ice hockey(0.03. playing poker(0.01) 59 punching bag(0.02), blowing out candles(0.02), pumping fist(0.02), dancing gangnam style(0.02), opening present(0.01) 60 feeding goats(0.02), blowing out candles(0.02), milking cow(0.02), arm wrestling(0.02. pumping fist(0.02), pushing cart(0.02), brushing teeth(0.02), eating ice cream(0.01) 62 clean and jerk(0.01), robot dancing(0.01), bench pressing(0.01), side kick(0.01), punching bag(0.01) 63 pull ups(0.01), gymnastics tumbling(0.01), punching bag(0.01), cracking neck(0.01), eating ice cream(0.01) 64 capoeira(0.01), riding elephant(0.01), feeding goats(0.01), feeding birds(0.01), crawling baby(0.01</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
