<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of deep learning models for multi-step ahead time series prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohitash</forename><surname>Chandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">UNSW Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Goyal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<postCode>110016</postCode>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Gupta</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Geology and Geophysics</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<postCode>721302</postCode>
									<settlement>Kharagpur</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of deep learning models for multi-step ahead time series prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recurrent neural networks</term>
					<term>LSTM</term>
					<term>Deep Learning</term>
					<term>Time Series Prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series prediction with neural networks have been focus of much research in the past few decades. Given the recent deep learning revolution, there has been much attention in using deep learning models for time series prediction, and hence it is important to evaluate their strengths and weaknesses. In this paper, we present an evaluation study that compares the performance of deep learning models for multi-step ahead time series prediction. Our deep learning methods compromise of simple recurrent neural networks, long short term memory (LSTM) networks, bidirectional LSTM, encoder-decoder LSTM networks, and convolutional neural networks. We also provide comparison with simple neural networks use stochastic gradient descent and adaptive gradient method (Adam) for training. We focus on univariate and multi-step-ahead prediction from benchmark time series datasets and compare with results from from the literature. The results show that bidirectional and encoder-decoder LSTM provide the best performance in accuracy for the given time series problems with different properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Apart from econometric models, machine learning methods became extremely popular for time series prediction or forecasting in the last few decades <ref type="bibr">[1,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>. Some of the popular categories include one-step, multi-step, and multivariate prediction. Recently some attention has been given to dynamic time series prediction where the size of the input to the model can dynamically change <ref type="bibr" target="#b6">[8]</ref>. Just as the term indicates, one-step prediction refers to the use of a model to make a prediction one-step ahead in time whereas a multi-step prediction refers to a series of steps ahead in time from an observed trend in a time series <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10]</ref>. In the latter case, the prediction horizon defines the extent of future prediction. The challenge is to develop models that produce low prediction errors as the prediction horizon increases given the chaotic nature and noise in the dataset <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b11">13]</ref>. There are two major strategies for multi-step-ahead prediction which include recursive and direct strategies. The recursive strategy features the prediction from a one-step-ahead prediction model as the input for future prediction horizon <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, where error in the prediction for the next horizon is accumulated in future horizons. The direct strategy encodes the multi-step-ahead problem as a multi-output problem <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref> which in the case of neural networks can be represented by multiple neurons in the output later, where each neuron denotes the prediction horizon. The major challenges in multi-step-ahead prediction include highly chaotic time series and those that have missing data which has been approached with non-linear filters and neural networks <ref type="bibr" target="#b16">[18]</ref>.</p><p>Email address: rohitash.chandra@unsw.edu.au (Rohitash <ref type="bibr">Chandra)</ref> Neural networks have been popular for time series prediction for various applications <ref type="bibr" target="#b17">[19]</ref>. Different neural network architectures have different strengths and weaknesses and given that time series prediction is concerned with careful integration of knowledge in temporal sequences according to different dimensions, it is important to choose the right neural network architecture and training algorithm.</p><p>Recurrent neural networks (RNNs) are known to be better suited for modelling temporal sequences <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23]</ref> and are also more suitable for modeling dynamical systems when compared to feedforward networks <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b24">26]</ref>. RNNs have shown to be robust methods for time series prediction <ref type="bibr" target="#b25">[27]</ref>. The Elman RNN <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b26">28]</ref> is one of the earliest architecture trained by backpropagation through-time which is an extension of the backpropagation algorithm for feedforward networks <ref type="bibr" target="#b19">[21]</ref>. The limitations in learning by RNNs for long-term dependencies in sequences that span hundreds or thousands of time-steps <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b28">30]</ref> were addressed by long short-term memory networks (LSTMs) <ref type="bibr" target="#b20">[22]</ref>.</p><p>More recently, with the deep learning revolution <ref type="bibr" target="#b21">[23]</ref>, there has been further improvements such as gated recurrent unit (GRU) <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref> networks, which provides similar performance than LSTMs, but are simpler to implement. Some of the other extensions include predictive state RNNs <ref type="bibr" target="#b31">[33]</ref> that combined RNNs with power of predictive state representations <ref type="bibr" target="#b32">[34]</ref>. Bidirectional RNNs connect two hidden layers of opposite directions to the same output where the output layer can get information from past and future states simultaneously <ref type="bibr" target="#b33">[35]</ref>. The idea was further extended into bidirectional-LSTMs for phoneme classification <ref type="bibr" target="#b34">[36]</ref> which performed better than standard RNNs and LSTMs. More work has been made by combining bidi-rectional LSTMS with convolutional neural networks (CNNs) for natural language processing with problem of named entity recognition <ref type="bibr" target="#b35">[37]</ref>. Further extensions have been made by encoder-decoder LSTM that used a LSTM to map the input sequence to a vector of a fixed dimensionality and used another LSTM to decode the target sequence from the vector for English to French translation task <ref type="bibr" target="#b36">[38]</ref>.</p><p>On the other hand, backpropgation neural network which is also slowly known as shallow learning as opposed to deep learning has a number of improvements for training and generalisation. Deep learning methods such as convolutional neural networks, presented idea of regularisation using dropouts during training that has been helpful for generalisation <ref type="bibr" target="#b37">[39]</ref>. Adaptive gradient methods such as the Adam optimiser has become popular for training shallow networks <ref type="bibr" target="#b38">[40]</ref>. Apart from these, other ways of training feedforward networks such as evolutionary algorithms have been used for time series problems <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b6">8]</ref>. Furthermore, RNNs have also been training by evolutionary algorithms with applications for time series prediction <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43]</ref>.</p><p>Noting these advances, it is important to evaluate the performance for a challenging problem which in our case is multistep time series prediction. We note that limited work has been done comparison done between FNN and RNNs for time series <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b43">45]</ref>. We note that while most of the LSTM applications have been natural language processing and signal processing applications such as phoneme recognition, there is no work that evaluates their performance for time series prediction, particularly multi-step ahead prediction. Since the underlying feature of LSTMs is in handing temporal sequences, it is worthwhile to investigate about their predictive power, i.e. as the prediction horizon increases. The recent advances in technologies such as Tensorflow have improved computational efficiency of RNNs <ref type="bibr" target="#b44">[46]</ref> and enabled easier implementation for providing comprehensive evaluation of their performance.</p><p>In this paper, we present an evaluation study that compares the performance of selected deep learning models for multi-step ahead time series prediction. Our deep learning methods compromise of standard LSTMs, bidirectional LSTMS, encoderdecoder LSTMs, and CNNs. Our shallow learning models use stochastic gradient descent and prominent adaptive gradient method known as Adam optimiser for training. We examine univariate time series prediction with selected models and learning algorithms for benchmark time series datasets. We also compare our results with other related machine learning methods for multi-step time series problems from the literature.</p><p>The rest of the paper is organised as follows. Section 2 presents a background and literature review of related work. Section 3 presents the details of the different deep learning models, and Section 4 presents experiments and results. Section 5 provides a discussion and Section 6 concludes the paper with discussion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-step time series prediction</head><p>The recursive strategy of multi-step-ahead prediction features the predicted value of the current prediction horizon as inputs to next prediction horizon which iterates, hence it is also known as iterated strategy. One of the first attempts for recursive strategy was using state-space Kalman filter and smoothing <ref type="bibr" target="#b45">[47]</ref> followed by recurrent neural networks <ref type="bibr" target="#b46">[48]</ref>. Later, a dynamic recurrent network used current and delayed observations as inputs to the network which reported excellent generalization performance <ref type="bibr" target="#b47">[49]</ref>. Then the non-parametric Gaussian process model was used to incorporate the uncertainty about intermediate regressor values <ref type="bibr" target="#b48">[50]</ref>. The Dempster-Shafer regression technique for prognosis of data-driven machinery used iterative strategy with promising performance <ref type="bibr" target="#b49">[51]</ref>. Lately, reinforced real-time recurrent learning was used with iterative strategy for flood forecasts <ref type="bibr" target="#b10">[12]</ref>.</p><p>The direct strategies for multi-step-ahead prediction feature all the prediction horizons during training typically as multiple outputs. Initial progress was made using recurrent neural networks trained by backpropagation through-time algorithm <ref type="bibr" target="#b11">[13]</ref>. A review of single-output vs. multiple-output approaches showed direct strategy more promising choice over recursive strategy <ref type="bibr" target="#b15">[17]</ref>. A multiple-output support vector regression (M-SVR) achieved better forecasts when compared to standard SVR using direct and iterated strategies <ref type="bibr" target="#b50">[52]</ref>.</p><p>The third strategy features the combination of recursive and direct strategies. Initial work featured multiple SVR models that were trained independently based on the same training data and with different targets <ref type="bibr" target="#b12">[14]</ref>. An optimally pruned extreme learning machine (OP-ELM) was used using recursive, direct and a combination of the two strategies in an ensemble approach where the combination gave better performance than standalone methods <ref type="bibr" target="#b51">[53]</ref>. Chandra et. al <ref type="bibr" target="#b52">[54]</ref> presented a recursive neural network inspired by multi-task learning and cascaded neural networks for multi-step ahead prediction, training by evolutionary algorithm which gave very promising performance when compared to the literature . Ye and Dai <ref type="bibr" target="#b53">[55]</ref> presented a multitask learning method which considers different prediction horizons as different tasks and explores the relatedness among horizons while forecasting them in parallel. The method consistently achieved lower error values over all horizons when compared to other related iterative and direct prediction methods.</p><p>A comprehensive study on the different strategies was given using a large experimental benchmark (NN5 forecasting competition) <ref type="bibr" target="#b54">[56]</ref>, and further comparison for macroeconomic time series where it was reported that the iterated forecasts typically outperformed the direct forecasts <ref type="bibr" target="#b55">[57]</ref> and relative performance of the iterated forecasts improved with the forecast horizon, with further comparison that presented an encompassing representation for derivation auto-regressive coefficients <ref type="bibr" target="#b56">[58]</ref>. A study on the properties shows that direct strategy provides prediction values that are relatively robust to breaks and the benefits increases with the prediction horizon <ref type="bibr" target="#b57">[59]</ref>.</p><p>Some applications of the different machine learning methods that apply multi-step-ahead prediction for real-world problems. These include 1.) auto-regressive models for predicting critical levels of abnormality in physiological signals <ref type="bibr" target="#b58">[60]</ref>, 2.) flood forecasting using recurrent neural networks <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b60">62]</ref>, 3.) emissions of nitrogen oxides using a neural network and related ap-proaches <ref type="bibr" target="#b61">[63]</ref>, 4.) photo-voltaic power forecasting using hybrid support vector machine <ref type="bibr" target="#b62">[64]</ref>, 5.) Earthquake ground motions and seismic response prediction <ref type="bibr" target="#b63">[65]</ref>, and 6. central-processing unit (CPU) load prediction <ref type="bibr" target="#b64">[66]</ref>. More recently, Wu <ref type="bibr" target="#b65">[67]</ref> employed an adaptive-network-based fuzzy inference system with uncertainty quantification the prediction of short-term wind and wave conditions for marine operations. Wang and Li <ref type="bibr" target="#b66">[68]</ref> presented multi-step ahead prediction for wind speed which was based on optimal feature extraction, deep learning with LSTMs, and error correction strategy. The method showed lower error values for one, three and five-step ahead predictions in comparison to related methods. Wang and Li <ref type="bibr" target="#b67">[69]</ref> also presented another hybrid approach to the multi-step ahead wind speed prediction with empirical wavelet transformation for feature extraction, autoregressive fractionally integrated moving average to detect long memory characteristics and swarm-based backpropagation neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep learning: LSTM and applications for time series</head><p>Deep learning naturally features robust spacial temporal information processing <ref type="bibr" target="#b68">[70,</ref><ref type="bibr" target="#b21">23]</ref> and has been popular for modelling temporal sequences. Deep learning has became very successful for computer vision <ref type="bibr" target="#b69">[71]</ref>, reinforcement learning for games <ref type="bibr" target="#b70">[72]</ref>, and big data related problems. Deep learning typically refer to recurrent networks (RNNs), convolutional neural networks (CNNs) <ref type="bibr" target="#b71">[73,</ref><ref type="bibr" target="#b70">72]</ref>, deep belief networks, and LSTM networks which are a special class of RNNs addressing longterm dependency problem in time series datasets <ref type="bibr" target="#b21">[23]</ref>. RNNs have been popular for forecasting time series with their ability to capture temporal information <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>. In terms of uncertainity quantification in predictions, Mirikitani and Nikolaev used <ref type="bibr" target="#b76">[77]</ref> variational inference for Bayesian RNNs for time series forecasting.</p><p>CNNs have gained attention recently in forecasting time series. Wang et. al <ref type="bibr" target="#b77">[78]</ref> used CNNs with wavelet transform probabilistic wind power forecasting. Xingjian et. al <ref type="bibr" target="#b78">[79]</ref> used CNNs in conjunction with LSTMs to capture spatial-temporal sequences for forecasting precipitation. Amarasinghe et al. <ref type="bibr" target="#b79">[80]</ref> employed CNNs for energy load forecasting, and Huang and Kuo <ref type="bibr" target="#b80">[81]</ref> combined CNNs and LSTMs for air pollution quality forecasting. Sudriani et al. <ref type="bibr" target="#b81">[82]</ref> employed LSTMs for forecasting discharge level of a river for managing water resources. Ding et al. <ref type="bibr" target="#b82">[83]</ref> employed CNNs to evaluate different events on stock price behavior, and Nelson et al. <ref type="bibr" target="#b83">[84]</ref> used LSTMs to forecast stock market trends. Chimmula and Zhand employed LSTMs for forecasting COVID-19 transmission in Canada <ref type="bibr" target="#b84">[85]</ref>. The authors predicted the possible ending point of the outbreak around June 2020, Canada reached the daily new cases peak by 2nd May 1 which further reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data reconstruction</head><p>The original time series data is reconstructed for multi-stepahead prediction. Taken's theorem expresses that the reconstruction can reproduce important features of the original time series <ref type="bibr" target="#b85">[86]</ref>. Therefore, given an observed time series x(t), an embedded phase space</p><formula xml:id="formula_0">Y(t) = [(x(t), x(t − T ), ..., x(t − (D − 1)T )]</formula><p>can be generated; where T is the time delay, D is the embedding dimension (window size) given t = 0, 1, 2, ..., N − DT − 1, and N is the length of the original time series. A study needs to be done to determine good values for D and T in order to efficiently apply Taken's theorem <ref type="bibr" target="#b86">[87]</ref>. We note that Taken's proved that if the original attractor is of dimension d, then D = 2d + 1 would be sufficient <ref type="bibr" target="#b85">[86]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Shallow learning via simple neural networks</head><p>We refer to the the backpropagation neural network and multiplayer perception as simple neural network which has been typically trained by the stochastic gradient descent (SGD) algorithm. SGD maintains a single learning rate for all weight updates which does not change during the training. Adaptive moment estimation (Adam) learning algorithm <ref type="bibr" target="#b87">[88]</ref> extends the stochastic gradient descent by maintaining and adapting the learning rate for each network weight as learning unfolds. Using first and second moments of the gradients, Adam computes individual adaptive learning rates which is inspired by the adaptive gradient algorithm (AdaGrad) <ref type="bibr" target="#b88">[89]</ref>. In the literature, Adam has shown better results when compared to stochastic gradient descent and AdaGrad, and our experiments will consider evaluating it further for multi-step time series prediction. Adam's learning procedure for iteration t is formulated as</p><formula xml:id="formula_1">Θ t−1 = [W t−1 , b t−1 ] g t = ∇ Θ J t (Θ t−1 ) m t = β 1 .m t−1 + (1 − β 1 ).g t v t = β 2 .v t−1 + (1 − β 2 ).g 2 t m t = m t /(1 − β t 1 ) v t = v t /(1 − β t 2 ) Θ t = Θ t−1 − α.m t /( v t + )<label>(1)</label></formula><p>where m t , v t are the respective first and second moment vectors for iteration t; β 1 , β 2 are constants ∈ [0, 1], α is the learning rate, and is a close to zero constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Simple recurrent neural networks</head><p>The Elman RNN [28] is a prominent example of simple recurrent networks that feature a context layer to act as memory and incorporate current state for propagating information into future states, given future inputs. The use context layer to store the output of the state neurons from computation of the previous time steps makes them applicable for time-varying patterns in data. The context layer maintains memory of the prior hidden layer result as shown in <ref type="figure">Figure ?</ref>?. A vectorised formulation can be given as follows</p><formula xml:id="formula_2">h t = σ h (W h x t + U h h t−1 + b h ) y t = σ y (W y h t + b y )<label>(2)</label></formula><p>where; x t input vector, h t hidden layer vector, y t output vector, W represent the weights for hidden and output layer, U is the context state weights, b is the bias, and σ h and σ y are the respective activation functions. Backpropagation through time (BPTT) <ref type="bibr" target="#b19">[21]</ref> which is an extension of the backpropagation algorithm is a prominent method for training simple recurrent networks which features gradient descent with the major difference that the error is backpropagated for a deeper network architecture that features states defined by time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">LSTM neural networks</head><p>Simple recurrent networks have the <ref type="bibr" target="#b20">[22]</ref> limitation of learning long-term dependencies with problems in vanishing and exploding gradients <ref type="bibr" target="#b89">[90]</ref> in simple recurrent neural networks. LSTMs employ using memory cells and gates for much better capabilities in remembering the long-term dependencies in temporal sequences as shown in <ref type="figure" target="#fig_1">Figure 2</ref> LSTM units are trained in a supervised fashion on a set of training sequences using an adaptation of the BPTT algorithm that considers the respective gates <ref type="bibr" target="#b20">[22]</ref>. Neuroevolution provides an alternate training method that does not request gradients <ref type="bibr" target="#b90">[91]</ref>. Policy gradient methods in reinforcement learning framework can be used in case where no training labels are present <ref type="bibr" target="#b91">[92]</ref>.</p><p>LSTM networks calculate a hidden state h t as</p><formula xml:id="formula_3">i t = σ x t U i + h t−1 W i f t = σ x t U f + h t−1 W f o t = σ x t U o + h t−1 W o C t = tanh x t U g + h t−1 W g C t = σ f t * C t−1 + i t * C t h t = tanh(C t ) * o t<label>(3)</label></formula><p>where, i t , f t and o t refer to the input, forget and output gates, at time t, respectively. x t and h t refer to the number of input features and number of hidden units, respectively. W and U is the weight matrices adjusted during learning along with b which is the bias. The initial values are c 0 = 0 and h 0 = 0. All the gates have the same dimensions d h , the size of your hidden state.C t is a "candidate" hidden state, and C t is the internal memory of the unit as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Note that we denote (*) as element-wise multiplication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Bi-directional LSTM networks</head><p>A major shortcoming of conventional RNNs is that they only make use of previous context state for determining future states. Bidirectional RNNs (BD-RNNs) <ref type="bibr" target="#b92">[93]</ref> process information in both directions with two separate hidden layers, which are then propagated forward to the same output layer. BD-RNNs hence consist of placing two independent RNNs together to allow the networks to have both backward and forward information about the sequence at every time step. BD-RNN computes the forward hidden sequence h f , the backward hidden sequence h b , and the output sequence y by iterating the backward layer from t = T to t = 1, the forward layer from t = 1 to t = T and then updating the output layer.</p><p>Originally proposed for world-embedding in natural language processing, bi-directional LSTM networks (BD-LSTM) <ref type="bibr" target="#b93">[94]</ref>, can access longer-range context or state in both directions similar to BD-RNNs. BD-LSTM would intake inputs in two ways, one from past to future and one from future to past which different from conventional LSTM since by running information backwards, state information from the future is preserved. Hence, with two hidden states combined, in any point in time the network can preserve information from both past and future as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. BD-LSTM networks have been used in several real-world sequence processing problems such as phoneme classification <ref type="bibr" target="#b93">[94]</ref>, continuous speech recognition <ref type="bibr" target="#b94">[95]</ref> and speech synthesis <ref type="bibr" target="#b95">[96]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Encoder-Decoder LSTM networks</head><p>Sutskever et. al <ref type="bibr" target="#b96">[97]</ref> introduced the encoder-decoder LSTM network (ED-LSTM) which is a sequence to sequence model for mapping a fixed-length input to a fixed-length output where the length of the input and output may differ, which is applicable in automatic language translation tasks (English to French for example). In the case of multi-step series prediction and multivariate analysis, both the input and outputs are of variable lengths. Hence, the input is the sequence of video frames (x 1 , ..., x n ), and the output is the sequence of words (y 1 , ..., y m ). Therefore, we estimate the conditional probability of an output sequence (y 1 , ..., y m ) given an input sequence (x 1 , ..., x n ) i.e. p(y 1 , ..., y m |x 1 , ..., x n ).</p><p>ED-LSTM networks handle variable-length input and outputs by first encoding the input sequences, one at a time, using a latent vector representation, and then decoding from that representation. In the encoding phase, given an input sequence, the ED-LSTM computes a sequence of hidden states In decoding phase, it defines a distribution over the output sequence given the input sequence as shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">CNNs</head><p>CNNs introduced by introduced by LeCun <ref type="bibr" target="#b98">[98,</ref><ref type="bibr" target="#b99">99]</ref> are prominent deep learning architecture inspired by the natural visual system of mammals. CNNs could classify handwritten digits and could be trained using backpropagation algorithm <ref type="bibr" target="#b100">[100]</ref> which later has been prominent in many computer vision and image processing tasks. More recently, CNNs have been applied for time series prediction <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b77">78]</ref> with promising results.</p><p>CNNs learn spatial hierarchies of features by using multiple building blocks, such as convolution, pooling layers, and fully connected layers. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of a CNN used for time series prediction, given a univariate time series input and multiple output neurons representing different prediction horizons. We note that multivariate time series is more appropriate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We present experiments and results that consider simple neural networks featuring SGD and Adam learning, and and deep learning methods that feature RNNs, LSTM networks, ED-LSTM, BD-LSTM and CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Design</head><p>The selected benchmark problems are are combination of simulated and real-world time series. The simulated time series are Mackey-Glass <ref type="bibr" target="#b101">[101]</ref>, Lorenz <ref type="bibr" target="#b102">[102]</ref>, Henon <ref type="bibr" target="#b103">[103]</ref>, and Rossler <ref type="bibr" target="#b104">[104]</ref>. The real-world time series are Sunspot <ref type="bibr" target="#b105">[105]</ref>, Lazer <ref type="bibr" target="#b106">[106]</ref> and ACI-financial time series <ref type="bibr" target="#b107">[107]</ref>. They have been used in our previous works and are prominent benchmarks for time series problems <ref type="bibr" target="#b108">[108,</ref><ref type="bibr" target="#b109">109,</ref><ref type="bibr" target="#b110">110]</ref>. The Sunspot time series indicates solar activities from November 1834 to June 2001 and consists of 2000 data points <ref type="bibr" target="#b105">[105]</ref>. The ACI-finance time series contains closing stock prices from December 2006 to February 2010, featuring 800 data points <ref type="bibr" target="#b107">[107]</ref>. The Lazer time series is from the Santa Fe competition that consists of 500 points <ref type="bibr" target="#b106">[106]</ref>.</p><p>The respective time series were pre-processed into a statespace vector <ref type="bibr" target="#b85">[86]</ref> with embedding dimension D = 5 and timelag T = 1 for 10-step-ahead prediction. We considered respective neural network models with number of hidden neurons, selected learning rate and other papers and in trial experiments to determine appropriate models. <ref type="table">Table 1</ref> gives details for the topology of the respective models in terms of input, hidden and output layers. In the respective datasets, we used first 1000 data points from which the first 60% was used for training and remaining for testing. All the respective time series are scaled in the range [0,1].</p><p>We use the root-mean-squared error (RMSE) in Equation 4 as the main performance measures for different prediction horizons.</p><formula xml:id="formula_4">RMS E = 1 N N i=1 (y i −ŷ i ) 2<label>(4)</label></formula><p>where y i ,ŷ i are the observed data, predicted data, respectively. N is the length of the observed data. We apply RMSE in Equation 4 for each prediction horizon, and also report the mean error for all the respective prediction horizons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We report the mean and 95 % confidence interval of RMSE for each prediction horizon for the respective problem for train and test datasets from 30 experimental runs with different initial neural network weights.</p><p>The results are shown in <ref type="figure">Figure 9</ref> to <ref type="figure" target="#fig_0">Figure 12</ref> for the simulated time series problems ( <ref type="table" target="#tab_8">Table 8</ref> to 11 in Appendix). <ref type="figure">Figure  6</ref> to 8 show results for the real-world time series problem (Table 5 to 7 in Appendix). We define robustness as the confidence interval which must be as low as possible to indicate high confidence in prediction. We consider scalability as the ability to provide consistent performance to some degree of error given the prediction horizon increases.</p><p>Note that the results are given in terms of the RMSE where the lower values indicate better performance. Note that each problem reports 10-step-ahead prediction results for 30 experiments with RMSE mean and 95% confidence interval as histogram and error bars, shown in Figures 6 to 12.</p><p>We first consider results for real world time series that naturally feature noise (ACI-Finance, Sunspot, Lazer). <ref type="figure">Figure 6</ref> shows the results for the ACI-fiancee problem. We observe that the test performance is better than the train performance in <ref type="figure">Figure 6</ref> (a) where deep learning models provide more reliable performance. The prediction error increases with the prediction horizon, and the deep learning methods do much better than simple learning methods (FNN-SGD and FNN-Adam). We find that LSTM provides the best overall performance as shown in <ref type="figure">Figure 6</ref> (b). The overall test performance shown in <ref type="figure">Figure 6</ref> (a) indicates that FNN-Adam and LSTM provide similar performance, which are better than rest of the problems. <ref type="figure" target="#fig_0">Figure 13</ref> shows ACI-finance prediction performance of the best experiment run with selected prediction horizons that indicate how the prediction deteriorates as prediction horizon increases.</p><p>Next, we consider the results for the Sunspot time series shown in <ref type="figure">Figure 7</ref> which follows a similar trend as the ACIfinance problem in terms of the increase in prediction error along with the prediction horizon. Also, the test performance is better than the train performance as evident from <ref type="figure">Figure 7</ref> (a). The LSTM methods (LSTM, ED-LSTM, BD-LSTM) gives better performance than the other methods as can be observed from <ref type="figure">Figure 7</ref> (a) and 7 (b). Note that the FNN-SGD gives the worst performance and the performance of RNN is better than  that of CNN, FNN-SGD, FNN-Adam but poorer than LSTM methods. <ref type="figure" target="#fig_0">Figure 14</ref> shows Sunspot prediction performance of the best experiment run with selected prediction horizons.</p><p>The results for Lazer time series is shown in <ref type="figure">Figure 8</ref>, which exhibits a similar trend in terms of the train and test performance as the other real-world time series problems. Note that the Lazer problem is highly chaotic (as visually evident in <ref type="figure" target="#fig_0">Figure 16)</ref>, which seems to be the primary reason behind the difference in performance for the prediction horizon in contrast to other problems as displayed in <ref type="figure">Figure 8 (b)</ref>. It is striking that none of the methods appear to be showing any trend for the prediction accuracy along the prediction horizon, as seen in previous problems. In terms of scalability, all the methods appear to be performing better in comparison with the other problems. The performance of CNN is better than that of RNN, which is different from other real-world time series. <ref type="figure" target="#fig_0">Figure 16</ref> shows Lazer prediction performance of the best experiment run using ED-LSTM with selected prediction horizons. We note that due to the chaotic nature of the time series, the prediction performance is visually not clear.</p><p>We now consider simulated time series that do not feature noise (Henon, Mackey-Glass, Rosssler, Lorenz). The Henon time series in <ref type="figure">Figure 9</ref> shows that ED-LSTM provides the best performance. Note that there is a more significant difference between the three LSTM methods when compared to other problems. The trends are similar to the ACI-finance and the Sunspot problem given the prediction horizon performance in <ref type="figure">Figure 9</ref> (a) and 9 (b) where the simple learning methods (FNN-SGD and FNN-Adam) appear to be more scalable than the other methods along the prediction horizon although they perform poorly. <ref type="figure" target="#fig_0">Figure 17</ref> and <ref type="figure" target="#fig_0">Figure 15</ref> shows Mackey-Glass and Henon prediction performance of the best experiment run using ED-LSTM with selected prediction horizons. The Henon prediction in <ref type="figure" target="#fig_0">Figure 15</ref> indicates that it is far more chaotic than Mackey-Glass and hence it faces more challenges. We show them since these are cases with no noise when compared to real-world time series previously shown that has a larger deviation or deterioration in prediction performance as the prediction horizon increases <ref type="figure" target="#fig_0">(Figures 13 and Figure 14</ref>).</p><p>For the Lorenz, Mackey-Glass and Rossler simulated time series, the deep learning methods are performing far better than the simple learning methods as can be seen in <ref type="figure" target="#fig_0">Figures 10, 11</ref> and 12. The trend along the prediction horizon is similar to previous problems, i.e., the prediction error increases along with the prediction horizon. If we consider scalability, the deep learning methods are more scalable in the Lorenz, Mackey-Glass and Rossler problems than the previous problems. This is the first instance where the CNN has outperformed LSTM for Mackey-Glass and Rossler time series.</p><p>We note that there have been distinct trends in prediction for the different types of problems. In the simulated time series problems, if we exclude Henon, we find similar trend for Mackey-Glass, Lorenz and Rossler time series. The trend indicates that simple neural networks face major difficulties and ED-LSTM and BD-LSTM networks provides the best performance, which also applies to Henon time series, except that it has close performance for simple neural networks when compared to deep learning models for 7-10 prediction horizons <ref type="figure">(Figure 9 b)</ref>. This difference reflects in the nature of the time series which is highly chaotic in nature ( <ref type="figure" target="#fig_0">Figure 15</ref>). We further note that the simple neural networks, in the Henon case ( <ref type="figure">Figure  9</ref>) does not deteriorate in performance as the prediction horizon increases when compared to Mackley-Glass, Lorenz and Rossler problems, although they give poor performance.</p><p>The performance of simple neural networks in Lazer problem shows a similar trend in Lazer time series, where the predictions are poor from the beginning and its striking that LSTM networks actually improve the performance as the prediction horizon increases <ref type="figure">(Figure 8 b)</ref>. This trend is a clear outline when compared to rest of real-world and simulated problems, as all of them have results where the deep learning models deteriorate as the prediction horizon increase. <ref type="table" target="#tab_2">Table 3</ref> and 4 show a comparison with related methods from the literature for simulated and real-world time series, respectively. We note that the comparison is not truly fair as other methods may have employed different models with different data processing and also in reporting of results with different measures of error as some papers report best experimental run and do not show mean and standard deviation. We highlight in bold the best performance for respective prediction horizon. <ref type="table" target="#tab_2">Table 3</ref>, we compare the Mackey-Glass and Lorenz time series performance for two-step-ahead prediction by real-time recurrent learning (RTRL) and echo state networks (ESN) <ref type="bibr" target="#b10">[12]</ref>. Note that the * in the results implies that the comparison was not very fair due to different embedding dimension in state-space reconstruction and it is not clear if the mean or best run has been reported. We show further comparison for Mackey-Glass for 5th prediction horizon using Extended Kalman Filtering (EKF), the Unscented Kalman Filtering (UKF) and the Gaussian Particle Filtering (GPF), along with their generalized versions G-EKF, G-UKF and G-GPF, respectively <ref type="bibr" target="#b111">[111]</ref>. Considering MultiTL-KELM <ref type="bibr" target="#b53">[55]</ref> for Mackey-Glass and Henon time series, it is observed that it is performing very well for the Mackey-Glass time series and beats all our proposed methods but fails to perform better for the Henon time series. In general, we find that our proposed deep learning methods (LSTM, BD-LSTM, ED-LSTM) have beaten most of the methods from the literature for the simulated time series, except for the Mackey-Glass time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the literature</head><p>In <ref type="table" target="#tab_4">Table 4</ref>, we compare the performance of Sunspot time series with support vector regression(SVR), iterated (SVR-I), direct (SVR-D), and multiple models (M-SVR) methods <ref type="bibr" target="#b12">[14]</ref>.</p><p>In the respective problems, we also compare with coevolutionary multi-task learning (CMTL) <ref type="bibr" target="#b52">[54]</ref>. We observe that our proposed deep learning methods have given the best performance for the respective problems for most of the prediction horizon. Moreover, we find the FNN-Adam overtakes CMTL in all time-series problems except in 8-step ahead prediction in Mackey-Glass and 2-step ahead prediction in Lazer time series. It should also be noted that except for the Mackey-Glass and ACI-Finance time series, the deep learning methods are the best which motivates further applications for these methods for challenging forecasting problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We provide a ranking of the methods in terms of performance accuracy over the test dataset across the prediction horizons in <ref type="table">Table 2</ref>. We observe that FNN-SGD gives the worst performance for all time-series problems followed by FNN-Adam in most cases, which is further followed by RNN and CNN. We observe that the BD-LSTM and ED-LSTM models provide one of the best performance across different problems with differed properties. We also note that in across all the problems, the confidence interval of RNN is the lowest followed by CNN which indicates that they provide more robust training performance given different initialisation in weight space.</p><p>We note that its natural for the results to deteriorate as the prediction horizons increases in multi-step ahead problems since the prediction is based on current values and the gap in the missing information increases as the horizon decreases since the next predictions are not used as inputs due to our problem formulated as direct strategy of multi-step ahead prediction, as opposed to iterated prediction strategies. ACI-finance problem is unique in since there is not major difference with simple neural networks and deep learning models <ref type="figure">(Figure 7</ref> b) from 7 -10 prediction horizon.</p><p>Long term dependency problems arise in the analysis of time series where the rate of decay of statistical dependence of two points with increasing time interval between the points. Canonical RNNs had difficulties in training with long-term dependencies <ref type="bibr" target="#b27">[29]</ref>, hence LSTM networks were proposed to address the learning imitates with memory cells for addressing vanishing error problem in learning long term dependencies <ref type="bibr" target="#b20">[22]</ref>. We note that the time series problems in our experiments are not long-term dependency problems, yet LSTM give better performance when compared to simple RNNs. It seems that the memory gates in LSTM networks help better capture information in temporal sequences, even though they do not have long-term dependencies. We note that the memory gates in LSTM networks were originally designed to cater for the vanishing gradient problem. It seems the memory gates of LSTM networks are helpful in capturing salient features in temporal series that help in predicting future tends much better than simple RNNs. We note that simple RNNs provided better results than simple neural networks (FNN-SGD and FNN-Adam) since they are more suited for temporal series. Moreover, we find striking results given that CNNs which are suited for image processing task performances better than simple RNNs in general. This could be due to the convolutional layers in CNNs that help in better capturing salient features for the temporal sequences.</p><p>Moving on, it is important to understand why the novel LSTM network model (ED-LSTM and BD-LSTM) have given much better results. ED-LSTMS were designed for language (a) RMSE across 10 prediction horizons (b) 10 step-ahead prediction   <ref type="table" target="#tab_2">ACI-finance  2  7  1  3  4  5  6  Sunspot  6  7  2  1  3  4  5  Lazer  6  7  1  2  3  5  4  Henon  6  7  3  2  1  5  4  Lorenz  6  7  2  3  1  5</ref>   <ref type="table">Table 2</ref>: Performance (rank) of different models for respective time-series problems. Note lower rank denotes better performance. Mackey-Glass 2SA-RTRL* <ref type="bibr" target="#b10">[12]</ref> 0.0035 ESN* <ref type="bibr" target="#b10">[12]</ref> 0.0052 EKF <ref type="bibr" target="#b111">[111]</ref> 0.2796 G-EKF <ref type="bibr" target="#b111">[111]</ref> 0.2202 UKF <ref type="bibr" target="#b111">[111]</ref> 0.1374 G-UKF <ref type="bibr" target="#b111">[111]</ref> 0.0509 GPF <ref type="bibr" target="#b111">[111]</ref> 0.0063 G-GPF <ref type="bibr" target="#b111">[111]</ref> 0.0022 Multi-KELM <ref type="bibr" target="#b53">[55]</ref> 0.0027 0.0031 0.0028 0.0029 MultiTL-KELM <ref type="bibr" target="#b53">[55]</ref> 0.0025 0.0029 0.0026 0.0028 CMTL <ref type="bibr" target="#b52">[54]</ref> 0.0550 0.0750 0.0105 0.1200 ANFIS(SL) <ref type="bibr" target="#b112">[112]</ref> 0.0051 0.0213 0.0547 R-ANFIS(SL) <ref type="bibr" target="#b112">[112]</ref> 0.0045 0.0195 0.0408 R-ANFIS(GL) <ref type="bibr" target="#b112">[112]</ref>    <ref type="bibr" target="#b12">[14]</ref> 0.2355 ± 0.0583 SVR-I <ref type="bibr" target="#b12">[14]</ref> 0.2729 ±0.1414 SVR-D <ref type="bibr" target="#b12">[14]</ref> 0.2151 ± 0.0538 CMTL <ref type="bibr" target="#b52">[54]</ref> 0.0473 0.0623 0.0771 0.0974 FNN-Adam 0.0236± 0.0015 0.0407± 0.0012 0.0582± 0.0019 0.0745± 0.0020 FNN-SGD 0.0352± 0.0022 0.0610± 0.0024 0.0856± 0.0023 0.1012± 0.0019 LSTM 0.0148± 0.0007 0.0321± 0.0006 0.0449± 0.0007 0.0587± 0.0010 BD-LSTM 0.0155± 0.0007 0.0318± 0.0007 0.0440± 0.0005 0.0576± 0.0010 ED-LSTM 0.0170± 0.0004 0.0348± 0.0004 0.0519± 0.0016 0.0673± 0.0022 RNN 0.0212± 0.0003 0.0395± 0.0002 0.0503± 0.0002 0.0641± 0.0003 CNN 0.0257± 0.0002 0.0419 ±0.0004 0.0555± 0.0006 0.0723± 0.0008 ACI-Finance CMTL <ref type="bibr" target="#b52">[54]</ref> 0.0486 0.0755 0.08783 0.1017 FNN-Adam 0.0203 ± 0.0012 0.0272 ± 0.0008 0.0323 ± 0.0004 0.0357 ± 0.0008 FNN-SGD 0.0242 ± 0.0020 0.0299 ± 0.0015 0.0350 ± 0.0021 0.0380 ± 0.0018 LSTM 0.0168 ± 0.0003 0.0248 ± 0.0006 0.0333 ± 0.0010 0.0367± 0.0015 BD-LSTM 0.0165 ± 0.0002 0.0253 ± 0.0004 0.0356 ± 0.0010 0.0409 ± 0.0015 ED-LSTM 0.0171 ± 0.0003 0.0271 ± 0.0010 0.0359 ± 0.0014 0.0395 ± 0.0014 RNN 0.0202 ± 0.0003 0.0284 ± 0.0004 0.0348 ± 0.0004 0.0384 ± 0.0003 CNN 0.0217 ±0.0004 0.0290 ±0.0002 0.0363 ±0.0006 0.0401± 0.0005 quence <ref type="bibr" target="#b36">[38]</ref>. In our case, the encoder maps an input time series to a fixed length vector and then the decoder LSTM maps the vector representation to the different prediction horizons. Although the application is different, the underlying task of mapping inputs to outputs remains the same and hence, ED-LSTM models have been very effective for multi-step ahead prediction. We note that conventional recurrent networks make use of only the previous context states for determining future states. BD-LSTMS on the other hand processes information using two LSTM models to feature forward and backward information about the sequence at every time step <ref type="bibr" target="#b93">[94]</ref>. Although these have been useful for language modelling tasks, our results show that they are applicable for mapping current and future states for time series modelling since information from past and future states are preserved that seems to be the key feature in achieving better performance for multi-step prediction problems when compared to conventional LSTM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we provide a comprehensive evaluation of emerging deep learning models for multi-step-ahead time series problems. Our results indicate that encoder-decoder and bidirectional LSTM networks provide best performance for both simulated and real-world time series problems. The results have significantly improved over other related time series prediction methods given in the literature.</p><p>In future work, it would be worthwhile to provide similar evaluation for multivariate time series prediction problems. Moreover, it is worthwhile to investigate the performance of given deep learning models for spatial-temporal time series, such as the prediction of behavior of storms and cyclone and also further applications in other real-world problems such as air pollution and energy forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software and Data</head><p>We provide open source implementation in Python along with data for the respective methods for further research 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FNN-Adam</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FNN-SGD</head><p>LSTM BD-LSTM ED-LSTM RNN CNN Train 0.1628 ± 0.0012 0.1703 ± 0.0018 0.1471 ± 0.0014 0.1454 ± 0.0021 0.1437 ± 0.0019 0.1930 ± 0.0018 0.1655 ±0.0013 Test 0.0885 ± 0.0011 0.0988 ± 0.0040 0.0860 ± 0.0025 0.0915 ± 0.0023 0.0923 ± 0.0032 0.0936 ± 0.0009 0.0978± 0.0011</p><p>Step-1 0.0165 ± 0.0011 0.0209 ± 0.0022 0.0127 ± 0.0003 0.0127 ± 0.0002 0.0130 ± 0.0005 0.0173 ± 0.0004 0.0193± 0.0006</p><p>Step-2 0.0203 ± 0.0012 0.0242 ± 0.0020 0.0168 ± 0.0003 0.0165 ± 0.0002 0.0171 ± 0.0003 0.0202 ± 0.0003 0.0217 ±0.0004</p><p>Step-3 0.0217 ± 0.0008 0.0266 ± 0.0032 0.0190 ± 0.0004 0.0194 ± 0.0002 0.0204 ± 0.0006 0.0228 ± 0.0003 0.0247± 0.0003</p><p>Step-4 0.0249 ± 0.0009 0.0277 ± 0.0023 0.0220 ± 0.0004 0.0229 ± 0.0003 0.0239 ± 0.0008 0.0258 ± 0.0004 0.0266± 0.0002 Step-5 0.0272 ± 0.0008 0.0299 ± 0.0015 0.0248 ± 0.0006 0.0253 ± 0.0004 0.0271 ± 0.0010 0.0284 ± 0.0004 0.0290 ±0.0002</p><p>Step-6 0.0289 ± 0.0006 0.0325 ± 0.0016 0.0281 ± 0.0008 0.0292 ± 0.0007 0.0302 ± 0.0012 0.0304 ± 0.0004 0.0315 ±0.0004</p><p>Step-7 0.0311 ± 0.0005 0.0342 ± 0.0020 0.0302 ± 0.0008 0.0331 ± 0.0010 0.0334 ± 0.0014 0.0327 ± 0.0004 0.0340± 0.0003 <ref type="bibr">Step 8</ref> 0.0323 ± 0.0004 0.0350 ± 0.0021 0.0333 ± 0.0010 0.0356 ± 0.0010 0.0359 ± 0.0014 0.0348 ± 0.0004 0.0363 ±0.0006 Step 9</p><p>0.0339 ± 0.0005 0.0357 ± 0.0012 0.0364 ± 0.0013 0.0388 ± 0.0011 0.0380 ± 0.0014 0.0371 ± 0.0003 0.0386± 0.0006 Step 10 0.0357 ± 0.0008 0.0380 ± 0.0018 0.0367± 0.0015 0.0409 ± 0.0015 0.0395 ± 0.0014 0.0384 ± 0.0003 0.0401± 0.0005 Step-1 0.0163± 0.0016 0.0281± 0.0032 0.0072± 0.0005 0.0086± 0.0006 0.0109± 0.0006 0.0132± 0.0004 0.0186± 0.0002</p><p>Step-2 0.0236± 0.0015 0.0352± 0.0022 0.0148± 0.0007 0.0155± 0.0007 0.0170± 0.0004 0.0212± 0.0003 0.0257± 0.0002 Step-3 0.0311± 0.0013 0.0441± 0.0025 0.0220± 0.0005 0.0222± 0.0006 0.0237± 0.0003 0.0285± 0.0002 0.0321± 0.0003</p><p>Step-4 0.0350± 0.0006 0.0518± 0.0022 0.0275± 0.0005 0.0276± 0.0006 0.0292± 0.0003 0.0346± 0.0002 0.0376± 0.0003 Step-5 0.0407± 0.0012 0.0610± 0.0024 0.0321± 0.0006 0.0318± 0.0007 0.0348± 0.0004 0.0395± 0.0002 0.0419 ±0.0004</p><p>Step-6 0.0464± 0.0016 0.0677± 0.0027 0.0360± 0.0006 0.0358± 0.0006 0.0402± 0.0006 0.0431± 0.0002 0.0457± 0.0004 Step-7 0.0514± 0.0019 0.0771± 0.0020 0.0397± 0.0006 0.0395± 0.0006 0.0458± 0.0011 0.0463± 0.0002 0.0498± 0.0005 Step 8 0.0582± 0.0019 0.0856± 0.0023 0.0449± 0.0007 0.0440± 0.0005 0.0519± 0.0016 0.0503± 0.0002 0.0555± 0.0006 Step 9 0.0653± 0.0016 0.0931± 0.0023 0.0509± 0.0009 0.0498± 0.0007 0.0590± 0.0020 0.0564± 0.0002 0.0633± 0.0007 Step 10 0.0745± 0.0020 0.1012± 0.0019 0.0587± 0.0010 0.0576± 0.0010 0.0673± 0.0022 0.0641± 0.0003 0.0723± 0.0008  Step-1 0.1465 ± 0.0058 0.1725 ± 0.0031 0.0287 ± 0.0045 0.0241 ± 0.0014 0.0226 ± 0.0039 0.0885 ± 0.0093 0.0650± 0.0018</p><p>Step-2 0.1606 ± 0.0024 0.1711 ± 0.0018 0.0682 ± 0.0058 0.0448 ± 0.0026 0.0454 ± 0.0069 0.1515± 0.0016 0.0859 ± 0.0038 Step-3 0.1610 ± 0.0008 0.1707 ± 0.0017 0.0920 ± 0.0066 0.0610 ± 0.0077 0.0517 ± 0.0122 0.1577 ± 0.0003 0.1411± 0.0021 Step-4 0.1714 ± 0.0009 0.1760 ± 0.0011 0.1386 ± 0.0044 0.0925 ± 0.0077 0.0609 ± 0.0154 0.1643 ± 0.0011 0.1519± 0.0022 Step-5 0.1731 ± 0.0005 0.1769 ± 0.0007 0.1584 ± 0.0010 0.1287 ± 0.0046 0.0694 ± 0.0161 0.1718 ± 0.0001 0.1601± 0.0007 Step-6 0.1758 ± 0.0006 0.1777 ± 0.0010 0.1642 ± 0.0007 0.1538 ± 0.0017 0.0868 ± 0.0165 0.1730 ± 0.0003 0.1674± 0.0004 Step-7 0.1786 ± 0.0006 0.1814 ± 0.0009 0.1684± 0.0011 0.1593 ± 0.0016 0.1120 ± 0.0139 0.1764 ± 0.0003 0.1721± 0.0006 <ref type="bibr">Step 8</ref> 0.1781 ± 0.0005 0.1805 ± 0.0012 0.1707 ± 0.0008 0.1697 ± 0.0008 0.1371 ± 0.0107 0.1768 ± 0.0001 0.1718± 0.0003 <ref type="bibr">Step 9</ref> 0.1757 ± 0.0004 0.1788 ± 0.0011 0.1723 ± 0.0005 0.1737 ± 0.0004 0.1524 ± 0.0077 0.1752 ± 0.0001 0.1752± 0.0003 Step 10 0.1762 ± 0.0009 0.1773 ± 0.0011 0.1756 ± 0.0005 0.1733 ± 0.0003 0.1689 ± 0.0046 0.1751 ± 0.0002 0.1737± 0.0002 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FNN-Adam</head><p>FNN-SGD LSTM BD-LSTM ED-LSTM RNN CNN Train 0.1932± 0.0124 0.2761± 0.0048 0.0242± 0.0086 0.0300± 0.0127 0.0225± 0.0031 0.0538± 0.0091 0.0354± 0.0032 Test 0.1809± 0.0119 0.2649± 0.0048 0.0254± 0.0093 0.0310± 0.0137 0.0234± 0.0031 0.0542± 0.0097 0.0347± 0.0029</p><p>Step-1 0.0183± 0.0042 0.0336± 0.0035 0.0025± 0.0006 0.0043± 0.0019 0.0051± 0.0015 0.0113± 0.0013 0.0055± 0.0006 Step-2 0.0206± 0.0046 0.0432± 0.0030 0.0033± 0.0010 0.0054± 0.0026 0.0044± 0.0012 0.0129± 0.0012 0.0067 ±0.0007</p><p>Step-3 0.0253± 0.0043 0.0547± 0.0028 0.0042± 0.0019 0.0064± 0.0031 0.0046± 0.0010 0.0143± 0.0016 0.0077± 0.0009 Step-4 0.0334± 0.0048 0.0651± 0.0032 0.0051± 0.0020 0.0074± 0.0035 0.0052± 0.0009 0.0151± 0.0018 0.0087± 0.0009 Step-5 0.0481± 0.0072 0.0787± 0.0030 0.0064± 0.0026 0.0079± 0.0036 0.0059± 0.0009 0.0155± 0.0024 0.0098± 0.0009 Step-6 0.0527± 0.0076 0.0866± 0.0033 0.0073± 0.0029 0.0094± 0.0046 0.0068± 0.0008 0.0164± 0.0029 0.0109 ±0.0010</p><p>Step-7 0.0613± 0.0064 0.0944± 0.0031 0.0089± 0.0033 0.0107± 0.0047 0.0079± 0.0009 0.0171± 0.0036 0.0120 ±0.0010 Step 8 0.0678± 0.0058 0.1027± 0.0025 0.0101± 0.0038 0.0125± 0.0057 0.0090± 0.0009 0.0186± 0.0042 0.0132± 0.0011 Step 9 0.0885± 0.0060 0.1116± 0.0027 0.0117± 0.0045 0.0133± 0.0056 0.0100± 0.0010 0.0199± 0.0049 0.0142± 0.0013 Step 10 0.0859± 0.0065 0.1178± 0.0026 0.0129± 0.0042 0.0146± 0.0059 0.0110± 0.0012 0.0226± 0.0058 0.0157± 0.0015 <ref type="table">Table 9</ref>: Lorenz reporting RMSE mean and 95 % confidence interval (±).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FNN-Adam</head><p>FNN-SGD LSTM BD-LSTM ED-LSTM RNN CNN Train 0.1810± 0.0097 0.2576± 0.0053 0.0890± 0.0076 0.0750± 0.0067 0.0587± 0.0090 0.1317± 0.0003 0.0854± 0.0052 Test 0.1822± 0.0098 0.2599± 0.0054 0.0897± 0.0075 0.0765± 0.0068 0.0602± 0.0090 0.1321± 0.0003 0.0868± 0.0048</p><p>Step-1 0.0162± 0.0037 0.0485± 0.0061 0.0056± 0.0011 0.0052± 0.0009 0.0059± 0.0010 0.0078± 0.0001 0.0075± 0.0007 Step-2 0.0256± 0.0038 0.0621± 0.0051 0.0080± 0.0014 0.0083± 0.0015 0.0076± 0.0014 0.0142± 0.0001 0.0120± 0.0010 Step-3 0.0398± 0.0051 0.0686± 0.0046 0.0120± 0.0018 0.0117± 0.0019 0.0103± 0.0020 0.0214± 0.0001 0.0167± 0.0013 Step-4 0.0457± 0.0045 0.0745± 0.0049 0.0178± 0.0020 0.0155± 0.0023 0.0133± 0.0024 0.0290± 0.0001 0.0216± 0.0015 Step-5 0.0520± 0.0044 0.0785± 0.0025 0.0238± 0.0024 0.0202± 0.0026 0.0168± 0.0027 0.0365± 0.0001 0.0262± 0.0016 Step-6 0.0581± 0.0042 0.0880± 0.0031 0.0298± 0.0025 0.0244± 0.0027 0.0200± 0.0031 0.0434± 0.0001 0.0301± 0.0017 Step-7 0.0680± 0.0047 0.0912± 0.0031 0.0341± 0.0028 0.0288± 0.0028 0.0227± 0.0033 0.0496± 0.0001 0.0332± 0.0018 Step 8 0.0727± 0.0050 0.0937± 0.0022 0.0381± 0.0029 0.0318± 0.0027 0.0248± 0.0036 0.0547± 0.0001 0.0354± 0.0018 Step 9 0.0761± 0.0046 0.0963± 0.0031 0.0406± 0.0030 0.0343± 0.0027 0.0261± 0.0038 0.0586± 0.0001 0.0364± 0.0017 Step 10 0.0777± 0.0043 0.0990± 0.0026 0.0418± 0.0033 0.0359± 0.0026 0.0271± 0.0040 0.0615± 0.0001 0.0364± 0.0017 ±  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Feed Forward Neural Network and Elman RNN for time series prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Long Short-Term Memory (LSTM) neural networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Bi-directional LSTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Encoder-Decoder LSTM to take advantage of feature extraction via the convolutional and the pooling layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>One-dimensional Convolutional Neural Network for time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :Figure 8 :Figure 9 :Figure 10 :</head><label>678910</label><figDesc>ACI-finance time series: performance evaluation of respective methods (a) RMSE across 10 prediction horizons (b) 10 step-ahead prediction Sunspot time series: performance evaluation of respective methods (RMSE mean and 95% confidence interval as error bar) (a) RMSE across 10 prediction horizons (b) 10 step-ahead prediction Lazer time series: performance evaluation of respective methods (RMSE mean and 95% confidence interval as error bar) (a) RMSE-Mean (b) 10 step-ahead prediction Henon time series: performance evaluation of respective methods (RMSE mean and 95% confidence interval as error bar) (a) RMSE across 10 prediction horizons (b) 10 step-ahead prediction Lorenz time series: performance evaluation of respective methods (RMSE mean and 95% confidence interval as error bar) (a) RMSE across 10 prediction horizons (b) 10 step-ahead prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Mackey-Glass time series: performance evaluation of respective methods (RMSE mean and 95% confidence interval as error bar) modeling tasks, primarily sequence to sequence model for language translation where encoder LSTM maps a source se-quence to a fixed-length vector, and the decoder LSTM maps the vector representation back to a variable-length target se-(a) RMSE across 10 prediction horizons (b) 10 step-ahead prediction Rossler time series: performance evaluation of respective methods (RMSE mean and 95% confidence interval as error bar) FNN-Adam FNN-SGD LSTM BD-LSTM ED-LSTM RNN CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>10 Figure 13 :</head><label>1013</label><figDesc>ACI-finance actual vs predicted values for Encoder-Decoder LSTM Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>10 Figure 14 : 10 Figure 15 : 10 Figure 16 : 10 Figure 17 :</head><label>1014101510161017</label><figDesc>Sunspot actual vs predicted values for Encoder-Decoder LSTM Model Henon actual vs predicted values for Encoder-Decoder LSTM Model Lazer actual vs predicted values for Encoder-Decoder LSTM Model Mackey-Glass actual vs predicted values for Encoder-Decoder LSTM Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with Literature for Simulated time series.</figDesc><table><row><cell>Problem</cell><cell>Method</cell><cell>2-step</cell><cell>5-step</cell><cell>8-step</cell><cell>10 steps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with Literature for Real World time series.</figDesc><table><row><cell>Problem</cell><cell>Method</cell><cell>2-step</cell><cell>5-step</cell><cell>8-step</cell><cell>10-step</cell></row><row><cell>Lazer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CMTL [54]</cell><cell>0.0762</cell><cell>0.1333</cell><cell>0.1652</cell><cell>0.1885</cell></row><row><cell></cell><cell>FNN-Adam</cell><cell>0.1043 ± 0.0018</cell><cell cols="3">0.0761 ± 0.0019 0.0642 ± 0.0020 0.0924± 0.0018</cell></row><row><cell></cell><cell>FNN-SGD</cell><cell>0.0983 ± 0.0046</cell><cell cols="2">0.0874 ± 0.0072 0.0864± 0.0053</cell><cell>0.0968± 0.0052</cell></row><row><cell></cell><cell>LSTM</cell><cell>0.0725 ± 0.0027</cell><cell cols="2">0.0512 ± 0.0015 0.0464± 0.0015</cell><cell>0.0561± 0.0044</cell></row><row><cell></cell><cell>BD-LSTM</cell><cell>0.0892 ± 0.0022</cell><cell cols="2">0.0596 ± 0.0036 0.0460± 0.0015</cell><cell>0.0631± 0.0037</cell></row><row><cell></cell><cell>ED-LSTM</cell><cell>0.0894 ± 0.0013</cell><cell cols="2">0.0694 ± 0.0073 0.0510± 0.0027</cell><cell>0.0615± 0.0030</cell></row><row><cell></cell><cell>RNN</cell><cell>0.1176 ± 0.0019</cell><cell cols="2">0.0755 ± 0.0011 0.0611± 0.0015</cell><cell>0.0947± 0.0027</cell></row><row><cell></cell><cell>CNN</cell><cell>0.0729± 0.0014</cell><cell>0.0701± 0.0020</cell><cell>0.0593± 0.0029</cell><cell>0.0577± 0.0018</cell></row><row><cell>Sunspot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>M-SVR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>ACI-finance reporting RMSE mean and 95 % confidence interval (±).</figDesc><table><row><cell></cell><cell>FNN-Adam</cell><cell>FNN-SGD</cell><cell>LSTM</cell><cell>BD-LSTM</cell><cell>ED-LSTM</cell><cell>RNN</cell><cell>CNN</cell></row><row><cell>Train</cell><cell cols="7">0.2043± 0.0047 0.3230± 0.0064 0.1418± 0.0034 0.1369± 0.0033 0.1210± 0.0057 0.1875± 0.0011 0.1695± 0.0019</cell></row><row><cell>Test</cell><cell cols="7">0.1510± 0.0024 0.2179± 0.0041 0.1160± 0.0021 0.1147± 0.0020 0.1322± 0.0032 0.1342± 0.0004 0.1487± 0.0015</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Sunspot reporting RMSE mean and 95 % confidence interval (±).</figDesc><table><row><cell></cell><cell>FNN-Adam</cell><cell>FNN-SGD</cell><cell>LSTM</cell><cell>BD-LSTM</cell><cell>ED-LSTM</cell><cell>RNN</cell><cell>CNN</cell></row><row><cell>Train</cell><cell cols="7">0.3371 ± 0.0026 0.4251 ± 0.0157 0.1954 ± 0.0082 0.1619 ± 0.0106 0.1166 ± 0.0147 0.3210 ± 0.0020 0.2151± 0.0029</cell></row><row><cell>Test</cell><cell cols="7">0.2537 ± 0.0024 0.2821 ± 0.0098 0.1910 ± 0.0042 0.2007 ± 0.0042 0.2020 ± 0.0057 0.2580 ± 0.0031 0.2240± 0.0025</cell></row><row><cell>Step-1</cell><cell cols="7">0.0746 ± 0.0027 0.0895 ± 0.0041 0.0577 ± 0.0021 0.0439 ± 0.0027 0.0490 ± 0.0039 0.0641 ± 0.0037 0.0942± 0.0025</cell></row><row><cell>Step-2</cell><cell cols="7">0.1043 ± 0.0018 0.0983 ± 0.0046 0.0725 ± 0.0027 0.0892 ± 0.0022 0.0894 ± 0.0013 0.1176 ± 0.0019 0.0729± 0.0014</cell></row><row><cell>Step-3</cell><cell cols="7">0.0820 ± 0.0026 0.0816 ± 0.0028 0.0807 ± 0.0007 0.0773 ± 0.0016 0.0707 ± 0.0014 0.0832 ± 0.0026 0.0684± 0.0022</cell></row><row><cell>Step-4</cell><cell cols="7">0.0764 ± 0.0017 0.0852 ± 0.0048 0.0697 ± 0.0015 0.0547 ± 0.0018 0.0601 ± 0.0029 0.0762 ± 0.0008 0.0671± 0.0013</cell></row><row><cell>Step-5</cell><cell cols="7">0.0761 ± 0.0019 0.0874 ± 0.0072 0.0512 ± 0.0015 0.0596 ± 0.0036 0.0694 ± 0.0073 0.0755 ± 0.0011 0.0701± 0.0020</cell></row><row><cell>Step-6</cell><cell cols="7">0.0691 ± 0.0013 0.0787 ± 0.0037 0.0540 ± 0.0016 0.0655 ± 0.0014 0.0606 ± 0.0041 0.0730 ± 0.0015 0.0677 ±0.0014</cell></row><row><cell>Step-7</cell><cell cols="7">0.0632 ± 0.0013 0.0740 ± 0.0061 0.0537 ± 0.0024 0.0601 ± 0.0007 0.0582 ± 0.0031 0.0643 ± 0.0009 0.0643 ± 0.0041</cell></row><row><cell>Step 8</cell><cell cols="2">0.0642 ± 0.0020 0.0864± 0.0053</cell><cell>0.0464± 0.0015</cell><cell>0.0460± 0.0015</cell><cell>0.0510± 0.0027</cell><cell>0.0611± 0.0015</cell><cell>0.0593± 0.0029</cell></row><row><cell>Step 9</cell><cell>0.0891± 0.0021</cell><cell>0.1032± 0.0042</cell><cell>0.0507± 0.0021</cell><cell>0.0599± 0.0019</cell><cell>0.0527± 0.0021</cell><cell>0.0882± 0.0023</cell><cell>0.0773± 0.0019</cell></row><row><cell cols="2">Step 10 0.0924± 0.0018</cell><cell>0.0968± 0.0052</cell><cell>0.0561± 0.0044</cell><cell>0.0631± 0.0037</cell><cell>0.0615± 0.0030</cell><cell>0.0947± 0.0027</cell><cell>0.0577± 0.0018</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Lazer reporting RMSE mean and 95 % confidence interval (±). ± 0.0023 0.5670 ± 0.0015 0.4542 ± 0.0071 0.4014 ± 0.0100 0.3235 ± 0.0316 0.5247 ± 0.0027 0.4728± 0.0038 Test 0.5378 ± 0.0022 0.5578 ± 0.0016 0.4516 ± 0.0052 0.4127 ± 0.0066 0.3294 ± 0.0290 0.5162 ± 0.0027 0.4779± 0.0027</figDesc><table><row><cell></cell><cell>FNN-Adam</cell><cell>FNN-SGD</cell><cell>LSTM</cell><cell>BD-LSTM</cell><cell>ED-LSTM</cell><cell>RNN</cell><cell>CNN</cell></row><row><cell>Train</cell><cell>0.5470</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Henon reporting RMSE mean and 95 % confidence interval (±).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Mackey-Glass reporting RMSE mean and 95 % confidence interval (±).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.worldometers.info/coronavirus/country/ canada/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/sydney-machine-learning/ deeplearning_timeseries</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time series forecasting for nonlinear and nonstationary processes: A review and comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sa-Ngasoongsong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beyca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Bukkapatnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iie Transactions</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1053" to="1071" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review and comparison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Taieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert systems with applications</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="7067" to="7083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical comparison of machine learning models for time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Gayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>El-Shishiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Reviews</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="594" to="621" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">25 years of time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>De Gooijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="473" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recent developments in econometric modeling and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Witt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Travel Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="99" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The econometric analysis of economic time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Hendry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review/Revue Internationale de Statistique</title>
		<imprint>
			<biblScope unit="page" from="111" to="148" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Co-evolutionary multi-task learning for dynamic time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="576" to="589" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature extraction, classification and forecasting of time series signal using fuzzy and garch techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Hemanth</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research &amp; Technology in the Coming Decades (CRT 2013), National Conference on Challenges in. IET</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cooperative coevolution of elman recurrent neural networks for chaotic time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A bias and variance analysis for multistepahead time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Taieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforced two-step-ahead weight adjustment technique for online training of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1269" to="1278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-step-ahead prediction with neural networks: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9emes rencontres internationales: Approches Connexionnistes en Sciences</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterated time series prediction with multiple support vector regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="411" to="422" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive and direct multi-step forecasting: the best of both worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben Taieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Monash University, Department of Econometrics and Business Statistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Methodology for long-term prediction of time series</title>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="2861" to="2869" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple-output modeling for multi-step-ahead time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Taieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1950" to="1957" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-step prediction of time series with random missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0307904X13007658" />
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Modelling</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3512" to="3522" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Time series prediction and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of intelligent and robotic systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="91" to="103" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning the hidden structure of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<ptr target="http://link.aip.org/link/?JAS/83/1615/1" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1615" to="1626" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fuzzy finite-state automata can be deterministically encoded into recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thornber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="89" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training second-order recurrent neural networks using hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Machine Learning</title>
		<meeting>the Ninth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="363" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Equivalence in knowledge representation: automata, recurrent neural networks, and dynamical fuzzy systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thornber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1623" to="1640" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent neural networks and robust time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="254" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>International Journal of Uncertainty</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predictive state recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6053" to="6064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predictive state representations: A new theory for modeling dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Rudary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th conference on Uncertainty in artificial intelligence</title>
		<meeting>the 20th conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The landlab v1.0 overlandflow component: a python tool for computing shallow-water flow across watersheds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Gasparini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E J</forename><surname>Hobley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W H</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Nudurupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Istanbulluoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geoscientific Model Development</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1645" to="1663" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coevolutionary multi-task learning for feature-based modular pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cripps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="164" to="175" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Time series prediction with recurrent neural networks trained by a hybrid pso-ea algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">13-15</biblScope>
			<biblScope unit="page" from="2342" to="2353" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cooperative coevolution of Elman recurrent neural networks for chaotic time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Time series prediction with multilayer perceptron, fir and elman neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lehtokangas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Congress on Neural Networks</title>
		<meeting>the World Congress on Neural Networks</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="491" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluation of co-evolutionary neural network architectures for time series prediction with mobile application in finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="462" to="473" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recursive estimation and forecasting of non-stationary time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="204" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long-term predictions of chemical processes using recurrent neural networks: a parallel training approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mcavoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Industrial &amp; Engineering Chemistry Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1338" to="1352" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-step-ahead prediction using dynamic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atiya</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608000000484" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="765" to="786" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Nonero Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2313-gaussian-process-priors-with-uncertain-inputs-application-to-multiple-step-apdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Becker, S. Thrun, and K. Obermayer</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dempster-shafer regression for multi-stepahead time-series prediction towards data-driven machinery prognosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0888327008002094" />
	</analytic>
	<monogr>
		<title level="j">Mechanical Systems and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="740" to="751" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-step-ahead time series prediction using multiple-output support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S092523121300917X" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="482" to="493" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Long-term time series prediction using op-elm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grigorievskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Ventelä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Séverin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="50" to="56" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Co-evolutionary multi-task learning with predictive recurrence for multi-step chaotic time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="page" from="21" to="34" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multitl-kelm: A multi-task learning algorithm for multi-step-ahead time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1568494619301656" />
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="227" to="253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A review and comparison of strategies for multi-step ahead time series forecasting based on the {NN5} forecasting competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Taieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0957417412000528" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7067" to="7083" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A comparison of direct and iterated multistep ar methods for forecasting macroeconomic time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcellino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of econometrics</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="499" to="526" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Direct and iterated multistep {AR} methods for difference stationary processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Proietti</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0169207010001068" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="280" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multistep forecasting in the presence of location shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chevillon</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0169207015000801" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-step ahead predictions for critical levels in physiological time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elmoaqet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tilbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1704" to="1714" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reinforced recurrent neural networks for multi-step-ahead flood forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0022169413004150" />
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="page" from="71" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Real-time multi-step-ahead water level forecasting by recurrent neural networks for urban flood control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y.</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0022169414004739" />
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">517</biblScope>
			<biblScope unit="page" from="836" to="846" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-step-ahead prediction of {NOx} emissions for a coal-based boiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smrekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Potočnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senegačnik</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0306261912007751" />
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Comparison of strategies for multi-step ahead photovoltaic power forecasting models based on hybrid group method of data handling networks and least square support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malvoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Congedo</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0360544216304261" />
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="360" to="373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-step prediction of strong earthquake ground motions and seismic responses of {SDOF} systems based on emd-elm method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0267726116000695" />
	</analytic>
	<monogr>
		<title level="j">Soil Dynamics and Earthquake Engineering</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A pattern fusion model for multi-step-ahead cpu load prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0164121212003354" />
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1257" to="1266" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Prediction of short-term wind and wave conditions for marine operations using a multi-step-ahead decomposition-ANFIS model and quantification of its uncertainty</title>
	</analytic>
	<monogr>
		<title level="j">Ocean Engineering</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page">106300</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-step ahead wind speed prediction based on optimal feature extraction, long short term memory neural network and error correction strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0306261918312844" />
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An innovative hybrid approach for multi-step ahead wind speed prediction</title>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1568494619300985" />
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="296" to="309" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep learning applications and challenges in big data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Najafabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Villanustre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muharemagic</surname></persName>
		</author>
		<idno>Train 0.1546± 0.0087 0.3757± 0.0100 0.0416± 0.0074 0.0281± 0.0098 0.0374± 0.0082 0.1088± 0.0009 0.0367± 0.0059 Test 0.1473± 0.0098 0.4314± 0.0122 0.0488± 0.0054 0.0349± 0.0070 0.0427± 0.0072 0.1030± 0.0008 0.0454± 0.0052 Step-1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<editor>FNN-Adam FNN-SGD LSTM BD-LSTM ED-LSTM RNN CNN</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Rossler reporting RMSE mean and 95 % confidence interval (±)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hüsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stagge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="223" to="235" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Competition and collaboration in cooperative coevolution of elman recurrent neural networks for time-series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2404823</idno>
		<ptr target="http://dx.doi.org/10.1109/TNNLS.2015.2404823" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3123" to="3136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deepar: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Recursive bayesian recurrent neural networks for time-series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Mirikitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nikolaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="274" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep learning based ensemble approach for probabilistic wind power forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied energy</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="56" to="70" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep neural networks for energy load forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 26th International Symposium on Industrial Electronics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1483" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A deep cnn-lstm model for particulate matter (pm2.5) forecasting in smart cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Long short term memory (lstm) recurrent neural network (rnn) for discharge level prediction and forecast in cimandiri river, indonesia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sudriani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ridwansyah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rustini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IOP Conference Series: Earth and Environmental Science</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">299</biblScope>
			<biblScope unit="page">12037</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep learning for event-driven stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2327" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Stock market&apos;s price movement prediction with lstm neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1419" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Time series forecasting of COVID-19 transmission in canada using lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K R</forename><surname>Chimmula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos, Solitons &amp; Fractals</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page">109864</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical Systems and Turbulence</title>
		<meeting><address><addrLine>Warwick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Chaos theory and transportation systems: Instructive example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kockelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record: Journal of the Transportation Research Board</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="9" to="17" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertain. Fuzziness Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Evolving deep lstm-based memory networks using an information maximization objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Reinforcement learning with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1475" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks : the official journal of the International Neural Network Society</title>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Tts synthesis with bidirectional lstm based recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a backpropagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hecht-Nielsen</surname></persName>
		</author>
		<title level="m">Theory of the backpropagation neural network,&quot; in International 1989 Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="593" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Oscillation and chaos in physiological control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">4300</biblScope>
			<biblScope unit="page" from="287" to="289" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Deterministic non-periodic flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="267" to="285" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A two-dimensional mapping with a strange attractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hénon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="77" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">An equation for continuous chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rössler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="397" to="398" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Solar cycle forecasting: A nonlinear dynamics approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy and Astrophysics</title>
		<imprint>
			<biblScope unit="volume">377</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Time series prediction: Forecasting the future and understanding the past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gershenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of a NATO Advanced Research Workshop on Comparative Time Series Analysis</title>
		<meeting>a NATO Advanced Research Workshop on Comparative Time Series Analysis<address><addrLine>Santa Fe, New Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">NASDAQ Exchange Daily: 1970-2010 Open, Close, High, Low and Volume</title>
		<idno>accessed: 02-02-2015</idno>
		<ptr target="http://www.nasdaq.com/symbol/aciw/stock-chart" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Langevin-gradient parallel tempering for bayesian neural learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cripps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Competition and collaboration in cooperative coevolution of Elman recurrent neural networks for time-series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3123" to="3136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Co-evolutionary multi-task learning with predictive recurrence for multi-step chaotic time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="page" from="21" to="34" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Multi-step prediction of chaotic timeseries with intermittent failures based on the generalized nonlinear filtering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0096300313002099" />
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="8584" to="8594" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Explore an evolutionary recurrent anfis for modelling multi-step-ahead flood forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0022169419300137" />
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">570</biblScope>
			<biblScope unit="page" from="343" to="355" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
