<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
							<email>lipeixuan@sia.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenyang Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes for Robotics and Intelligent Manufacturing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Opto-Electronic Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Liaoning Province</orgName>
								<orgName type="laboratory">Key Lab of Image Understanding and Computer Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
							<email>hczhao@sia.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenyang Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes for Robotics and Intelligent Manufacturing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Opto-Electronic Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Liaoning Province</orgName>
								<orgName type="laboratory">Key Lab of Image Understanding and Computer Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<email>liupengfei@sia.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenyang Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes for Robotics and Intelligent Manufacturing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Opto-Electronic Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Liaoning Province</orgName>
								<orgName type="laboratory">Key Lab of Image Understanding and Computer Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
							<email>caofeidao@sia.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenyang Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes for Robotics and Intelligent Manufacturing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Opto-Electronic Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Liaoning Province</orgName>
								<orgName type="laboratory">Key Lab of Image Understanding and Computer Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose an efficient and accurate monocular 3D detection framework in single shot. Most successful 3D detectors take the projection constraint from the 3D bounding box to the 2D box as an important component. Four edges of a 2D box provide only four constraints and the performance deteriorates dramatically with the small error of the 2D detector. Different from these approaches, our method predicts the nine perspective keypoints of a 3D bounding box in image space, and then utilize the geometric relationship of 3D and 2D perspectives to recover the dimension, location, and orientation in 3D space. In this method, the properties of the object can be predicted stably even when the estimation of keypoints is very noisy, which enables us to obtain fast detection speed with a small architecture. Training our method only uses the 3D properties of the object without the need for external networks or supervision data. Our method is the first real-time system for monocular image 3D detection while achieves state-ofthe-art performance on the KITTI benchmark. Code will be released at https://github.com/Banconxuan/RTM3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is an essential component of scene perception and motion prediction in autonomous driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Currently, most powerful 3D detectors heavily rely on 3D LIDAR laser scanners for the reason that it can provide scene locations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31]</ref>. However, the LiDAR-based systems are expensive and not conducive to embedding into the current vehicle shape. In comparison, monocular camera devices are cheaper and convenient which makes it drawing an increasing attention in many application scenarios <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref>. In this paper, the scope of our research lies in 3D object detection from only monocular RGB image.</p><p>Monocular 3D object detection methods can be roughly divided into two categories by the type of training data: one utilizes complex features, such as instance segmentation, vehicle shape prior and even depth map to select best proposals in multi-stage fusion module <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42]</ref>. These features require additional annotation work to train some stand-alone networks which will consume plenty of computing resources in the training and inferring stages. Another one only employs 2D bounding box and properties of a 3D object as the supervised data <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>. In this case, an intuitional idea is to building a deep regression network to predict directly 3D information of the object. This can cause performance bottlenecks due to the large search space. For this reason, recent works have clearly pointed out that apply geometric constraints from 3D box vertexes to 2D box edges to refine or directly predict object parameters <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>. However, four edges of a 2D bounding box provide only four constraints on recovering a 3D bounding box while each vertex of a 3D bounding box might correspond to any edges in the 2D box, which will takes 4,096 of the same calculations to get one result <ref type="bibr" target="#b27">[28]</ref>. Meanwhile, the strong reliance on the 2D box causes a sharp decline in 3D detection performance when predictions of 2D detectors even have a slight error. Therefore, most of these methods take advantage of two-stage detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref> to ensure the accuracy of 2D box prediction, which limit the upper-bound of the detection speed.</p><p>In this paper, we propose an efficient and accurate monocular 3D detection framework in the form of onestage, which be tailored for 3D detection without relying on 2D detectors. The framework can be divided into two main parts, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. First, we perform a onestage fully convolutional architecture to predict 9 of the 2D keypoints which are projected points from 8 vertexes and central point of 3D bounding box. This 9 keypoints provides 18 geometric constrains on the 3D bounding box. Inspired by CenterNet <ref type="bibr" target="#b46">[47]</ref>, we model the relationship between the eight vertexes and the central point to solve the keypoints grouping and the vertexes order problem. The SIFT, SUFT and other traditional keypoint detection methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref>computed an image pyramid to solve the scaleinvariant problem. A similar strategy was used by Center-Net as a post-processing step to further improve detection accuracy, which slows the inference speed. Note that the Feature Pyramid Network(FPN) <ref type="bibr" target="#b22">[23]</ref> in 2D object detection is not applicable to the network of keypoint detection, because adjacent keypoints may overlap in the case of smallscale prediction. We propose a novel multi-scale pyramid of keypoint detection to generate a scale-space response. The final activate map of keypoints can be obtained by means of the soft-weighted pyramid. Given the 9 projected points, the next step is to minimize the reprojection error over the perspective of 3D points that parameterized by the location, dimension, and orientation of the object. We formulate the reprojection error as the form of multivariate equations in se 3 space, which can generate the detection results accurately and efficiently. We also discuss the effect of different prior information on our keypoint-based method, such as dimension, orientation, and distance. The prerequisite for obtaining this information is not to add too much computation so as not to affect the final detection speed. We model these priors and reprojection error term into an overall energy function in order to further improve 3D estimation.</p><p>To summarize, our main contributions are the following:</p><p>• We formulate the monocular 3D detection as the keypoint detection problem and combine the geometric constrains to generate properties of 3D objects more efficiently and accurately.</p><p>• We propose a novel one-stage and multi-scale network for 3D keypoint detection which provide the accurate project points for multi-scale object.</p><p>• We propose an overall energy function that can jointly optimize the prior and 3D object information.</p><p>• Evaluation on the KITTI benchmark, We are the first real-time 3D detection method using only images and achieves better accuracy under the same running time in comparing other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The 3D detection can be divided into two groups by the type of data: LiDAR-, and image-based methods. LiDAR-based method. LiDAR-based systems can provide accuracy and reliable point cloud of object surfaces in 3D scene. Therefor, most of the recent 3D object detection employ it in different representation to obtain the state-of-theart model <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b20">21]</ref>. Extra Data or Network for Image-based 3D Object Detection. In the last years, many studies develop the 3D detection in an image-based method for the reason that camera devices are more convenient and much cheaper. To complement the lacked depth information in image-based detection, most of the previous approaches heavily relied on the stand-alone network or additional labeling data, such as instance segmentation, stereo, wire-frame model, CAD prior , and depth, as shown in <ref type="table">Table.</ref> 1. Among them, monocular 3D detection is a more challenging task due to the difficulty of obtaining reliable 3D information from a single image. One of the first examples <ref type="bibr" target="#b6">[7]</ref> enumerate a multitude of 3D proposals from pre-defined space where the objects may appear as the geometrical heuristics. Then it takes the other complex prior, such as shape, instance segmentation, contextual feature, to filter out dreadful proposals and scoring them by a classifier. To make up for the lack of depth, <ref type="bibr" target="#b41">[42]</ref> embed a pre-trained stand-alone module to estimate the disparity and 3D point cloud. The disparity map concatenates the front view representation to help the 2D proposal network and the 3D detection can be boosted by fusing the extracted feature after RoI pooling and point cloud. As a followup, <ref type="bibr" target="#b26">[27]</ref> combines the 2D detector and monocular depth estimation model to obtain the 2D box and corresponding point cloud. The final 3D box can be obtained by the regression of PointNet <ref type="bibr" target="#b31">[32]</ref> after the aggregation of the image feature and 3D point information through attention mechanism, which achieves the best performance in the monocular image. Intuitively, these methods would certainly increase the accuracy of the detection, but the additional network and annotated data would lead to more computation and labor-intensive work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Real Time Stereo Depth Shape/CAD Segmentation Mono3D <ref type="bibr" target="#b6">[7]</ref> 3DOP <ref type="bibr" target="#b7">[8]</ref>, stereoRCNN <ref type="bibr" target="#b21">[22]</ref> MF3D <ref type="bibr" target="#b41">[42]</ref>, Peseudo-LiDAR <ref type="bibr" target="#b38">[39]</ref>,</p><p>MonoPSR <ref type="bibr" target="#b16">[17]</ref> AM3D <ref type="bibr" target="#b26">[27]</ref> Mono3D++ <ref type="bibr" target="#b13">[14]</ref>,Deep-MENTA <ref type="bibr" target="#b5">[6]</ref>, 3DVP <ref type="bibr" target="#b39">[40]</ref> Deep3DBox <ref type="bibr" target="#b27">[28]</ref>,GS3D <ref type="bibr" target="#b19">[20]</ref>,MonoGRNet <ref type="bibr" target="#b32">[33]</ref>, FQNet <ref type="bibr" target="#b24">[25]</ref>, M3D-RPN <ref type="bibr" target="#b3">[4]</ref> Shift-RCNN <ref type="bibr" target="#b29">[30]</ref> Ours(RTM3D) Image-only in Monocular 3D Object Detection. Recent works have tried to fully explore the potency of RGB images for 3D detection. Most of them include geometric constraints and 2D detectors to explicitly describe the 3D information of the object. <ref type="bibr" target="#b27">[28]</ref> uses CNN to estimate the dimension and orientation extracted feature from the 2D box, then it proposes to obtain the location of an object by using the geometric constraints of the perspective relationship between 3D points and 2D box edges. This contribution is followed by most image-based detection methods either in refinement step or as direct calculation on 3D objects <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>. All we know in this constraint is that certain 3D points are projected onto 2D edges, but the corresponding relationship and the exact location of the projection are not clear. Therefore, it needs to exhaustively enumerate 8 4 = 4096 configurations to determine the final correspondence and can only provide four constraints, which is not sufficient for fully 3D representation in 9 parameters. It led to the need to estimate other prior information. Nevertheless, possible inaccuracies in the 2D bounding boxes may result in a grossly inaccurate solution with a small number of constraints. Therefore, most of these methods obtain more accurate 2D box through a two-stage detector, which is difficult to get realtime speed. Keypoints in Monocular 3D Object Detection. It is believed that the detection accuracy of occluded and truncated objects can be improved by deducing complete shapes from vehicle keypoints <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b28">29]</ref>. They represent the regularshape vehicles as a wire-frame template , which is obtained from a large number of CAD models. To train the keypoint detection network, they need to re-label the data set and even use depth maps to enhance the detection capability. <ref type="bibr" target="#b13">[14]</ref> is most related to our work, which also considers the wire-frame model as prior information. Furthermore, It jointly optimizes the 2D box, 2D keypoints, 3D orientation, scale hypotheses, shape hypotheses, and depth with four different networks. This has limitations in run time.</p><p>In contrast to prior work, we reformulate the 3D detection as the coarse keypoints detection task. Instead of predicting the 3D box based on an off-the-shelf 2D detectors or other data generators, we build a network to predict 9 of 2D keypoints projected by vertexes and center of 3D bounding box while minimize the reprojection error to find an optimal result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section. We first describe the overall architecture for keypoint detection. Then we detail how to find the 3D vehicles from the generated keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Keypoint Detection Network</head><p>Our keypoint detection network takes an only RGB image as the input and generates the perspective points from vertexes and center of the 3D bounding box. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, it consists of three components: backbone, keypoint feature pyramid, and detection head. The main architecture adopts a one-stage strategy that shares a similar layout with the anchor-free 2D object detector <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref>, which allows us to get a fast detection speed. Details of the network are given below. Backbone. For the trade-off between speed and accuracy, we use two different structures as our backbones: ResNet-18 <ref type="bibr" target="#b12">[13]</ref> and DLA-34 <ref type="bibr" target="#b44">[45]</ref>. All models take a single RGB image I ∈ R W ×H×3 and downsample the input with factor S = 4. The ResNet-18 and DLA-34 build for image classification network, the maximal downsample factor is ×32. We upsample the bottleneck thrice by three bilinear interpolations and 1 × 1 convolutional layer. Before the upsampling layers, we concatenate the corresponding feature maps of the low level while adding one 1 × 1 convolutional layers for channel dimension reduction. After three upsampling layers, the channels are 256, 128, 64, respectively. Keypoint Feature Pyramid. Keypoint in the image have no difference in size. Therefore, the keypoint detection is not suitable for using the Feature Pyramid Network(FPN) <ref type="bibr" target="#b22">[23]</ref>, which detect multi-scale 2D box in different pyramid layers. We propose a novel approach Keypoint Feature Pyramid Network (KFPN) to detect scale-invariant keypoints in the point-wise space, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Assuming we have F scale feature maps, we first resize each  scale f, 1 &lt; f &lt; F back to the size of maximal scale, which yields the feature mapsf 1&lt;f &lt;F . Then, we generate soft weight by a softmax operation to denote the importance of each scale. The finally scale-space score map S score is obtained by linear weighing sum. In detail, it can be defined as:</p><formula xml:id="formula_0">S score = ff sof tmax(f )<label>(1)</label></formula><p>where denote element-wise product. Detection Head. The detection head is comprised of three fundamental components and six optional components which can be arbitrarily selected to boost the accuracy of 3D detection with a little computational consumption. Inspired by CenterNet <ref type="bibr" target="#b46">[47]</ref>, we take a keypoint as the maincenter for connecting all features. Since the 3D projection point of the object may exceed the image boundary in the case of truncation, the center point of the 2D box will be  Although the 18 constraints by the 9 keypoints have an ability to recover the 3D information of the object, more prior information can provide more constraints and further improve the detection performance. We offer a number of options to meet different needs for accuracy and speed. The center offset M os ∈ R Training. The all heatmaps of keypoint training strategy follow the <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b18">19]</ref>. The loss solves the imbalance of positive and negative samples with focal loss <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_1">L K kp = − 1 N K k=1 H/S x=1 W/S y=1 (1 −p kxy ) α log(p kxy ) if p kxy = 1 (1 − p kxy ) βp kxy log(1 −p kxy ) otherwise<label>(2)</label></formula><p>where K is the channels of different keypoints, K = C in maincenter and K = 9 in vertexes. N is the number of maincenter or vertexes in an image, and α and β are the hyper-parameters to reduce the loss weight of negative and easy positive samples. We set is α = 2 and β = 4 in all experiments following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b18">19]</ref>. p kxy can be defined by Gaussian kernel p xy = exp − x 2 +y 2 2σ centered by ground truth keypointp xy . For σ, we find the max area A max and min area A min of 2D box in training data and set two hyper-parameters σ max and σ min . We then define the σ = A( σmax−σmin Amax−Amin ) for a object with size A. For regression of dimension and distance, we define the residual term as: denotes if maincenter appears in position x, y. The offset of maincenter, vertexes are trained with an L1 loss following <ref type="bibr" target="#b46">[47]</ref>:</p><formula xml:id="formula_2">L D = 1 3N</formula><formula xml:id="formula_3">L m of f = 1 2N H/S x=1 W/S y=1 1 obj xy M xy os − p m S −p m L v of f = 1 2N H/S x=1 W/S y=1 1 ver xy V xy os − p v S −p v<label>(4)</label></formula><p>where p m , p v are the position of maincenter and vertexes in the original image. The regression coordinate of vertexes with an L1 loss as:</p><formula xml:id="formula_4">L ver = 1 N 8 k=1 H/S x=1 W/S y=1 1 ver xy V (2k−1):(2k)xy c − p v − p m S<label>(5)</label></formula><p>Finial, we define the multi-task loss for keypoint detection as:</p><formula xml:id="formula_5">L =ω main L C kp + ω kpver L 8 kp + ω ver L ver + ω dim L D + ω ori L ori + ω Z L dis + ω m of f L m of f + ω v of f L v of f (6)</formula><p>We empirical set ω main = 1, ω kpver = 1, ω dim = 1, ω ori = 0.5, ω dis = 0.1, ω m of f = 0.5 and ω v of f = 0.5 in our experimental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Bounding Box Estimate</head><p>Consider an image I, a set of i = 1...N object are represented by 9 keypoints and other optional prior, given by our keypoint detection network. We define this keypoints as kp ij for j ∈ 1...9, dimension as D i , orientation asθ i , and distance as Z i . The corresponding 3D bounding box B i can be defined by its rotation R i (θ), position</p><formula xml:id="formula_6">T i = [T x i , T y i , T z i ] T , and dimensions D i = [h i , w i , l i ] T .</formula><p>Our goal is to estimate the 3D bounding box B i , whose projections of center and 3D vertexes on the image space best fit the corresponding 2D keypoints kp ij . This can be solved by minimize the reprojection error of 3D keypoints and 2D keypoints. We formulate it and other prior errors as a nonlinear least squares optimization problem:</p><formula xml:id="formula_7">R * , T * , D * = arg max {R,T,D} Ri,Ti,Di e cp R i , T i , D i , kp i 2 Σi +ω d e d D i , D i 2 2 + ω r e r R i ,θ i 2 2 (7)</formula><p>where e cp (..), e d (..), e r (..) are measurement error of camera-point, dimension prior and orientation prior respectively. We set ω d = 1 and ω r = 1 in our experimental. Σ is the covariance matrix of keypoints projection error. It is the confidence extracted from the heatmap corresponding to the keypoints:</p><formula xml:id="formula_8">Σ i = diag Sof tmax V ( kp 1:8 i ), M ( kp 9 i ) (8)</formula><p>In the rest of the section, we will first define this error item, and then introduce the way to optimize the formulation. Camera-Point. Following the <ref type="bibr" target="#b9">[10]</ref>, we define the homogeneous coordinate of eight vertexes and 3D center as:</p><formula xml:id="formula_9">P i 3D = diag(D i )Cor Cor = 0 0 0 0 −1 −1 −1 −1 −1/2 1/2 −1/2 −1/2 1/2 1/2 −1/2 −1/2 1/2 0 1/2 1/2 −1/2 −1/2 1/2 1/2 −1/2 −1/2 0 1 1 1 1 1 1 1 1 1 (9)</formula><p>Given the camera intrinsics matrix K, the projection of these 3D points into the image coordinate is:</p><formula xml:id="formula_10">kp i = 1 s i K R T 0 T 1 diag(D i )Cor = 1 s i K exp(ξ ∧ )diag(D i )Cor<label>(10)</label></formula><p>where ξ ∈ se 3 and exp maps the se 3 into SE 3 space. The projection coordinate should fit tightly into 2D keypoints detected by the detection network. Therefore, the camerapoint error is then defined as:</p><formula xml:id="formula_11">e cp = kp i − kp i<label>(11)</label></formula><p>Minimizing the camera-point error needs the Jacobians in se 3 space. It is given by:</p><formula xml:id="formula_12">∂e cp ∂δξ = −   fx Z 0 − fxX Z 2 0 fy Z 0 − fyY Z 2   · I, −P ∧ ∂e cp ∂D i = − 1 9 9 col=1   fx Z 0 − fxX Z 2 0 fy Z 0 − fyY Z 2   · R · Cor col (12) where P = [X , Y , Z ] T = (exp(ξ ∧ P )) 1:3 .</formula><p>Dimension-Prior: The e d is sample defined as:</p><formula xml:id="formula_13">e d = D i − D i<label>(13)</label></formula><p>Rotation-Prior: We define e r in SE3 space and use log to map the error into its tangent vector space:</p><formula xml:id="formula_14">e r = log(R −1 R(θ)) ∨ se3<label>(14)</label></formula><p>These multivariate equations can be solved via the Gaussnewton or Levenberg-Marquardt algorithm in the g2o library <ref type="bibr" target="#b17">[18]</ref>. A good initialisation is mandatory using this optimization strategy. We adopt the prior information generated by keypoint detection network as the initialization value, which is very important in improving the detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We evaluated our experiments on the KITTI 3D detection benchmark <ref type="bibr" target="#b9">[10]</ref>, which has a total of 7481 training images and 7518 test images. We follow the <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b40">[41]</ref> to split the training set as train1, val 1 and train2, val 2 respectively. We comprehensively compare our framework and other method on this two validation as well as test set.</p><p>We implemented our network using PyTorch, with the machine i7-8086K CPU and 2 1080Ti GPUs. We pad the original image to 1280 × 384 for training and testing. We project the 3D bounding box of Ground Truths in the left and right images to obtain Ground Truth keypoints and use the flipping as image augmentation, which makes our dataset is quadruple with the origin training set. We run Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with a base learning rate of 0.0002 for 300 epochs and reduce 10× at 150 and 180 epochs. For standard deviation of Gaussian kernel, we set σ max = 19 and σ min = 3. Based on the statistics of KITTI dataset, we setl = 3.89,w = 1.62,h = 1.53 and σl = 0.41, σw = 0.1, σh = 0.13. In the inference step, after 3×3 max pooling, we filter the maincenter and keypoints with threshold 0.4 and 0.1 respectively, and only keypoints that in the image size range is sent into the geometric constraint module. The backbone networks are initialized by a classification model pre-trained on the ImageNet data set. Finally, The ResNet-18 takes about three days with batch size 16 and DLA-34 for four days with batch size 30 in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Other Methods</head><p>To fully evaluate the performance of our keypoint-based method, for each task three official evaluation metrics be reported in KITTI: average precision for 3D intersectionover-union (AP 3D ), average precision for Birds Eye View (AP BEV ), and Average Orientation Similarity (AOS) if 2D bounding box available. We evaluate our method at three difficulty settings: easy, moderate, and hard, according to the object's occlusion, truncation, and height in the image space <ref type="bibr" target="#b9">[10]</ref>. AP 3D and AP BEV . We compare our method with current image-based SOTA approaches and also provide a comparison about running time. However, it is not realistic to list the running times of all previous methods because most of them do not report their efficiency. The results AP 3D , AP BEV and running time are shown in <ref type="table" target="#tab_3">Table 2</ref> and 3, respectively. ResNet-18 as the backbone achieves the best speed while our accuracy outperforms most of the image-only method. In particular, it is more than 100 times faster than Mono3D <ref type="bibr" target="#b6">[7]</ref> while outperforms over 10% for both AP BEV and AP 3d across all datasets. In addition, our ResNet-18 method is more than 75 times faster while  <ref type="table">Table 3</ref>. Comparison of our method to image-based 3D detection frameworks for car category, evaluated using metric APBEV on val1 / val2 of KITTI data set.</p><p>having a comparable accuracy than 3DOP <ref type="bibr" target="#b7">[8]</ref>, which employs stereo images as the input. DLA-34 as the backbone achieves the best accuracy while having relatively good speed. It is faster about 3 times than the recently proposed M3D-RPN <ref type="bibr" target="#b3">[4]</ref> while achieves the improvement in most of the metrics. Note that comparing our method with this all approaches is unfair because most of these approaches rely on extra stand-alone network or data in addition to monocular images. Nevertheless, we achieve the best speed with better performance. Results on the KITTI testing set. We also evaluate our results on the KITTI testing set, as shown in <ref type="table">Table.</ref> 4. More details can be found on the KITTI website 1 . <ref type="figure" target="#fig_5">Fig. 4</ref> shows some qualitative results of our method. We visualize the keypoint detection network outputs, geometric constraint module outputs and BEV images. The results of the projected 3D box on image demonstrate than our method can handle crowded and truncated objects. The  results of the BEV image show that our method has an accuracy localization in different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Effect of Optional Components. Three optional components be employed to enhance our method: dimension, orientation, distance and keypoints offset. We experiment with different combinations to demonstrate their effect on 3D detection. The results are shown in <ref type="table">Table.</ref>5, we train our network with DLA-34 backbone and evaluate it using AP 3D and AP BEV . The combinations of dimension, orientation, distance and keypoints offset achieve the best accuracy meanwhile have a faster running speed. This is be- cause we take the output predicted by our network as the initial value of the geometric optimization module, which can reduce the search space of the gradient descent method.   2D Detection and Orientation. Although our focus is on 3D detection, we also compare the performance of our methods in 2D detection and orientation evaluation. We report the AOS and AP with a threshold IoU=0.7 for comparison. The results are shown in <ref type="table">Table.</ref> 7, the Deep3DBox train MS-CNN <ref type="bibr" target="#b4">[5]</ref> in KITTI to produce 2D bounding box and adopt VGG16 <ref type="bibr" target="#b36">[37]</ref> for orientation prediction, which gives him the highest accuracy. Deep3Dbox takes advantage of better 2D detectors, however, our AP 3D outperforms it by about 20% in moderate sets, which emphasize the importance of customizing the network specifically for 3D detection. Another interesting finding is that the 2D accuracy of back-projection 3D results is better than the direct prediction, thanks to our method that can infer the occlusive area of the object.</p><p>Method AP 2D AOS Mono3D <ref type="bibr" target="#b6">[7]</ref> 88.67/ -86.28/ -3DOP <ref type="bibr" target="#b7">[8]</ref> 88.07/ -85.80/ -Deep3DBox <ref type="bibr" target="#b27">[28]</ref> -/97.20 -/96.68 DeepMANTA <ref type="bibr" target="#b5">[6]</ref>   <ref type="table">Table 7</ref>. Comparing of 2D detection AP2D with IoU=0.7 and orientation AOS results for car category evaluated on val1 / val2 of KITTI data set. Only the results under the moderate criteria are shown. Ours(2D) represents the results from the keypoint detection network, and Ours(3D) is the 2D bounding box of the projected 3D box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a faster and more accurate monocular 3D object detection method for autonomous driving scenarios. We reformulate 3D detection as the keypoint detection problem and show how to recover the 3D bounding box by using keypoints and geometric constraints. We specially customize the point detection network for 3D detection, which can output keypoints of the 3D box and other prior information of the object using only images. Our geometry module formulates this prior to easy-to-optimize loss functions. Our approach generates a stable and accurate 3D bounding box without containing stand-alone networks, additional annotation while achieving real-time running speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of proposed method: We first predict ordinal keypoints projected in the image space by eight vertexes and a central point of a 3D object. We then reformulate the estimation of the 3D bounding box as the problem of minimizing the energy function by using geometric constraints of perspective projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An overview of proposed keypoint detection architecture: It takes only the RGB images as the input and outputs main center heatmap, vertexes heatmap, and vertexes coordinate as the base module to estimate 3D bounding box. It can also predict other alternative priors to further improve the performance of 3D detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our keypoint feature pyramid network(KFPN). selected more appropriately. The heatmap can be define as M ∈ [0, 1] H S × W S ×C , where C is the number of object cat-egories. Another fundamental component is the heatmap V ∈ [0, 1] H S × W S ×9 of nine perspective points projected by vertexes and center of 3D bounding box. For keypoints association of one object, we also regress an local offset V c ∈ R H S × W S ×18 from the maincenter as an indication. Keypoints of V closest to the coordinates from V c are taken as a group of one object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>S</head><label></label><figDesc>×2 and vertexes offset V os ∈ R H S × W S ×2 are discretization error for each keypoint in heatmaps. The dimension D ∈ R H S × W S ×3of 3D object have a smaller variance, which makes it easy to predict. The rotation R(θ) of an object only by parametrized by orientation θ (yaw) in the autonomous driving scene. We employ the Multi-Bin based method<ref type="bibr" target="#b27">[28]</ref> to regress the local orientation. We classify the probability with cosin and sine offset of the local angle in one bin, which generates feature map of orientation O ∈ R H S × W S ×8 with two bins. We also regress the depth Z ∈ R H S × W S ×1 of 3D box center, which can be used as the initialization value to speed up the solution in Sec .3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 ( 3 )</head><label>23</label><figDesc>xy ) − log( Z xy ) We set ∆ D xy = log Dxy−D Dσ , whereD and D σ are the mean and standard deviation dimensions of training data. 1 obj xy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of our 3D detection. From top to bottom are keypoints, projections of the 3D bounding box and bird's eye view image, ground truths in green and our results in blue. The crimson arrows, green arrows, and red arrows point to occluded, distant, and truncated objects, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the real-time status and the requirements of additional data in different image-based detection approaches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our method to image-based 3D detection frameworks for car category evaluated using metric AP3D on val1 / val2 of KITTI data set. Extra means the extra data used in training.</figDesc><table><row><cell>Method</cell><cell>Extra</cell><cell>Time</cell><cell>Easy</cell><cell></cell><cell cols="2">AP 3D (IoU=0.5) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP 3D (IoU=0.7) Moderate</cell><cell>Hard</cell></row><row><cell>Mono3D [7]</cell><cell>Mask</cell><cell>4.2 s</cell><cell cols="2">25.19/ -</cell><cell>18.20/ -</cell><cell></cell><cell>15.52/ -</cell><cell cols="2">2.53 / -</cell><cell>2.31 / -</cell><cell>2.31 / -</cell></row><row><cell>3DOP [8]</cell><cell>Stereo</cell><cell>3 s</cell><cell cols="2">46.04/ -</cell><cell>34.63/ -</cell><cell></cell><cell>30.09/ -</cell><cell cols="2">6.55 / -</cell><cell>5.07 / -</cell><cell>4.10 / -</cell></row><row><cell>MF3D [42]</cell><cell>Depth</cell><cell>-</cell><cell cols="2">47.88/45.57</cell><cell>29.48/30.03</cell><cell></cell><cell>26.44/23.95</cell><cell cols="2">10.53/ 7.85</cell><cell>5.69 / 5.39</cell><cell>5.39 / 4.73</cell></row><row><cell>Mono3D++ [14]</cell><cell cols="2">Depth+Shape &gt;0.6s</cell><cell cols="2">42.00/ -</cell><cell>29.80/ -</cell><cell></cell><cell>24.20/ -</cell><cell cols="2">10.60/ -</cell><cell>7.90 / -</cell><cell>5.70 / -</cell></row><row><cell>GS3D [20]</cell><cell>None</cell><cell>2.3s</cell><cell cols="2">32.15/30.60</cell><cell>29.89/26.40</cell><cell></cell><cell>26.19/22.89</cell><cell cols="2">13.46/11.63</cell><cell>10.97/10.51</cell><cell>10.38/10.51</cell></row><row><cell>M3D-RPN [4]</cell><cell>None</cell><cell>0.16s</cell><cell cols="2">48.96/49.89</cell><cell>39.57/36.14</cell><cell></cell><cell>33.01/28.98</cell><cell cols="2">20.27/20.40</cell><cell>17.06/16.48</cell><cell>15.21/13.34</cell></row><row><cell>Deep3DBox [28]</cell><cell>None</cell><cell>-</cell><cell cols="2">27.04/ -</cell><cell>20.55/ -</cell><cell></cell><cell>15.88/ -</cell><cell cols="2">5.85 / -</cell><cell>4.10 / -</cell><cell>3.84 / -</cell></row><row><cell>MonoGRNet [33]</cell><cell>None</cell><cell>0.06s</cell><cell cols="2">50.51/ -</cell><cell>36.97/ -</cell><cell></cell><cell>30.82/ -</cell><cell cols="2">13.88/ -</cell><cell>10.19/ -</cell><cell>7.62 / -</cell></row><row><cell>FQNet[25]</cell><cell>None</cell><cell>3.33s</cell><cell cols="2">28.16/28.98</cell><cell>21.02/20.71</cell><cell></cell><cell>19.91/18.59</cell><cell cols="2">5.98 / 5.45</cell><cell>5.50 / 5.11</cell><cell>4.75 / 4.45</cell></row><row><cell>Ours (ResNet18)</cell><cell>None</cell><cell>0.035 s</cell><cell cols="2">47.43/46.52</cell><cell>33.86/32.61</cell><cell></cell><cell>31.04/30.95</cell><cell cols="2">18.13/18.38</cell><cell>14.14/14.66</cell><cell>13.33/12.35</cell></row><row><cell>Ours (DLA34)</cell><cell>None</cell><cell>0.055 s</cell><cell cols="2">54.36/52.59</cell><cell>41.90/40.96</cell><cell></cell><cell>35.84/34.95</cell><cell cols="2">20.77/19.47</cell><cell>16.86/16.29</cell><cell>16.63/15.57</cell></row><row><cell>Method</cell><cell>Extra</cell><cell>Time</cell><cell>Easy</cell><cell cols="2">AP BEV (IoU=0.5) Moderate</cell><cell></cell><cell>Hard</cell><cell>Easy</cell><cell>AP BEV (IoU=0.7) Moderate</cell><cell>Hard</cell></row><row><cell>Mono3D [7]</cell><cell>Mask</cell><cell>4.2 s</cell><cell>30.50/ -</cell><cell></cell><cell>22.39/ -</cell><cell cols="2">19.16/ -</cell><cell>5.22 / -</cell><cell>5.19 / -</cell><cell>4.13 / -</cell></row><row><cell>3DOP [8]</cell><cell>Stereo</cell><cell>3 s</cell><cell>55.04/ -</cell><cell></cell><cell>41.25/ -</cell><cell cols="2">34.55/ -</cell><cell>12.63/ -</cell><cell>9.49 / -</cell><cell>7.59 / -</cell></row><row><cell>MF3D [42]</cell><cell>Depth</cell><cell>-</cell><cell>55.02/54.18</cell><cell></cell><cell>36.73/38.06</cell><cell cols="2">31.27/31.46</cell><cell>22.03/19.20</cell><cell>13.63/12.17</cell><cell>11.60/10.89</cell></row><row><cell>Mono3D++ [14]</cell><cell>Depth+Shape</cell><cell>&gt;0.6s</cell><cell>46.70/ -</cell><cell></cell><cell>34.30/ -</cell><cell cols="2">28.10/ -</cell><cell>16.70/ -</cell><cell>11.50/ -</cell><cell>10.10/ -</cell></row><row><cell>GS3D [20]</cell><cell>None</cell><cell>2.3s</cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>M3D-RPN [4]</cell><cell>None</cell><cell>0.16s</cell><cell>55.37/55.87</cell><cell></cell><cell>42.49/41.36</cell><cell cols="2">35.29/34.08</cell><cell>25.94/26.86</cell><cell>21.18/21.15</cell><cell>17.90/17.14</cell></row><row><cell>Deep3DBox [28]</cell><cell>None</cell><cell>-</cell><cell>30.02/ -</cell><cell></cell><cell>23.77/ -</cell><cell cols="2">18.83/ -</cell><cell>9.99 / -</cell><cell>7.71 / -</cell><cell>5.30 / -</cell></row><row><cell>MonoGRNet [33]</cell><cell>None</cell><cell>0.06s</cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>FQNet[25]</cell><cell>None</cell><cell>3.33s</cell><cell>32.57/33.37</cell><cell></cell><cell>24.60/26.29</cell><cell cols="2">21.25/21.57</cell><cell>9.50 /10.45</cell><cell>8.02 / 8.59</cell><cell>7.71 / 7.43</cell></row><row><cell>Ours(ResNet18)</cell><cell>None</cell><cell>0.035s</cell><cell>52.79/41.91</cell><cell></cell><cell>35.92/34.28</cell><cell cols="2">33.02/28.88</cell><cell>20.81/10.84</cell><cell>16.60/16.48</cell><cell>15.80/15.45</cell></row><row><cell>Ours (DLA34)</cell><cell>None</cell><cell>0.055 s</cell><cell>57.47/56.90</cell><cell></cell><cell>44.16/44.69</cell><cell cols="2">42.31/41.75</cell><cell>25.56/24.74</cell><cell>22.12/22.03</cell><cell>20.91/18.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparing 3D detection AP3D on KITTI testing set. We use the DLA-34 as the backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>51.21 40.73 35.00 18.23 17.05 15.94 √ 0.061 25.35 22.33 21.18 3.12 3.43 2.97 √ √ 0.057 54.18 41.34 34.89 20.23 16.02 15.94 √ √ √ 0.055 54.20 41.56 35.13 20.76 16.80 16.25 √ √ √ √ 0.055 54.36 41.90 35.84 20.77 16.86 16.36</figDesc><table><row><cell>dim ori dist off time(s) √ 0.058</cell><cell>AP 3D (IoU=0.5) Easy Mode Hard Easy Mode Hard AP 3D (IoU=0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of different optional selecting results on val1 set. We use the DLA-34 as the backbone.Effect of Keypoint FPN. We propose keypoint FPN as a strategy to improve the performance of multi-scale keypoint detection. To better understand its effect, we compare the AP 3D and AP BEV with and without KFPN. The details are shown inTable. 6, using KFPN achieves the improvement across all sets while no significant change in time consumption. .054 50.14 40.73 34.94 17.47 15.99 15.36 w/ 0.055 54.36 41.90 35.84 20.77 16.86 16.36</figDesc><table><row><cell>KFPN time</cell><cell>AP 3D (IoU=0.7) Easy Mode Hard Easy Mode Hard AP 3D (IoU=0.5)</cell></row><row><cell>w/o 0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparing 3D detection AP3D of w/o KFPN and w/ KFPN for car category on val1 set. We use the DLA-34 as the backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>90.89/91.01 90.66/90.66 GS3D [20] 88.85/90.02 87.52/89.13 Ours(2D) 90.14/91.85 89.58/89.22 Ours(3D) 90.41/92.08 89.95/89.40</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cvlibs.net/datasets/kitti/eval_ object.php?obj_benchmark=3d</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3d scene flow estimation in autonomous driving scenarios?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">Abu</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2574" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Birdnet: a 3d object detection framework from lidar information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">Miguel</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cruzado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo De La</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3517" to="3523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">european conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03446</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<title level="m">A method for stochastic optimization. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11867" to="11876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">G 2 o: A general framework for graph optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Kummerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hauke</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3607" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6851" to="6860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falak</forename><surname>Gv Sai Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Madhava</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andretti</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Byeong-Moon Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d object localisation from multi-view image detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosimo</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Crocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio Del</forename><surname>Bue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1281" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cubeslam: Monocular 3-d object slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Muhammad Zeeshan Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3678" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
