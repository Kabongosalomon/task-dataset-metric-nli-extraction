<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speeding up Word Mover&apos;s Distance and its variants via properties of distances between embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Werner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Laber</surname></persName>
						</author>
						<title level="a" type="main">Speeding up Word Mover&apos;s Distance and its variants via properties of distances between embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Word Mover's Distance (WMD) proposed by Kusner et al. is a distance between documents that takes advantage of semantic relations among words that are captured by their embeddings. This distance proved to be quite effective, obtaining state-of-art error rates for classification tasks, but is also impracticable for large collections/documents due to its computational complexity. For circumventing this problem, variants of WMD have been proposed. Among them, Relaxed Word Mover's Distance (RWMD) is one of the most successful due to its simplicity, effectiveness, and also because of its fast implementations.</p><p>Relying on assumptions that are supported by empirical properties of the distances between embeddings, we propose an approach to speed up both WMD and RWMD. Experiments over 10 datasets suggest that our approach leads to a significant speed-up in document classification tasks while maintaining the same error rates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Document comparison is a fundamental step in several applications such as recommendation, clustering, search, and categorization. In its simplest version, this task consists of computing the distance between a single pair of documents.</p><p>The document representation is an essential factor in the definition of a distance. Arguably, the most employed document representations due to its simplicity and good results are the Bag-of-Words (BOW) and the Term Frequency -Inverse Document Frequency (TF-IDF). These representations are based on word counts, and so they may lose information that is relevant for some applications, such as the ordering among words in a document, co-occurrence, and semantic relations between different words. Thus, richer representations that take into account some of this information have been proposed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Up to a few years ago, semantic relations were barely used because there was no adequate methodology of how to obtain them. Consequently, researchers eventually decided to use ontologies as a way to mitigate this issue <ref type="bibr" target="#b11">[12]</ref>, although this makes applications dependent on an external knowledge base. This scenario changed with the emergence of Word2Vec <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and its variants <ref type="bibr" target="#b19">[20]</ref>, a class of methods that allow us to efficiently identify the relationship between words and embed them into vectors, called word embeddings. As a result, researchers have been searching for ways to use these embeddings to refine existing models in the literature. The results of these efforts can already be seen in works such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>, and indeed, improvements are obtained.</p><p>In particular, Kusner et al. <ref type="bibr" target="#b13">[14]</ref> propose the Word Mover's Distance (WMD), an application of the classic Earth Mover's Distance (EMD) <ref type="bibr" target="#b22">[23]</ref> for the domain of documents that takes advantage of the semantic relations captured by the embeddings associated with their words. The idea is to compute the minimum cost required to transform one document representation into another by using the distance between embeddings as the cost of transforming words. In fact, the distance is given by the cost of an optimal solution of a transportation problem defined on a complete bipartite graph where the nodes correspond to the distinct words of the documents, and the edge costs are the distance between embeddings. In the same paper, they show that this approach obtained outstanding results on document classification tasks, outperforming many competitors. The major drawback of WMD, however, is its high computational cost since solving the transportation problem in a complete bipartite graph is costly, requiring super cubic time.</p><p>Since the proposal of WMD, there has been a considerable amount of research focusing on improving its performance while keeping its effectiveness <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. The Relaxed Word Mover's Distance (RWMD) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>, due to its simplicity and speed, is arguably one of the most successful outcomes of this research effort. In fact, experiments reported in the literature show that it achieves quality (test error) competitive with those obtained by WMD with the advantage of being much faster. However, despite its good performance, further improvements are relevant because there are applications in which this kind of distance needs to be calculated very quickly.</p><p>Motivated by this scenario, we focus on developing an approach to derive distances that are as effective as WMD and its variants with the advantage of allowing a faster computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>To achieve this goal, in contrast to other approaches available, we explore the properties of the application domain, more specifically the distribution of distances among word embeddings. Our key observation is that one can assume, without incurring a significant loss, that the set of distances between word embeddings is split into two sets: the set of distances between related words and the set of distances between non-related words, with the distances in the latter having the same value. We show that this assumption, which is supported by empirical data, can be used to: (i) obtain a more compact formulation for the transportation problem that is used to calculate WMD and its variants and (ii) dramatically reduce the memory required to cache the distances between embeddings, which is essential for the fast computation of RWMD for large vocabularies since the evaluation of the distance between a pair of words requires hundreds of operations for typical sizes of embeddings.</p><p>By relying on the previous observation, we propose a simple approach for speeding up WMD and distances with a similar flavour. More concretely, we show how to derive new distances between docu-ments by applying our approach to both WMD and RWMD. The time and space complexities required to compute these distances depend on a parameter r that has to do with the number of related words. This parameter can be set to a small value which leads to complexity improvements over WMD and RWMD. In addition, experiments executed over 10 datasets, for two distinct tasks, suggest that these distances yield to test errors as good as those obtained by WMD/RWMD, with a significant gain in terms of execution time. Indeed, with regards to efficient implementations of RWMD, we obtained an average speed-up of almost 5 times for one task and 15 times for the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Our work is closely related to some approaches that have been proposed to circumvent the high computational cost of WMD <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Kusner et al. <ref type="bibr" target="#b13">[14]</ref> propose the Relaxed Word Mover's Distance (RWMD), a distance that is defined over a relaxation of the transportation problem in which some constraints are dropped. Given the distance matrix between the words embeddings of documents D and D , the RWMD can be calculated in O(|D| · |D |) time, where |D| and |D | are the number of distinct words of D and D , respectively. Thus, the bottleneck of RWMD is to compute the distance matrix which costs O(|D| · |D | · d) time, where d is the dimension of the word embeddings space. Such cost can be prevented by caching the O(n 2 ) distances between all the n words of the vocabulary, an approach that could be prohibitive for large n. Experiments from <ref type="bibr" target="#b13">[14]</ref> shows that RWMD achieves test error competitive with WMD for document classification tasks while incurring a lower computational cost, even without using cache.</p><p>Atasu et al. <ref type="bibr" target="#b2">[3]</ref> show how to compute RWMD for any two documents D and D from a collection C in O(|D| + |D |) time. To achieve this running time, they need to pre-compute and store the distance of word w to the nearest word in document D, for each w in the vocabulary and each D in the collection. Thus, it consumes O(n|C|) memory, where |C| is the number of documents in C, which may be infeasible for large collections. Furthermore, this linear time complexity does not hold for dynamic collections since the method requires O(|D new | · n · d) preprocessing time before calculating the RWMD from a new document D new to some document D. Further work from Atasu et al. <ref type="bibr" target="#b1">[2]</ref> discusses a limitation of RWMD for documents that share many words and then proposes a family of variants of RWMD that better address this scenario.</p><p>In a broader scope, our work is also related to some proposals to speed up EMD <ref type="bibr" target="#b18">[19]</ref> and approximate solutions for transportation problems in general <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Pele et al. <ref type="bibr" target="#b18">[19]</ref> present an optimized solution of the EMD for instances in which the costs of the edges satisfies certain properties that are motivated by the way human perceive distances. The optimization introduced by this approach consists of reducing the number of edges in the transportation network and, as a consequence, the running time. This work resembles ours in the sense that both optimize the time complexity to solve the transportation problem by taking into account how the costs behave in the domains under consideration.</p><p>Cuturi et al. <ref type="bibr" target="#b4">[5]</ref> use an entropic regularization term to smooth out the transportation problem so that it can be solved much faster via Sinkhorn's matrix scaling algorithm. This algorithm has O(|D| · |D |) empirical time according to <ref type="bibr" target="#b4">[5]</ref> and it was used in a supervised version of WMD <ref type="bibr" target="#b12">[13]</ref>. As RWMD, this method needs an O(n 2 ) space cache in order to prevent the O(|D| · |D | · d) time required to compute the distances between the words in D and D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Paper Organization</head><p>The paper is organized as follows. In Section 2, we introduce our notation and discuss some background that is important to the understanding of our work. In the next section, we develop our approach. In Section 4, we present our experimental study comparing the new distance with WMD and RWMD both in terms of test error and computational performance. Finally, in Section 5, we present our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we introduce some notation and explain some concepts that are important to understand our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>We assume that we have a vocabulary of n words {1, . . . , n} and a collection of documents. Throughout the text, we need to refer to arbitrary documents D and D to explain existing distances and the new ones that we propose. Hence, unless otherwise stated, we assume that the set of distinct words of D and D are, respectively, {w1, . . . , w |D| } and {w 1 , . . . , w |D | }. Note that |D| (resp. |D |) is the number of distinct words of document D (resp. D ). Moreover, we use Di to denote the normalized frequency of wi, that is, the number of occurrences of wi in D over the total number of words in D. We use D j to refer to the normalized frequency of w j analogously. Note that i Di = j D j = 1.</p><p>We use x(i) to denote the embedding of a word i in a vector space of dimension d and we use c(i, j) to denote the Euclidean distance between the embeddings of words i and j, that is,</p><formula xml:id="formula_0">c(i, j) = x(i) − x(j) 2.</formula><p>To make sure that our notation is clearly understood, we present a simple example involving the following documents:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D:</head><p>John likes algorithms. Mary likes algorithms too. D : John also likes data structures.</p><p>Ignoring the stopwords, and assuming that D and D are the only doc's in our collection, we have the following vocabulary {1 : "John", 2 : "likes", 3 : "algorithms",4 : "Mary", 5 : "too", 6 : "also", 7 : "data", </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Mover's Distance</head><p>By exploring the behaviour of the Word Embeddings, Kusner et al. <ref type="bibr" target="#b13">[14]</ref> define the distance between two documents as the minimum cost of converting the words of one document into the words of the other, where the cost c(wi, w j ) of transforming the word wi into word w j is given by the distance between the word embeddings of wi and w j .</p><p>Formally, the WMD between documents D and D is defined as the value of the optimal solution of the following transportation problem:</p><formula xml:id="formula_1">min |D| i=1 |D | j=1 c(wi, w j )Tij (1) s.t.: |D | j=1 Tij = Di ∀i ∈ {1, . . . , |D|} (2) |D| i=1 Tij = D j ∀j ∈ {1, . . . , |D |} (3) Tij ≥ 0 for all i, j (4)</formula><p>In the above formulation, T is the flow matrix. The variable Tij gives the amount of word wi that is transformed into word w j . The equation <ref type="formula">(2)</ref>, for a fixed i, assures that each unit of word wi is transformed into a unit of a word in D while Equation <ref type="formula">(3)</ref>, for each j, assures that the total units of words in D transformed into w j is D j .</p><p>The WMD, although well founded, suffers from efficiency problems since solving the transportation problem on a complete bipartite graph is costly, requiring super cubic time using the best known minimum cost flow algorithms <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relaxed Word Mover's Distance</head><p>To overcome the high computational cost of solving the transportation problem, Kusner et al. <ref type="bibr" target="#b13">[14]</ref> propose the RWMD, a variation of WMD whose computation relies on optimally solving relaxations of the transportation problem. These relaxations are obtained by either ignoring the set of constraints (2) or the set (3). In fact, the RMWD between D and D can be calculated by evaluating the expression</p><formula xml:id="formula_2">max    |D| i=1 Di min 1≤j≤|D | c(wi, w j ), |D | j=1 D j min 1≤i≤|D| c(wi, w j )   </formula><p>(5) where the left and the right terms in the maximum are the optimum values of the relaxations that ignore constraints <ref type="formula">(3)</ref> and <ref type="formula">(2)</ref>, respectively.</p><p>By examining the above equation we conclude that, given the costs c(wi, w j )'s, RWMD can be calculated for a pair of documents D and D in O(|D| × |D |) time, which is a significant improvement over WMD. Therefore, RWMD's bottleneck is the computation of the costs</p><formula xml:id="formula_3">c(wi, w j )'s since it requires O(|D| · |D | · d) time.</formula><p>We also note that by performing a simple preprocessing step before applying equation <ref type="formula">(5)</ref> we can obtain a tighter relaxation of WMD that corresponds to the OMR distance proposed in <ref type="bibr" target="#b1">[2]</ref>. The motivation is better handling cases in which D and D share many words.</p><p>The preprocessing consists of first identifying pairs of words (wi, w j ), with wi = w j . Then, for each of these pairs we do the following: (i) we replace Di with its excess max{Di − D j , 0} and, if Di ≤ D j , we remove index i from the range where the minimum iterates in the right term of the max; (ii) similarly, we replace D j with its excess max{D j − Di, 0} and, if D j ≤ Di, we remove index j from the range where the minimum iterates in the left term of the max. The impact of this preprocessing is associating in equation <ref type="formula">(5)</ref> the excesses of Di and D j with the second closest word to wi and w j , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">RWMD in linear time</head><p>In <ref type="bibr" target="#b2">[3]</ref>, it is proposed an implementation that computes the RWMD between two documents D and D in O(|D| + |D |) time, improving upon the O(|D| · |D| · d) time required by the original proposal. This improvement, however, comes at the expense of some potentially costly preprocessing.</p><p>To explain the implementation, denoted here by RWMD(L), let C be a collection of documents and let Ci(j) be the word, among those in the i-th document of C, that is closest to some given word j in the vocabulary. RWMD(L) builds, at the preprocessing phase, a matrix M with |C| rows and n columns, where the entry Mij stores the distance between Ci(j) and word j. To fill the row of M associated with document D we have to pay O(n · |D| · d) time.</p><p>Having the matrix M available, it is possible to compute the RWMD between documents D and D in O(|D| + |D |) time. The reason is that the terms minjc(wi, w j ) and minic(wi, w j ) of (5) can be computed in O(1) time. The former is obtained by accessing the entry M r ,i , where r is the row corresponding to document D , while for the latter we need to access the entry Mr,j, where r is the row corresponding to document D.</p><p>In addition to the time required to build matrix M , another potential problem of RWMD(L) is its space requirement since the matrix M can be prohibitively large when either the vocabulary or the collection of documents is huge.</p><p>We note that in order to properly use the preprocesssing for equation <ref type="bibr" target="#b4">(5)</ref> described in the previous section, we should store in matrix M the distance of w to its second closest word among those that belong D, when w ∈ D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AN APPROACH FOR SPEEDING UP WMD AND ITS VARIANTS</head><p>For large vocabularies the methods discussed in the previous section may have to cope with an enormous amount of distances between embeddings, which may be a problem either in terms of memory requirements or in terms of running time. In fact, for applications (e.g. document classification via k-NN) that use the distance between the same pair of embeddings several times caching turns out to be crucial for achieving computational efficiency. However, when the vocabulary is large, caching becomes prohibitive.</p><p>In this section, we show that we can significantly attenuate this problem by taking into account the distribution of distances among word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">On the distances between word embeddings</head><p>Here we discuss the assumptions in which our approach and the new distances, derived from it, rely on. For that, we present examples of distances between word embeddings. These embeddings, used here and in the next sections, were made available by Google 2 . To obtain them, they trained the vectors with d = 300 using the Word2Vec template of Word Embeddings <ref type="bibr" target="#b17">[18]</ref> on top of a Google News document base containing altogether about 100 billion words and 3 million tokens. We also refer to some datasets that will be detailed in our experimental section.</p><p>From a semantic perspective, it is reasonable to consider that words are closely related to only a few other words in general. As the word embeddings were designed to simulate semantic relations, it is expected that they present a similar behaviour; that is, each vector should be close to a few other vectors and far away from the remaining ones.</p><p>As an example, if the words are ranked according to their distances to the embedding corresponding to "cat" one should expect "dog" and "rabbit" preceding both "moon" and "guitar". However, it is not clear whether "moon" or "guitar" comes first in the ranking since neither of them has an obvious relation with "cat". <ref type="figure" target="#fig_1">Figure 1</ref> illustrates this behaviour by displaying the distances between the embedding for "cat" and the embeddings from the words of the Amazon dataset sorted by increasing order of distance. We note that there are few words with small distances while the vast majority has distance concentrated in the range [1.2, 1.4]. For checking whether this behaviour persists for other words, we computed all the distances between embeddings from the words of the Reuters dataset. <ref type="figure" target="#fig_2">Figure 2</ref> shows the distribution of these distances clustered in bins for better visualization. Once again, we observe a high concentration of the distances around the interval [1.2, 1.4], behaving similarly to a Normal distribution. Based on this discussion, we make the following assumptions:</p><p>(i) Given a word w, the remaining words can be split into two groups: RELATED(w) and UNRELATED(w), with the former (latter) containing the words related (unrelated) with w;</p><p>(ii) The distances from every word in UNRELATED(w) to w, for every w, is the same "large" value cmax.</p><p>Although the number of related words may vary according to the word of reference, in order to make our approach simpler and thus, more practical, we assume that all words have the same number of related words and we use r to denote this number. The value of r can either be set manually or automatically estimated using a training set as we discuss in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithms exploiting distance assumptions</head><p>Algorithms to compute distances with the flavour of WMD can benefit from our assumptions because, by using them, they just need to handle a much smaller set of distances between embeddings, that is, the set of distances between related words. As a result, caching distances becomes feasible even for large vocabularies, which prevent these methods of calculating the distance between the same pair of words more than once. In addition, the transportation problem in which WMD and related distances as RWMD rely on can be solved in a sparse bipartite graph rather than on a complete bipartite graph.</p><p>In the next subsections we discuss how WMD and RWMD can be adapted to make use of our assumptions. These adaptations lead to new distances between documents, namely Rel-WMD and Rel-RWMD. We start with the explanation of a preprocessing phase that is required to calculate these new distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Preprocessing Phase</head><p>In this phase, we build a structure (cache) C that stores for each word w, from a vocabulary of n words, the r closest words to w as well as its distances to w.</p><p>Choose a word i in the vocabulary. The procedure computes its Euclidean distance x(i)−x(j) 2 to every other word j and add these distances, as well as the corresponding words, to a list Li. Next, it selects the r words that are closest to i in Li and adds them, with their distances, to cache C. This selection can be performed in expected linear time using QuickSelect <ref type="bibr" target="#b10">[11]</ref>. The distances that were not included in C are then added to a global accumulator A with the goal of calculating cmax. This procedure is repeated for every word i in the vocabulary and the value cmax is given by the average of all values added to A.</p><p>The cache C requires O(n · r) space and its construction requires O(n 2 ·d) time, where the term d is due to the time required to compute the distance between a pair of embeddings.</p><p>For large vocabularies the construction of the cache C, as above described, may be costly due to the O(n 2 · d) time complexity. This construction, however, can be optimized by clustering the embeddings and then considering only words in the same cluster to find the related words.</p><p>We discuss this approach using the traditional k-means clustering algorithm <ref type="bibr" target="#b0">[1]</ref>. On the one hand, this algorithm allows the user to define the number of clusters k and it performs O(n · k · d · I) operations to cluster n points in R d into k clusters, where I is the maximum number of iterations allowed. On the other hand, if the n embeddings are uniformly distributed among k clusters then the construction of cache C requires O((n/k) 2 · d) time per cluster which implies on O(n 2 · d/k) overall time. Hence, let f (k) = n · d · k · I + (n 2 · d/k) be an estimation of the running time required to execute k-means and then the construction of cache C. By minimizing f (k) we get k = n/I. Thus, if n is large, in order to speed up the preprocessing phase, we run k-means algorithm with k = n/I, before building the cache C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Related Word Mover's Distance</head><p>The Related Word Mover's Distance (Rel-WMD) between D and D is defined as the optimum value of the transportation problem given by equations (1)-(4), where the costs of the edges are as follows:</p><formula xml:id="formula_4">c(wi, w j ) =      0, if wi = w j x(wi) − x(w j ) 2, if (wi, w j ) ∈ C cmax, otherwise<label>(6)</label></formula><p>For small values of parameter r many costs are equal to cmax. In this case, it is possible to replace the formulation given by (1)- <ref type="bibr" target="#b3">(4)</ref> with an equivalent and more compact one. This new formulation is given by (7)-(10) and its key idea is using variable Ti,t to represent the number of units of word wi that is transformed into words that are at a distance cmax from wi. Thus, the single variable Ti,t replaces all variable Ti,j in the original formulation for which (wi, w j ) does not belong to cache C. Similarly Tt,j represents the number of units transformed into w j from words that are at a distance cmax of word w j . The underlying graph of this new formulation is much sparser (for small values of r) so that the transportation problem can be solved significantly faster.</p><formula xml:id="formula_5">min (w i ,w j )∈C c(wi, w j )Ti,j + |D| i=1 cmaxTi,t<label>(7)</label></formula><p>s.t.:</p><formula xml:id="formula_6">Ti,t + j|(w i ,w j )∈C Ti,j = Di i = 1, . . . , |D|<label>(8)</label></formula><formula xml:id="formula_7">Tt,j + i|(w i ,w j )∈C Ti,j = D j j = 1, . . . , |D | (9)</formula><p>Ti,j, Ti,t, Tt,j ≥ 0 for all i, j</p><p>We shall note that this formulation has been used before by <ref type="bibr" target="#b18">[19]</ref> to speed up EMD <ref type="bibr" target="#b22">[23]</ref> in the context of image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Related Relaxed Word Mover's Distance</head><p>The Related Relaxed Word Mover 's Distance (Rel-RWMD) is a variation of the Rel-WMD, in which we drop constraints of the original formulation in order to obtain a relaxation that can be computed more efficiently. Rel-RWMD is to Rel-WMD as RWMD is to WMD.</p><p>The Rel-RWMD can be computed using equation <ref type="formula">(5)</ref> with a cost structure given by <ref type="bibr" target="#b5">(6)</ref>. Let R (wi) (resp. R(w j ) ) be the set of related words of wi (resp. w j ) that belong to D (resp. D). Thus, the Rel-RWMD between documents D and D is given by</p><formula xml:id="formula_9">max    |D| i=1 Di min w j ∈R (w i ) c(wi, w j ), |D | j=1 D j min w i ∈R(w j ) c(wi, w j )    (11)</formula><p>Although not explicit in the above equation, if R (wi) (resp. R(w j )) is empty then Di (resp. D j ) is multiplied by cmax.</p><p>To efficiently evaluate the first term of the maximum in equation <ref type="bibr" target="#b10">(11)</ref> we need to obtain for each w in D its related words that belong to D , that is, the set R (w). By storing D as a hash table we can find them in O(r) time. For that, it is enough to traverse the list of words related to w in cache C and for each word w in the list we use the hash table of D to verify whether w belongs to D . Since the second term in the maximum can be calculated analogously we conclude that the Rel-RWMD between D and D can be computed in O((|D| + |D |) · r) time, which is a significant improvement over the (|D| · |D |) time required by RWMD when the size of the documents is considerably larger than r.</p><p>Finally, we mention that the linear time implementation of RWMD presented in Section 2.3.1 can also benefit from our assumptions. The first advantage is that the matrix M can be computed faster since, in order to fill the row associated with a document D, we just need to consider the words in the vocabulary that are related to D because for the other words the corresponding entries have value cmax. Thus, the addition of the row associated with document D costs O(|D| · r) time rather than the O(n · |D| · d) required by RWMD(L). The second advantage is the sparsity of matrix M which allows handling larger collections/vocabularies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To evaluate our methods, we employ two tasks that involve the computation of distances between documents. The first one is the document classification task via k-Nearest Neighbors (k-NN) that was used to evaluate the WMD algorithm in <ref type="bibr" target="#b13">[14]</ref>. The second one is the task of identifying related pairs of documents, employed to evaluate the performance of paragraph vector <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>We compared in terms of test error and computational performance our new distance, Rel-RWMD, against WMD, RWMD, Cosine distance and Word Centroid Distance (WCD) <ref type="bibr" target="#b13">[14]</ref>. The WCD between two documents is given by the Euclidean distance between their centroids, where the centroid of a document D is defined as |D| i=1 Dix(wi). When reporting computational times we use RWMD(S) and RWMD(L) to distinguish between the Standard implementation of RWMD and the one that requires Linear time. Rel-RWMD(S) and Rel-RWMD(L) are used analogously. We note that for all RWMD's implementations the preprocessing described at the end of Section 2.3 is applied.</p><p>Although we have also implemented/evaluated Rel-WMD, its results are omitted in the next sections since, in general, it is competitive with Rel-RWMD in terms of test error while being much slower.</p><p>The methods were implemented in C++. The Eigen library <ref type="bibr" target="#b9">[10]</ref> was used for matrix manipulation and Linear Algebra while the OR-Tools library <ref type="bibr" target="#b20">[21]</ref> was used for the resolution of flow problems. All experiments were executed using a single core of an Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz, with 8 GB of RAM. The code and datasets are available in a GitHub repository 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document classification via k-NN</head><p>Our experimental setting follows Kusner et al. <ref type="bibr" target="#b13">[14]</ref>, where different distances are evaluated according to their performance when they are employed by the k-NN method to address document classification tasks.</p><p>In order to classify a document D from some testing set, k-NN computes the distance of D to each document in the corresponding training set and then it returns the most frequent class among the k closest documents to D. As stated in <ref type="bibr" target="#b13">[14]</ref>, motivations for using this evaluation approach, based on k-NN , include its reproducibility and simplicity.</p><p>We run k-NN using k = 19, and, in case of ties, k is divided by two until there are no more ties. This setting is slightly different from <ref type="bibr" target="#b13">[14]</ref>, where k is selected from the set {1, 3, . . . , 19} based on the lower error rate obtained.</p><p>The parameter r that defines the number of related words was selected from the set S = {1, 2, 4, . . . , 128} using a 5-fold crossvalidation on top of the training set. Because we are prioritizing computational performance and, the smaller the r the faster the method, we choose the lowest r whose test error in the cross validation is at most 1% larger than the minimum one found among all the possibilities in the set S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets description</head><p>We used the following eight preprocessed datasets 4 provided by <ref type="bibr" target="#b13">[14]</ref>:</p><p>• 20NEWS: Posts on discussion boards for 20 different topics.</p><p>• AMAZON: Product reviews from Amazon for 4 product categories. For all datasets 70% is used for training and 30% for testing, respecting the partitions provided. <ref type="table" target="#tab_0">Table 1</ref> presents relevant statistics for each of these datasets.  <ref type="table" target="#tab_1">Table 2</ref> presents the test errors obtained by the distances under consideration over the eight datasets. We averaged the results for the datasets 4 https://github.com/mkusner/wmd AMAZON, BBCSPORT, CLASSIC, RECIPE, and TWITTER following the 5 predefined train/test splits. The remaining datasets have only one split, and so the average is not necessary. Some observations are in order: clearly, WCD and Cosine presented the worst results. Among WMD, RWMD, and Rel-RWMD, there is a balance. The behaviour of WCD and Cosine, as well as the balance between WMD and RWMD, are compatible with the findings/conclusions reached in <ref type="bibr" target="#b13">[14]</ref> while the results of Rel-RWMD suggest that our simplifying assumptions work very well. The values selected for r ranged from 2 (20NEWS and RECIPE) to 128 (AMAZON) with a median equal 19.5.  <ref type="table" target="#tab_2">Table 3</ref> presents the running times in minutes for all distances and datasets examined. First, as expected, WCD and Cosine are the fastest distances since they run in linear time and their preprocessing phases are very cheap while WMD is the slowest distance since it has to solve a transportation problem optimally. We note that the times of Cosine were omitted due to the lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>It is interesting to examine how the distances/implementations related to RWMD perform. RWMD(S), the original implementation of <ref type="bibr" target="#b13">[14]</ref>, is the slowest of them while Rel-RWMD(L) is the fastest one, being on average 4.7 times faster than RWMD(L), which is the second fastest. The main advantage of Rel-RWMD(L) over RWMD(L) is due to the time required to build the matrix M since Rel-RWMD(L) is, on average, 10 times faster. With regards to the time required to evaluate two doc's we can also observe a small advantage of Rel-RWMD(L) which is probably related to the sparsity of M . It is important to mention that the values in <ref type="table" target="#tab_2">Table 3</ref> do not include the time required to estimate the value of r. In fact, the execution of a 5-fold cross validation on the training set for each potential r incurs a high cost. However, in practice one can estimate the value of r using a much smaller set or, even better, set r to a small value, without estimating it, as suggested by the results of <ref type="table" target="#tab_3">Table 4</ref>. This table presents the test errors for r = 1, 2, 16, 128 and also for the value estimated via cross validation. We observe that the test errors remain at the same level, in particular for r ≥ 16. The running times, though not presented, change very little as expected since the value of r has a small effect in the time complexity of the linear implementation of Rel-RWMD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Identifying related documents</head><p>For the second task our experimental setting is inspired on Dai et al. <ref type="bibr" target="#b5">[6]</ref>, where representations/distances are evaluated according to their capacity of recognizing whether a document D 1 is more related to a document D 2 or to a document D 3 . For achieving this goal, testing sets are used which contain many triples of documents, namely triplets. In each triplet, only two documents are related and a given distance succeeds if its smallest value is achieved for the related pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets description</head><p>For our experiments, we first downloaded the documents in the two testing sets of triplets 5 provided in <ref type="bibr" target="#b5">[6]</ref>. The first set uses papers from Arxiv while the second one uses articles from Wikipedia. Then, we preprocessed them to remove all non-alphanumeric characters and words contained in a list of stopwords due to its little semantic value. Finally, to represent the documents we just consider the words that have embeddings in the set that Google made available. It is important to note that we are not using the embeddings of [6] since they were not provided. <ref type="table" target="#tab_4">Table 5</ref> presents relevant statistics for each of the datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>In this experiment, in contrast to the previous one, each document has its distance evaluated a few times on average, indeed less than twice. 5 http://cs.stanford.edu/ quocle/triplets-data.tar.gz Thus, building the cache M required for the linear time implementations of RWMD does not pay off. In addition, its size would be huge, around 10 10 entries for Wikipedia as an example. Therefore, we only executed RWMD(S) and Rel-RWMD(S). By comparing the statistics of the datasets in <ref type="table" target="#tab_0">Tables 1 and 5</ref>, we observe that the number of tokens (word embeddings) of the latter is one order of magnitude higher than the former and, as a consequence, the preprocessing phase of Rel-RWMD becomes expensive, harming the performance gain achieved while computing the distances. Thus, following our approach, we cluster the embeddings before building the cache C. We run k-means using a limit of I = 5 iterations and setting k = 289 ≈ 415, 967/5 for Wikipedia and k = 229 ≈ 260, 640/5 for Arxiv. Moreover, motivated by the discussion/results of the previous section we used r = 16 for Rel-RWMD. <ref type="table" target="#tab_5">Table 6</ref> presents the test errors achieved by the different methods. We can observe a behaviour similar to the previous task. Once again, both Cosine and WCD achieve the largest test errors while the others display competitive results. The computational times (in minutes) are displayed in <ref type="table" target="#tab_6">Table 7</ref>. Again WCD and Cosine are the fastest. The former is slower because it has to compute the centroids of the documents in its preprocessing phase while the latter does not. Among the others, Rel-RWMD(S) and RWMD(S), as expected, are much faster than WMD. For Wikipedia Rel-RWMD(S) is 3 times faster than RWMD(S) while for Arxiv Rel-RWMD(S) it is 27 times faster.</p><p>By taking a more in-depth examination of the running times we can also observe that the time consumption of REL-RWMD(S) is highly concentrated on its preprocessing phase when the cache C is built. Having this structure available, it computes the distances, on average, 25 and 60 times faster than RWMD(S) for Wikipedia and Arxiv, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented an approach to speed up the computation of WMD and its variants that relies on the properties of the distances between embeddings. The improvements in time and space complexities together with our experimental evaluation provide strong evidence that this approach should be employed if one is aiming to compute these distances efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>8 :</head><label>8</label><figDesc>"structures"} Hence, the set of distinct words of D and D are, respectively, {w1 = 1, w2 = 2, w3 = 3, w4 = 4, w5 = 5} and {w 1 = 1, w 2 = 6, w 3 = 2, w 4 = 7, w 5 = 8}. Finally, the normalized Bag-of-Words representations of D and D are, respectively, D: {D1, D2, D3, D4, D5} = {1/7, 2/7, 2/7, 1/7, 1/7} D : {D 1 , D 2 , D 3 , D 4 , D 5 } = {1/5, 1/5, 1/5, 1/5, 1/5}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Distances from embeddings of all words in the vocabulary of Amazon dataset to the word "cat".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Distribution of the distances between all words in the vocabulary of Reuters dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>• BBCSPORT: BBC Sport sports section articles for 5 sport between 2004 and 2005. • CLASSIC: Sentences from academic works from 4 different publishers. • OHSUMED: Medical summaries categorized by different cardiovascular diseases. For computational performance issues, only the first 10 categories of the database were used. • RECIPE: Culinary recipes separated by 15 regions of origin. • REUTERS: News from the Reuters news agency in 1987. The original database contains 90 classes, however, due to problems of imbalance between them, a reduced version with only the 8 most frequent ones was created. • TWITTER: Collection of tweets labeled by feelings "negative", "positive" and "neutral".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets statistics including training and testing sets.</figDesc><table><row><cell>NAME</cell><cell cols="2">#DOCS #TOKENS</cell><cell>AVG. TOKENS PER DOC</cell><cell>CLASSES</cell></row><row><cell>20NEWS</cell><cell>18,820</cell><cell>22,439</cell><cell>69.3</cell><cell>20</cell></row><row><cell>AMAZON</cell><cell>8,000</cell><cell>30,249</cell><cell>44.5</cell><cell>4</cell></row><row><cell>BBCSPORT</cell><cell>737</cell><cell>10,103</cell><cell>116.5</cell><cell>5</cell></row><row><cell>CLASSIC</cell><cell>7,093</cell><cell>18,080</cell><cell>38.6</cell><cell>4</cell></row><row><cell>OHSUMED</cell><cell>9,152</cell><cell>19,954</cell><cell>60.2</cell><cell>10</cell></row><row><cell>RECIPE</cell><cell>4,370</cell><cell>5,225</cell><cell>48.3</cell><cell>15</cell></row><row><cell>REUTERS</cell><cell>7,674</cell><cell>15,115</cell><cell>36.0</cell><cell>8</cell></row><row><cell>TWITTER</cell><cell>3,108</cell><cell>4,489</cell><cell>9.9</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Test error (in %) for different distances and datasets. The datasets with more than one partition had their error rates averaged. The best results are bold faced.</figDesc><table><row><cell cols="6">DATASET COSINE WCD WMD RWMD REL-RWMD</cell></row><row><cell>20NEWS</cell><cell>30.45</cell><cell cols="2">36.2 24.09</cell><cell>24.79</cell><cell>25.22</cell></row><row><cell>AMAZON</cell><cell>12.90</cell><cell>9.04</cell><cell>7.21</cell><cell>6.87</cell><cell>6.98</cell></row><row><cell>BBCSPORT</cell><cell>4.82</cell><cell>11.9</cell><cell>5.36</cell><cell>5.09</cell><cell>4.82</cell></row><row><cell>CLASSIC</cell><cell>6.34</cell><cell>8.93</cell><cell>3.04</cell><cell>2.91</cell><cell>3.15</cell></row><row><cell>OHSUMED</cell><cell cols="4">45.74 47.00 42.85 43.49</cell><cell>41.26</cell></row><row><cell>RECIPE</cell><cell cols="4">45.71 49.20 46.56 43.63</cell><cell>43.20</cell></row><row><cell>REUTERS</cell><cell>8.95</cell><cell>4.98</cell><cell>3.84</cell><cell>3.97</cell><cell>4.39</cell></row><row><cell>TWITTER</cell><cell>31.97</cell><cell cols="2">29.4 29.14</cell><cell>28.95</cell><cell>28.95</cell></row><row><cell>AVERAGE</cell><cell cols="4">23.36 24.59 20.26 19.94</cell><cell>19.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Computational runtime (in minutes) for different distances and datasets. The datasets with more than one partition have their computational times averaged.</figDesc><table><row><cell cols="5">DATASET WCD WMD RWMD(S) RWMD(L) REL-RWMD(L)</cell></row><row><cell cols="2">20NEWS 1.87 6,244</cell><cell>842</cell><cell>68.0</cell><cell>13.4</cell></row><row><cell cols="2">AMAZON 0.30 351</cell><cell>71.7</cell><cell>22.1</cell><cell>4.47</cell></row><row><cell cols="2">BBCSPORT 0.01 21</cell><cell>3.72</cell><cell>1.38</cell><cell>0.27</cell></row><row><cell>CLASSIC</cell><cell>0.24 213</cell><cell>45.6</cell><cell>10.8</cell><cell>2.26</cell></row><row><cell cols="2">OHSUMED 0.47 1,002</cell><cell>158</cell><cell>25.1</cell><cell>5.20</cell></row><row><cell>RECIPE</cell><cell>0.09 106</cell><cell>27.6</cell><cell>2.13</cell><cell>0.55</cell></row><row><cell cols="2">REUTERS 0.27 181</cell><cell>47.7</cell><cell>10.9</cell><cell>1.67</cell></row><row><cell cols="2">TWITTER 0.04 3.32</cell><cell>1.23</cell><cell>0.43</cell><cell>0.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Test Error (in %) for Rel-RWMD, with r = 1, 2, 16, 128 and values estimated via cross-validation, for the different datasets.</figDesc><table><row><cell>DATASET</cell><cell>CROSS VAL.</cell><cell>R=1</cell><cell>R=2</cell><cell>R=16</cell><cell>R=128</cell></row><row><cell>20NEWS</cell><cell>25.22</cell><cell cols="3">25.27 25.22 24.90</cell><cell>25.22</cell></row><row><cell>AMAZON</cell><cell>6.98</cell><cell>9.66</cell><cell>9.21</cell><cell>7.94</cell><cell>7.02</cell></row><row><cell>BBCSPORT</cell><cell>4.82</cell><cell>4.00</cell><cell>3.64</cell><cell>4.91</cell><cell>5.55</cell></row><row><cell>CLASSIC</cell><cell>3.15</cell><cell>3.62</cell><cell>3.56</cell><cell>3.20</cell><cell>3.18</cell></row><row><cell>OHSUMED</cell><cell>41.26</cell><cell cols="3">42.44 42.83 41.26</cell><cell>41.55</cell></row><row><cell>RECIPE</cell><cell>43.20</cell><cell cols="3">43.57 43.20 43.20</cell><cell>43.52</cell></row><row><cell>REUTERS</cell><cell>4.39</cell><cell>4.93</cell><cell>4.52</cell><cell>4.02</cell><cell>4.20</cell></row><row><cell>TWITTER</cell><cell>28.95</cell><cell cols="3">31.52 30.60 28.91</cell><cell>29.16</cell></row><row><cell>AVERAGE</cell><cell>19.75</cell><cell cols="4">20.63 20.35 19.79 19.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Datasets statistics.</figDesc><table><row><cell>NAME</cell><cell cols="3">#DOCS #TRIPLETS #TOKENS</cell><cell>AVG. TOKENS PER DOC</cell></row><row><cell>ARXIV</cell><cell>47,080</cell><cell>19,998</cell><cell>260,640</cell><cell>1,043.9</cell></row><row><cell cols="2">WIKIPEDIA 58,015</cell><cell>19,336</cell><cell>415,967</cell><cell>429.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Test Error (in %) for different distances and datasets. The best results are bold faced. DATASET COSINE WCD WMD RWMD REL-RWMD ARXIV 28.83 29.99 22.77 23.43 23.16 WIKIPEDIA 27.83 29.23 26.74 27.01 26.90</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Computational runtime (in minutes) for different distances and datasets.</figDesc><table><row><cell cols="4">DATASET COSINE WCD WMD RWMD(S) REL-RWMD(S)</cell></row><row><cell>ARXIV</cell><cell>0.01 0.18 1,996</cell><cell>74.8</cell><cell>2.72</cell></row><row><cell cols="2">WIKIPEDIA 0.01 0.09 302</cell><cell>11.0</cell><cell>3.36</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Departamento de Informática, PUC-Rio, Rio de Janeiro, Brazil, email: {mwerner, laber}@inf.puc-rio.br</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://code.google.com/archive/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/matwerner/fast-wmd</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The second author is partially supported by CNPq under grant 307572/2017-0 and by FAPERJ, grant Cientista do Nosso Estado E-26/202.823/2018.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linear-complexity dataparallel earth movers distance approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubilay</forename><surname>Atasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mittelholzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linear-complexity relaxed word mover&apos;s distance with gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubilay</forename><surname>Atasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Parnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celestine</forename><surname>Dünner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Sifalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haralampos</forename><surname>Pozidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Vasileiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michail</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Berrospi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Labbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Document embedding with paragraph vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07998</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaussian lda for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis to improve access to textual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>Acm</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="281" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic optimization for large-scale optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3440" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Eigen v3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Jacob</surname></persName>
		</author>
		<ptr target="http://eigen.tuxfamily.org" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithm 65: find</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="321" to="322" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ontologies improve text document clustering&apos;, in Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerd</forename><surname>Stumme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM 2003. Third IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="541" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised word mover&apos;s distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4862" to="4870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topic modeling for short texts with auxiliary word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and robust earth mover&apos;s distances&apos;, in Computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Pele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Or-tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Perron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Furnon</surname></persName>
		</author>
		<ptr target="https://developers.google.com/optimization/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Computational optimal transport&apos;, Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A metric for distributions with applications to image databases&apos;, in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">Elwood</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Word mover&apos;s embedding: From word2vec to document embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witbrock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
