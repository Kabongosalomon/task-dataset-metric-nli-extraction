<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Representations by Maximizing Mutual Information Across Views</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
							<email>phil.bachman@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
							<email>devon.hjelm@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Representations by Maximizing Mutual Information Across Views</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatiotemporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on Im-ageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning useful representations from unlabeled data is a challenging problem and improvements over existing methods can have wide-reaching benefits. For example, consider the ubiquitous use of pre-trained model components, such as word vectors <ref type="bibr" target="#b29">[Mikolov et al., 2013</ref><ref type="bibr" target="#b31">, Pennington et al., 2014</ref> and context-sensitive encoders <ref type="bibr" target="#b32">[Peters et al., 2018</ref><ref type="bibr" target="#b5">, Devlin et al., 2019</ref>, for achieving state-of-the-art results on hard NLP tasks. Similarly, large convolutional networks pre-trained on large supervised corpora have been widely used to improve performance across the spectrum of computer vision tasks <ref type="bibr" target="#b8">[Donahue et al., 2014</ref><ref type="bibr" target="#b34">, Ren et al., 2015</ref><ref type="bibr" target="#b18">, He et al., 2017</ref><ref type="bibr" target="#b3">, Carreira and Zisserman, 2017</ref>. Though, the necessity of pre-trained networks for many vision tasks has been convincingly questioned in recent work <ref type="bibr" target="#b19">[He et al., 2018]</ref>. Nonetheless, the core motivations for unsupervised learning -namely minimizing dependence on potentially costly corpora of manually annotated data -remain strong.</p><p>We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. This is analogous to a human learning to represent observations generated by a shared cause, e.g. the sights, scents, and sounds of baking, driven by a desire to predict other related observations, e.g. the taste of cookies. For a more concrete example, the shared context could be an image from the ImageNet training set, and multiple views of the context could be produced by repeatedly applying data augmentation to the image. Alternatively, one could produce multiple views of an image by repeatedly partitioning its pixels into "past" and "future" sets, with the considered partitions corresponding to a fixed autoregressive ordering, as in Contrastive Predictive Coding <ref type="bibr">[CPC, van den Oord et al., 2018]</ref>. The key idea is that maximizing mutual information between features extracted from multiple views of a shared context forces the features to capture information about higher-level factors (e.g., presence of certain objects or occurrence of certain events) that broadly affect the shared context. We introduce a model for self-supervised representation learning based on local Deep InfoMax <ref type="bibr">[DIM, Hjelm et al., 2019]</ref>. Local DIM maximizes mutual information between a global summary feature vector, which depends on the full input, and a collection of local feature vectors pulled from an intermediate layer in the encoder. Our model extends local DIM in three key ways: it predicts features across independently-augmented versions of each input, it predicts features simultaneously across multiple scales, and it uses a more powerful encoder. Each of these modifications provides improvements over local DIM. Predicting across independently-augmented copies of an input and predicting at multiple scales are two simple ways of producing multiple views of the context provided by a single image. We also extend our model to mixture-based representations, and find that segmentation-like behaviour emerges as a natural side-effect. Section 3 discusses the model and training objective in detail. We evaluate our model using standard datasets: CIFAR10, CIFAR100, STL10 <ref type="bibr" target="#b4">[Coates et al., 2011]</ref>, ImageNet 1 <ref type="bibr" target="#b35">[Russakovsky et al., 2015]</ref>, and Places205 <ref type="bibr" target="#b44">[Zhou et al., 2014]</ref>. We evaluate performance following the protocol described by <ref type="bibr" target="#b24">Kolesnikov et al. [2019]</ref>. Our model outperforms prior work on these datasets. Our model significantly improves on existing results for STL10, reaching over 94% accuracy with linear evaluation and no encoder fine-tuning. On ImageNet, we reach over 68% accuracy for linear evaluation, which beats the best prior result by over 12% and the best concurrent result by 7%. We reach 55% accuracy on the Places205 task using representations learned with ImageNet data, which beats the best prior result by 7%. Section 4 discusses the experiments in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One characteristic which distinguishes self-supervised learning from classic unsupervised learning is its reliance on procedurally-generated supervised learning problems. When developing a selfsupervised learning method, one seeks to design a problem generator such that models must capture useful information about the data in order to solve the generated problems. Problems are typically generated from prior knowledge about useful structure in the data, rather than from explicit labels.</p><p>Self-supervised learning is gaining popularity across the NLP, vision, and robotics communities -e.g., <ref type="bibr" target="#b5">[Devlin et al., 2019</ref><ref type="bibr" target="#b26">, Logeswaran and Lee, 2018</ref><ref type="bibr" target="#b36">, Sermanet et al., 2017</ref><ref type="bibr" target="#b10">, Dwibedi et al., 2018</ref>. Some seminal work on self-supervised learning for computer vision involves predicting spatial structure or color information that has been procedurally removed from the data. E.g., <ref type="bibr" target="#b7">Doersch et al. [2015]</ref> and <ref type="bibr" target="#b30">Noroozi and Favaro [2016]</ref> learn representations by learning to predict/reconstruct spatial structure. <ref type="bibr" target="#b43">Zhang et al. [2016]</ref> introduce the task of predicting color information that has been removed by converting images to grayscale. <ref type="bibr" target="#b12">Gidaris et al. [2018]</ref> propose learning representations by predicting the rotation of an image relative to a fixed reference frame, which works surprisingly well.</p><p>We approach self-supervised learning by maximizing mutual information between features extracted from multiples views of a shared context. For example, consider maximizing mutual information between features extracted from a video with most color information removed and features extracted from the original full-color video. <ref type="bibr" target="#b40">Vondrick et al. [2018]</ref> showed that object tracking can emerge as a side-effect of optimizing this objective in the special case where the features extracted from the fullcolor video are simply the original video frames. Similarly, consider predicting how a scene would look when viewed from a particular location, given an encoding computed from several views of the scene from other locations. This task, explored by Eslami et al. <ref type="bibr">[2018]</ref>, requires maximizing mutual information between features from the multi-view encoder and the content of the held-out view. The general goal is to distill information from the available observations such that contextually-related observations can be identified among a set of plausible alternatives. Closely related work considers learning representations by predicting cross-modal correspondence <ref type="bibr">Zisserman, 2017, 2018]</ref>. While the mutual information bounds in <ref type="bibr" target="#b40">[Vondrick et al., 2018</ref><ref type="bibr" target="#b11">, Eslami et al., 2018</ref> rely on explicit density estimation, our model uses the contrastive bound from CPC [van den <ref type="bibr" target="#b39">Oord et al., 2018]</ref>, which has been further analyzed by <ref type="bibr" target="#b28">McAllester and Stratos [2018]</ref>, and <ref type="bibr" target="#b33">Poole et al. [2019]</ref>.</p><p>Evaluating new self-supervised learning methods presents some challenges. E.g., performance gains may be largely due to improvements in model architectures and training practices, rather than advances in the self-supervised learning component. This point was addressed by <ref type="bibr" target="#b24">Kolesnikov et al. [2019]</ref>, who found massive gains in standard metrics when existing methods were reimplemented with up-to-date architectures and optimized to run at larger scales. When evaluating our model, we follow their protocols and compare against their optimized results for existing methods. Some potential shortcomings with standard evaluation protocols have been noted by <ref type="bibr" target="#b13">Goyal et al. [2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method Description</head><p>Our model, which we call Augmented Multiscale DIM (AMDIM), extends the local version of Deep InfoMax introduced by Hjelm et al. <ref type="bibr">[2019]</ref> in several ways. First, we maximize mutual information between features extracted from independently-augmented copies of each image, rather than between features extracted from a single, unaugmented copy of each image. 2 Second, we maximize mutual information between multiple feature scales simultaneously, rather than between a single global and local scale. Third, we use a more powerful encoder architecture. Finally, we introduce mixture-based representations. We now describe local DIM and the components added by our new model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local DIM</head><p>Local DIM maximizes mutual information between global features f 1 (x), produced by a convolutional encoder f , and local features {f 7 (x) ij : ∀i, j}, produced by an intermediate layer in f . The subscript d ∈ {1, 7} denotes features from the top-most encoder layer with spatial dimension d × d, and the subscripts i and j index the two spatial dimensions of the array of activations in layer d. <ref type="bibr">3</ref> Intuitively, this mutual information measures how much better we can guess the value of f 7 (x) ij when we know the value of f 1 (x) than when we do not know the value of f 1 (x). Optimizing this relative ability to predict, rather than absolute ability to predict, helps avoid degenerate representations which map all observations to similar values. Such degenerate representations perform well in terms of absolute ability to predict, but poorly in terms of relative ability to predict.</p><p>For local DIM, the terms global and local uniquely define where features come from in the encoder and how they will be used. In AMDIM, this is no longer true. So, we will refer to the features that encode the data to condition on (global features) as antecedent features, and the features to be predicted (local features) as consequent features. We choose these terms based on their role in logic.</p><p>We can construct a distribution p(f 1 (x), f 7 (x) ij ) over (antecedent, consequent) feature pairs via ancestral sampling as follows: (i) sample an input x ∼ D, (ii) sample spatial indices i ∼ u(i) and j ∼ u(j), and (iii) compute features f 1 (x) and f 7 (x) ij . Here, D is the data distribution, and u(i)/u(j) denote uniform distributions over the range of valid spatial indices into the relevant encoder layer. We denote the marginal distributions over per-layer features as p(f 1 (x)) and p(f 7 (x) ij ).</p><p>Given</p><formula xml:id="formula_0">p(f 1 (x)), p(f 7 (x) ij ), and p(f 1 (x), f 7 (x) ij ), local DIM seeks an encoder f that maximizes the mutual information I(f 1 (x); f 7 (x) ij ) in p(f 1 (x), f 7 (x) ij ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noise-Contrastive Estimation</head><p>The best results with local DIM were obtained using a mutual information bound based on Noise-Contrastive Estimation (NCE - <ref type="bibr" target="#b14">[Gutmann and Hyvärinen, 2010]</ref>), as used in various NLP applications <ref type="bibr" target="#b27">[Ma and Collins, 2018]</ref>, and applied to infomax objectives by van den Oord et al. <ref type="bibr">[2018]</ref>. This class of bounds has been studied in more detail by <ref type="bibr" target="#b28">McAllester and Stratos [2018]</ref>, and <ref type="bibr" target="#b33">Poole et al. [2019]</ref>.</p><p>We can maximize the NCE lower bound on I(f 1 (x); f 7 (x) ij ) by minimizing the following loss:</p><formula xml:id="formula_1">E (f1(x),f7(x)ij ) E N7 [L Φ (f 1 (x), f 7 (x) ij , N 7 )] .<label>(1)</label></formula><p>The positive sample pair (f 1 (x), f 7 (x) ij ) is drawn from the joint distribution p(f 1 (x), f 7 (x) ij ). N 7 denotes a set of negative samples, comprising many "distractor" consequent features drawn independently from the marginal distribution p(f 7 (x) ij ). Intuitively, the task of the antecedent feature is to pick its true consequent out of a large bag of distractors. The loss L Φ is a standard log-softmax, where the normalization is over a large set of matching scores Φ(f 1 , f 7 ). Roughly speaking, Φ(f 1 , f 7 ) maps (antecedent, consequent) feature pairs onto scalar-valued scores, where higher scores indicate higher likelihood of a positive sample pair. We can write L Φ as follows:</p><formula xml:id="formula_2">L Φ (f 1 , f 7 , N 7 ) = − log exp(Φ(f 1 , f 7 )) f 7 ∈N7∪{f7} exp(Φ(f 1 ,f 7 )) ,<label>(2)</label></formula><p>where we omit spatial indices and dependence on x for brevity. Training in local DIM corresponds to minimizing the loss in Eqn. 1 with respect to f and Φ, which we assume to be represented by parametric function approximators, e.g. deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient NCE Computation</head><p>We can efficiently compute the bound in Eqn. 1 for many positive sample pairs, using large negative sample sets, e.g. |N 7 | 10000, by using a simple dot product for the matching score Φ:</p><formula xml:id="formula_3">Φ(f 1 (x), f 7 (x) ij ) φ 1 (f 1 (x)) φ 7 (f 7 (x) ij ).<label>(3)</label></formula><p>The functions φ 1 /φ 7 non-linearly transform their inputs to some other vector space. Given a sufficiently high-dimensional vector space, in principle we should be able to approximate any (reasonable) class of functions we care about -which correspond to belief shifts like log p <ref type="formula" target="#formula_1">(f7|f1)</ref> p <ref type="formula" target="#formula_10">(f7)</ref> in our case -via linear evaluation. The power of linear evaluation in high-dimensional spaces can be understood by considering Reproducing Kernel Hilbert Spaces (RKHS). One weakness of this approach is that it limits the rank of the set of belief shifts our model can represent when the vector space is finite-dimensional, as was previously addressed in the context of language modeling by introducing mixtures <ref type="bibr" target="#b41">[Yang et al., 2018]</ref>. We provide pseudo-code for the NCE bound in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>When training with larger models on more challenging datasets, i.e. STL10 and ImageNet, we use some tricks to mitigate occasional instability in the NCE cost. The first trick is to add a weighted regularization term that penalizes the squared matching scores like: λ(φ 1 (f 1 (x)) φ 7 (f 7 (x) ij )) 2 . We use NCE regularization weight λ = 4e −2 for all experiments. The second trick is to apply a soft clipping non-linearity to the scores after computing the regularization term and before computing the log-softmax in Eqn. 2. For clipping score s to range [−c, c], we applied the non-linearity s = c tanh( s c ), which is linear around 0 and saturates as one approaches ±c. We use c = 20 for all experiments. We suspect there may be interesting formal and practical connections between regularization that restricts the variance/range/etc of scores that go into the NCE bound, and things like the KL/information cost in Variational Autoencoders <ref type="bibr" target="#b23">[Kingma and Welling, 2013]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Augmentation</head><p>Our model extends local DIM by maximizing mutual information between features from augmented views of each input. We describe this with a few minor changes to our notation for local DIM. We construct the augmented feature distribution p A (f 1 (x 1 ), f 7 (x 2 ) ij ) as follows: (i) sample an input x ∼ D, (ii) sample augmented images x 1 ∼ A(x) and x 2 ∼ A(x), (iii) sample spatial indices i ∼ u(i) and j ∼ u(j), (iv) compute features f 1 (x 1 ) and f 7 (x 2 ) ij . We use A(x) to denote the distribution of images generated by applying stochastic data augmentation to x. For this paper, we apply some standard data augmentations: random resized crop, random jitter in color space, and random conversion to grayscale. We apply a random horizontal flip to x before computing x 1 and x 2 .</p><p>Given p A (f 1 (x 1 ), f 7 (x 2 ) ij ), we define the marginals p A (f 1 (x 1 )) and p A (f 7 (x 2 ) ij ). Using these, we rewrite the infomax objective in Eqn. 1 to include prediction across data augmentation:</p><formula xml:id="formula_4">E (f1(x 1 ),f7(x 2 )ij ) E N7 L Φ (f 1 (x 1 ), f 7 (x 2 ) ij , N 7 ) ,<label>(4)</label></formula><p>where negative samples in N 7 are now sampled from the marginal p A (f 7 (x 2 ) ij ), and L Φ is unchanged. <ref type="figure" target="#fig_0">Figure 1a</ref> illustrates local DIM with prediction across augmented views. </p><formula xml:id="formula_6">P na i=1 P nc j=1 snce[i, i, j]</formula><p>Algorithm ImageNet Encoder Architecture ReLU( Conv2d( 3, ndf, 5, 2, 2) ) ReLU <ref type="figure" target="#fig_0">( Conv2d(ndf, ndf, 3, 1, 0</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multiscale Mutual Information</head><p>Our model further extends local DIM by maximizing mutual information across multiple feature scales. Consider features f 5 (x) ij taken from position (i, j) in the top-most layer of f with spatial dimension 5 × 5. Using the procedure from the preceding subsection, we can construct joint distributions over pairs of features from any position in any layer like:</p><formula xml:id="formula_7">p A (f 5 (x 1 ) ij , f 7 (x 2 ) kl ), p A (f 5 (x 1 ) ij , f 5 (x 2 ) kl ), or p A (f 1 (x 1 ), f 5 (x 2 ) kl ).</formula><p>We can now define a family of n-to-m infomax costs:</p><formula xml:id="formula_8">E (fn(x 1 )ij ,fm(x 2 ) kl ) E Nm L Φ (f n (x 1 ) ij , f m (x 2 ) kl , N m ) ,<label>(5)</label></formula><p>where N m denotes a set of independent samples from the marginal p A (f m (x 2 ) ij ) over features from the top-most m × m layer in f . For the experiments in this paper we maximize mutual information from 1-to-5, 1-to-7, and 5-to-5. We uniformly sample locations for both features in each positive sample pair. These costs may look expensive to compute at scale, but it is actually straightforward to efficiently compute Monte Carlo approximations of the relevant expectations using many samples in a single pass through the encoder for each batch of (x 1 , x 2 ) pairs. <ref type="figure" target="#fig_0">Figure 1b</ref> illustrates our full model, which we call Augmented Multiscale Deep InfoMax (AMDIM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Encoder</head><p>Our model uses an encoder based on the standard ResNet <ref type="bibr">[He et al., 2016a,b]</ref>, with changes to make it suitable for DIM. Our main concern is controlling receptive fields. When the receptive fields for features in a positive sample pair overlap too much, the task becomes too easy and the model performs worse. Another concern is keeping the feature distributions stationary by avoiding padding.</p><p>The encoder comprises a sequence of blocks, with each block comprising multiple residual layers. The first layer in each block applies mean pooling with kernel width w and stride s to compute a base output, and computes residuals to add to the base output using a convolution with kernel width w and stride s, followed by a ReLU and then a 1 × 1 convolution, i.e. w = s = 1. Subsequent layers in the block are standard 1 × 1 residual layers. The mean pooling compensates for not using padding, and the 1 × 1 layers control receptive field growth. Exhaustive details can be found in our code: https://github.com/Philip-Bachman/amdim-public. We train our models using 4-8 standard Tesla V100 GPUs per model. Other recent, strong self-supervised models are nonreproducible on standard hardware.</p><p>We use the encoder architecture in <ref type="figure" target="#fig_0">Figure 1c</ref> when working with ImageNet and Places205. We use 128 × 128 input for these datasets due to resource constraints. The argument order for Conv2d is (input dim, output dim, kernel width, stride, padding). The argument order for ResBlock is the same as Conv2d, except the last argument (i.e. ndepth) gives block depth rather than padding. Parameters ndf and nrkhs determine encoder feature dimension and output dimension for the embedding functions φ n (f n ). The embeddings φ 7 (f 7 ) and φ 5 (f 5 ) are computed by applying a small MLP via convolution. We use similar architectures for the other datasets, with minor changes to account for input sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Mixture-Based Representations</head><p>We now extend our model to use mixture-based features. For each antecedent feature f 1 , we compute a set of mixture features {f 1 1 , ..., f k 1 }, where k is the number of mixture components. We compute these features using a function m k : {f 1 1 , ..., f k 1 } = m k (f 1 ). We represent m k using a fully-connected network with a single ReLU hidden layer and a residual connection between f 1 and each mixture feature f i 1 . When using mixture features, we maximize the following objective:</p><formula xml:id="formula_9">maximize f,q E (x 1 ,x 2 )   1 n c nc i=1 k j=1 q(f j 1 (x 1 )|f i 7 (x 2 )) s nce (f j 1 (x 1 ), f i 7 (x 2 )) + αH(q)   . (6)</formula><p>For each augmented image pair (x 1 , x 2 ), we extract k mixture features {f 1 1 (x 1 ), ..., f k 1 (x 1 )} and n c consequent features {f 1 7 (x 2 ), ..., f nc 7 (x 2 )}. s nce (f j 1 (x 1 ), f i 7 (x 2 )) denotes the NCE score between f j 1 (x 1 ) and f i 7 (x 2 ), computed as described in <ref type="figure" target="#fig_0">Figure 1c</ref>. This score gives the log-softmax term for the mutual information bound in Eqn. 2. We also add an entropy maximization term αH(q).</p><p>In practice, given the k scores {s nce (f 1 1 , f i 7 ), ..., s nce (f k 1 , f i 7 )} assigned to consequent feature f i 7 by the k mixture features {f 1 1 , ..., f k 1 }, we can compute the optimal distribution q as follows:</p><formula xml:id="formula_10">q(f j 1 |f i 7 ) = exp(τ s nce (f j 1 , f i 7 )) j exp(τ s nce (f j 1 , f i 7 )) ,<label>(7)</label></formula><p>where τ is a temperature parameter that controls the entropy of q. We motivate Eqn. 7 by analogy to Reinforcement Learning. Given the scores s nce (f j 1 , f i 7 ), we could define q using an indicator of the maximum score. But, when q depends on the stochastic scores this choice will be overoptimistic in expectation, since it will be biased towards scores which are pushed up by the stochasticity (which comes from sampling negative samples). Rather than take a maximum, we encourage q to be less greedy by adding the entropy maximization term αH(q). For any value of α in Eqn. 6, there exists a value of τ in Eqn. 7 such that computing q using Eqn. 7 provides an optimal q with respect to Eqn. 6. This directly relates to the formulation of optimal Boltzmann-type policies in the context of Soft Q Learning. See, e.g. <ref type="bibr" target="#b15">Haarnoja et al. [2017]</ref>. In practice, we treat τ as a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on standard benchmarks for self-supervised visual representation learning. We use CIFAR10, CIFAR100, STL10, ImageNet, and Places205. To measure performance, we first train an encoder using all examples from the training set (sans labels), and then train linear and MLP classifiers on top of the encoder features f 1 (x) (sans backprop into the encoder). The final performance metric is the accuracy of these classifiers. This follows the evaluation protocol described by <ref type="bibr" target="#b24">Kolesnikov et al. [2019]</ref>. Our model outperforms prior work on these datasets.</p><p>On CIFAR10 and CIFAR100 we trained small models with size parameters: (ndf=128, nrkhs=1024, ndepth=10), and large models with size parameters: (ndf=320, nrkhs=1280, ndepth=10). On CI-FAR10, the large model reaches 91.2% accuracy with linear evaluation and 93.1% accuracy with MLP evaluation. On CIFAR100, it reaches 70.2% and 72.8%. These are comparable with slightly older fully-supervised models, and well ahead of other work on self-supervised feature learning. See <ref type="table" target="#tab_4">Table 2</ref> for a comparison with standard fully-supervised models. On STL10, using size parameters: (ndf=192, nrkhs=1536, ndepth=8), our model significantly improves on prior self-supervised results. STL10 was originally intended to test semi-supervised learning methods, and comprises 10 classes with a total of 5000 labeled examples. Strong results have been achieved on STL10 via semi-supervised learning, which involves fine-tuning some of the encoder parameters using the available labeled data. Examples of such results include <ref type="bibr" target="#b22">[Ji et al., 2019]</ref> and <ref type="bibr" target="#b2">[Berthelot et al., 2019]</ref>   <ref type="bibr" target="#b12">[Gidaris et al., 2018</ref><ref type="bibr" target="#b9">, Dosovitskiy et al., 2014</ref><ref type="bibr" target="#b6">, Doersch and Zisserman, 2017</ref><ref type="bibr" target="#b30">, Noroozi and Favaro, 2016</ref><ref type="bibr" target="#b39">, van den Oord et al., 2018</ref><ref type="bibr" target="#b20">, Hénaff et al., 2019</ref>. The non-CPC results are from updated versions of the models by <ref type="bibr" target="#b24">Kolesnikov et al. [2019]</ref>. The (sup) models were fully-supervised, with no self-supervised costs. The small and large AMDIM models had size parameters: (ndf=192, nrkhs=1536, ndepth=8) and (ndf=320, nrkhs=2560, ndepth=10). AMDIM outperforms prior and concurrent methods by a large margin. We trained AMDIM models for 150 epochs on 8 NVIDIA Tesla V100 GPUs. When we train the small model using a shorter 50 epoch schedule, it achieves 62.7% accuracy in 2 days on 4 GPUs.  <ref type="bibr" target="#b16">, He et al., 2016a</ref><ref type="bibr" target="#b42">, Zagoruyko and Komodakis, 2016</ref>. The small and large AMDIM models had size parameters: (ndf=128, nrkhs=1024, ndepth=10) and (ndf=256, nrkhs=2048, ndepth=10). AMDIM features performed on par with classic fully-supervised models.  <ref type="bibr">(ndf=192, nrkhs=1536, ndepth=8)</ref>. We trained these models for 50 epochs, thus the ImageNet models were smaller and trained for one third as long as our best result (68.1%). We perform ablations against a baseline model that applies basic data augmentation which includes resized cropping, color jitter, and random conversion to grayscale. We ablate different aspects of the data augmentation as well as multiscale feature learning and NCE cost regularization. Our strongest results used the Fast AutoAugment augmentation policy from <ref type="bibr" target="#b25">Lim et al. [2019]</ref>, and we report the effects of switching from basic augmentation to stronger augmentation as "+strong aug". Data augmentation had the strongest effect by a large margin, followed by stability regularization and multiscale prediction. which achieve 88.8% and 94.4% accuracy respectively. Our model reaches 94.2% accuracy on STL10 with linear evaluation, which compares favourably with semi-supervised results that fine-tune the encoder using the labeled data.</p><p>On ImageNet, using a model with size parameters: (ndf=320, nrkhs=2536, ndepth=10), and a batch size of 1008, we reach 68.1% accuracy for linear evaluation, beating the best prior result by over 12% and the best concurrent results by 7% <ref type="bibr" target="#b24">[Kolesnikov et al., 2019</ref><ref type="bibr" target="#b20">, Hénaff et al., 2019</ref><ref type="bibr" target="#b38">, Tian et al., 2019</ref>. Our model is significantly smaller than the models which produced those results and is reproducible on standard hardware. Using MLP evaluation, our model reaches 69.5% accuracy. Our linear and MLP evaluation results on ImageNet both surpass the original AlexNet trained end-to-end by a large margin. <ref type="table" target="#tab_5">Table 3</ref> provides results from single ablation tests on STL10 and ImageNet. We perform ablations on individual aspects of data augmentation and on the use of multiscale feature learning and NCE cost regularization. See <ref type="table" target="#tab_3">Table 1</ref> for a comparison with well-optimized results for prior and concurrent models. We also tested our model on an Imagenet→Places205 transfer task, which involves training the encoder on ImageNet and then training the evaluation classifiers on the Places205 data. Our model also beat prior results on that task. Performance with the transferred features is close to that of features learned on the Places205 data. See <ref type="table" target="#tab_3">Table 1</ref>.</p><p>We include additional visualizations of model behaviour in <ref type="figure" target="#fig_1">Figure 2</ref>. See the figure caption for more information. Briefly, though our model generally performs well, it does exhibit some characteristic weaknesses that provide interesting subjects for future work. Intriguingly, when we incorporate mixture-based representations, segmentation behaviour emerges as a natural side-effect. The mixturebased model is more sensitive to hyperparameters, and we have not had time to tune it for ImageNet. Each left-most column shows a query image, whose f 1 was used to retrieve 7 most similar images. For each query, we visualize similarity between its f 1 and the f 7 s from the retrieved images. On ImageNet, we see that good retrieval is often based on similarity focused on the main object, while poor retrieval depends more on background similarity. The pattern is more diffuse for Places205. (c) and (d) visualize the data augmentation that produces paired images x 1 and x 2 , and three types of similarity: between f 1 (x 1 ) and f 7 (x 2 ), between f 7 (x 1 ) and f 7 (x 2 ), and between f 5 (x 1 ) and f 5 (x 2 ). (e, f, g, h): we visualize models trained on STL10 with 2, 3, 3, and 4 components in the top-level mixtures. For each x 1 (left) and x 2 (right), the mixture components were inferred from x 1 and we visualize the posteriors over those components for the f 7 features from x 2 . We compute the posteriors as described in Section 3.7.</p><p>However, the qualitative behaviour on STL10 is exciting and we observe roughly a 1% boost in performance with a simple bag-of-features approach for using the mixture features during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We introduced an approach to self-supervised learning based on maximizing mutual information between arbitrary features extracted from multiple views of a shared context. Following this approach, we developed a model called Augmented Multiscale Deep InfoMax (AMDIM), which improves on prior results while remaining computationally practical. Our approach extends to a variety of domains, including video, audio, text, etc. E.g., we expect that capturing natural relations using multiple views of local spatio-temporal contexts in video could immediately improve our model.</p><p>Worthwhile subjects for future research include: modifying the AMDIM objective to work better with standard architectures, improving scalability and running on better infrastructure, further work on mixture-based representations, and examining (formally and empirically) the role of regularization in the NCE-based mutual information bound. We believe contrastive self-supervised learning has a lot to offer, and that AMDIM represents a particularly effective approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a): Local DIM with predictions across views generated by data augmentation. (b): Augmented Multiscale DIM, with multiscale infomax across views generated by data augmentation. (c)-top: An algorithm for efficient NCE with minibatches of n a images, comprising one antecedent and n c consequents per image. For each true (antecedent, consequent) positive sample pair, we compute the NCE bound using all consequents associated with all other antecedents as negative samples. Our pseudo-code is roughly based on pytorch. We use dynamic programming in the log-softmax normalizations required by nce . (c)-bottom: Our ImageNet encoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualizing behaviour of AMDIM. (a) and (b) combine two things -KNN retrieval based on cosine similarity between features f 1 , and the matching scores (i.e., φ 1 (f 1 ) φ 7 (f 7 )) between features f 1 and f 7 . (a) is from ImageNet and (b) is from Places205.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm Compute and Memory-E cient NCE // na: # antecedents, nc: # consequents/antecedent // s: array of (fa) &gt; (fc) scores, with size (na, na, nc) // the tuple after each statement gives result size sshift = max(max(s, dim=2), dim=1) // (na, 1, 1) sexp = exp(s sshift) / / ( na, na, nc) sself = sum(sexp, dim=2) // (na, na, 1) sfull = sum(sself , dim=1) // (na, 1, 1) sother = sfull sself // (na, na, 1) slse = log(sexp + sother) / / ( na, na, nc) snce = s sshift slse // (na, na, nc)</figDesc><table><row><cell>nce =</cell><cell>1 nanc</cell></row><row><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>,</figDesc><table><row><cell>Method</cell><cell cols="2">ImageNet Places205</cell><cell></cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell></cell><cell>STL10</cell><cell>ImageNet</cell></row><row><cell>ResNet50v2 (sup)</cell><cell>74.4</cell><cell>61.6</cell><cell></cell><cell>(linear, MLP)</cell><cell>(linear, MLP)</cell><cell></cell><cell cols="2">(linear, MLP) (linear, MLP)</cell></row><row><cell>AMDIM (sup)</cell><cell>71.3</cell><cell>57.4</cell><cell>Highway Network</cell><cell>92.28</cell><cell>67.61</cell><cell>AMDIM</cell><cell>93.4, 93.8</cell><cell>61.7, 62.6</cell></row><row><cell>Rotation</cell><cell>55.4</cell><cell>48.0</cell><cell>ResNet:101</cell><cell>93.58</cell><cell>74.84</cell><cell>+strong aug</cell><cell>94.2, 94.5</cell><cell>62.7, 63.1</cell></row><row><cell>Exemplar</cell><cell>46.0</cell><cell>42.7</cell><cell>WideResNet:40-4</cell><cell>95.47</cell><cell>79.82</cell><cell>color jitter</cell><cell>90.3, 90.6</cell><cell>57.7, 58.8</cell></row><row><cell>Patch O↵set</cell><cell>51.4</cell><cell>45.3</cell><cell>AMDIM -small</cell><cell>89.5, 91.4</cell><cell>68.1, 71.5</cell><cell cols="2">random gray 88.3, 89.4</cell><cell>53.6, 54.9</cell></row><row><cell>Jigsaw</cell><cell>44.6</cell><cell>42.2</cell><cell>AMDIM -large</cell><cell>91.2, 93.1</cell><cell>70.2, 72.8</cell><cell cols="2">random crop 86.0, 87.1</cell><cell>53.2, 54.9</cell></row><row><cell>CPC -large CPC -huge</cell><cell>48.7 61.0</cell><cell>n/a n/a</cell><cell></cell><cell>(b)</cell><cell></cell><cell>multiscale stabilize</cell><cell>92.6, 93.0 93.5, 93.8</cell><cell>59.9, 61.2 57.2, 59.5</cell></row><row><cell>CMC -large AMDIM -small</cell><cell>60.1 63.5</cell><cell>n/a n/a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell></row><row><cell>AMDIM -large</cell><cell>68.1</cell><cell>55.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>(a): Comparing AMDIM with prior results for the ImageNet and Imagenet→Places205 transfer tasks using linear evaluation. The competing methods are from:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>(b): comparing AMDIM with fully-supervised models on CIFAR10 and CIFAR100, using linear and MLP evaluation. Supervised results are from [Srivastava</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>(c): Results of single ablations on STL10 and ImageNet. The size parameters for all models on both datasets were:</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ILSVRC2012 version</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We focus on images in this paper, but the approach directly extends to, e.g.: audio, video, and text. 3 d refers to the layer's spatial dimension and should not be confused with its depth in the encoder.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Look, listen and learn. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Objects that sound. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of single layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jost Tobias Springenberg, and Thomas Brox Martin Riedmiller. Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<title level="m">Learning actionable representations from visual observations. International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Sm Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avraham</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Koray Kavukcuoglu, and Demis Hassabis. Neural scene representation and rendering. Science</title>
		<meeting><address><addrLine>Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, Daan Wierstra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling and benchmarking selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01235</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Identity mappings in deep residual networks. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mask r-cnn. International Conference on Computer Vision (ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883[vs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Rethinking imagenet pre-trainining</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Rezavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S M Ali</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06653</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbim</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00397</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Fast autoaugment.</note>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Formal limitations on the measurement of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04251</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.IT]</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Breaking the softmax bottleneck: A high-rank rnn language model. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audo</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
