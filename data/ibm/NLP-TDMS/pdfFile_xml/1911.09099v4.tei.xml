<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Modules and Information Blocking Decoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Lowe</forename><surname>Sj√∂sund</surname></persName>
							<email>lars.sjosund@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
							<email>youngjoon.yoo@navercorp.com</email>
							<affiliation key="aff2">
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Monet</surname></persName>
							<email>nicolas.monet@naverlabs.com</email>
							<affiliation key="aff3">
								<orgName type="department">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Bang</surname></persName>
							<email>jihwan.bang@navercorp.com</email>
							<affiliation key="aff4">
								<address>
									<country>Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.kr</email>
							<affiliation key="aff5">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Modules and Information Blocking Decoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing a lightweight and robust portrait segmentation algorithm is an important task for a wide range of face applications. However, the problem has been considered as a subset of the object segmentation problem and less handled in the semantic segmentation field. Obviously, portrait segmentation has its unique requirements. First, because the portrait segmentation is performed in the middle of a whole process of many real-world applications, it requires extremely lightweight models. Second, there has not been any public datasets in this domain that contain a sufficient number of images with unbiased statistics. To solve the first problem, we introduce the new extremely lightweight portrait segmentation model SINet, containing an information blocking decoder and spatial squeeze modules. The information blocking decoder uses confidence estimates to recover local spatial information without spoiling global consistency. The spatial squeeze module uses multiple receptive fields to cope with various sizes of consistency in the image. To tackle the second problem, we propose a simple method to create additional portrait segmentation data which can improve accuracy on the EG1800 dataset. In our qualitative and quantitative analysis on the EG1800 dataset, we show that our method outperforms various existing lightweight segmentation models. Our method reduces the number of parameters from 2.1M to 86.9K (around 95.9% reduction), while maintaining the accuracy under an 1% margin from the state-of-the-art portrait segmentation method. We also show our model is successfully executed on a real mobile device with 100.6 FPS. In addition, we demonstrate that our method can be used for general semantic segmentation on the Cityscapes dataset. The code and dataset are available in https://github.com/HYOJINPARK/ExtPortraitSeg . Figure 1. Accuracy (mIoU) vs. complexity (number of parameters) on the EG1800 validation set. Our proposed SINet has high accuracy with small complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Developing algorithms targeting face data has been considered as an important task in the computer vision field, and many related vision algorithms including detection, recognition, and key-point extraction are actively studied. Among them, portrait segmentation is commonly used in real-world applications such as background editing, security checks, and face resolution enhancement <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, giving rise to the need for fast and robust segmentation models.</p><p>The challenging point of the segmentation task is that the model have to solve two contradictory problems simultaneously; (1) Handling long-range dependencies or global consistency and (2) preserving detailed local information. <ref type="figure" target="#fig_0">Figure 2</ref> shows two common segmentation errors. First, the blue blob in <ref type="figure" target="#fig_0">Figure 2</ref>  son for this problem is that the segmentation model fails to get global context information which prevents wrong representation. Second, the red blobs in <ref type="figure" target="#fig_0">Figure 2</ref> (b) show the model's failure to accurately segment fine details. The lateral part of hair needs fine segmentation due to its small size and similar color to the wood. The model is not able to produce a sharp segmentation image because of the lack of detail information about hair. This is because the usage of stride convolution or pooling layer. These techniques induces the model can capture global information by enlarging receptive field size. However, the local information might be destroyed.</p><p>Researchers have developed several strategies to solve these problems and the first one is to produce multiple receptive fields for each layer. This multi-path structure is able to enhance both global and local information, but comes at the cost of increased latency due to fragmented parallel operations <ref type="bibr" target="#b9">[10]</ref>. Another method is using a twobranch network, which consists of a deeper branch that is employed to produce global context, and a shallow branch that preserves detailed local features by keeping high resolution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>. Even though the shallow branch has few convolutional layers, it is computationally heavy due to its high-resolution feature maps. Also, this method has to extract features two times, once for each branch.</p><p>The portrait segmentation problem comes with a set of additional challenges. The first one being the small amount of available data. The EG1800 dataset <ref type="bibr" target="#b18">[19]</ref>, an accessible public portrait segmentation dataset, contains only around 1,300 training images, and has large biases with regard to attributes such as race, age, and gender. Second, portrait segmentation is usually used just as one of several steps in real-world applications. Since many of these applications run on mobile devices, the segmentation model needs to be lightweight to ensure real-time speeds. Researchers have developed plenty of lightweight segmentation methods, but most of them are still not lightweight enough for portrait segmentation tasks. For example, PortraitNet <ref type="bibr" target="#b28">[29]</ref>, the current state-of-the-art model on the EG1800 dataset, has 2.1M parameters. A few examples of general lightweight segmentation models are ESPNetV2 <ref type="bibr" target="#b10">[11]</ref> with 0.78M parameters, and MobileNet V3 <ref type="bibr" target="#b5">[6]</ref> with 0.47M parameters.</p><p>In this paper, we propose a new extremely lightweight portrait segmentation model called SINet with an information blocking decoder structure and spatial squeeze modules (S2-module). Furthermore, we collect additional portrait segmentation data to overcome the aforementioned dataset problems.</p><p>The proposed SINet has 86.9K parameters, achieving 100.6 FPS in iPhone XS without any low-floating point operations or pruning methods. Compared with the baseline model, PortraitNet, which has 2.1M parameters, the accuracy degradation is just under 1% on the EG1800 dataset, as can be seen in <ref type="figure">Figure 1</ref>. Our contributions can be summarized as follows: <ref type="formula" target="#formula_0">(1)</ref> We introduce the information blocking scheme to the decoder. It measures the confidence in a low-resolution feature map, and blocks the influence of high-resolution feature maps in highly confident pixels. This prevents noisy information to ruin already certain areas, and allows the model to focus on regions with high uncertainty. We show that this information blocking decoder is robust to translation and can be applied to general segmentation tasks. <ref type="bibr" target="#b1">(2)</ref> We propose a spatial squeeze module (S2-module), an efficient multipath network for feature extraction. Existing multi-path structures deal with the various size of long-range dependencies by managing multiple receptive fields. However, this increases latency in real implementations, due to having unsuitable structure with regard to kernel launching and synchronization. To mitigate this problem, we squeeze the spatial resolution from each feature map by average pooling, and show that this is more effective than adopting multi-receptive fields. (3) The public portrait segmentation dataset has a small number of images compared to other segmentation datasets, and is highly biased. We propose a simple and effective data generation method to augment the EG1800 dataset with a significant amount of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Portrait Application: PortraitFCN+ <ref type="bibr" target="#b18">[19]</ref> built a portrait dataset from Flickr and proposed a portrait segmentation model based on FCN <ref type="bibr" target="#b8">[9]</ref>. After that, PortraitNet proposed a real-time portrait segmentation model with higher accuracy than PortraitFCN+. <ref type="bibr" target="#b12">[13]</ref> integrated two different segmentation schemes from Mask R-CNN and DensePose, and generated matting refinement based on FCN. <ref type="bibr" target="#b4">[5]</ref> introduced a boundary-sensitive kernel to enhance semantic boundary shape information. While these works achieved good segmentation results, their models are still too heavy for embedded systems. Global consistency: Global consistency and long range of dependencies are critical factors for the segmentation task, and models without a large enough receptive field will produce error-prone segmentation maps. One way of creating a large receptive field is to use large kernels. However, this is not suitable for lightweight models due to their large number of parameters. Another method is to reduce the size of feature maps through downsampling, but this leads to difficulties in segmenting small or narrow objects.</p><p>To resolve this problem, dilated convolutions (or atrous convolutions) have been introduced as an effective solution to get a large receptive field while preserving localization information <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>, keeping the same amount of computation as the normal convolution. However, as the dilation rate is increased the count of valid weights decreases, to finally degenerate to a 1 √ó 1 convolution <ref type="bibr" target="#b2">[3]</ref>. Also, the grid effect degrades the segmentation result with checkerboard pattern. Another method is to use spatial pyramid pooling to get a larger receptive field. The spatial pyramid pooling uses different sizes of pooling and concatenates each resultant feature map to obtain a multi-scale receptive field. Similarly, the Atrous Spatial Pyramid Pooling layer <ref type="bibr" target="#b3">[4]</ref> replaces the pooling with dilated convolutions to get a multi-scale representation. To get a multi-scale representation, some works use a multi-path structure for feature extraction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Each module splits the input feature map and translates the feature map with a different dilation rate. This method is well suited for lightweight models, but suffers from high latency. Recently, the asymmetric non-local block <ref type="bibr" target="#b29">[30]</ref> was proposed, inspired by the non-local block <ref type="bibr" target="#b22">[23]</ref> and spatial pyramid pooing. Because the non-local block calculates all the pairwise pixel dependencies, it is computationally heavy. Asymmetric non-local block approximates the calculation with spatial pyramid pooling. However, the computational cost is still too large to fit a lightweight model.Recently, some works adopt average pooling to reduce complexity more <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>. Detail local information: Recovering detailed local information is crucial to generating sharp segmentation maps. Conventionally, an encoder-decoder structure based on deconvolution (or transposed convolution) is applied <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. By concatenating the high-resolution feature, they recover the original resolution step by step. Also, some works use global attention for upsampling. The feature pyramid attention <ref type="bibr" target="#b7">[8]</ref> uses global pooling to enhance the high resolution feature map from the low-resolution. However, the attention vector can not reflect the local information well due to global pooling. Recently, the two-branch method is suggested for better segmentation. ContextNet <ref type="bibr" target="#b15">[16]</ref> and FastSCNN <ref type="bibr" target="#b16">[17]</ref> designed a two-path network, each branch of which is for global context and detailed information, respectively. BiSeNet <ref type="bibr" target="#b26">[27]</ref> also proposed a similar two-path network for preserving spatial information as well as acquiring a large enough receptive field. However, it needs to calculate features twice, once for each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we explain the structure of the proposed SINet which consists of a Spatial squeeze module and a Information blocking decoder. The spatial squeeze block (S2-module) handles global consistency by using the multireceptive field scheme, and squeezes the feature resolution to mitigate the high latency of multi-path structures. The information blocking decoder is designed to only take the necessary information from the high-resolution feature maps by utilizing the confidence score of the low-resolution feature maps. The information blocking in the decoder is important for increasing robustness regarding translation (Section 3.1) and the S2-module can handle global consistency without heavy computation (Section 3.2). We also demonstrate a simple data generation framework to solve the lack of data in two situations: 1) having human segmentation ground truths and 2) having only raw images (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Information Blocking Decoder</head><p>An encoder-decoder structure is the most commonly used structure for segmentation. An encoder extracts semantic features of the incoming images according to semantic information, and a decoder recovers detailed local information and resolution of the feature map. For designing the decoder, bilinear upsampling or transposed convolution upsampling blocks are commonly used to expand the low-resolution feature maps from the encoder. Also, recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref> re-use additional high-resolution feature maps from the encoder to make more accurate segmentation results. To the best of our knowledge, most studies take all the information of high-resolution feature maps from the encoder by conducting concatenation, element-wise summation, or by enhancing high-resolution feature maps via attention vectors from low-resolution. However, using the high-resolution feature maps means that we give nuisance local information, which is already removed by the encoder. Therefore, we have to take only the necessary clue and avert the nuisance noise.</p><p>Here, we introduce a new concept of a decoder structure using information blocking. We measure the confidence score in the low-resolution feature map and block the information flow from the high-resolution feature into the region where the encoder successfully segmented with high confidence. The information blocking process removes nuisance information from the image and makes the high-resolution feature map concentrate only on the low confidence regions.   <ref type="figure" target="#fig_1">Figure 3</ref> shows the overall architecture of SINet and the detailed process of the information blocking decoder. The model projects the last set of feature maps of the encoder to the size of the number of classes by a pointwise convolution and uses a bilinear upsampling to make the same resolution as the high-resolution target segmentation map. The model employs a softmax function to get a probability of each class and calculates each pixel's confidence score c by taking maximum value among the probabilities of each class. Finally, we generate an information blocking map by computing (1 ‚àí c). We perform pointwise multiplication between the information blocking map and the highresolution feature maps. This ensures that low confidence regions get more information from the high-resolution feature maps, while high confidence regions keep their original values in the subsequent pointwise addition operation. <ref type="figure" target="#fig_2">Figure 4</ref> (b) is an example of the information blocking map, and (c) is the confidence map from the model output. As shown in <ref type="figure" target="#fig_2">Figure 4</ref> (b), the boundary and clothing have high uncertainty while the inner parts of the foreground and background already have a high confidence score. This indicates that the high uncertainty regions need more detailed Dilated rate rate=2 rate=6 rate=12 rate=18 Latency (ms) 6.7 6.63 11.84 12.03 local information to reduce uncertainty. However, the inner parts of the face, such as the beard and nose, do not need to get more local information for making a segmentation map. If the local information was embedded, it could be harmful to the global consistency due to nuisance information as noise. In the final confidence map of the model ( <ref type="figure">Figure.</ref> 4 (c)), the uncertainty region of the boundary has shrunk, and the confidence score of the inner part is highly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Squeeze module</head><p>A multi-path structure have an advantage of high accuracy with less parameters <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>, but it suffers from increased latency proportional to the number of sub-paths <ref type="bibr" target="#b9">[10]</ref>. The proposed spatial squeeze module (S2-module) resolves this problem and <ref type="figure" target="#fig_3">Figure 5</ref> shows the structure. We utilize average pooling for adjusting the size of the receptive field and reducing the latency.</p><p>The S2-module is also following a kind of splittransform-merge scheme like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> for covering multi-receptive field with two spatial squeeze blocks (S2block). First, we uses a pointwise convolution to reduce the number of feature maps by half. For further reduction of computation, we use a group pointwise convolution with channel shuffle. The reduced feature maps pass through each S2-block, and the results are merged through concatenation.We also adopt a residual connection between the in-(a) Spatial Squeeze Block (S2-block).</p><p>(b) Spatial Squeeze Module (S2-module). put feature map and the merged feature map. Finally, PRelu is utilized for non-linearity.</p><p>For S2-block, we select average pooling rather than dilated convolution for making a multi-receptive field structure for two reasons. First, the latency time is affected from the dilated rate, as shown in <ref type="table" target="#tab_0">Table 1</ref>, and dilated convolution can not be free from the problem of grid effects <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>. Second, the multi-path structure is not friendly to GPU parallel computing <ref type="bibr" target="#b9">[10]</ref>. Thus, we squeeze the resolution of each feature map to avoid the sacrifice of the latency time. The S2-block squeezes the resolution of a feature map by an average pooling, with kernel size up to 4. Then, a depthwise separable convolution with the kernel size 3 or 5 is used.</p><p>Between the depthwise convolution and the pointwise convolution, we use a PRelu non-linear activation function. Empirically, placing the pointwise convolution before or after the bilinear upsampling does not have a critical effect on the accuracy. Therefore, we put it before the bilinear upsampling to further reduce computation. We also insert a batch normalization layer after the depthwise convolution and the bilinear upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Design for SINet</head><p>In this part, we explain the overall structure of SINet. SINet uses S2-modules as bottlenecks and depthwise separable convolution (ds-conv) with stride 2 for reducing the resolution of feature maps. Empirically, applying the S2module with stride 2 for downsampling improves accuracy,  <ref type="table">Table 2</ref>. Detailed settings for the SINet encoder. k denotes the kernel size of the depthwise convolution and p denotes the kernel size of average pooling the S2-block.</p><p>but we found that it has longer latency time than S2-module with stride 1 under the same output size conditions. Therefore, for downsampling, instead of the S2-module with stride 2, we use ds-conv with Squeeze-and-Excite blocks.</p><p>For the first bottleneck we use two sequential S2-modules and for the second bottleneck we use eight. The detailed setting of the S2-module is described in <ref type="table">Table 2</ref>. We add a residual connection for each bottleneck, concatenating the bottleneck input with its output. A 3 √ó 3 convolution is used for classification and finally bilinear upsampling is applied to recover the original input resolution.</p><p>We found that a weighted auxiliary loss for the boundary part is helpful in improving the accuracy. The final loss is as follows:</p><formula xml:id="formula_0">B = (f ‚äï y * ) ‚àí (f y * ) Loss = CE i‚ààP (y * i ,≈∑) + ŒªCE j‚ààB (y * j ,≈∑ j ).<label>(1)</label></formula><p>Here, f is a 15 √ó 15 filter used for the morphological dilation (‚äï) and erosion ( ) operations. P denotes all the pixels of the ground truth, and B denotes the pixels in the boundary area as defined by the morphology operation. y * is a binary ground truth value and≈∑ is a predicted label from a segmentation model. Œª is a hyperparameter that controls the balance between the loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Data Generation</head><p>Annotating data often comes with high costs, and the annotation time per instance varies a lot depending on the task type. For example, the annotation time per instance for PASCAL VOC is estimated to be 20.0 seconds for image classification and 239.7 seconds for segmentation, an order of magnitude difference as mentioned in <ref type="bibr" target="#b0">[1]</ref> . To mitigate the cost of annotation for portrait segmentation, we consider a couple of plausible situations: 1) having images with ground truth human segmentation. 2) having only raw images. We make use of either an elaborate face detector model (case 1) or a segmentation model (case 2) for generating pseudo ground truths to each situation.</p><p>When we have human images and ground truths, the only thing we need is a bounding box around the portrait area. We took images from Baidu dataset <ref type="bibr" target="#b23">[24]</ref>, which contains 5,382 human full body segmentation images covering various poses, fashions and backgrounds. To get the bounding box and portrait area, we detect the face location of the images using a face detector <ref type="bibr" target="#b25">[26]</ref>. Since the face detector tightly bounds the face region, we increase the bounding box size to include parts of the upper body and background before cropping the image and ground truth segmentation.</p><p>We also create a second augmentation from portrait images scraped from the web, applying a more heavyweight segmentation model to generate pseudo ground truth segmentation masks. This segmentation model consists of a DeepLabv3+ <ref type="bibr" target="#b3">[4]</ref> architecture with a SE-ResNeXt-50 <ref type="bibr" target="#b24">[25]</ref> backbone. The model is pre-trained on ImageNet and finetuned on a proprietary dataset containing around 2,500 fine grained human segmentation images. The model is trained for general human segmentation rather than for the specific purpose of portrait segmentation.</p><p>Finally, human annotators just check the quality of each pseudo ground truth image, removing obvious failure cases. This method reduces the annotation effort per instance from several minutes to 1.25 seconds by transforming the segmentation task into a binary classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluated the proposed method on the public dataset EG1800 <ref type="bibr" target="#b18">[19]</ref>, which collected images from Flickr with manually annotated labels. The dataset has a total of 1, 800 images and is divided into 1, 500 train and 300 validation images. However, we could access only 1,309 images for train and 270 for validation, since some of the URLs are broken. We built an additional 10,448 images using the proposed data generation method mentioned in Section 3.4. We trained our model using ADAM optimizer with initial learning rate to 7.5e‚àí3, and weight decay to 2e‚àí4, for a total of 600 epochs. We followed the data augmentation method in <ref type="bibr" target="#b28">[29]</ref> with 224√ó224 images. We used a two-stage training method;for the first 300 epochs, we only trained until the encoder with the batch size set to 36. Then, we initialize the encoder with the best parameters from the previous step, and trained the overall SINet model for an additional 300 epochs with the batch size to 24. We evaluated our model followed by various ablations using mean intersection over union (mIoU) and F1-score in the boundary part, and compared with SOTA portrait segmentation models including other lightweight segmentation models. To define the boundary region, we subtract the eroded ground truths from the dilated ground truths, using a kernel with size 15 √ó 15. We demonstrated the robustness of the information blocking decoder on randomly rotated EG1800 validation images, and the importance of multi-receptive structure on EG1800 validation images in Section 4.2 Also, we showed that the proposed method can be used for general tasks by evaluating it on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Results on the EG1800 Dataset</head><p>We compared the proposed model to PortraitNet <ref type="bibr" target="#b28">[29]</ref>, which has SOTA accuracy in the portrait segmentation field. Since some sample URLs in the EG1800 dataset are missing, we re-trained the PortraitNet following the original method in paper and using the official code on the remaining samples in EG1800 dataset. PortraitNet compared their work to BiseNet and Enet. Therefore, we also re-trained BiSeNet and ENet following the method of PortraitNet for a fair comparison. As shown in <ref type="table">Table 3</ref>, the accuracies of the re-trained models are slightly decreased due to the reduced size of the training dataset. We measured latency time on an Intel Core i5-7200U CPU environment with the PyTorch framework on an LG gram laptop.</p><p>Among the compared methods, DS-ESPNet has the same structure as ESPNet, with only changing the standard dilated convolutions of the model into depth-wise separable dilated convolutions. For ESPNetV2 (2.0) and ESP-NetV2 (1.5), we changed the number of channels of the convolutional layers to reduce the model size as following official code. We also reduced the number of channels for the convolutions in the DS-ESPNet (0.5) by half from the orig-Method Parameters (M) FPS FLOPs (G) F1-score mIOU mIOU <ref type="bibr" target="#b28">[29]</ref> Enet (2016) <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>. EG1800 validation results for the proposed SINet and other segmentation models. DS denotes depth-wise separable convolution. We measure FPS on an Intel Core 15-7200 CPU environment with input size 224 √ó 224. The results in the last column are from the PortraitNet <ref type="bibr" target="#b28">[29]</ref> paper. SINet+ is the result of using the augmented dataset as descibed in Section 3.4 <ref type="figure">Figure 6</ref>. Qualitative comparison results on the EG1800 validation dataset.</p><p>inal model to make it less than 0.1M parameters and 0.2G FLOPs. The original ContextNet used 4 pyramid poolings but we used only 3 due to the small feature map size.</p><p>From <ref type="table">Table 3</ref>, we see that our proposed method achieved comparable or better performance than the other models, while having less parameters and FLOPs, and higher FPS. The SOTA PortraitNet showed the highest accuracy in all the experimental results, and has achieved even better performance than the heavier BiSeNet. However, PortraitNet requires a large number of parameters, which is a disadvantage for using it on smaller devices. The proposed SINet has reduced the number of parameters by 95%, and FLOPs by 80% compared to PortraitNet, while maintaining accuracy. ESPNet and ESPNet V2 have similar accuracy, but showed a trade-off between the number of parameters and FLOPs. ESPNet V2 has more parameters than ESPNet, but ESP-Net needs more FLOPs than ESPNet V2. Enet shows better performance than both models but requires more FLOPs. In our comparison, the proposed method has less number of parameters and FLOPs, but still achieved better accuracy than ESPNet and ESPNet V2. In particular, our SINet has the highest accuracy in an extremely lightweight environment. <ref type="figure">Figure 6</ref> shows that the quality of our model is superior to other extremely lightweight models.</p><p>We compared the execution speed of the proposed model with SOTA segmentation model MobileNet V3 on an iPhone XS using the CoreML framework. MobileNet V3 has 60.7 FPS, and our SINet has 100.6 FPS. The FLOPs  in MobileNet V3 and SINet are similar, but SINet is much faster than MobileNet V3. We conjecture that the SE block and h-swish activation function are the main reasons for the increase in latency in MobileNet V3. In summary, the proposed SINet showed outstanding performance among the various segmentation model in terms of accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Information blocking decoder: <ref type="table">Table 4</ref> shows the accuracy improvement from using the information blocking decoder. We randomly rotated validation images and evaluated mIOU over the whole image. Reverse IB denotes that we multiply the high-resolution feature maps with the confidence score c instead of (1 ‚àí c), thus enhancing highconfident pixels rather than low-confidence ones. Remove IB means that we did not use any information blocking, and instead conducted element-wise summation between the low-resolution feature maps and the high-resolution feature maps from a middle layer of a encoder. GAU uses global pooling to enhance high-resolution feature map from low-resolution feature map before applying element-wise summation. GAU has better performance than Reverse IB and Remove IB, but it still fails to get a tight boundary and to get better performances in translated images than IB. From the result, we can see that the information blocking decoder shows outstanding performance compared to the other methods. Qualitatively, it prevents segmentation errors of the background region as shown in <ref type="figure" target="#fig_4">Figure 7</ref>.</p><p>Multi-receptive field: <ref type="table">Table 5</ref> shows the performance depending on the multi-receptive structures. SINet used vari-  ous combinations of kernel sizes for convolution and pooling. We re-designed the S2-module to always use the same kernel sizes within the S2-block for all convolutional and pooling layers respectively. As shown in <ref type="table">Table 5</ref>, our SINet achieved higher mIOU and F1-score than the other combinations. Therefore, a multi-receptive field structure has an advantage for accuracy than a single-receptive field one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">General Segmentation Dataset</head><p>We also demonstrate that our proposed method is suitable not only for the binary segmentation problem but also for general segmentation problems by testing the model on the Cityscapes dataset. We increased the number of layers and channels a little bit to cope with the increased complexity compared to the binary segmentation task, and we factorized the depthwise convolution in the S2-blocks for reducing the number of parameters. Here, SINet has only 0.12M parameters and 1.2GFLOPs for input of size 2048 √ó 512, but our model showed better accuracy than any other lightweight segmentaiton model except MobileNet V3 and MobileNet V2. The accuracy of SINet decreases by 2.9% with respect to MobileNet V3, but the number of parameters and FLOPs are much lower than MobileNet V3. <ref type="table">Table 7</ref> is a detailed setting of the encoder model for the Cityscape segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed an extremely lightweight portrait segmentation model, SINet, which consists of an in- <ref type="table">Table 7</ref>. Detailed settings for the SINet encoder. k denotes the kernel size of the depthwise convolution and p denotes the kernel size of average pooling the S2-block.</p><p>formation blocking decoder and spatial squeeze modules. SINet executes well in mobile device with 100.6FPS and has high accuracy with 95.29. The information blocking decoder prevents nuisance information from high-resolution features and induce the model to concentrate more on high uncertainty regions. The spatial squeeze module has multireceptive field to handle the various sizes of global consistency in an image. We also proposed a simple data generation framework covering the two situations: 1) having human segmentation ground truths 2) having only raw images. Not only on the specific portrait dataset but also on the general segmentation dataset, our model obtained outstanding performance compared to the existing lightweight segmentation models from the experiments. The proposed method shows appropriate accuracy (66.5 %) with only 0.12M number of parameter and 1.2G FLOP on the Cityscapes dataset.  The goal of this experiment is to show exact performances on different types of layers or models that run on iOS. We are running on all the experiments with iPhone XS Max and iOS 13.1.2. We did 100 iterations and waited 2 seconds between each run, and all performance values are reported in milliseconds. Each model in <ref type="table">Table 8</ref> are tested on CPU/GPU/NPU configuration and each model in <ref type="table">Table  9</ref> are executed in different configuration (CPU, CPU/GPU, CPU/GPU/NPU ). As shown in <ref type="table">Table 8</ref>, a large dilated rate convolution layer is slower than others regardless of input size. <ref type="table">Table 9</ref> proves that our SINet is faster than any other model in every configuration. This result demonstrates that our model can be applied to any configurations well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(b) is classified as foreground, even though it is easily recognized as a wood region. The rea-arXiv:1911.09099v4 [cs.CV] 9 Feb 2020 (a) Input image (b) Typical segmentation errors (c) Ground truth (d) Example of Ours Typical examples of segmentation errors. (b) The wood region is failed to be suppressed and the boundary of the boy's hair is not sharply segmented. (d) Our method solves the problem without heavy computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The overall architecture of SINet. The S2-module is the bottleneck in SINet and the information blocking decoder makes fine segmentation results. DSConv+SE means depthwise separable convolution with a Squeeze-and-Excitation block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>From left to right,(a) an input image, (b) an information blocking map, (c) a confidence map of last block in model and (d) the segmentation results from the proposed method. The information blocking map helps to prevent inflow of nuisance features, and makes the model focus more on regions with high uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>(a) An input feature map is squeezed by an N √ó N average pooling before a depthwise convolution. Then, a bilinear upsampling recovers the original input resolution of the feature maps. (b) The S2-module has multi-receptive structures by using different combinations of convolution kernels and pooling for the S2-blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Confidence maps of the last features from the model and segmentation results according to the decoding method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Latency running depthwise separable dilated convolution with different dilation rates on an iPhone XS. The input size is 128 √ó 120 √ó 120. Additional experiments with other input sizes are reported in the supplementary material.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Semantic segmentation results on the Cityscapes test set. FLOP(f) means that the number of FLOPs was measured with fullresolution input, 2048 √ó 1024. FLOP(h) denotes that the number of FLOPs was measured with half-resolution, 1024 √ó 512.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Latency time experiments about depthwise separable dilated convolution on iPhone CPU/GPU/NPU environment. All performance value are reported in milliseconds. d denotes a dilated rate of convolution filter. Latency time experiments under different environments. We converted ESPNet V2 Pascal VOC segmentation model by CoreML framework from their official converting code for baseline of latency time in a mobile environment. The input size of SINet and MobileNet V3 is 224 √ó 224 and the input size of ESP-Net V2 is 256 √ó 256. The number of FLOPs of SINet, MobileNet V3 and ESPNet V2 are 0.064G, 0.066 and 0.338G. Our SINet is faster than MobileNet V3 and ESPNet V2 in all the environments. All performance value are reported in milliseconds</figDesc><table><row><cell>CPU</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boundary-sensitive network for portrait segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Upright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<title level="m">Searching for mobilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03888</idno>
		<title level="m">Hbonet: Harmonious bottleneck on two orthogonal dimensions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11431</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Portrait segmentation by deep refinement of image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Estopi√±√°n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beltr√°n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1495" to="1499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">C3: Concentrated-comprehensive convolution and its application to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04920</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Contextnet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04554</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast-scnn: fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C L S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic portrait segmentation for image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Elastic: Improving cnns with dynamic scaling policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2258" to="2267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Early hierarchical contexts learned by convolutional networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1538" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Extd: Extremely tiny face detector via iterative filter reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Portraitnet: Real-time portrait segmentation network for mobile device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="104" to="113" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07678</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
