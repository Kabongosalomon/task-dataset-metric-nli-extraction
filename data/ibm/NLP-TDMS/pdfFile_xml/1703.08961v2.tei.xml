<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling the Scattering Transform: Deep Hybrid Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-04-04">4 Apr 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
							<email>edouard.oyallon@ens.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Département Informatique</orgName>
								<orgName type="institution">Ecole Normale Supérieure Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
							<email>eugene.belilovsky@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">-Saclay INRIA and KU</orgName>
								<orgName type="institution">University of Paris</orgName>
								<address>
									<settlement>Leuven</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
							<email>sergey.zagoruyko@enpc.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">Université</orgName>
								<address>
									<addrLine>Paris-Est Ecole des Ponts ParisTech</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling the Scattering Transform: Deep Hybrid Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-04-04">4 Apr 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 × 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their endto-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification is a high dimensional problem that requires building lower dimensional representations that reduce the non-informative images variabilities. For example, some of the main source of variability are often due to geometrical operations such as translations and rotations. An efficient classification pipeline necessarily builds invariants to these variabilities. Deep architectures build representations that lead to state-of-the-art results on image classification tasks <ref type="bibr" target="#b12">[13]</ref>. These architectures are designed as very deep cascades of non-linear end-to-end learned modules <ref type="bibr" target="#b21">[22]</ref>. When trained on large-scale datasets they have been shown to produce representations that are transferable to other datasets <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15]</ref>, which indicate they have captured generic properties of a supervised task that consequently do not need to be learned. Indeed several works indicate geometrical structures in the filters of the earlier layers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> of Deep CNNs. However, understanding the precise operations performed by those early layers is a complicated <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26]</ref> and possibly intractable task. In this work we investigate if it is possible to replace these early layers, by simpler cascades of non-learned operators that reduce variability while retaining discriminative information.</p><p>Indeed, there can be several advantages to incorporating pre-defined geometric priors, via a hybrid approach of combining pre-defined and learned representations. First, endto-end pipelines can be data hungry and ineffective when the number of samples is low. Secondly, it could permit to obtain more interpertable classification pipelines which are amenable to analysis. Finally, it can reduce the spatial dimensions and the required depth of the learned modules.</p><p>A potential candidate for an image representation is the SIFT descriptor <ref type="bibr" target="#b22">[23]</ref> that was widely used before 2012 as a feature extractor in classification pipelines <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. This representation was typically encoded via an unsupervised Fisher Vector (FV) and fed to a linear SVM. However, several works indicate that this is not a generic enough representation to build further modules on top of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref>. Indeed end-to-end learned features produce substantially better classification accuracy. A major improvement over SIFT can be found in the scattering transform <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref>, which is a type of deep convolutional network, which permits to retain discriminative information normally discarded by methods like SIFT while introducing geometric invariances and stability. Scattering transforms have been shown to already produce representations that lead to the top results on complex image datasets when compared to other unsupervised representations (even learned ones) <ref type="bibr" target="#b26">[27]</ref>. This makes them an excellent candidate for the initial layers of a deep network. We thus investigate the use of scattering as a generic representation to combine with deep neural networks.</p><p>Related to our work <ref type="bibr" target="#b27">[28]</ref> proposed a hybrid representation for large scale image recognition combining a prede-x |W 1 | |W 2 | A J Sx <ref type="figure">Figure 1</ref>. A scattering network. AJ concatenates the averaged signals.</p><p>fined representation and Neural Networks (NN), that uses Fisher Vector encoding of SIFT and leverages NNs as scalable classifiers. In contrast we use the scattering transform in combination with convolutional architectures. Our main contributions are as follows: First, we demonstrate that using supervised local descriptors, obtained by shallow 1 × 1 convolutions, with very small spatial window sizes permits to obtain AlexNet accuracy on the imagenet classification task (Subsection 2.3). We show empirically these encoders build explicit invariance to local rotations (Subsection 3.2). Second, we propose hybrid networks that combine scattering with modern CNNs (Section 4) and show that using scattering and a ResNet of reduced depth, we obtain similar accuracy to ResNet-18 on Imagenet (Subsection 4.1). Finally, we demonstrate in Subsection 4.3 that scattering permits a substantial improvement in accuracy in the setting of limited data.</p><p>Our highly efficient GPU implementation of the scattering transform is, to our knowledge, orders of magnitude faster than any other implementations, and allows training very deep networks applying scattering on the fly. Our scattering implementation 1 and pre-trained hybrid models 2 are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Scattering Networks and Hybrid Architectures</head><p>We introduce the scattering transform and motivate its use as a generic input for supervised tasks. A scattering network belongs to the class of CNNs whose filters are fixed as wavelets <ref type="bibr" target="#b26">[27]</ref>. The construction of this network has strong mathematical foundations <ref type="bibr" target="#b23">[24]</ref>, meaning it is well understood, relies on few parameters and is stable to a large class of geometric transformations. In general, the parameters of this representation do not need to be adapted to the bias of the dataset <ref type="bibr" target="#b26">[27]</ref>, making its output a suitable generic representation.</p><p>We then propose and motivate the use of supervised CNNs built on top of the scattering network. Finally we propose a supervised encodings of scattering coefficients using 1x1 convolutions, that can retain interpertability and locality properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Scattering Networks</head><p>In this section, we recall the definition of the scattering transform. Consider a signal x(u), with u the spatial position index and an integer J ∈ N, which is the spatial scale of our scattering transform. Let φ J be a local averaging filter with a spatial window of scale 2 J (here, a Gaussian smoothing function). Applying the local averaging operator, A J x(u) = x ⋆ φ J (2 J u) we obtain the zeroth order scattering coefficient, S 0 x(u) = A J x(u). This operation builds an approximate invariant to translations smaller than 2 J , but it also results in a loss of high frequencies that are necessary to discriminate signals.</p><p>A solution to avoid the loss of high frequency information is provided by the use of wavelets. A wavelet is an integrable and localized function in the Fourier and space domain, with zero mean. A family of wavelets is obtained by dilating a complex mother wavelet ψ (here, a Morlet wavelet) such that ψ j,θ (u) = 1 2 2j ψ(r −θ u 2 j ), where r −θ is the rotation by −θ, and j ≥ 0 is the scale of the wavelet. A given wavelet ψ j,θ has thus its energy concentrated at a scale j, in the angular sector θ. Let L ∈ N be an integer parametrizing a discretization of [0, 2π]. A wavelet transform is the convolution of a signal with the family of wavelets introduced above, with an appropriate downsampling:</p><formula xml:id="formula_0">W 1 x(j 1 , θ 1 , u) = {x ⋆ ψ j1,θ1 (2 j1 u)} j1≤J,θ1=2π l L ,1≤l≤L</formula><p>Observe that j 1 and θ 1 have been discretized: the wavelet is chosen to be selective in angle and localized in Fourier. With appropriate discretization <ref type="bibr" target="#b26">[27]</ref>, {A J x, W 1 x} is approximatively an isometry on the set of signals with limited bandwidth, and this implies the energy of the signal is preserved. This operator then belongs to the category of multiresolution analysis operators, each filter being excited by a specific scale and angle, but with the output coefficients not being invariant to translation. To achieve invariance we can not apply A J to W 1 x since it gives a trivial invariant, namely zero.</p><p>To tackle this issue, we apply a non-linear point-wise complex modulus to W 1 x, followed by an averaging A J , which builds a non trivial invariant. Here, the mother wavelet is analytic, thus |W 1 x| is regular <ref type="bibr" target="#b0">[1]</ref> which implies that the energy in Fourier of |W 1 x| is more likely to be contained in a lower frequency domain than W 1 x. Thus, A J preserves more energy of |W 1 x|. It is possible to define S 1 x = A J |W 1 |x, which can also be written as:</p><formula xml:id="formula_1">S 1 x(j 1 , θ 1 , u) = |x ⋆ ψ j1,θ1 | ⋆ φ J (2 J u)</formula><p>; this is the first order scattering coefficients. Again, the use of the averaging builds an invariant to translation up to 2 J . Once more, we apply a second wavelet transform W 2 , with the same filters as W 1 , on each channel. This permits the recovery of the high-frequency lost due to the averaging applied to the first order, leading to S 2 x = A J |W 2 ||W 1 |, which can also be written as S 2 x(j 1 , j 2 , θ 1 , θ 2 , u) = |x ⋆ ψ j1,θ1 | ⋆ ψ j2,θ2 | ⋆ φ J (2 J u). We only compute increasing paths, e.g. j 1 &lt; j 2 because non-increasing paths have been shown to bear no energy <ref type="bibr" target="#b5">[6]</ref>. We do not compute higher order scatterings, because their energy is negligible <ref type="bibr" target="#b5">[6]</ref>. We call Sx(u) the final scattering coefficient corresponding to the concatenation of the order 0, 1 and 2 scattering coefficients, intentionally omitting the path index of each representation. In the case of colored images, we apply independently a scattering transform to each RGB channel of the image, which means Sx(u) has a size equal to 3 × 1 + JL + 1 2 J(J − 1)L 2 , and the original image is down-sampled by a factor 2 J <ref type="bibr" target="#b5">[6]</ref>.</p><p>This representation is proved to linearize small deformations <ref type="bibr" target="#b23">[24]</ref> of images, be non-expansive and almost complete <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref>, which makes it an ideal input to a deep network algorithm, that can build invariants to this local variability via a first linear operator. We discuss it as an ideal initialization in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cascading a supervised Deep architecture</head><p>We now motivate the use of a supervised architecture on top of a scattering network. Scattering transforms have yielded excellent numerical results <ref type="bibr" target="#b5">[6]</ref> on datasets where the variabilities are completely known, such as MNIST or FERET. In these task, the problems encountered are linked to sample and geometric variance and handling these variances leads to solving these problems. However, in classification tasks on more complex image datasets, such variabilities are only partially known as there are also non geometrical intra-class variabilities. Although applying the scattering transform on datasets like CIFAR or Caltech leads to nearly state-of-the-art results in comparison to other unsupervised representations there is a large gap in performance when comparing to supervised representations <ref type="bibr" target="#b26">[27]</ref>. CNNs fill in this gap, thus we consider the use of deep neural networks utilizing generic scattering representations in order to reduce more complex variabilities than geometric ones.</p><p>Recent works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref> have suggested that deep networks could build an approximation of the group of symmetries of a classification task and apply transformations along the orbits of this group, like convolutions. This group of symmetry corresponds to some of the non-informative intra class variabilities, which must be reduced by a supervised classifier. <ref type="bibr" target="#b24">[25]</ref> motivates that to each layer corresponds an approximated Lie group of symmetry, and this approximation is progressive, in the sense that the dimension of these groups is increasing with depth. For instance, the main linear Lie group of symmetry of an image is the translation group, R 2 . In the case of a wavelet transform obtained by rotation of a mother wavelet, it is possible to recover a new subgroup of symmetry after a modulus non-linearity, the rotation SO 2 , and the group of symmetry at this layer is the roto-translation group: R 2 ⋉ SO 2 . If no non-linearity was applied, a convolution along R 2 ⋉ SO 2 would be equivalent to a spatial convolution. Discovering explicitly the next new and non-geometrical groups of symmetry is however a difficult task <ref type="bibr" target="#b16">[17]</ref>; nonetheless, the roto-translation group seems to be a good initialization for the first layers. In this work, we investigate this hypothesis and avoid learning those well-known symmetries.</p><p>Thus, we consider two types of cascaded deep network on top of scattering. The first, referred to as the Shared Local Encoder (SLE), learns a supervised local encoding of the scattering coefficients. We motivate and describe the SLE in the next subsection as an intermediate representation between unsupervised local pipelines, widely used in computer vision prior to 2012, and modern supervised deep feature learning approaches. The second, referred to as a hybrid CNN, is a cascade of a scattering network and a standard CNN architecture, such as a ResNet <ref type="bibr" target="#b12">[13]</ref>. In the sequel we empirically analyse hybrid CNNs, which permits to greatly reduce the spatial dimensions on which convolutions are learned and can reduce sample complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Shared Local Encoder for Scattering Representations</head><p>We now discuss the spatial support of different approaches, in order to motivate our local encoder for scattering. In CNNs constructed for large scale image recognition, the representations at a specific spatial location and depth depend upon large parts of the initial input image and thus mixes global information. For example, at depth 2 of <ref type="bibr" target="#b18">[19]</ref>, the effective spatial support of the corresponding filter is already 32 pixels (out of 224). The specific representations derived from CNNs trained on large scale image recognition are often used as representations in other computer vision tasks or datasets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>On the other hand prior to 2012 local encoding methods led to state of the art performance on large scale visual recognition tasks <ref type="bibr" target="#b29">[30]</ref>. In these approaches local neighborhoods of an image were encoded using method such as SIFT descriptors <ref type="bibr" target="#b22">[23]</ref>, HOG <ref type="bibr" target="#b8">[9]</ref>, and wavelet transforms <ref type="bibr" target="#b31">[32]</ref>. They were also often combined with an unsupervised encoding, such as sparse coding <ref type="bibr" target="#b3">[4]</ref> or Fisher Vectors(FVs) <ref type="bibr" target="#b29">[30]</ref>. Indeed, many works in classical image processing or classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref> suggests that the local encoding of an image permit to describe efficiently an image. Additionally for some algorithms that rely on local neighbourhoods, the use of local descriptors is essential <ref type="bibr" target="#b22">[23]</ref>. Observe that a representation based on local non overlapping spatial neighborhood is simpler to analyze, as there is no ad-hoc mixing of spatial information. Nevertheless, on large scale classification, this approach was surpassed by fully supervised learned methods <ref type="bibr" target="#b18">[19]</ref>.</p><p>We show that it is possible to apply, a similarly local, yet supervised encoding algorithm to a scattering transform, as suggested in the conclusion of <ref type="bibr" target="#b27">[28]</ref>. First observe that at each spatial position u, a scattering coefficient S(u) corresponds to a descriptor of a local neighborhood of spatial size 2 J . As explained in the first Subsection 2.1, each of our scattering coefficients are obtained using a stride of 2 J , which means the final representation can be interpreted as a non-overlapping concatenation of descriptors. Then, let f be a cascade of fully connected layers that we identically apply on each Sx(u). Then f is a cascade of CNN operators with spatial support size 1 × 1, thus we write f Sx {f (Sx(u))} u . In the sequel, we do not make any distinction between the 1 × 1 CNN operators and the operator acting on Sx(u), ∀u. We refer to f as a Shared Local Encoder. We note that similarly to Sx, f Sx corresponds to non-overlapping encoded descriptors. To learn a supervised classifier on a large scale image recognition task, we cascade fully connected layers on top of the SLE.</p><p>Combined with a scattering network, the supervised SLE, has several advantages. Since the input corresponds to scattering coefficients, whose channels are structured, the first layer of f is as well structured. We further explain and investigate this first layer in Subsection 3.2. Unlike standard CNNs, there is no linear combinations of spatial neighborhoods of the different feature maps, thus the analysis of this network need only focus on the channel axis. Observe that if f was fed with raw images, for example in gray scale, it could not build any non-trivial operation except separating different level sets of these images.</p><p>In the next section, we investigate empirically this supervised SLE trained on the ILSVRC2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Local Encoding of Scattering</head><p>We evaluate the supervised SLE on the Imagenet ILSVRC2012 dataset. This is a large and challenging natural color image dataset consisting of 1.2 million training images and 50, 000 validation images, divided into 1000 classes. We then show some unique properties of this network and evaluate its features on a separate task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Shared Local Encoder on Imagenet</head><p>We first describe our training pipeline, which is similar to <ref type="bibr" target="#b40">[41]</ref>. We trained our network for 90 epochs to minimize the standard cross entropy loss, using SGD with momentum 0.9 and a batch size of 256. We used a weight decay of 1 × 10 −4 . The initial learning rate is 0.1, and is dropped off by 0.1 at epochs 30, 50, 70, 80. During the training process, each image is randomly rescaled, cropped, and flipped as in <ref type="bibr" target="#b12">[13]</ref>. The final crop size is 224 × 224. At testing, we rescale <ref type="figure">Figure 2</ref>. Architecture of the SLE, which is a cascade of 3 1 × 1 convolutions followed by 3 fully connected layers. The ReLU non-linearity are included inside the Fi blocks for clarity.</p><formula xml:id="formula_2">. . . . . . . . . . . . Sx(u − 2 J ) Sx(u) Sx(u + 2 J ) F 4 F 5 F 6 F 1 F 2 F 3 F 1 F 2 F 3 F 1 F 2 F 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top 1 Top 5 FV + FC <ref type="bibr" target="#b27">[28]</ref> 55.6 78.4 FV + SVM <ref type="bibr" target="#b29">[30]</ref> 54 the image to a size of 256, and extract a center crop of size 224 × 224.</p><p>We use an architecture which consists of a cascade of a scattering network, a SLE f , followed by fully connected layers. <ref type="figure">Figure 2</ref> describes our architecture. We select the parameter J = 4 for our scattering network, which means the output representation has size 224 2 4 × 224 2 4 = 14 × 14 spatially and 1251 in the channel dimension. f is implemented as 3 layers of 1x1 convolutions F 1 , F 2 , F 3 with layer size 1024. There are 2 fully connected layers of ouput size 1524. For all learned layers we use batch normalization <ref type="bibr" target="#b15">[16]</ref> followed by a ReLU <ref type="bibr" target="#b18">[19]</ref> non-linearity. We compute the mean and variance of the scattering coefficients on the whole Imagenet, and standardized each spatial scattering coefficients with it. <ref type="table">Table 3</ref>.1 reports our numerical accuracies obtained with a single crop at testing, compared with local encoding methods, and the AlexNet that was the state-of-the-art approach in 2012. We obtain 20.4% at Top 5 and 43.0% Top 1 errors. The performance is analogous to the AlexNet <ref type="bibr" target="#b18">[19]</ref>. In term of architecture, our hybrid model is analogous, and comparable to that of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28]</ref>, for which SIFT features are extracted followed by FV <ref type="bibr" target="#b30">[31]</ref> encoding. Observe the FV is an unsupervised encoding compared to our supervised encoding. Two approaches are then used: either the spatial localization is handled either by a Spatial Pyramid Pooling <ref type="bibr" target="#b19">[20]</ref>, which is then fed to a linear SVM, either the spatial variables are directly encoded in the FVs, and classified with a stack of four fully connected layers. This last method is a major difference with ours, as the obtained de-scriptor does not have a spatial indexing anymore which are instead quantified. Furthermore, in both case, the SIFT are densely extracted which correspond to approximatively 2 10 4 descriptors, whereas in our case, only 14 2 = 196 scattering coefficients are extracted. Indeed, we tackle the nonlinear aliasing (due to the fact the scattering transform is not oversampled) via random cropping during training, allowing to build an invariant to small translations. In Top 1, <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b27">[28]</ref> obtain respectively 44.4% and 45.7%. Our method brings a substantial improvement of 1.4% and 2.7% respectively.</p><p>The BVLC AlexNet 3 obtains a of 43.1% single-crop Top 1 error, which is nearly equivalent to the 43.0% of our SLE network. The AlexNet has 8 learned layers and as explained before, large receptive fields. On the contrary, our training pipeline consists in 6 learned layers with constant receptive field of size 16 × 16, except for the fully connected layers that build a representation mixing spatial information from different locations. This is a surprising result, as it seems to suggest context information is only necessary at the very last layers, to reach AlexNet accuracy.</p><p>We study briefly the local SLE, which has only a spatial extent of 16 × 16, as a generic local image descriptor. We use the Caltech-101 benchmark which is a dataset of 9144 image and 102 classes. We followed the standard protocol for evaluation <ref type="bibr" target="#b3">[4]</ref> with 10 folds and evaluate per class accuracy, with 30 training samples per class, using a linear SVM used with the SLE descriptors. Applying our raw scattering network leads to an accuracy of 62.8 ± 0.7, and the outputs features from F 1 , F 2 , F 3 brings respectively an absolute improvement of 13.7, 17.3, 20.1. The accuracy of the final SLE descriptor is thus 82.9 ± 0.4, similar to that reported for the final AlexNet final layer in <ref type="bibr" target="#b41">[42]</ref> and sparse coding with SIFT <ref type="bibr" target="#b3">[4]</ref>. However in both cases spatial variability is removed, either by Spatial Pyramid Pooling <ref type="bibr" target="#b19">[20]</ref>, or the cascade of large filters. By contrasts the concatenation of SLE descriptors are completely local.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interprating SLE's first layer</head><p>Finding structure in the kernel of the layers of depth less than 2 [39, 42] is a complex task, and few empirical analyses exist that shed light on the structure <ref type="bibr" target="#b16">[17]</ref> of deeper layers. A scattering transform with scale J can be interpreted as a CNN with depth J <ref type="bibr" target="#b26">[27]</ref>, whose channels indexes correspond to different scattering frequency indexes, which is a structuration. This structure is consequently inherited by the first layer F 1 of our SLE f . We analyse F 1 and show that it builds explicitly invariance to local rotations, yet also that the Fourier bases associated to rotation are a natural bases of our operator. It is a promising direction to understand the nature of the two next layers. We first establish some mathematical notions linked to the rotation group that we use in our analysis. For the sake of clarity, we do not consider the roto-translation group. For a given input image x, let r θ .x(u) x(r −θ (u)) be the image rotated by angle θ, which corresponds to the linear action of rotation on images. Observe the scattering representation is covariant with the rotation in the following sense:</p><formula xml:id="formula_3">S 1 (r θ .x)(θ 1 , u) = S 1 x(θ 1 − θ, r −θ u) r θ .(S 1 x)(θ 1 , u) S 2 (r θ .x)(θ 1 , θ 2 , u) = S 2 x(θ 1 − θ, θ 2 − θ, r −θ u) r θ .(S 2 x)(θ 1 , θ 2 , u)</formula><p>Besides, in the case of the second order coefficients, (θ 1 , θ 2 ) is covariant with rotations, but θ 2 − θ 1 is an invariant to rotation that correspond to a relative rotation.</p><p>Unitary representation framework <ref type="bibr" target="#b35">[36]</ref> permits the building of a Fourier transform on compact group, like rotations. It is even possible to build a scattering transform on the rototranslation group <ref type="bibr" target="#b32">[33]</ref>. Fourier analysis permits the measurement of the smoothness of the operator and, in the case of CNN operator, it is a natural basis.</p><p>We can now numerically analyse the nature of the operations performed along angle variables by the first layer F 1 of f , with output size K = 1024. Let us define as {F 0 1 S 0 x, F 1 1 S 1 x, F 2 1 S 2 x} the restrictions of F 1 to the order 0,1,2 scattering coefficients respectively. Let 1 ≤ k ≤ K an index of a feature channel and 1 ≤ c ≤ 3 the color index. In this case, F 0 1 S 0 x is simply the weights associated to the smoothing S 0 x. F 1 1 S 1 x depends only (k, c, j 1 , θ 1 ), and F 2 1 depends on (k, c, j 1 , j 2 , θ 1 , θ 2 ). We would like to characterize the smoothness of these operators with respect to the variables (θ 1 , θ 2 ), because Sx is covariant to rotations. To this end, we define byF 1 1 ,F 2 1 the Fourier transform of these operators along the variables θ 1 and (θ 1 , θ 2 ) respectively. These operator are expressed in the tensorial Frequency domain, which corresponds to a change of basis. In this experiment, we normalized each filter of F such that they have a ℓ 2 norm equal to 1, and each order of the scattering coefficients are normalized as well. <ref type="figure">Figure 3</ref> shows the distribution of the amplitude ofF 1 1 ,F 2 2 . We observe that the distribution is shaped as a Laplace distribution, which is an indicator of sparsity.</p><p>To illustrate that this is a natural basis we explicitly sparsify this operator in its frequency basis and verify that empirically the network accuracy is minimally changed. We do this by thresholding by ǫ the coefficients of the operators in the Fourier domain. Specifically we replace the opera-torsF <ref type="bibr">1 1</ref>  . We select an ǫ that sets 80% of the coefficients to 0, which is indicated on <ref type="figure">Figure 3</ref>. Without retraining our network performance degrades by only an absolute value of 2% worse on Top 1 and Top 5 ILSVRC2012. We have thus shown that this basis permits a sparse approximation of the first layer, F 1 . We now show evidence that this operator builds an explicit invariant to local rotations.</p><p>To aid our analysis we introduce the following quantities:</p><formula xml:id="formula_4">Ω 1 {F }(ω 1 ) k,j1,c |F 1 1 (k, c, j 1 , ω θ1 )| 2 (1) Ω 2 {F }(ω θ1 , ω θ2 ) k,c,j1,j2 |F 2 1 (k, c, j 1 , j 2 , ω θ1 , ω θ2 )| 2</formula><p>They correspond to the energy propagated by F 1 for a given frequency, and permit to quantify the smoothness of our first layer operator w.r.t. the angular variables. <ref type="figure">Figure 4</ref> shows variation of Ω 1 {F } and Ω 2 {F } along frequencies.</p><p>For example, if F 1 1 and F 2 1 were convolutional along θ 1 and (θ 1 , θ 2 ), these quantities would correspond to their respective singular values. One sees that the energy is concentrated in the low frequency domain, which indicates that F 1 builds explicitly an invariant to local rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Numerical performances of hybrid networks</head><p>We now demonstrate cascading modern CNN architectures on top of the scattering network can produce high performance classification systems. We apply hybrid convolutional networks on the Imagenet ILSVRC 2012 dataset as well as the CIFAR-10 dataset and show that they can achieve performance comparable to modern end-to-end learned approaches. We then evaluate the hybrid networks in the setting of limited data by utilizing a subset of CIFAR-10 as well as the STL-10 dataset and show that we can  <ref type="bibr" target="#b40">[41]</ref> 78.3 94.2 64.7M <ref type="table">Table 2</ref>. ILSVRC-2012 validation accuracy (single crop) of hybrid scattering and 10 layer resnet, a comparable 18 layer resnet, and other well known benchmarks. We obtain comparable performance using analogous amount of parameters while learning parameters at a spatial resolution of 28 × 28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy Unsupervised Representations</head><p>Roto-Scat + SVM <ref type="bibr" target="#b26">[27]</ref> 82.3 ExemplarCNN <ref type="bibr" target="#b10">[11]</ref> 84.3 DCGAN <ref type="bibr" target="#b28">[29]</ref> 82.8 Scat + FC (ours) 84.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised and Hybrid</head><p>Scat + Resnet (ours) 93.1 Highway network <ref type="bibr" target="#b34">[35]</ref> 92.4 All-CNN <ref type="bibr" target="#b33">[34]</ref> 92.8  95.7 WRN 28 -10 <ref type="bibr" target="#b40">[41]</ref> 96.0 <ref type="table">Table 3</ref>. Accuracy of scattering compared to similar architectures on CIFAR10. We set a new state-of-the-art in the unsupervised case and obtain competitive performance with hybrid CNNs in the supervised case.</p><p>obtain substantial improvement in performance over analogous end-to-end learned CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep Hybrid CNNs on ILSVRC2012</head><p>We showed in the previous section that a SLE followed by FC layers can produce results comparable with the AlexNet [19] on the Imagenet classification task. Here we consider cascading the scattering transform with a modern CNN architecture, such as Resnet <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b12">13]</ref>. We take the Resnet-18 <ref type="bibr" target="#b40">[41]</ref>, as a reference and construct a similar architecture with only 10 layers on top of the scattering network. We utilize a scattering transform with J = 3 such that the CNN is learned over a spatial dimension of 28 × 28 and a channel dimension of 651 (3 color channels of 217 each). The ResNet-18 typically has 4 residual stages of 2 blocks each which gradually decrease the spatial resolution <ref type="bibr" target="#b40">[41]</ref>. Since we utilize the scattering as a first stage we remove two blocks from our model. The network is described in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>We use the same optimization and data augmentation procedure described in Section 3.1 but with learning rate drops at 30, 60, and 80. We find that, when both methods are trained with the same settings of optimization and data   For sizes and stage details if settings vary we list CIFAR-10 and then the STL-10 network information. All convolutions are of size 3 × 3 and the channel width is shown in brackets for both the network applied to STL-10 and CIFAR-10. For CIFAR-10 we use n = 2 and for the larger STL-10 we use n = 4.</p><p>augmentation, and when the number of parameters is similar (12.8M versus 11.7 M) the scattering network combined with a resnet can achieve analogous performance (11.4% Top 5 for our model versus 11.1 %), while utilizing fewer layers. The accuracy is reported in <ref type="table">Table 2</ref> and compared to other modern CNNs. This demonstrates both that the scattering networks does not lose discriminative power and that it can be used to replace early layers of standard CNNs. We also note that learned convolutions occur over a drastically reduced spatial resolution without resorting to pre-trained early layers which can potentially lose discriminative information or become too task specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hybrid Supervised and Unsupervised Representations on CIFAR-10</head><p>We now consider the popular CIFAR-10 dataset consisting of colored images composed of 5×10 4 images for training, and 1 × 10 4 images for testing divided into 10 classes. We perform two experiments, the first with a cascade of fully connected layers, that allows us to evaluate the scattering transform as an unsupervised representation. In a sec-ond experiment, we again use a hybrid CNN architecture with a ResNet built on top of the scattering transform.</p><p>For the scattering transform we used J = 2 which means the output of the scattering stage will be 8 × 8 spatially and 243 in the channel dimension. We follow the training procedure prescribed in <ref type="bibr" target="#b40">[41]</ref> utilizing SGD with momentum of 0.9, batch size of 128, weigh decay of 5 × 10 −4 , and modest data augmentation of the dataset by using random cropping and flipping. The initial learning rate is 0.1, and we reduce it by a factor of 5 at epochs 60, 120 and 160. The models are trained for 200 epochs in total. We used the same optimization and data augmentation pipeline for training and evaluation in both case. We utilize batch normalization techniques at all layers which lead to a better conditioning of the optimization <ref type="bibr" target="#b15">[16]</ref>. <ref type="table" target="#tab_4">Table 4</ref>.1 reports the accuracy in the unsupervised and supervised settings and compares them to other approaches.</p><p>In the unsupervised comparison we consider the task of classification using only unsupervised features. Combining the scattering transform with a NN classifier consisting of 3 hidden layers, with width 1.1 × 10 4 , we show that one can obtain a new state of the art classification for the case of unsupervised features. This approach outperforms all methods utilizing learned and not learned unsupervised features further demonstrating the discriminative power of the scattering network representation.</p><p>In the case of the supervised task we compare to stateof-the-art approaches on CIFAR-10, all based on end-to-end learned CNNs. We use a similar hybrid architecture to the successful wide residual network (WRN) <ref type="bibr" target="#b40">[41]</ref>. Specifically we modify the WRN of 16 layers which consists of 4 convolutional stages. Denoting the widening factor, k, after the scattering output we use a first stage of 32 × k. We add intermediate 1 × 1 to increase the effective depth, without increasing too much the number of parameters. Finally we apply a dropout of 0.2 as specified in <ref type="bibr" target="#b40">[41]</ref>. Using a width of 32 we achieve an accuracy of 93.1%. This is superior to several benchmarks but performs worse than the original ResNet <ref type="bibr" target="#b12">[13]</ref> and the wide resnet <ref type="bibr" target="#b40">[41]</ref>. We note that training procedures for learning directly from images, including data augmentation and optimization settings, have been heavily optimized for networks trained directly on natural images, while we use them largely out of the box we do believe there are regularization techniques, normalization techniques, and data augmentation techniques which can be designed specifically for the scattering networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Limited samples setting</head><p>A major application of a hybrid representation is in the setting of limited data. Here the learning algorithm is limited in the variations it can observe or learn from the data, such that introducing a geometric prior can substantially improve performance. We evaluate our algorithm on the  <ref type="table">Table 6</ref>. Mean accuracy of a hybrid scattering in a limited sample situation on CIFAR-10 dataset. We find that including a scattering network is significantly better in the smaller sample regime of 500 and 100 samples. limited sample setting using a subset of CIFAR-10 and the STL-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">CIFAR-10</head><p>We take subsets of decreasing size of the CIFAR dataset and train both baseline CNNs and counterparts that utilize the scattering as a first stage. We perform experiments using subsets of 1000, 500, and 100 samples, that are split uniformly amongst the 10 classes.</p><p>We use as a baseline the Wide ResNet [41] of depth 16 and width 8, which shows near state-of-the-art performance on the full CIFAR-10 task in the supervised setting. This network consists of 4 stages of progressively decreasing spatial resolution detailed in <ref type="table">Table 1</ref> of <ref type="bibr" target="#b40">[41]</ref>. We construct a comparable hybrid architecture that removes a single stage and all strides, as the scattering already down-sampled the spatial resolution. This architecture is described in <ref type="table" target="#tab_5">Table 5</ref>. Unlike the baseline, refereed from here-on as WRN 16-8, our architecture has 12 layers and equivalent width, while keeping the spatial resolution constant through all stages prior to the final average pooling.</p><p>We use the same training settings for our baseline, WRN 16-8, and our hybrid scattering and WRN-12. The settings are the same as those described for CIFAR-10 in the previous section with the only difference being that we apply a multiplier to the learning rate schedule and to the maximum number of epochs. The multiplier is set to 10,20,100 for the 1000,500, and 100 sample case respectively. For example the default schedule of 60,120,160 becomes 600,1200,1600 for the case of 1000 samples and a multiplier of 10. Finally in the case of 100 samples we use a batch size of 32 in lieu of 128. <ref type="table">Table 6</ref> corresponds to the averaged accuracy over 5 different subsets, with the corresponding standard error. In this small sample setting, a hybrid network outperforms the purely CNN based baseline, particularly when the sample size is smaller. This is not surprising as we incorporate a geometric prior in the representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">STL-10</head><p>The SLT-10 dataset consists of colored images of size 96 × 96, with only 5000 labeled images in the training set divided</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised methods</head><p>Scat + WRN 19-8 76.0 ± 0.6 CNN <ref type="bibr" target="#b36">[37]</ref> 70.1 ± 0.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised methods</head><p>Exemplar CNN <ref type="bibr" target="#b10">[11]</ref> 75.4 ± 0.3 Stacked what-where AE <ref type="bibr" target="#b42">[43]</ref> 74.33 Hierarchical Matching Pursuit (HMP) <ref type="bibr" target="#b2">[3]</ref> 64.5±1 Convolutional K-means Network <ref type="bibr" target="#b7">[8]</ref> 60.1±1 <ref type="table">Table 7</ref>. Mean accuracy of a hybrid CNN on the STL-10 dataset.</p><p>We find that our model is better in all cases even compared to those utilizing the large unsupervised part of the dataset. equally in 10 classes and 8000 images in the test set. The larger size of the images and the small number of available samples make this a challenging image classification task.</p><p>The dataset also provides 100 thousand unlabeled images for unsupervised learning. We do not utilize these images in our experiments, yet we find we are able to outperform all methods which learn unsupervised representations using these unlabeled images, obtaining very competitive results on the STL-10 dataset. We apply a hybrid convolutional architecture, similar to the one applied in the small sample CIFAR task, adapted to the size of 96 × 96. The architecture is described in Table 5 and is similar to that used in the CIFAR small sample task. We use the same data augmentation as with the CIFAR datasets. We apply SGD with learning rate 0.1 and learning rate decay of 0.2 applied at epochs 1500,2000,3000,4000. Training is run for 5000 epochs. We use at training and evaluation the standard 10 folds procedure which takes 1000 training images. The averaged result for 10 folds is reported in <ref type="table">Table 7</ref>. Unlike other approaches we do not use the 4000 remaining training image to perform hyper-parameter tuning on each fold, as this is not representative of typical small sample situations, instead we train the same settings on each fold. The best reported result in the purely supervised case is a CNN <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b10">11]</ref> whose hyper parameters have been automatically tuned using 4000 images for validation achieving 70.1% accuracy. The other competitive methods on this dataset utilize the unlabeled data to learn in an unsupervised manner before applying supervised methods. To compare with <ref type="bibr" target="#b13">[14]</ref> we also train on the full training set of 5000 images obtaining an accuracy of 87.6% on the test set, which is substantially higher than 81.3% reported in <ref type="bibr" target="#b13">[14]</ref> using unsupervised learning and the full unlabeled and labeled training set. The competing techniques add several hyper parameters and require an additional engineering process. Applying a hybrid network is on the other hand straightforward and is very competitive with all the existing approaches, without using any unsupervised learning.</p><p>In addition to showing hybrid networks perform well in the small sample regime these results, along with our unsupervised CIFAR-10 results, suggest that completely unsupervised feature learning on natural image data, for downstream discriminative tasks, may still not outperform supervised learning methods and pre-defined representations. One possible explanation is that in the case of natural images, learning in an unsupervised way more complex variabilities than geometric ones ( e.g the rototranslation group), might be very challenging or possibly ill-posed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work demonstrates a competitive approach for large scale visual recognition, based on scattering networks, in particular for ILSVRC2012. When compared with unsupervised representation on CIFAR-10 or small data regimes on CIFAR-10 and STL-10, we demonstrate state-of-the-art results. We build a supervised Shared Local Encoder that permits the scattering networks to surpass other local encoding methods on ILSVRC2012. This network of just 3 learned layers permits analysis on the operation performed.</p><p>Our work also suggests that pre-defined features are still of interest and can provide enlightenment on deep learning techniques and to allow them to be more interpretable. Combined with appropriate learning methods, they could permit having more theoretical guarantees that are necessary to engineer better deep models and stable representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>3 https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0Histogram ofF1 amplitude for first and second order coefficients. The vertical lines indicate a threshold that is used in Subsection 3.2 to sparsifyF1. Best viewed in color. Energy Ω1{F } (left) and Ω2{F } (right) from Eq. 1 for given angular frequencies. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Structure of Scattering and Resnet-10 used in imagenet experiments. Taking the convention of<ref type="bibr" target="#b40">[41]</ref> we describe the convolution size and channels in the Stage details</figDesc><table><row><cell>Stage</cell><cell>Output size</cell><cell cols="2">Stage details</cell></row><row><cell cols="2">scattering 8 × 8, 24 × 24</cell><cell>J = 2</cell><cell></cell></row><row><cell>conv1</cell><cell>8×8, 24×24</cell><cell cols="2">16×k , 32×k</cell></row><row><cell>conv2</cell><cell>8×8, 24×24</cell><cell>32×k 32×k</cell><cell>×n</cell></row><row><cell>conv3</cell><cell>8×8, 12×12</cell><cell>64×k 64×k</cell><cell>×n</cell></row><row><cell>avg-pool</cell><cell>1 × 1</cell><cell cols="2">[8 × 8], [12 × 12]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Structure of Scattering and Wide ResNet hybrid used in small sample experiments. Network width is determined by factor k.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>± 0.8 46.5 ±1.4 60.0 ±1.8 Scat + WRN 12-8 38.9 ± 1.2 54.7±0.6 62.0±1.1</figDesc><table><row><cell>Method</cell><cell>100</cell><cell>500</cell><cell>1000</cell></row><row><cell>WRN 16-8</cell><cell>34.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://github.com/edouardoyallon/pyscatwave 2 http://github.com/edouardoyallon/scalingscattering</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Mathieu Andreux, Matthew Blaschko, Carmine Cella, Bogdan Cirstea, Michael Eickenberg, Stéphane Mallat for helpful discussions and support. The authors would also like to thank Rafael Marini and Nikos Paragios for use of computing resources. We would like to thank Florent Perronnin for providing important details of their work. This work is funded by the ERC grant InvariantClass 320959, via a grant for PhD Students of the Conseil régional d'Ile-de-France (RDM-IdF), Internal Funds KU Leuven, FP7-MC-CIG 334380, an Amazon Academic Research Award, and DIGITEO 2013-0788D -SOPRANO.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized analytic signals in image processing: comparison, theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Bouchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quaternion and Clifford Fourier Transforms and Wavelets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="221" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="660" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for rgb-d based object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ask the locals: multi-way local pooling for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2651" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.0407</idno>
		<title level="m">Audio texture synthesis with scattering moments</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3537</idno>
		<title level="m">Learning stable group invariant representations with convolutional networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dokmanić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Hoop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05502</idno>
		<title level="m">Inverse problems with invariant multiscale statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning through spatial contrasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00243</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01775</idno>
		<title level="m">Smeulders. Multiscale hierarchical convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The structure of locally orderless images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCAS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the seventh IEEE international conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20150203</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Building a regular decision boundary with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01775</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep roto-translation scattering for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2865" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fisher vectors meet neural networks: A hybrid classification architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3743" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Realistic modeling of simple and complex cell tuning in the hmax model, and implications for invariant object recognition in cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unitary representations and harmonic analysis: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">These de doctorat de lEcole normale supérieure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Waldspurger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>École normale supérieure</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02351</idno>
		<title level="m">Stacked what-where auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
