<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Multi-Input Deep Learning Architecture for Biological Activity Prediction: An Application in EGFR Inhibitors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Huy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><forename type="middle">Hoang</forename><surname>Le</surname></persName>
							<email>le.hg.trung@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>City</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vietnam</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Research &amp; Development OPC Pharmaceutical Company Ho Chi Minh City</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Research Engineer Trusting Social</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Multi-Input Deep Learning Architecture for Biological Activity Prediction: An Application in EGFR Inhibitors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Neural network</term>
					<term>Deep learning</term>
					<term>CNN</term>
					<term>Attention</term>
					<term>EGFR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning and deep learning have gained popularity and achieved immense success in Drug discovery in recent decades. Historically, machine learning and deep learning models were trained on either structural data or chemical properties by separated model. In this study, we proposed an architecture training simultaneously both type of data in order to improve the overall performance. Given the molecular structure in the form of SMILES notation and their label, we generated the SMILES-based feature matrix and molecular descriptors. These data were trained on a deep learning model which was also integrated with the Attention mechanism to facilitate training and interpreting. Experiments showed that our model could raise the performance of prediction comparing to the reference. With the maximum MCC 0.58 and AUC 90% by cross-validation on EGFR inhibitors dataset, our architecture was outperforming the referring model. We also successfully integrated Attention mechanism into our model, which helped to interpret the contribution of chemical structures on bioactivity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Machine learning was applied widely in drug discovery, especially in virtual screening for hit identification. The most popular techniques are Support Vector Machine (SVM), Decision Tree (DT), k-Nearest Neighbor (k-NN), Naive Bayesian method (NB), and Artificial Neural network (ANN). <ref type="bibr" target="#b0">[1]</ref>. In these methods, ANNs need not assume that there was any type of relationship between activity and molecular descriptors, and ANNs are usually outperforming in traditional Quantitative structure -activity relationship problem because they can deal with both nonlinear and linear relationship. As a result, ANNs rose to become a robust tool for Drug Discovery and Development <ref type="bibr" target="#b1">[2]</ref>. However, ANNs are usually sensitive to overfitting and difficult to design an optimal model. Additionally, ANNs also require huge computation resources and their results usually are unable to be interpreted. Those weaknesses could be a reason for limited use of neural network comparing to Decision Tree or Naive Bayesian algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Above algorithms can be applied to various types of chemical features. These features are either structural information or chemical properties. The structural information could be represented as fingerprint vector by using specific algorithms (e.g, Extended-Connectivity Fingerprints <ref type="bibr" target="#b2">[3]</ref>, Chemical Hashed Fingerprint <ref type="bibr" target="#b3">[4]</ref>) while chemistry information could be described by various molecular descriptors (e.g, logP, dipole moment). The ideas that combine some types of features to improve the overall performance was also mentioned in a number of studies <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In these models, each set of chemical features was trained by specific algorithms (SVM, DT, k-NN, NB, ANN or any other algorithms) to generate one particular output. After that, these outputs were pushed to the second model which was usually another multi-layers perceptron model before giving the final result. The disadvantage of this approach is that we need to train each feature set separately because of completely different algorithms. As a result, it is difficult to build a pipeline for all training algorithms. In other words, the automation of the training procedure was reduced.</p><p>Regarding the interpretation of neural network model, there are some interests in making neural network models more explainable and interpretable. A worthy approach needs to be mentioned is the use of Attention mechanisms in sequence-tosequence model. With the encoder-decoder architecture, the attention approach not only improves the performance but reveals the alignment between input and output <ref type="bibr" target="#b6">[7]</ref>.</p><p>To deal with the problem of both automation and interpretation in predicting biological activity, we made an effort to combine different types of chemical features in one deep learning architecture and also integrate Attention mechanism. As a result, our model could train concurrently several feature sets and explain the interaction between the features and outcomes. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>A. Overview of Neural network 1) Artificial neural network: The Artificial neural network is computing architecture which enables a computer to learn from historical data. Nowadays, it is one of the main tools used in machine learning. As the name suggests, artificial neural networks are inspired by how biological neurons work, however, an artificial neural network is a composition of many differentiable functions chained together. Mathematically, a neural network is a non-linear mapping which assumes the output variable y as a non-linear function of its input variables</p><formula xml:id="formula_0">x 1 , x 2 , ...x n y = f (x 1 , x 2 , ..., x n ; θ) +<label>(1)</label></formula><p>where θ is the parameters of the neural network and is model's inreducible error. A very simple neural network which contains only input and output is described as follows:</p><formula xml:id="formula_1">y = σ (w 0 + x 1 w 1 + x 2 w 2 + ... + x n w n )<label>(2)</label></formula><p>whereŷ is an approximation of y. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, each input variable x i is represented as a node in the input layer and connects to the output node through a connection with weight w i . Note that a node with value 1 is attached to the input layer with the corresponding connection weight w 0 to represent the offset term. The σ function is called activation function or squash function which introduce nonlinear relationship between input x and output y. The Sigmoid f (x) = 1 1+e −x and ReLU (f (x) = max(0, x)) function are the most widely used activation functions. The model in <ref type="figure" target="#fig_0">Fig.  1</ref> is referred as generalized linear model. The generalized linear model is simple, thus may not be able to describe complex relationship between inputs and outputs. Therefore, this architecture can be extended by building multiple generalized linear model as the form of layers (or fully connected layers or hidden layers) and stacking those layers together to build a neural network. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates a two layered neural network, we also add neuron 1 to the second layer as we do for the generalized linear model below. Let l (k) andŷ represent the k-th hidden layer and output layer respectively, neural network in <ref type="figure" target="#fig_1">Fig. 2</ref> can be described mathematically as follows:</p><formula xml:id="formula_2">l (1) i = σ w (1) 10 + w<label>(1)</label></formula><formula xml:id="formula_3">1i x i l (2) i = σ w (2) 20 + w (2) 2i l (1) i ŷ = σ w (3) 30 + w (3) 3i l (2) i</formula><p>Note that a neural network may consist of an arbitrary number of many layers by simply stacking more layers. A network containing more than one layers is usually called a deep neural network. 2) Convolutional neural network: Convolutional neural network (CNN) <ref type="bibr" target="#b7">[8]</ref> is a class of neural network. The models using CNN are usually designed to operate on data with gridlike topology. CNN models are usually considered as the state-of-the-art architectures in the computer vision related tasks. CNNs are also applied in biological tasks and achieved remarkable results <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Basically, a CNN block is a combination of convolution layers followed by non-linear activation and pooling layers.</p><p>• Convolutional layer (CONV): A convolutional layer is composed of a set of kernels to extract local features from the previous input. Each kernel is represented as a 3D tensor F k ∈ R w×w×c , where w is the size of the kernel (typically 3 or 5) and c denotes the total number of kernels. Since c is equal to the input's third dimension, it is frequently omitted when referring to the kernel shape. For an input X ∈ R d1×d2×d3 , each kernel F k convolves with X to attain a single feature map</p><formula xml:id="formula_4">O k ∈ R ((d1−w+)/s+1)×(d2−w)/s+1) where O k i,j = w i =1 w j =1 c l=1 [X] i+i −1,j+j −1,l [F k ] i ,j ,l<label>(3)</label></formula><p>where [X] i,j ∈ R w×w×d3 is a small block (known as receptive field) of X around location (i, j); s is the stride which is the interval of the receptive fields of neighboring units. • Pooling layer (POOL): Pooling layer creates a summary of learned features from CONV layers by aggregating the information of nearby features into a single one. The most common design of pooling operation is max-pooling. For example, a 2 × 2 max-pooling filter operating on a particular feature map F with size (n, m) will compute max{F i,j , F i+1,j , F i,j+1 , F i+1,j+1 } for each coordinate i, j in F . This will result in a new features map with size (m/2, n/2). Since a CNN typically contains multiple stacking of CONV layers, pooling is used to reduce data dimension which causes the model less computationally expensive. Pooling can also make the model invariant to small positional and translational changes. A typical CNN architecture is generally made up of series of CNN blocks followed by one or more fully connected layers at the end. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates a simple CNN architecture for image classification problem.</p><p>3) Training Neural Networks: The goal of learning is to minimize the loss function with respect to the network parameters θ. To do that, we need to find an estimate for the parametersθ by solving an optimization problem of the form</p><formula xml:id="formula_5">θ = arg min θ J(θ) = 1 n n i=1 L(x i , y i ; θ)<label>(4)</label></formula><p>where n is the number of instances in the training dataset; L is the loss function which measures the discrepancy between model output and the ground truth. Because the optimization problem does not have closed form solution, the method of gradient descent is used. Firstly, the parameters θ are randomly constructed, for every iteration, the parameters are updated as follow</p><formula xml:id="formula_6">θ t+1 = θ t − γ∇ θ J(θ)<label>(5)</label></formula><p>This process continues until some criterion is satisfied. Here, γ is a constant called the learning rate which is the amount that the weights are updated during training. As presented in the equation 4, the loss function is computed over all the examples, which is computationally extensive. In practice, we use a modified version of gradient descent called stochastic gradient descent , that means, we do not use the whole dataset for gradient computation but a subset of data called a mini-batch. Typically, a mini-batch contains from dozens to hundreds of samples depending on system memory. Since the neural network is a composition of multiple layers, the gradient with respect to all the parameters can be methodically computed using the chain rule of differentiation also known as back-propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Regularization:</head><p>One of the major issues in training neural networks is overfitting. Overfitting happens when a network performs too well on the data it has been trained on but poorly on the test set which it has never seen before. This phenomenon is due to the large number of parameters in the network. Regularization is able to regulate a network activity to ensure the model actually learns the underlying mapping function not just memorizing the input and output. Recently, there are two advanced regularizers which are widely used in the deep neural network.</p><p>• Dropout: During training, some weights in the network at a particular layer could be co-adapted together which may lead to overfitting. Dropout tackles this issue by randomly skipped some weights (explicitly set them zero) with a probability p (usually p = 0.5 or 0.8). During inference, dropout is disabled and the weights are scaled with a factor of p <ref type="bibr" target="#b10">[11]</ref>. • Batch normalization: Recall in regression analysis, one often standardizes the designed matrix so that the features have zero mean and unit variance. This action called normalization speeds up the convergence and make initialization easier. Batch normalization spread this procedure to not only input layer but all of the hidden layers. During training, let x i is values across a minibatch B = {x 1 , x 2 , ..., x k }, the batch norm layer calculate normalized versionx i of x i via:</p><formula xml:id="formula_7">x i = x i − µ B σ 2 B + where µ B = 1 k k i=1 x i ; σ 2 B = 1 k k i=1 (x i − µ B ) 2</formula><p>are mini-batch mean and variance respectively, is a constant to help computational efficiency. To make it more versatile, a batch norm layer usually has two additional learnable parameters γ and β which stand for scale and shift factor such that:</p><formula xml:id="formula_8">x i = γx i + β</formula><p>During inference, mini-batch mean and variance are replaced by population mean and variance which are estimated during training <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Attention mechanism:</head><p>Neural networks could be considered as a "black box" optimization algorithm since we do not know what happens inside them. Attention mechanism enables us to visualize and interpret the activity of neural networks by allowing the network to look back to what it has passed through. This mechanism is motivated by how we, human, pay visual attention to certain regions of images or important words in a sentence. In the neural network, we can simulate this behavior by putting attention weights to express the importance of an element such as pixel in an image or a word in a sentence.</p><p>Attention mechanism was applied widely and now becomes a standard in many tasks such as Neural machine translation <ref type="bibr" target="#b12">[13]</ref>, Image captioning <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of EGFR</head><p>Epidermal Growth Factor Receptor (EGFR) is a member of ErbB receptor family that consists of 4 types: EGFR, HER1, HER2/new, HER3, and HER4. They are located in the cell membrane with the intrinsic tyrosine kinase. The binding of ligands (TGF-α, amphiregulin, and other ligands) and EGFR triggers the signal amplification and diversification which lead to cell proliferation, apoptosis, tumor cell mobility, and angiogenesis. In some type of cancer (such as lung cancer), the overexpression and constitute activation cause the dysregulation of EGFR pathway that activates the tumor process <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>For two decades, there was a great deal of effort in studying this target to discover novel medicine. 3D-QSAR was studied widely to analysis the molecular filed of ligands, which reveals the relationship between various substituents on molecules and biological activity <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Other methods were also useful. R. Bathini et al. employed the molecular docking and molecular mechanics with generalized born surface area (MM/GBSA) to calculate the binding affinities of protein-ligand complexes <ref type="bibr" target="#b18">[19]</ref>. G. Verma et al. conducted pharmacophore modeling in addition to 3D-QSAR to generate a new model which was used for screening novel inhibitors <ref type="bibr" target="#b21">[22]</ref>.</p><p>Regarding the application of machine learning techniques in EGFR inhibitors discovery, H. Singh et al. <ref type="bibr" target="#b22">[23]</ref> used Random Forest algorithms to classify EGFR inhibitors and non-inhibitors. In their study, the authors collected a set of diverse chemical and their activity on EGFR. A model with high accuracy was trained and validated by 5-fold crossvalidation (0.49 in MCC and 0.89 in AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Overview of Features set</head><p>1) SMILES Feature matrix: SMILES (Simplified Molecular Input Line Entry System) is a way to represent the molecules in in silico study. This method uses a strict and detailed set of rules to interpret the molecular structure into the chemical notation which is user-friendly but also machine-friendly. In particularly, SMILES notation of a molecule is a chain of character which is specified for atoms, bonds, branches, cyclic structures, disconnected structures, and aromaticity <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure">Fig. 4</ref>. The steps to generate the feature matrix from chemical structure using SMILES notation Based on this representation, M. Hirohara et al. developed a SMILES-based feature matrix to train a convolutional neural network model for predicting toxicity. His model outperformed the conventional approach and performed comparably against the winner of Tox21 data challenge <ref type="bibr" target="#b24">[25]</ref>. In this dataset, each molecule was represented in the form of SMILES notation and the output consisted of 12 tasks to predict <ref type="bibr" target="#b25">[26]</ref>.</p><p>2) Molecular descriptors: Molecular descriptors are terms that characterize a specific aspect of a molecular, including substituent constants and whole molecular descriptors <ref type="bibr" target="#b26">[27]</ref>. The calculation of former type derived from the difference in functional group substitution into the main core of the compound. Based on this approach, the latter is the expansion of the substituent constant. However, some whole molecular descriptors are developed from totally new methods or based on physical experiments <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The dataset used in this study was collected by H. Singh et al. <ref type="bibr" target="#b22">[23]</ref>. This dataset contains 3492 compounds which is classified as inhibitor or non-inhibitor of EGFR. The inhibition activity of a particular substance was assigned if its IC50 is less than 10 nM . The ratio of inhibitors over non-inhibitor is 506 : 2986 ≈ 1 : 6. The information of chemical includes ID, SMILES representation, and class (1 for inhibitor and 0 for non-inhibitor).</p><p>1) SMILES Feature matrix generation: Based on the collected dataset, the chemical structure data in the form of SMILES notation was preprocessed and converted to the canonical form which is unique for each molecule by the package rdkit <ref type="bibr" target="#b28">[29]</ref>. In this study, the SMILES Feature matrix generation method developed by M. Hirohasa et al. was used to encode the chemical notation. The maximum length of each input was 150 and thus the input strings with length below 150 were padded with zeros at the tail. In their method, for each character in the SMILES string, a 42-dimensional vector was computed <ref type="table" target="#tab_0">(Table  I</ref>). The first 21 features represent the data about the atom and the last 21 features contain SMILES syntactic information <ref type="bibr" target="#b24">[25]</ref>. 2) Descriptor calculation: We used the package mordred built by H. Moriwaki, Y. Tian, N. Kawashita et al. <ref type="bibr" target="#b29">[30]</ref> to generate molecular descriptor data. Because the SMILES notation do not provide exact 3D conformation, the 2D descriptors were only calculated with total of 1613 features . The generated data was preprocessed by imputing the meaningless features or the variables which are same for whole dataset. A standard scaler was also used to normalize the molecular descriptors dataset. We used package numpy <ref type="bibr" target="#b30">[31]</ref>, pandas <ref type="bibr" target="#b31">[32]</ref> and scikit-learn <ref type="bibr" target="#b32">[33]</ref> for this process.  2) Attention mechanism: The idea of using the attention mechanism came from the fact that each chemical's atoms contribute differently to the drug's effect. In other words, we put attention or weight to the atoms in the chemical which are represented by rows in the SMILES feature matrix. The larger the weight of an atom is, the more contribution of which atom contribute to the drug. By doing this, we can extract each components weights for interpreting the results and analysis.</p><p>Let m and R denote the vector obtained by feeding SMILE vector through a linear layer and the SMILE feature matrix, respectively. The model uses the similarity between m and the i-th row R i as a measure of the importance of R i . Particularly, let a i denote the attention weight vector, it is formulated as follows:</p><formula xml:id="formula_9">a i = 1 1 + exp R i · m<label>(6)</label></formula><p>After that, the vector a is then used as the coefficient of a linear combination of rows in the SMILES feature matrix. Therefore, the output of the attention layer f is expressed as:</p><formula xml:id="formula_10">f = a i × R i<label>(7)</label></formula><p>3) Molecular Descriptors (MD) branch: The MD branch is used to train the molecular descriptors data. Let t is the vector obtained by feeding molecular descriptors vector through 3 blocks of fully connected layers each consisted of a fully connected layer, a batch normalization layer, and a dropout layer. t is considered as a high-level representation of molecular descriptors data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Concatenation:</head><p>The three vectors: f , m, t are then concatenated as in <ref type="figure" target="#fig_4">Fig. 5</ref> to form the final representation vector. This vector combines information from both molecule structural information and descriptors information. It is then fed through a linear layer and squash by sigmoid function to make the final prediction as the probability of the molecule as an inhibitor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyper-parameter tuning</head><p>In this study, PyTorch platform <ref type="bibr" target="#b33">[34]</ref> was used in order to implement our model and the 5-fold cross-validation was conducted to evaluate the performance. In this method, the dataset was split into 5 parts. For each fold, the model was trained on the set of 4 parts and tested on the remaining part. The choice of the training set was permuted through all divided parts of the dataset, thus the model was trained 5 times and the average performance metrics of each time was used to evaluate. The ending point of Training step was determined by Early-stopping technique <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Thus, for each fold, the model would stop training if the loss value increases continuously 30 epochs.</p><p>The second column of <ref type="table" target="#tab_0">Table IV</ref> presents the hyperparameters and their considered values in the tuning step. Grid Search technique was conducted to determine the best combination of hyperparameters which had the best performance. However, in the case of discovering the suitable batch size for training, several suggested values were tested and the chosen was the value which utilized the system efficiently. Additionally, the threshold of the classifier was determined by analyzing the ROC plot and the Precision-Recall curve. The most optimal threshold was the point nearest to top-left of the ROC plot and gave the balance between Precision and Recall in the latter plot. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Evaluation</head><p>In order to assess the performance of each model, several metrics were calculated during training and validation steps ( <ref type="table" target="#tab_3">Table V)</ref>.</p><p>The ROC analysis and AUC are usually considered as the most popular metrics for imbalanced dataset because they   are not biased against the minor label <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. However, these metrics show the overall performance in the whole domain of threshold. In other words, ROC or AUC are not represented for a particular classifier. In our study, MCC was the most preferred criteria to evaluate model performance. This is because MCC considers all classes in the confusion matrix whereas other metrics (eg, accuracy or F1-score) do not fully use four classes in the confusion matrix <ref type="bibr" target="#b38">[39]</ref>. The remaining metrics were still useful for the benchmark. The training batch size is 128 which gave the best utility on the GPU Tesla K80. When comparing the effect of two types of the optimizer, we observed that Adaptive Moment Estimation (ADAM) showed a better result than Stochastic gradient descent (SGD) in both running time and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyper-parameters optimization</head><p>The optimal collection of hyper-parameters was listed in the third column of <ref type="table" target="#tab_0">Table IV</ref>. These values were used for evaluating the performance of three considered models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance</head><p>Our architecture was trained on the EGFR dataset and evaluated by mentioned cross-validation method. The final result represents in <ref type="table" target="#tab_0">Table VI</ref>. When using structure information in the CNN branch only, the performance was slightly better than H. Singh et al. model in all metrics excepting AUC. However, the CNN + MD model and CNN + MD + ATT model was outperforming to the reference model. By comparing two important indicators, it can be seen that there was an improvement in MCC and AUC. Additionally, when training with more branch (CNN + MD and CNN + MD + ATT), the Running time was also reduced significantly to around a half.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention mechanism</head><p>The Attention mechanism was successfully implemented in our architecture. From the attention weights vector, the weights representing to each atom in the molecules were extracted and used to indicate their contribution to the compound's activity. By using visualization on the package rdkit, these attention weights could be used to visualize the distribution of contribution over the molecular structure. <ref type="figure">Figure 7</ref> illustrates some example from the model.</p><p>Additionally, the integration of attention mechanism to the model did not give much more improvement in performance. We believed that this problem could be improved by optimizing the hyperparameters. However, we did not focus on hyperparameters optimization for this model because the aim of Attention vector was to make the model more interpretable.</p><p>The visualization of Attention vectors revealed some key findings in chemical structure, such as the Nitrogens in the hetero-cyclic structure are usually highlighted by the model and the halobenzyl substitution on heterocyclic structure contributed positively to the bioactivity. <ref type="figure">Fig. 7</ref>. Chemical structure interpretation using Attention weight V. DISCUSSION There are two major advantages to our architecture. The first strong point is the combination of both structure information and chemical attributes in a single learning model. As a result, this advancement made a significant improvement in both performance and automation. Another worthy innovation was the integration of attention mechanism which facilitated the interpretation of the model. In fact, attention weight generated by the model would help explain the contribution of each atom on the overall biological activity.</p><p>Comparing to another effort to make deep learning model more interpretable, our architecture has an advantage in computation because it is easier to generate the SMILES Feature matrix than other algorithms. For example, Sanjoy Dey et al. <ref type="bibr" target="#b39">[40]</ref> used ECFP fingerprint algorithms to transform the chemical structure into the matrix feature. This method does not treat the molecule as a whole structure but calculate on each fragment of a chemical with a particular radius. Additionally, there are required calculation to generate the features including tuning the hyper-parameters of the algorithms (e.g the radius of calculation).</p><p>Regarding to our implementation of Attention mechanism, we observed that each atom in a substance was treated separately; as a result, the connection between atom was not highlighted in our model, as well as the contribution of some functional groups which contain many atoms (e.g, carbonyl, carboxylic, etc) was not clearly illustrated. We proposed a solution for this limitation that is to add another branch to the architecture which embeds the substructure patterns (e.g, Extended-Connectivity Fingerprints <ref type="bibr" target="#b2">[3]</ref>, Chemical Hashed Fingerprint <ref type="bibr" target="#b3">[4]</ref>).</p><p>Additionally, the lower performance of the CNN model comparing to the baseline model was another interesting finding. This could be due to the sparsity of SMILES feature matrix. In fact, the CNN as well as other deep learning algorithms require much data to accumulate the information in the training step. In case of SMILES feature data, because of zero paddings to justify the length of encoding vectors, the feature matrix became sparse. This led to the fact that the model required more data for training but the dataset was quite small for deep learning. However, in the case of CNN + MD or CNN + MD + ATT model, because of the addition of another input data, the models acquired information more easily. As a result, the performance was improved in terms of all metrics.</p><p>When considering the running time between different models, it is clear that the longest running time was that of model with only CNN branch (59 min) while the more complicated model with more data like CNN + MD and CNN + MD + ATT took just a half of running time with 31 min and 37 min, respectively. This could be because the SMILES Feature matrix in the CNN model was sparse so the model should train longer to achieve the convergence of loss function. In the CNN + MD and CNN + MD + ATT model, there could be a complement between different input branches and we supposed that there was an information flow transferring between two branches, which facilitated the training stage and performance improvement. In other studies which also used several types of data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, the model trained model separately and did not use this information connection. This phenomenon might represent an advantage of our architecture.</p><p>In conclusion, the combination of different source of features is definitely useful for bioactivity prediction, especially when using deep learning model. The attention-based multiinput architecture we proposed achieved a superior score comparing to referring model. Additionally, the attention mechanism would help to interpret the interaction between each element of chemical structures and their activity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example of an ordinary neuron</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Example of an 2 layered neural network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Example of a CNN for the Image Classification task on CIFAR10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>B. Model architecture 1 )</head><label>1</label><figDesc>Convolutional neural network (CNN) branch: The SMILES Feature matrix was flown through 2 CNN blocks each consisted of a 2D convolution layer, one Batch normalization layer, a Dropout layer, and a Max pooling layer before being flattened and fully connected via a linear layer. The detail of the hyper-parameters of each layer is represented inTable II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The Architecture of Attention based Multi-Input deep learning model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>√(</head><label></label><figDesc>TP+FP)(TP+FN)(TN+FP)(TN+FN) AUC the area under the ROC curve † TP: True Positive, FN: False Negative, TN: True Negative, FP: False Positive, MCC: The Matthews correlation coefficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Despite of imbalanced classes, the loss function of model (binary cross-entropy) still converged with the loss value of 0.10 − 0.16 for Training set and 0.22 − 0.28 for Validation set at the end of each fold when cross-validating the model CNN + MD + ATT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>in 5-fold Cross validation, SENS: Sensitivity, SPEC: Specificity, ACC: Accuracy, MCC: The Matthews correlation coefficient, AUC: The Area under the ROC curve, RT: Running time. ‡ CNN: using CNN branch only, CNN + MD: using both CNN and MD branch, CNN + MD + ATT: using both CNN and MD branch with Attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>The Loss value of each model on the Validation steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FEATURESTABLE</head><label>I</label><figDesc></figDesc><table><row><cell>Features</cell><cell>Description</cell><cell>Size</cell></row><row><cell>Type of atom</cell><cell>H, C, O, N, or others</cell><cell>5</cell></row><row><cell>No. of Hs</cell><cell cols="2">Total number of attached Hydrogen atoms 1</cell></row><row><cell>Degree</cell><cell>Degree of unsaturation</cell><cell>1</cell></row><row><cell>Charge</cell><cell>Formal charge</cell><cell>1</cell></row><row><cell>Valence</cell><cell>Total valence</cell><cell>1</cell></row><row><cell>Ring</cell><cell>Included in a ring or not?</cell><cell>1</cell></row><row><cell>Aromaticity</cell><cell>Included in a aromatic ring or not?</cell><cell>1</cell></row><row><cell>Chirality</cell><cell>R, S, or others</cell><cell>3</cell></row><row><cell>Hybridization</cell><cell>s, sp, sp 2 , sp 3 , sp 3 d, sp 3 d 2 , or others</cell><cell>7</cell></row><row><cell cols="2">SMILES symbol ( ) [ ] . : = # \ / @ + -</cell><cell>21</cell></row><row><cell></cell><cell>Ion_charge Start End</cell><cell></cell></row><row><cell>Total</cell><cell></cell><cell>42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">HYPER-PARAMETER IN CNN BRANCH</cell></row><row><cell>Layer</cell><cell cols="2">Hyper-parameter</cell><cell>Value</cell></row><row><cell>1 st conv2d</cell><cell cols="2">No. of input channels</cell><cell>1</cell></row><row><cell></cell><cell cols="2">No. of output channels</cell><cell>6</cell></row><row><cell></cell><cell cols="2">Kernel size</cell><cell>(3,3)</cell></row><row><cell cols="3">2 nd conv2d No. of input channels</cell><cell>6</cell></row><row><cell></cell><cell cols="2">No. of output channels</cell><cell>16</cell></row><row><cell></cell><cell cols="2">Kernel size</cell><cell>(3,3)</cell></row><row><cell>Dropout</cell><cell cols="2">Dropout rate</cell><cell>Be tuned</cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell></row><row><cell cols="4">HYPER-PARAMETER IN MD BRANCH</cell></row><row><cell>Layer</cell><cell></cell><cell cols="2">Hyper-parameter Value</cell></row><row><cell cols="2">1 st Linear layer</cell><cell>No. of neurons</cell><cell>512</cell></row><row><cell cols="3">2 nd Linear layer No. of neurons</cell><cell>128</cell></row><row><cell cols="2">3 rd Linear layer</cell><cell>No. of neurons</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="3">HYPER-PARAMETER TUNING</cell></row><row><cell>Hyper-parameter</cell><cell>Value</cell><cell>Optimal value</cell></row><row><cell>Batch size</cell><cell>32; 64; 128; 512</cell><cell>128</cell></row><row><cell>Dropout rate</cell><cell>0; 0.2; 0.5</cell><cell>0.5</cell></row><row><cell>Optimizer</cell><cell>SGD; ADAM</cell><cell>ADAM</cell></row><row><cell>Learning rate</cell><cell>1e-4; 1e-5; 1e-6</cell><cell>1e-5</cell></row><row><cell>Threshold</cell><cell>0.2; 0.5; 0.8</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell>PERFORMANCE METRICS</cell></row><row><cell>Metrics</cell><cell>Formulas  †</cell></row><row><cell>Sensitivity Specificity Accuracy MCC</cell><cell>TP TP+FN TN TN+FP TP+TN TP+TN+FP+FN TP×TN−FP×FN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">PERFORMANCE COMPARISON</cell><cell></cell></row><row><cell cols="2">Metrics  † H. Singh</cell><cell>CNN</cell><cell cols="2">CNN + MD CNN + MD</cell></row><row><cell></cell><cell>et al.</cell><cell></cell><cell></cell><cell>+ ATT  ‡</cell></row><row><cell>LOSS</cell><cell>N.A</cell><cell>0.2727</cell><cell>0.2434</cell><cell>0.2399</cell></row><row><cell>SENS</cell><cell>69.89%</cell><cell>74.31%</cell><cell>75.29%</cell><cell>74.11%</cell></row><row><cell>SPEC</cell><cell>86.03%</cell><cell>85.77%</cell><cell>89.75%</cell><cell>90.15%</cell></row><row><cell>ACC</cell><cell>83.66%</cell><cell>84.10%</cell><cell>87.66%</cell><cell>87.83%</cell></row><row><cell>MCC</cell><cell>0.49</cell><cell>0.50</cell><cell>0.58</cell><cell>0.57</cell></row><row><cell>AUC</cell><cell>89.00%</cell><cell>87.52%</cell><cell>90.32%</cell><cell>90.84%</cell></row><row><cell cols="3">† LOSS: Average Best Loss value</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine-learning approaches in drug discovery: methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavecchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="318" to="331" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks as robust tools in drug lead discovery and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Biochemistry and Biotechnology -Part B Molecular Biotechnology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="167" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extended-Connectivity Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chemical Hashed Fingerprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Al-Lazikani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dictionary of Bioinformatics and Computational Biology</title>
		<meeting><address><addrLine>Chichester, UK; Ltd</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scrutinizing MHC-I Binding Peptides and Their Limits of Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Perna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pillong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Todoroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Folkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1003088</biblScope>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exhaustive Proteome Mining for Functional MHC-I Ligands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Perna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weissmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pillong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Baleeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reutlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Folkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Walden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Waibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Chemical Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1876" to="1881" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="255" to="258" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Learning in Drug Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gawehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The biology of epidermal growth factor receptor in lung cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Scagliotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Selvaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Novello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Cancer Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EGFR signaling and drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lurje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="400" to="410" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D-QSAR and docking studies on 4-anilinoquinazoline and 4-anilinoquinoline epidermal growth factor receptor (EGFR) tyrosine kinase inhibitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Assefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Buolamwini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="475" to="493" />
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Receptor-Guided Alignment-Based Comparative 3D-QSAR Studies of Benzylidene Malonitrile Tyrphostins as EGFR and HER-2 Kinase Inhibitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Buolamwini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="4657" to="4668" />
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Molecular docking, MM/GBSA and 3D-QSAR studies on EGFR inhibitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bathini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fatima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Sciences</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1163" to="1173" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2D-QSAR and 3D-QSAR Analyses for EGFR Inhibitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed Research International</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D-QSAR, molecular docking, and dynamics simulation of quinazoline-phosphoramidate mustard conjugates as EGFR inhibitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amelia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Megantara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Pharmaceutical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pharmacophore modeling, 3D-QSAR, docking and ADME prediction of quinazoline based EGFR inhibitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shaquiquzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal of Chemistry</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">QSAR based model for discriminating EGFR inhibitors and noninhibitors using Random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Raghava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biology Direct</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural network based on SMILES representation of compounds for detecting chemical motif</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">526</biblScope>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tox21 Data Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>National Center of Advancing Translational Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glossary of terms used in dysmorphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">U</forename><surname>Of</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chemistry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Desk Reference -Clinical Genetics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1137" to="1152" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chemical Information and Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Understanding the Basics of QSAR for Applications in Pharmaceutical Sciences and Risk Assessment</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="47" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">RDKit: Open-source cheminformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mordred: A molecular descriptor calculator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moriwaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kawashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<title level="m">Guide to NumPy. USA: CreateSpace Independent Publishing Platform</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The NumPy Array: A Structure for Efficient Numerical Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving model selection by nonconvergent methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Finnoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hergert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="1993-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic early stopping using cross validation: quantifying the criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="1998-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Handling imbalanced datasets : A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Transactions on Computer Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the Class Imbalance Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ten quick tips for machine learning in computational biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioData Mining</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting adverse drug reactions through interpretable deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Suppl 21</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
