<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RMS-Net: Regression and Masking for Soccer Event Spotting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Tomei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bronzin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Metaliquid S.R.L</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RMS-Net: Regression and Masking for Soccer Event Spotting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently proposed action spotting task consists in finding the exact timestamp in which an event occurs. This task fits particularly well for soccer videos, where events correspond to salient actions strictly defined by soccer rules (a goal occurs when the ball crosses the goal line). In this paper, we devise a lightweight and modular network for action spotting, which can simultaneously predict the event label and its temporal offset using the same underlying features. We enrich our model with two training strategies: the first one for data balancing and uniform sampling, the second for masking ambiguous frames and keeping the most discriminative visual cues. When tested on the SoccerNet dataset and using standard features, our full proposal exceeds the current state of the art by 3 Average-mAP points. Additionally, it reaches a gain of more than 10 Average-mAP points on the test set when fine-tuned in combination with a strong 2D backbone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Understanding videos has been one of the most attractive and challenging areas of Computer Vision of the last few years. While a significant research effort has been carried out to design novel architectures and training approaches for enhancing the effectiveness of spatio-temporal features extraction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, there is also a growing interest in bringing these techniques to more application-oriented domains <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Among these, the soccer industry could greatly benefit from the automatic understanding of soccer matches. Soccer videos are indeed used by professionals for statistics generation, for analyzing and developing strategies and for understanding failures. Broadcast video providers, on the other hand, often need to automatically generate summaries and highlights of soccer matches, currently still achieved via manual annotation in most cases.</p><p>For these reasons, there has been an increasing effort to develop architectures for spotting actions in soccer videos <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The task requires to temporally localize all significant events happening inside the match like goals or yellow/red card events. As these events are sparse within a video, the task couples the need for effective feature extraction with that of properly handling the data imbalance and event sparsity issues. Also, an effective action spotting approach needs to provide accurate temporal localization, which in turn requires proper architectural and training strategies.</p><p>In this paper, we treat the action spotting as a detection problem and devise a novel network which takes inspiration from the regression strategies used in object detection. Specifically, our network takes a short video snippet as input and predicts the presence of a candidate event together with its class and relative temporal position inside the input snippet. This choice is innovative compared to recent works in the field, which usually assign the event to the central frame of the video chunk, thus preventing the model from learning an accurate localization of actions. We also deal with the sparsity and class imbalance issues through a sampling approach that ensures a uniform distribution in training batches in terms of ground-truth class and action locations.</p><p>Further, we develop a strategy to increase the generalization capabilities of the network, which is inspired by the masking strategies used in self-supervised pre-training techniques <ref type="bibr" target="#b6">[7]</ref>. During training, indeed, we randomly mask and replace the frames preceding an event and constraint the network to focus on the most relevant parts of the action.</p><p>We conduct extensive evaluations on the recently released SoccerNet dataset, which features 500 full broadcast soccer matches, annotated with relevant events. We provide experiments to validate the architectural choices behind the proposal and the effectiveness of the masking and data sampling strategies. When compared to the state of the art, our network achieves a gain of 3 Average-mAP points exploiting the same features used by previous works, while being significantly more lightweight. We also conduct experiments with different feature extraction backbones, and investigate the role of finetuning such backbones, devising a combination which further pushes the state of the art of 10 Average-mAP points.</p><p>II. RELATED WORK Video activity understanding. Video Understanding is one of the most challenging and broad areas of Computer Vision, and many research efforts have been dedicated to this field in the last few years. As understanding the complexity of videos often requires to learn data-driven models with many parameters, the effort of researchers has also focused on the collection of proper datasets in recent times. Some of the datasets have collected action clips from movies or usergenerated videos, like HMDB <ref type="bibr" target="#b7">[8]</ref>, UCF101 <ref type="bibr" target="#b8">[9]</ref>, YouTube-8m <ref type="bibr" target="#b9">[10]</ref>, or Kinetics <ref type="bibr" target="#b10">[11]</ref>, while others have focused on more domain-specific actions, like those belonging to soccer events and sports in general <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>On the technical and architectural side, most of the recent methods employ variants of temporal convolutions to integrate motion and appearance features, either by using full 3D kernels <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> or separating the extraction of spatial and temporal features in different layers <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Other techniques integrate motion in a second stream of the architecture which processes the optical flow of the video <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Recently, the literature has also concentrated on the role of the spatial and temporal resolution of the video <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Besides classifying the actions performed inside a given video, the literature has also investigated the prediction of the temporal boundaries of the action <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, with datasets such as ActivityNet <ref type="bibr" target="#b21">[22]</ref> or Thumos <ref type="bibr" target="#b22">[23]</ref>, and on the spatio-temporal location of actions <ref type="bibr" target="#b23">[24]</ref>. Soccer event spotting. On a related line of research, the action spotting task has been proposed for soccer videos along with the SoccerNet dataset in <ref type="bibr" target="#b2">[3]</ref>, with the aim of finding the exact anchor time (or spot) of an event and to recognize it.</p><p>The final objective of these approaches is primarily highlights identification and generation. With this purpose, Bag-of-Words and SIFT features have been adopted by <ref type="bibr" target="#b24">[25]</ref>, together with an LSTM for soccer video classification, while deep convolutional features have been used in <ref type="bibr" target="#b25">[26]</ref>. Tsagkatakis et al. <ref type="bibr" target="#b26">[27]</ref> adopted an optical flow and appearance feature fusion strategy for goal and no goal event detection. After the spotting problem definition by Giancola et al. baseline, several approaches have been proposed for this task: audio stream integration has been explored in <ref type="bibr" target="#b27">[28]</ref>, showing promising spotting results. Vats et al. <ref type="bibr" target="#b4">[5]</ref> introduced a multitower temporal convolutional network, while a context-aware loss function has been defined in <ref type="bibr" target="#b5">[6]</ref>, observing that frames just after an event contain most of the visual cues for event recognition. Large-scale Soccer datasets. Gathering a large number of realistic soccer videos is not easy, because of the limited public availability of broadcast content. Nevertheless, a number of datasets for soccer analysis have been recently proposed. SoccerNet <ref type="bibr" target="#b2">[3]</ref> is a collection of 500 broadcast soccer games with one second resolution event annotations, and is at present the largest dataset for soccer action spotting. Yu et al. <ref type="bibr" target="#b28">[29]</ref> collected 222 soccer matches with shot transitions, event boundaries, and players bounding box annotations. SoccerDB <ref type="bibr" target="#b29">[30]</ref> includes 346 soccer games with event segments and players, ball, goalposts bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In the following, we present our approach for soccer event spotting. Our formulation features a lightweight network, which can be integrated with any existing backbone, and which jointly predicts classification scores and temporal offsets. This is combined with a masking strategy which increases the training performance, and with effective handling of data imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proposed architecture</head><p>A soccer match can be represented as a sequence of frames (x 1 , x 2 , ..., x N ), with x i ∈ R H×W ×Ch , extracted with a given frame rate from the original video clip. The goal of our approach is to spot and classify a set of interesting events inside the match, i.e. to predict a set of pairs {(t j , e j )} n j=1 , where t j indicates the timestamp of the action from the beginning of the video, and e j ∈ {0, ..., C − 1} indicates its class label.</p><p>Taking inspiration from the regression strategies used in object detection <ref type="bibr" target="#b30">[31]</ref>, our model takes as input a short video chunk X = (x 1 , x 2 , ..., x T ) with length T , and predicts the temporal offset of a possible action inside the chunk, plus a classification score over C + 1 classes, where the additional class indicates the background one. At test time, predictions can be obtained by applying the model on chunks extracted from the input video with a given stride, converting relative offsets to absolute timestamps, and accumulating predictions over time.</p><p>In our network, the T input frames are independently passed to a 2D backbone to extract a feature vector for each frame, which is obtained by averaging the activation map's spatial dimensions from the last convolutional layer and linearly projecting the result to a common dimensionality. The model then applies a stack of two 1D convolutions over the temporal dimension to combine the features of different frames. After the convolutions, a maximum operation is applied to remove the time axis, and a stack of linear layers is added. The tail of the model consists of two sibling fully-connected layers, one for action classification and the other for temporal offset regression. Our model is visually depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Given a "foreground" input clip containing an event (t j , e j ), the classification layer outputs per-class probabilities and a cross-entropy loss is applied:</p><formula xml:id="formula_0">L cls = − C c=0 1 c=ej log(p c ),<label>(1)</label></formula><p>where C + 1 is the number of different event labels, including the background label, 1 c=ej is the indicator function, and p c is the model output probability for event label c. On the other hand, the regression layer predicts a single scalar per clip, which is projected in the range [0, 1] through a sigmoid function and trained to predict the normalized relative offset of the event in the video chunk. Formally, we apply a squarederror loss as follows:</p><formula xml:id="formula_1">L regr = (σ(o) − r j ) 2 ,<label>(2)</label></formula><p>where o is the output of the regression branch of the network, and the normalized relative offset r j is computed as (t j −s)/T , where s is the starting timestamp of the video chunk. The final loss for a foreground chunk is a weighted sum of L cls and L regr :</p><formula xml:id="formula_2">L = L cls + λL regr ;<label>(3)</label></formula><p>for "background" video chunks, i.e. chunks which do not contain any event, we instead apply only the classification loss, using the background class as ground truth label. As it can be noticed, the proposed method predicts a single event per clip. It could be easily extended to predict more than one event, e.g. including for instance a bipartite matching strategy between predictions and ground truth labels <ref type="bibr" target="#b31">[32]</ref>. In practice, we noticed that interesting events are very sparse in a soccer match. The only rare circumstances in which more than one action occurs in a few seconds interval correspond to double substitutions or double yellow/red cards. While the choice of having a single action predictor could slightly affect spotting performances, it does not affect automatic highlight generation, which is the final goal of event spotting in soccer videos and allows us to maintain a simple and lightweight architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Masking strategy</head><p>The majority of visual cues that contribute to the recognition of an event occur just after the event itself <ref type="bibr" target="#b5">[6]</ref>. This is particularly evident in the case of soccer matches, in which reactions to an event are often a good indicator of the presence of the event itself, like in the case of the celebrations following a goal event. Taking inspiration from the masking strategies used in self-supervised pre-training approaches <ref type="bibr" target="#b6">[7]</ref>, we endow our approach with a masking policy that allows the network to focus on the most relevant portions of the clips at training time.</p><p>In our formulation, a foreground training clip is masked with a given probability by replacing the frames before the event with a randomly chosen background sequence (i.e. which does not contain any event). We also make sure that there is a sufficient number of frames after the event, by avoiding the application of masking on clips in which the event comes too late. Background training clips, instead, are not masked at all.</p><p>Formally, we define a masking function M (p, q), in which p indicates the masking probability and q is the maximum normalized relative temporal offset of the event inside the clip. Given a foreground clip X containing an event (t j , e j ), the masking function M (p, q) is defined as follows:</p><formula xml:id="formula_3">M (p, q)(X) =                z 1 , ..., z tj −s−1 , x tj −s , ..., x T if r j ≤ q, u &lt; p x 1 , ..., x tj −s−1 , x tj −s , ..., x T otherwise,<label>(4)</label></formula><p>where s is the starting timestamp of the video chunk, r j is the normalized relative offset of the event inside the video chunk,</p><formula xml:id="formula_4">(z i ) tj −s−1 i=1</formula><p>is a sequence of consecutive frames randomly selected from a background clip, and u is a random value sampled from the uniform distribution U [0, 1]. A visualization of the masking strategy is also reported in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Through this masking approach, we randomly force the model to recognize an event using just the frames following the event itself, and without relying on the previous ones. We will experimentally show how this masking strategy, which does not require any architectural change or additional resources, is robust to different values of the masking probability p, and how it can increase the recognition and localization accuracy of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Sampling and Balancing</head><p>The training set of our architecture is composed of all possible clips with length T which can be extracted from a set of matches. Since relevant events in a soccer match are quite sparse, a balancing strategy is needed to make sure that the network can learn to properly classify relevant events, without losing the capability of distinguishing background clips. At the same time, we need to make sure that the distribution of the offsets used to train the regression branch is sufficiently uniform.</p><p>Given a training soccer match and an anchor event (t j , e j ), we extract all clips with length T containing the event, sliding a window along the time axis with stride 1. Doing so, we generate many clips for the same event, where each of them contains the event in a different relative temporal location.</p><p>Repeating this procedure for each event in a match, and then for each match in the training set, we collect a set of interesting events, where the distribution of the relative position of events inside clips is balanced by construction.</p><p>The remaining parts of the matches, which do not contain any event, are sliced with a window of size T and stride T (thus avoiding overlapping), to build the set of background video chunks. In each training epoch, we randomly sample n F foreground clips from the above mentioned set, and n B = n F /C clips (where C is the number of classes) from the set of background clips, to balance the number of samples per class. During inference we extract non-overlapping clips, each with T frames, in a sliding-window manner, assuming that T is small enough to prevent that more than one action occurs inside a clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>A. Dataset and evaluation protocol Data. We train and evaluate the proposed method on the SoccerNet dataset <ref type="bibr" target="#b2">[3]</ref>. SoccerNet gathers 500 full broadcast soccer matches, spanning 764 hours of video, which are split into 300 games for training, 100 for validation, and 100 for testing. Interesting events belong to three categories (goal, card, substitution) and are manually annotated with a temporal resolution of one second. The average separation between events is 6.9 minutes, thus leading to a very sparse annotation.</p><p>For fairness of comparison with previously published baselines and state-of-the-art approaches, all experiments are performed with the pre-computed ResNet-152 <ref type="bibr" target="#b32">[33]</ref> features released with the dataset itself, unless otherwise specified. These have been obtained by Giancola et al. <ref type="bibr" target="#b2">[3]</ref> by resizing and cropping videos at a resolution of 224 × 224 and extracting frames at a frequency of 25 fps. Starting from this extraction, a feature vector was computed every 0.5 seconds. They also applied a PCA step to reduce the dimensionality to 512. The feature extraction backbone was pre-trained on ImageNet <ref type="bibr" target="#b33">[34]</ref>. Evaluation metric. The action spotting task requires to correctly predict the anchor spot that identifies an event. For instance, a ground truth spot for a card event is defined as the timestamp in which the referee extracts the card. Following previous literature, we use the Average-mAP defined in <ref type="bibr" target="#b2">[3]</ref> as the evaluation metric, which accounts for multiple temporal tolerances. Given a temporal tolerance δ, we compute the average precision for a class by considering a prediction as positive if the distance from its closest ground truth spot is less than δ. The mean average precision is then obtained by averaging the AP of each class. The final metric is computed as the area under the mAP curve obtained by varying δ in the interval ranging from 5 to 60 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In all our experiments, the model takes T = 41 frames as input, therefore spanning 20 seconds of the match. The weight λ of the regression loss (defined in Eq. 3) is set to 10, unless otherwise specified. <ref type="table" target="#tab_0">Table I reports</ref>   Each 1D convolutional layer has a kernel size of 9, a stride of 1, and zero-padding to keep the temporal dimension shape. The drop rate of the dropout layer is set to 0.1.</p><p>Training. Following our data sampling strategy, we obtain a total of around 150,000 foreground and 72,000 background video chunks. At each training epoch, we randomly sample 30,000 foreground sequences and 10,000 background sequences, being the number of foreground classes C equal to three in SoccerNet. During training, we also drop all substitution events occurring at half time, since no visual cues suggest that a substitution is happening. The masking probability p is set to 1/3, while the maximum relative temporal offset q is set to 0.5, unless otherwise specified. We train our model for a maximum of 50 epochs using an SGD optimizer with momentum 0.9. The batch size is set to 64 and we apply a learning rate of 0.05, with a linear warm-up during the first epoch and a cosine annealing scheduling from the second epoch. A weight decay of 10 −4 is also adopted. Early stopping on the Average-mAP computed over the validation set is applied. Training is done on a NVIDIA RTX 2080Ti GPU.</p><p>Inference. During inference, we slide a window containing T frames on the input video with a stride of T , and the clip class, together with the event relative temporal offset, is predicted by the two sibling fully connected layers. The predicted relative offset is then converted to absolute timestamp, to obtain the predicted spot location. No masking is applied during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Main results and comparison with the state of the art</head><p>In the following, we validate the proposed approach for action spotting in soccer video. We firstly perform a comparison with baselines and state-of-the-art methods and then analyze and ablate the key components of the approach. Finally, we will investigate the performance of the proposed architecture when using different 2D convolutional backbones.</p><p>Comparison with baselines and previous methods. In Table II we report the performance of our model, in comparison with previous approaches for action spotting. All the reported approaches adopt the ResNet-152 features released with the dataset, and we do the same for fairness of comparison. The only exception is the approach presented by Vanderplaetse et al. <ref type="bibr" target="#b27">[28]</ref>, which enriched the same visual features by encoding the audio stream through a VGG network <ref type="bibr" target="#b34">[35]</ref>.  As it can be noticed, our approach outperforms the best SoccerNet baseline (with T = 20 seconds) by 15.8 Average-mAP points and the to-date best performing method by 3 Average-mAP points on the test set of SoccerNet. Noticeably, this result is achieved without any matching strategy between predictions and ground truth, and without post-processing steps like non-maxima-suppression.</p><p>We also conduct a grid search to find the optimal clip duration T , as done in Giancola et al. <ref type="bibr" target="#b2">[3]</ref> on the SoccerNet baselines (and reported in <ref type="table" target="#tab_0">Table II</ref>). Also with our architecture, the optimal clip duration is 20 seconds.</p><p>Per-class performances. In <ref type="figure" target="#fig_4">Fig. 4</ref> we report the AP computed for each class, as a function of the spotting tolerance. The best performing events are goals, with a margin of around 5 AP points compared to the substitution class when allowing high spotting tolerances. This is in line with previous literature, and can be attributed to the richness of visual cues which are usually found after a goal event (e.g. celebration, replays). Card events are, instead, the most difficult to spot, as the presence of a yellow or red card is the only visual indication that can distinguish those events from a general foul. Finally, it can be observed that goals are also the events which can be localized with the greatest temporal precision, compared to the other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation studies</head><p>Removing key components. We investigate the role of the key components of our approach by conducting an ablation study. Firstly, we assess the role of employing a uniformly distributed normalized relative offset in foreground clips. To this aim, we modify our data sampling strategy by considering only the chunks that contain an event in the middle of the clip (thus, with a normalized relative offset of 0.5), and removing the regression output. In this case, we still balance training batches by ensuring a uniform number of clips for each class, including background. At prediction time, we assume a normalized relative offset of 0.5. Noticeably, this evaluation setting is similar to the one adopted in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b4">[5]</ref>. As can be seen from <ref type="table" target="#tab_0">Table III</ref>, this leads to a significant performance drop, thus testifying the need for uniformly distributing relative offsets.</p><p>In a second ablation experiment, we instead maintain the original data sampling strategy and exclude the regression branch, to estimate its contribution to the final performance. Also in this case, we assume that the relative temporal offset of a predicted event is always 0.5. This leads to a drop of the test Average-mAP of almost 10 points, as reported in <ref type="table" target="#tab_0">Table III</ref>. <ref type="figure" target="#fig_5">Fig. 5</ref> also shows a more detailed comparison for different values of the tolerance threshold δ using this regression-free model (in blue) and our full proposal (in red). We notice that, for high values of δ, the resulting mAP is similar for both models. When δ decreases (and in particular when δ is lower than the clip duration) our model avoids an abrupt decrease of the mAP, which instead occurs when removing the regression branch. This confirms that the regression loss can increase the localization accuracy of the prediction.</p><p>Further, we also test the contribution given by the masking strategy. When removing the masking, we observe a decrease of 1.5 Average-mAP points on the test set, as reported in <ref type="table" target="#tab_0">Table III</ref>. We also underline how masking represents a costfree improvement, which does not require any additional resource or model parameter.</p><p>Masking analysis. In <ref type="table" target="#tab_0">Table IV</ref> we show how performances vary when changing the masking probability p and the maximum relative temporal offset q for masking. We achieve the  best configuration with p = 1/3 and q = 0.5, which means that clips with the event in their first half have a 1/3 probability of being masked during training. Keeping q = 0.5 fixed and varying p, the Average-mAP always exceeds the performance of the masking-free model (64.0 Avg-mAP) except for p = 1.</p><p>When p = 1, indeed, a clip is always masked if it has the event in the first half. This creates a big gap between the train and test distributions and performances drop as expected. Similarly, the model is robust to different values of q, except when all the clips have probability p = 1/3 to be masked, independently of where the event lies inside it (i.e. when q = 1). In this case, clips with too little context after the event can be masked too, resulting in lower Average-mAP. One can question if the majority of visual cues actually occurs just after the event, and not before. <ref type="table" target="#tab_4">Table V</ref> shows the results when masking frames just after the event with different values of p, while always keeping the frames before the event. Under this setting, masking is not beneficial and lowers the final performance.</p><p>Additional analysis. Finally, we look for the best value of hyperparameter λ, which controls the relative weight of the regression loss with respect to the classification loss (see Eq. 3). <ref type="figure" target="#fig_6">Fig. 6</ref> shows the test Average-mAP when varying λ in the interval between 0 and 100. The best value is achieved  with λ = 10. When λ = 0 there is no temporal offset regression in training, and we fall back in a setting similar to that of <ref type="table" target="#tab_0">Table III</ref>, third row. In this case, however, the relative temporal offset is still predicted by the network (without any supervision) and not fixed to the clip center timestamp. The gap between the setting with λ = 0 and λ = 1 exceeds 10 Average-mAP points. Performances decrease, instead, when λ &gt; 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Changing the convolutional backbone</head><p>Further, we present additional results when changing the convolutional backbone used for feature extraction, and when fine-tuning part of it during the training of the action spotting model. Here, we employ different variants of ResNet, pretrained on ImageNet. For assessing the role of fine-tuning, only the parameters belonging to the last residual block and our model are trained, while the other parameters of the backbone are kept fixed, as we did not observe significant improvements when fine-tuning larger portions of the backbone. <ref type="table" target="#tab_0">Table VI</ref> shows the performances obtained when finetuning ResNet-18, ResNet-50 and ResNet-152. We train these models on two RTX 2080Ti GPUs, with a batch size of 24 clips and a base learning rate of 0.025. RGB frames are extracted from the low resolution videos (224 × 398) at a rate of 2 fps, thus keeping the entire frame instead of center cropping as done in the features released with SoccerNet. No spatial augmentation is performed, and all the other implementation details are kept unchanged.</p><p>When using ResNet-152 as our backbone, we observe a boost of 9.6 Average-mAP points on the test set when compared to our best performing model trained on the precomputed ResNet-152 features. A similar performance level is obtained when finetuning ResNet-50, while a more significant  performance loss is visible when using ResNet-18. While precomputed features are a good starting point for research and comparison purposes, our findings underline that end-to-end training still guarantees a significant performance boosting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative analysis</head><p>Finally, we qualitatively assess the spotting capabilities of our model. For this purpose, we extract frames from the raw video with a resolution of 2 fps and create overlapping clips having a length of T frames, with stride 1. For each clip, our model predicts the event temporal offset and its label, and we count the number of times a frame is predicted as a spot. As can be seen from <ref type="figure" target="#fig_7">Fig. 7</ref>, the most voted frame index (the peak of the blue plot) is often very close to the ground truth spot (highlighted in red), and usually lies in an interval of 10 frames around it, corresponding to the lowest tolerance value δ which is considered for the Average-mAP computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a novel approach for spotting relevant actions inside soccer matches. Our proposal is a lightweight network including a simple stack of temporal operations after the extraction of frame-level features, and which jointly predicts classification scores and temporal offsets. The training of the network is paired with a masking strategy, which constraints the network to focus on the most relevant regions of the input chunks. Further, we devised an effective data sampling and balancing strategy which increases the recognition performance. The proposed approach has been extensively tested on SoccerNet, demonstrating the appropriateness of the components and the design choices. Our full model surpasses the state of the art by a significant margin when using the same features. Finally, we have conducted an experimental evaluation on the use of different backbones, which leads to an increase of about 10 Average-mAP points over the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>We propose a lightweight and effective network for spotting relevant events in soccer matches. Our model features a masking strategy which increases training performance by constraining the network to focus on the most relevant parts of the video and a data sampling solution which handles class imbalance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Architecture of RMS-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc><ref type="bibr" target="#b2">[3]</ref> and a first M(p, q) Goal t Masking strategy. Frames before the event are masked with probability p, if the event lies before the offset q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the architectural details of the model, including the number of input and output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Per-class Average Precision, as a function of spotting tolerance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Mean average precision when varying the tolerance δ in the interval between 5 and 60 seconds, for our complete model (red curve) and when removing the offset regression branch (blue curve). The Average-mAP gain can be quantified as the area under the red curve and above the blue one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Performance when varying the regression weight λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative results. The ground truth action timestamp is shown in red, while the blue curve shows the number of times a frame index was predicted as spot. A goal, a substitution and a card events are shown, from top to bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Structure of the proposed architecture.</figDesc><table><row><cell>Layer</cell><cell>Input Channels</cell><cell>Output Channels</cell></row><row><cell>FC 1</cell><cell>512</cell><cell>256</cell></row><row><cell>Conv 1</cell><cell>9 × 256</cell><cell>256</cell></row><row><cell>Conv 1</cell><cell>9 × 256</cell><cell>128</cell></row><row><cell>Dropout</cell><cell>-</cell><cell>-</cell></row><row><cell>Max over time</cell><cell>-</cell><cell>-</cell></row><row><cell>FC 2</cell><cell>128</cell><cell>64</cell></row><row><cell>FC cls</cell><cell>64</cell><cell>C (number of classes)</cell></row><row><cell>FCregr</cell><cell>64</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Comparison with baselines and state-of-the-art approaches.</figDesc><table><row><cell>Model</cell><cell>Clip length (s)</cell><cell>Features</cell><cell cols="2">Val Avg-mAP Test Avg-mAP</cell></row><row><cell>SoccerNet baseline [3]</cell><cell>5</cell><cell>ResNet-152 (PCA)</cell><cell>-</cell><cell>34.5</cell></row><row><cell>SoccerNet baseline [3]</cell><cell>60</cell><cell>ResNet-152 (PCA)</cell><cell>-</cell><cell>40.6</cell></row><row><cell>SoccerNet baseline [3]</cell><cell>20</cell><cell>ResNet-152 (PCA)</cell><cell>-</cell><cell>49.7</cell></row><row><cell>Vanderplaetse et al. [28]</cell><cell>20</cell><cell>ResNet-152 (PCA) + Audio</cell><cell>-</cell><cell>56.0</cell></row><row><cell>Vats et al. [5]</cell><cell>15</cell><cell>ResNet-152 (PCA)</cell><cell>-</cell><cell>60.1</cell></row><row><cell>Cioppa et al. [6]</cell><cell>120</cell><cell>ResNet-152 (PCA)</cell><cell>-</cell><cell>62.5</cell></row><row><cell>Ours</cell><cell>20</cell><cell>ResNet-152 (PCA)</cell><cell>67.8</cell><cell>65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance of the proposed model when removing key components.</figDesc><table><row><cell>Model</cell><cell cols="2">Val Avg-mAP Test Avg-mAP</cell></row><row><cell>Ours</cell><cell>67.8</cell><cell>65.5</cell></row><row><cell>Ours w/o uniformly distributed offsets</cell><cell>48.7</cell><cell>46.2</cell></row><row><cell>Ours w/o offset regression branch</cell><cell>58.5</cell><cell>55.7</cell></row><row><cell>Ours w/o masking</cell><cell>66.5</cell><cell>64.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Performance when varying the masking probability p and the maximum relative temporal offset q for masking.</figDesc><table><row><cell>p</cell><cell>q</cell><cell cols="2">Val Avg-mAP Test Avg-mAP</cell></row><row><cell>1/5</cell><cell>0.5</cell><cell>66.8</cell><cell>64.6</cell></row><row><cell>1/4</cell><cell>0.5</cell><cell>67.0</cell><cell>64.4</cell></row><row><cell>1/3</cell><cell>0.5</cell><cell>67.8</cell><cell>65.5</cell></row><row><cell>1/2</cell><cell>0.5</cell><cell>67.1</cell><cell>64.4</cell></row><row><cell>1</cell><cell>0.5</cell><cell>64.7</cell><cell>60.7</cell></row><row><cell>1/3</cell><cell>0.1</cell><cell>65.5</cell><cell>63.4</cell></row><row><cell cols="2">1/3 0.25</cell><cell>67.4</cell><cell>64.7</cell></row><row><cell>1/3</cell><cell>0.5</cell><cell>67.8</cell><cell>65.5</cell></row><row><cell cols="2">1/3 0.75</cell><cell>66.5</cell><cell>64.0</cell></row><row><cell>1/3</cell><cell>1</cell><cell>64.7</cell><cell>62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Performance when varying p, keeping q = 0.5 fixed and masking frames after the event.</figDesc><table><row><cell></cell><cell>p</cell><cell>q</cell><cell cols="4">Val Avg-mAP Test Avg-mAP</cell></row><row><cell></cell><cell cols="2">1/5 0.5</cell><cell>65.2</cell><cell></cell><cell>62.5</cell></row><row><cell></cell><cell cols="2">1/4 0.5</cell><cell>64.6</cell><cell></cell><cell>62.9</cell></row><row><cell></cell><cell cols="2">1/3 0.5</cell><cell>63.8</cell><cell></cell><cell>61.8</cell></row><row><cell></cell><cell cols="2">1/2 0.5</cell><cell>61.4</cell><cell></cell><cell>60.7</cell></row><row><cell></cell><cell>1</cell><cell>0.5</cell><cell>54.1</cell><cell></cell><cell>54.1</cell></row><row><cell></cell><cell>66</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Avg-mAP</cell><cell>58 60 62</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>λ</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Performance analysis when finetuning different variants of ResNet.</figDesc><table><row><cell>Model</cell><cell>Pre-train</cell><cell cols="2">Val Avg-mAP Test Avg-mAP</cell></row><row><cell>ResNet-18 + Our</cell><cell>ImageNet</cell><cell>73.8</cell><cell>70.9</cell></row><row><cell>ResNet-50 + Our</cell><cell>ImageNet</cell><cell>76.6</cell><cell>74.9</cell></row><row><cell cols="2">ResNet-152 + Our ImageNet</cell><cell>77.5</cell><cell>75.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has been partially funded by Metaliquid Srl, Milano (Italy). We also acknowledge NVIDIA AI Technology Center for providing technical support and computational resources used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multigrid method for efficiently training video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soccernet: A scalable dataset for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dghaily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic annotation and transcoding of soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Event detection in coarsely annotated sports videos via parallel multi-receptive field 1d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="882" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A context-aware loss function for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deliege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action classification in soccer videos with long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="154" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic soccer video event detection based on a deep neural network combined cnn and rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="490" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Goal!! event detection in sports video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsagkatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsakalides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="15" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved soccer action spotting using both audio and video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanderplaetse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="896" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Comprehensive dataset of broadcast soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Comprehensive soccer video understanding: Towards human-comparable video understanding system in constrained environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04465</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
