<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXPR at SemEval-2018 Task 9: A Combined Approach for Hypernym Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 5-6, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Issa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Aldine</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Lebanese University</orgName>
								<address>
									<country key="LB">Lebanon</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounira</forename><surname>Harzallah</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">LINA -University of Nantes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berio</forename><surname>Giuseppe</surname></persName>
							<email>giuseppe.berio@univ-ubs.frnicolas.bechet@irisa.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Béchet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Faour</surname></persName>
							<email>ahmad.faour@ul.edu.lb</email>
							<affiliation key="aff2">
								<orgName type="institution">Lebanese University</orgName>
								<address>
									<country key="LB">Lebanon</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IRISA -University Bretagne Sud</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EXPR at SemEval-2018 Task 9: A Combined Approach for Hypernym Discovery</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
						<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018) <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="919" to="923"/>
							<date type="published">June 5-6, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present our proposed system (EXPR) to participate in the hypernym discovery task of SemEval 2018. The task addresses the challenge of discovering hyper-nym relations from a text corpus. Our proposal is a combined approach of path-based technique and distributional technique. We use dependency parser on a corpus to extract candidate hypernyms and represent their dependency paths as a feature vector. The feature vector is concatenated with a feature vector obtained using Wikipedia pre-trained term embedding model. The concatenated feature vector fits a supervised machine learning method to learn a classifier model. This model is able to classify new candidate hypernyms as hyper-nym or not. Our system performs well to discover new hypernyms not defined in gold hy-pernyms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hypernymy is an important lexical-semantic relation that is useful for many applications such as question answering, machine translation, information retrieval, and so on. In addition, hypernym relations are the backbone for building ontologies.</p><p>Various methods have been proposed to detect hypernym relation from text corpora. Most of these techniques are either path-based techniques or distributional techniques. In path-based methods, the detection of hypernym relations is based on the lexico-syntactic paths connecting a pair of terms in a corpus. Conversely, distributional methods are based on the distribution of term pair contexts. Most of these methods were unsupervised. Recently, focus shifted towards supervised methods.</p><p>This task inherits complexity and is far from being solved. The SemEval organizers address the same task but with a novel formulation <ref type="bibr" target="#b2">(Camacho- Collados et al., 2018</ref>). They reformulate the task from hypernym detection into hypernym discovery. This novel formulation makes the task more realistic in terms of actual downstream application, while also enabling the benefits of information retrieval evaluation metrics. Hypernym detection focuses on deciding whether a hypernymic relation holds between a given pair of terms or not. Hypernym discovery focuses on discovering a set containing the best hypernyms for a given term from a given vocabulary search space. The task is divided into two subtasks: General-Purpose Hypernym Discovery and Domain-Specific Hypernym Discovery. The first consists of discovering hypernym in a general-purpose corpus, thus the SemEval organizers provide the participants with data for three languages: English, Italian, and Spanish. The second consists of discovering hypernym in a domain-specific corpus, thus they provide the participants with data for two specific domains: Medical and Music. The data contains a list of training terms along with gold hypernyms, a list of testing terms, and a vocabulary search space. The term is either a concept or an entity.</p><p>To tackle this task, we propose an approach that combines a path-based technique and distributional technique via concatenating two feature vectors: a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings. Then, by using the concatenated vector we create a binary supervised classifier model based on support vector machine (SVM) algorithm. The model predicts if a term and its candidate hypernym are hypernym related or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most of the previous approaches for hypernymy detection are either path-based (patterns) or distributional based. Recently, some approaches are taking advantages of the combination of pathbased and distributional techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Path-Based</head><p>Path-based approaches are heuristic methods that predict hypernymy between a pair of terms if they match a particular pattern in a sentence of the corpus. These patterns are either manually identified <ref type="bibr" target="#b3">(Hearst, 1992)</ref> or automatically extracted ( <ref type="bibr" target="#b19">Snow et al., 2005;</ref><ref type="bibr" target="#b12">Navigli and Velardi, 2010;</ref><ref type="bibr" target="#b17">Sheena et al., 2016)</ref>. Approaches based on handcrafted patterns yield a good precision, but their recall is very low ( <ref type="bibr" target="#b1">Buitelaar et al., 2005</ref>). Approaches based on automatic learning of patterns achieve better performance by a small improvement in terms of precision and a considerable improvement in terms of recall, but the main limitation of these approaches is the sparsity of the feature space ( <ref type="bibr" target="#b18">Shwartz et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributional</head><p>Distributional approaches predict hypernym relations between terms based on their distributional representation, by either unsupervised or supervised models. The early unsupervised distributional models are based on symmetric measures <ref type="bibr" target="#b7">(Lin, 1998)</ref>. Later, asymmetric measures are introduced based on the Distributional Inclusion Hypothesis (DIH) ( <ref type="bibr" target="#b21">Weeds and Weir, 2003;</ref><ref type="bibr" target="#b6">Kotlerman et al., 2010</ref>). More recent, <ref type="bibr" target="#b16">Santus et al. (2014)</ref>; <ref type="bibr" target="#b14">Rimell (2014)</ref> introduce new measures based on assumption that DIH is not correct for all cases. While, most of the supervised models rely on term embedding ( <ref type="bibr" target="#b10">Mikolov et al., 2013;</ref><ref type="bibr">Penning- ton et al., 2014</ref>) to represent the feature vector between the terms x and y. Various vector representations have been used such as concatenation x ⊕ y ( <ref type="bibr" target="#b0">Baroni et al., 2012</ref>) and difference y − x ( <ref type="bibr" target="#b15">Roller et al., 2014;</ref><ref type="bibr" target="#b20">Weeds et al., 2014</ref>). More recent, <ref type="bibr" target="#b22">Yu et al. (2015)</ref>; <ref type="bibr" target="#b8">Luu et al. (2016)</ref> suggested that models rely on term embedding are useful to indicate similarity between words, not to indicate hypernymy relations. Consequently, they learn their own term embedding models that are more relevant to indicate hypernym relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combined Approaches</head><p>Combined approaches of distributional and lexicosyntactic paths are proposed based on the assumption that distributional approaches and path-based approaches have certain complementary properties. To our best knowledge, there are little works on integrating them ( <ref type="bibr" target="#b11">Mirkin et al., 2006;</ref><ref type="bibr" target="#b5">Kaji and Kitsuregawa, 2008)</ref>. The recent work on integrating them is proposed by <ref type="bibr" target="#b18">Shwartz et al. (2016)</ref>. They use a long short-term memory (LSTM) network <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>) to encode dependency paths into a feature vector, then they concatenate the feature vector by the term embedding vectors of term x and term y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Description</head><p>As a preliminary step, we split each corpus into a training corpus and a testing corpus. Training corpus is a corpus of all sentences that contains training data terms (Concept/Entity), while testing corpus is a corpus of all sentences that contains testing data terms (Concept/Entity). Some sentences may contain training and testing data terms. These sentences will exist in both training and testing corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Hypernyms</head><p>The first step in the system is to extract candidate hypernyms for the given training and testing data terms from a training corpus and a testing corpus respectively. We consider a term as a candidate hypernym if:</p><p>1. The term and its candidate occur in the same sentence.</p><p>2. The candidate exists in the vocabulary list.</p><p>3. The term and its candidate are noun phrases.</p><p>4. The term and its candidate are linked by short dependency path.</p><p>We consider a dependency path as short if it doesn't exceed two grammatical dependency relations. Using the short dependency path, we are capable representing paths similar to Hearst Patterns and other patterns. For example of short dependency paths, the dependency path between X and Y in the sentence S 1 "X such as Y " is {nmod:such as(X , Y )} and in the sentence S 2 "X includes Y " is {nsubj(includes , X), dobj(includes , Y )}. We use Stanford dependency parser <ref type="bibr">1 (Marneffe et al., 2006</ref>) to extract dependency paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Vector</head><p>The feature vector used to learn a model capable of predicting hypernym relations between a term and a candidate hypernym consists of the concatenation of two vectors: the first one is a vector extracted using a path-based technique while the second is extracted using a distributional technique.</p><p>The path-based vector consists of a set of features representing the short dependency path between a term y and its candidate hypernym x.</p><p>The</p><note type="other">feature set is: [T ag(x), GRel(x), HR, F req, T ag(y), GRel(y)]. T ag(x) and T ag(y) are the POS tag of x and y, GRel(x) and GRel(y) are the grammatical dependency relation of x and y, HR is the hypernym ratio of a dependency path and it is equal to the number of occurrences of a dependency path when indicating hypernm relation divided by the total occurrences of the same dependency path, and F req is the relative frequency of a dependency path and it is equal to the occurrence of a dependency path divided by the total occurrences of all dependency paths.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR =</head><p>hypernym DP occurrences DP occurrences F req = DP occurrences T otal DP s occurrences For a distributional based vector, We use pretrained 300 dimensional Word2Vec 2 term embeddings, trained on Wikipedia ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>). We apply the difference between the embedding vector of term y and the embedding vector of term x ( y − x)( <ref type="bibr" target="#b15">Roller et al., 2014;</ref><ref type="bibr" target="#b20">Weeds et al., 2014</ref>). The term is either a single word or a multi-word expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Learning and Hypernym Discovery</head><p>In each training corpus, we extract a set of candidate hypernyms for each training term and label them if they are hypernym related or not using the gold hypernym data. Next, we represent each term and its candidate hypernym by a concatenated feature vector. These concatenated vectors are used for training the model. The classification method we used is SVM 3 with RBF kernel (C = 1.0, gamma = 1/F eatureSize). The training dataset was unbalanced, the ratio of hypernym instances w.r.t. not hypernym is less than 0.05. To represent the two categories (hypernym and not hypernym) in the training set, we improved this ratio to 0.2 by random elimination of not hypernym instances (20% hypernym instances and 80% not hypernym instances). The classifier model is then used to discover hypernyms from a set of candidate hypernyms extracted from a testing corpus for each testing term by predicting if a term and its candidate hypernym are hypernym related or not. Each predicted hypernym is associated with a probability value. These values are used as ranking values to select the best fifteen hypernyms for each term (from higher to lower probability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>We submit our systems predictions for three corpora: English, Medical, and Music. The table 1 (a,b and c) below shows the result of our system and other supervised systems to discover hypernyms for Concept terms only. For the three corpora, our system performs better than STJU system, and it performs better than the MFH system on the English corpora. In addition, the result shows that our system performs well in discovering new hypernyms not defined in the gold hypernyms where it yields good False Positive values in the three corpora and we achieve the best False Positive value in Medical corpus (40) with a large difference to the second value <ref type="formula">(20)</ref>   The evaluation results of our system and other supervised systems.</p><p>Our system result was beneath the expectation. By a short look into the output result files, we notice a lot of empty lines, meaning that our system was unable to discover any hypernym for a lot of terms and unexpectedly these terms correspond to all entity terms. In other words, our system lacks the ability to discover hypernyms for entity terms.</p><p>The table 2 (a,b and c) below shows the coverage of Wikipedia pre-trained term embedding model (TEM) and the coverage of candidate hypernym extraction (CHE) for the training and testing terms of the three corpora (English, Medical, and Music). The table shows that our system is unable to discover hypernyms for a considerable number of terms due to two main reasons. The first reason is that Wikipedia pre-trained term embedding model is limited in coverage, where many terms (Concepts/Entities) are not covered by the pre-trained embeddings, which leads to failure to discover hypernyms for these terms. For example, the term embedding (TEM) coverage of Medical Testing terms is 249 (50%), which means the system is unable to discover hypernyms for 251 (50%) terms not covered by the pre-trained term embedding. The second reason is that some conditions used to extract candidate hypernyms restrict the number of candidate hypernyms. For instance, the condition of the existence of a short dependency link between the term and its candidate causes the system to miss many candidate hypernyms if they are not linked by a short dependency path with the terms. In addition, the term and its candidate hypernym must occur as noun phrases in the sentence. This condition leads to failure to extract candidate hypernyms for some entity terms that can't be identified as noun phrases in the corpus such as "Up All Night", "Someday Came Suddenly", "Now What", etc. As shown in the table 2, the candidate hypernym extraction (CHE) coverage for English testing terms is 950 (63%), that means our system is unable to extract any candidate hypernym for 550 (37%) terms (398 entities and 152 concepts). Furthermore, our system suffers from a major computational issue when applied to a large corpus. Parsing the corpus took to long and failed to complete before the submission deadline. Approximately, we processed 50% sentences of English corpus and 80% sentences of Music corpus, while we processed all sentences of Medical corpus. This explains why the performance of our  The coverage of wikipedia pre-trained term embedding model and candidate hypernym extraction.</p><p>system on Medical corpus is better than its performance on the two others corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented our proposed system (EXPR) that is a combination of path-based technique and distributional technique to participate in Hypernym Discovery task of SemEval 2018. In this work, two feature vectors were extracted and concatenated: the first one is obtained using dependency parser on sentences and the second vector is obtained using pre-trained term embedding. A supervised classifier model based on SVM is built using training dataset composed of concatenated vectors. This model is used to discover hypernyms for new terms. The result was good but didnt fulfill our ambition due to several issues. Our future work is to improve our approach for hypernym discovery by solving several issues. We believe that relying on term embedding model learned from the corpus provided in this task may be a good choice. In addition, we will work on the definition of a new dependency links not only those defined in this paper. Also, we will work to propose an unsupervised approach by using sequential pattern mining technique to automatically extract frequent sequential pattern between hyponym terms and their given hypernyms from the corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>achieved by CRIM system.</figDesc><table>Systems 
MAP 
MRR 
P@1 
P@3 
P@5 
P@15 
False + 
CRIM 
16.08 
30.04 
23.94 
17.23 
15.41 
14.88 
20 
MSCG 
9.36 
18.9 
13.81 
10.67 
9.38 
8.31 
28 
UMDuluth 
8.13 
18.93 
15.33 
8.83 
7.53 
7.07 
20 
NLP HZ 
7.17 
13.13 
8.99 
7.69 
7.11 
6.71 
24 
Vanilla 
6.99 
16.05 
12.3 
7.69 
6.55 
6.18 
Begab 
6.41 
13.92 
9.74 
6.75 
6.33 
5.86 
24 
EXPR 
4.94 
11.64 
10.12 
5.27 
4.52 
4.28 
16 
MFH 
4.73 
12.48 
11.92 
4.84 
4.13 
3.93 
SJTU 
3.29 
5.68 
0.28 
3.45 
3.57 
3.54 
0 

(a) English Corpus. 

Systems 
MAP 
MRR 
P@1 
P@3 
P@5 
P@15 
False + 
CRIM 
34.05 
54.64 
49.2 
40.13 
36.77 
27.1 
20 
MFH 
28.93 
35.8 
32.6 
34.27 
34.2 
21.39 
Begab 
20.75 
40.6 
31.6 
23.5 
21.43 
17.05 
16 
Vanilla 
18.84 
41.07 
35.4 
27.07 
20.71 
12.4 
EXPR 
13.77 
40.76 
38.2 
17.17 
12.76 
9.34 
40 
STJU 
11.69 
25.95 
15.2 
13.57 
11.69 
10.24 
12 

(b) Medical Corpus. 

Systems 
MAP 
MRR 
P@1 
P@3 
P@5 
P@15 
False + 
CRIM 
43.38 
63.79 
52.79 
47.16 
43.87 
40.14 
24 
MFH 
33.56 
56.82 
46.65 
38.41 
35.22 
27.47 
Begab 
23.52 
39.26 
24.02 
23.23 
22.66 
23.13 
16 
Vanilla 
11.53 
35.78 
31.28 
13.59 
10.28 
8.46 
EXPR 
6.74 
20.15 
15.64 
9.22 
6.65 
4.64 
20 
SJTU 
4.71 
9.15 
2.23 
4.98 
4.91 
4.67 
4 

(c) Music Corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://stanfordnlp.github.io/CoreNLP/</note>

			<note place="foot" n="2"> https://radimrehurek.com/gensim/ 3 We use a machine learning python library scikit-learn (http://scikit-learn.org/stable/)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entailment above the 922 word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ontology learning from text: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ontology Learning from Text: Methods</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vered Shwartz, Roberto Navigli, and Horacio Saggion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
		<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018)<address><addrLine>New Orleans, LA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018 Task 9: Hypernym Discovery</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Computational Linguistics</title>
		<meeting>the 14th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using hidden markov random fields to combine distributional and pattern-based word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NLE</title>
		<imprint>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning term embeddings for taxonomic relation identification using dynamic weighting neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006)</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation (LREC 2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating pattern-based and distributional similarity methods for lexical entailment acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Shachar Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING and ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning word-class lattices for definition and hypernym extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine Schulte Im</forename><surname>Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic extraction of hypernym and meronym relations in english sentences using dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sheena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smitha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelbi</forename><surname>Jasmine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procedia Computer Science</title>
		<meeting>edia Computer Science</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno>abs/1603.06076</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMLP</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning term embeddings for hypernymy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJ-CAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJ-CAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1390" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
