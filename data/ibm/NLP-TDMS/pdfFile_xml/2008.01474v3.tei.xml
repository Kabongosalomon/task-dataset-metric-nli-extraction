<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-21">21 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengzuo</forename><surname>Huang</surname></persName>
							<email>hmengzuo@mail.dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Netease Games AI Lab</orgName>
								<address>
									<settlement>HangZhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
							<email>lifeng06@corp.netease.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Netease Games AI Lab</orgName>
								<address>
									<settlement>HangZhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuhe</forename><surname>Zou</surname></persName>
							<email>zouwuhe@corp.netease.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Netease Games AI Lab</orgName>
								<address>
									<settlement>HangZhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhang</surname></persName>
							<email>zhangweidong02@corp.netease.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Netease Games AI Lab</orgName>
								<address>
									<settlement>HangZhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-21">21 Dec 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dialogue systems in open domain have achieved great success due to the easily obtained single-turn corpus and the development of deep learning, but the multi-turn scenario is still a challenge because of the frequent coreference and information omission. In this paper, we investigate the incomplete utterance restoration which has brought general improvement over multi-turn dialogue systems in recent studies. Meanwhile, jointly inspired by the autoregression for text generation and the sequence labeling for text editing, we propose a novel semi autoregressive generator (SARG) with the high efficiency and flexibility. Moreover, experiments on two benchmarks show that our proposed model significantly outperforms the state-of-the-art models in terms of quality and inference speed. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Dialogue systems in open-domain have attracted increasing attention <ref type="bibr" target="#b17">(Li 2020;</ref><ref type="bibr" target="#b14">Huang, Zhu, and Gao 2020)</ref>, and been widely utilized in real-world applications <ref type="bibr" target="#b0">(Adiwardana et al. 2020;</ref><ref type="bibr" target="#b11">Gong et al. 2019;</ref><ref type="bibr" target="#b12">Hewitt and Beaver 2020)</ref>. However, due to frequently occurred coreference and information omission, as shown in <ref type="table">Table 1</ref>, there still exists a major challenge: it is hard for machines to understand the real intention from the original utterance without the context. A series of models of retrieval-based and generative-based have been studied for multi-turn systems <ref type="bibr" target="#b34">(Yan, Song, and Wu 2016;</ref><ref type="bibr" target="#b36">Zhang et al. 2018;</ref><ref type="bibr" target="#b39">Zhou et al. 2018;</ref>, and they generally combine the context and the original utterance as input to retrieve or generate responses. However, these methods lack great generalizations since they have a strong reliance on the size of the multi-turn corpus. <ref type="bibr" target="#b27">Su et al. 2019 and</ref><ref type="bibr" target="#b21">Pan et al. 2019</ref> propose their utterance restoration models, respectively, which are aimed at restoring the semantic information of the original utterance based on the history of the session from a different perspective. Restoration methods decouple multi-turn systems into the single-turn problems, which alleviate the dependence on <ref type="table">Table 1</ref>: An example of utterance restoration in humanmachine dialogue system. Utterance 3 ′ is the restored sentence based on Utterance 3. Red means coreference and blue means omission. multi-turn dialogue corpus and also achieve leading performance. Specifically, <ref type="bibr" target="#b27">Su et al. 2019</ref> employ transformerbased Seq2Seq architecture and pointer network to rewrite the original utterance, and they split the whole session into history and original utterance for capturing different attentions. <ref type="bibr" target="#b21">Pan et al. 2019</ref> propose a cascade frame of "pick-andcombine" to restore the incomplete utterance from history. And both of them generate restored utterance from scratch in an autoregressive manner of Seq2Seq, which is highly timeconsuming during inference.</p><p>Unlike some traditional end-to-end text generation task, where the apparent disparity exists between the sources and targets, utterance restoration always has some considerable overlapping regions between inputs and outputs. Intuitively, some sequence labeling methods can be utilized to speed up the inference stage in this task, since Seq2Seq from scratch is time wasteful. Further, <ref type="bibr" target="#b19">Malmi et al. 2019</ref> introduce LaserTagger, a sequence labeling method, which casts text generation as a text editing task. However, the insertions of LaserTagger are restricted to a fixed phrase vocabulary that is derived from the training data. In multi-turn dialogue, some rare phrases are habitually omitted by the speaker without affecting the listening comprehension; as shown in <ref type="table">Table 1</ref>, "Li Chunfeng" is a rare phrase and omitted in Utterance 3 of Context 1. And LaserTagger can not solve such a coreference problem well, since the rare phrase is discarded when constructing the fixed phrase vocabulary.</p><p>As a first attempt to combine the sequence labeling and autoregression in utterance restoration, we propose a semi autoregressive generator (SARG), which can well tackle the challenges brought by highly time-consuming and discarded rare words or phrases. SARG retains the flexibility of autoregression and takes advantage of the fast inference speed of sequence labeling.</p><p>First, we employ a tagger to predict the editing labels, which involves three main operations: KEEP a token, DELETE a token, CHANGE a token with other phrases. Then, instead of adding phrases from a pre-defined phrase vocabulary, we utilize an autoregressive decoder based on LSTM with copy mechanism for generating the added phrases. Moreover, inspired by the great success of the pretrained transformer models <ref type="bibr" target="#b29">(Vaswani et al. 2017)</ref>, we also design an encoder based on BERT <ref type="bibr" target="#b7">(Devlin et al. 2018</ref>) to obtain the contextual encodings. Finally, we perform experiments on two benchmarks: the Restoration-200k <ref type="bibr" target="#b21">(Pan et al. 2019)</ref> and CANARD <ref type="bibr" target="#b9">(Elgohary, Peskov, and Boyd-Graber 2019)</ref>, the SARG shows superiorities on the automatic evaluation, the human evaluation, and the inference speed respectively. In summary, our contributions are:</p><p>• SARG is a creative fusion of sequence labeling and autoregressive generation, which is suitble for utterance restoration task;</p><p>• SARG solves the restoration problem by a joint way and can easily load the pretrained BERT weights for the overall model;</p><p>• SARG obtains a competitive performance and faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-turn Dialogue systems</head><p>Recently, building a chatbot with data-driven approaches in open-domain has drawn significant attention <ref type="bibr" target="#b23">(Ritter, Cherry, and Dolan 2011;</ref><ref type="bibr" target="#b15">Ji, Lu, and Li 2014;</ref><ref type="bibr" target="#b1">Athreya, Ngonga Ngomo, and Usbeck 2018)</ref>. Most of works on conversational systems can be divided into retrieval-based methods <ref type="bibr" target="#b15">(Ji, Lu, and Li 2014;</ref><ref type="bibr" target="#b34">Yan, Song, and Wu 2016;</ref><ref type="bibr" target="#b30">Wu, Wang, and Xue 2016;</ref><ref type="bibr" target="#b39">Zhou et al. 2018;</ref><ref type="bibr" target="#b36">Zhang et al. 2018</ref>) and generation-based methods <ref type="bibr" target="#b26">(Serban et al. 2016;</ref><ref type="bibr" target="#b33">Xing et al. 2016;</ref><ref type="bibr" target="#b25">Serban et al. 2017;</ref><ref type="bibr" target="#b38">Zhao, Xu, and Wu 2020;</ref><ref type="bibr" target="#b18">Lin et al. 2020</ref>). Though the above methods are enlightening, there is a lack of high-quality multi-turn dialogue data to train them.</p><p>In multi-turn dialogue systems, existing methods are still far from satisfactory compared to the single-turn ones, since the coreference and information omission frequently occur in our daily conversation, which makes machines hard to understand the real intention <ref type="bibr" target="#b27">(Su et al. 2019)</ref>. Recent studies suggest simplifying the multi-turn dialogue modeling into a single-turn problem by restoring the incomplete utterance <ref type="bibr" target="#b27">(Su et al. 2019;</ref><ref type="bibr" target="#b21">Pan et al. 2019)</ref>. <ref type="bibr" target="#b27">Su et al. 2019</ref> rewrite the utterance based on transformer-based Seq2Seq and pointer network from context with two-channel attentions. <ref type="bibr" target="#b21">Pan et al. 2019</ref> propose a cascaded "pick-and-combine" model to restore the incomplete utterance from its context. Moreover, <ref type="bibr" target="#b21">Pan et al. 2019</ref> release the high quality datasets Restoration-200k for the study of incomplete utterance restoration in open-domain dialogue systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Rewriting</head><p>Sentence rewriting is a general task which has high overlap between input text and output text, such as: text summarization <ref type="bibr" target="#b24">(See, Liu, and Manning 2017;</ref><ref type="bibr" target="#b4">Chen and Bansal 2018;</ref><ref type="bibr" target="#b3">Cao et al. 2018</ref>), text simplification <ref type="bibr" target="#b32">(Wubben, Krahmer, and van den Bosch 2012;</ref><ref type="bibr" target="#b35">Zhang and Lapata 2017)</ref>, grammatical error correction <ref type="bibr" target="#b20">(Ng et al. 2014;</ref><ref type="bibr" target="#b10">Ge, Wei, and Zhou 2018;</ref><ref type="bibr" target="#b5">Chollampatt and Ng 2018;</ref><ref type="bibr" target="#b37">Zhao et al. 2019</ref>) and sentence fusion <ref type="bibr" target="#b28">(Thadani and McKeown 2013;</ref><ref type="bibr" target="#b16">Lebanoff et al. 2019)</ref>, ect. Seq2Seq model, which provides a powerful framework for learning to translate source texts into target texts, is the main approach for sentence rewriting. However, conventional Seq2Seq approaches require large amounts of training data and take low-efficiency on inference. <ref type="bibr" target="#b19">Malmi et al. 2019</ref> propose a sequence labeling approach for sentence rewriting that casts text generation as a text editing task. And the method is fast enough at inference time with performance comparable to the state-of-the-art Seq2Seq models. However, it can't be applied to our incomplete utterance restoration well, due to some limitations of inflexibility.</p><p>To make full use of the flexibility of autoregressive models and the efficiency of sequence labeling models, we combine the autoregressive generation and the sequence labeling for the trade-off between inference time and model flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we demonstrate our proposed SARG for the multi-turn incomplete utterance restoration. The restoration problem can be denoted as f (H,</p><formula xml:id="formula_0">U ) = R, where H = {w h 1 , w h 2 , ..., w h m } is the history of dialogue (context), U = {w u 1 , w u 2 , .</formula><p>.., w u n } is the original utterance (source) to be rewritten and R is the restored utterance (target). The overall architecture of SARG is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Instead of generating the restored utterance from scratch as traditional Seq2Seq, we first determine the editing operation sequence across the original utterance; then generate the potential phrases according to the operation sequence; finally convert the operation sequence and the generated phrases to Label for supervised learning <ref type="figure" target="#fig_0">Figure 1</ref>: The overall architecture of the proposed SARG. In the constructed label, D means the DELETE operation, K means the KEEP, C means the CHANGE and the phrase of "Li Chunfeng" is the added phrase for this CHANGE operation. In the input, the blue words are the history of the session, the red words are the original utterance and the &lt;ui&gt; is the dummy token. In the dataflow, the black means encoding, the orange means tagging, the green means decoding, and the blue means the realization.</p><p>text. The detailed descriptions are as follows. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tagging Operations</head><p>First of all, meaningless dummy tokens are inserted between every two tokens in the original utterance, as shown in first column of <ref type="figure" target="#fig_0">Figure 1</ref>. We can directly add the phrases in the gaps between every two tokens by the insertion of dummy tokens, which eliminates the ambiguity of possible editing operations to some extent. Moreover, we recommend that the original tokens can only be kept or deleted, and the dummy tokens can only be deleted or changed by other phrases. Formally, three editing operations are defined in this work: KEEP, DELETE and CHANGE. Intuitively, KEEP means that the token remains in the restored utterance, DELETE means that the token is undesired, and CHANGE A means that the token should be replaced by the informative phrase A.</p><p>The following steps are employed to construct the supervised labels: (1) first compute the longest common subsequence (LCS) between original and restored utterance; (2) then greedily attempt to align the original utterance, restored utterance and the LCS; (3) finally replace the undesired tokens in original utterance with the added tokens in restored utterance. The detailed descriptions are demonstrated in Algorithm 1, and the constructed labels can be referenced in    <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, the first column of labels is used to supervise the tagger and other columns are used for the decoder. Moreover, the comparison of average length between added phrase and the restored utterance is listed in <ref type="table" target="#tab_1">Table 2</ref>, which indicates that SARG saves at least three-quarters of the time for decoding compared to those complete autoregressive model.</p><formula xml:id="formula_1">i ∈ [1, 2n + 1] do 6 if S[i] = K[k] then 7 L[i] = KEEP 8 while T [j] = K[k] do 9 A = A + T [j] 10 j = j + 1 11 end 12 k = k + 1 13 if A = ∅ then 14 L[i − 1] = CHANGE A 15 A = [ ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Since pretrained transformers <ref type="bibr" target="#b29">(Vaswani et al. 2017)</ref> have been shown to be beneficial in many downstream NLP tasks <ref type="bibr" target="#b22">(Radford et al. 2018;</ref><ref type="bibr" target="#b7">Devlin et al. 2018)</ref>, in this work, we utilize the standard transformer blocks as the backbone of the encoder, like the black lines in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In the embedding module, we concatenate the history H and the original utterance U (involved dummy tokens) as the input sequence W = {w 1 , w 2 , ..., w k }, then embed them into continuous space by looking up the following embedding tables: • Word Embedding: the word embedding table is built on a pre-defined wordpiece vocabulary from pretrained transformers.</p><p>• Position Embedding: the position embedding table is also initialized by pretrained transformers.</p><p>• Turn Embedding: turn embedding is used to indicate which turn each token belongs to. The looking-up table is randomly initialized. For each token w i , we sum and normalize (Ba, Kiros, and Hinton 2016) the above three embeddings, then acquire the input embedding:</p><formula xml:id="formula_2">E (0) i = LN(WE(w i ) + PE(w i ) + TE(w i )),<label>(1)</label></formula><p>where WE is the word embedding, PE is the position embedding and TE is the turn embedding. Once the input embedding is acquired, we feed such representation into the L stacked transformer blocks:</p><formula xml:id="formula_3">E (l) = TransformerBlock(E (l−1) ).<label>(2)</label></formula><p>At last, we obtain the final encodings E (L) , which can be further divided into two parts according to the partitions of history and original utterance:</p><formula xml:id="formula_4">E h = {h 1 , h 2 , · · · , h m },<label>(3)</label></formula><formula xml:id="formula_5">E u = {u 1 , u 2 , · · · , u 2n+1 },<label>(4)</label></formula><p>where E h is the encodings of history and E u is the encodings of original utterance. There are n + 1 dummy tokens in the original utterance, which collect the information from those original tokens by the self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tagger</head><p>Tagger takes the encodings E u as the input and predicts the editing labels on each token in original utterance. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the orange lines stand for the dataflow of tagger. In our setting, a single linear transformation layer with softmax activation function is employed for projecting the encoding to the space of editing labels, the formula is as follows:</p><formula xml:id="formula_6">p(y i |u i ) = softmax(W t · u i + b t ),<label>(5)</label></formula><p>where W t and b t are parameters to be learned, and the following W and b are all learnable. Finally, the loss provided by the tagger is defined as negative log-likelihood:</p><formula xml:id="formula_7">loss tag = − i log p(y i |u i ),<label>(6)</label></formula><p>where i is corresponding to the index of token in original utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>Different from the general autoregressive decoder that performs decoding from scratch, in our setting, the decoder, as green lines in <ref type="figure" target="#fig_0">Figure 1</ref>, works in parallel on the tokens which get CHANGE operations in tagger. Specifically, the decoder is only one and shared by these tokens.</p><p>For the consideration of efficiency, we employ one layer of unidirectional LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997)</ref> as the backbone of our decoder. For each token in original utterance, the related initial state s 0 is initialized with the according hidden representation 3 :</p><formula xml:id="formula_8">s 0 = u i ∈ E u .<label>(7)</label></formula><p>Then the autoregressive generation is described as follows:</p><formula xml:id="formula_9">s t = LSTM(WE(x t ), s t−1 ),<label>(8)</label></formula><p>where x t is the output of decoder in the previous step, and the x 1 is initialized by a special start token. Moreover, in order to dynamically choose copying from the history or sampling from the overall vocabulary, we introduce the recurrent attention and coverage mechanism as in pointer-generator network <ref type="bibr" target="#b24">(See, Liu, and Manning 2017)</ref>. At each decoding step, we utilize the output s t to collect information from the encodings of history E h . The detailed calculations are as follows:</p><formula xml:id="formula_10">e t j = v T tanh(W s s t + W h h j + w c c t j + b attn ),<label>(9)</label></formula><formula xml:id="formula_11">a t = softmax(e t ),<label>(10)</label></formula><p>where j is corresponding to the index of token in the history, t is corresponding to the decoding steps and the c t is the coverage vector in t-th step. Specifically, the coverage vector is initialized by zero at the beginning of decoding and accumulated as follow:</p><formula xml:id="formula_12">c t j = t−1 t ′ =0 a t ′ j .<label>(11)</label></formula><p>Model  Once the normalized weights a t are obtained, we can calculate the results of attention:</p><formula xml:id="formula_13">s * t = j a t j · h j .<label>(12)</label></formula><p>Then, the s * t is forwarded into the subsequent modules for acquiring the predicted word:</p><formula xml:id="formula_14">g = σ(w T s * s * t + w T s s t + w T x WE(x t ) + b g ), (13) p vocab = softmax(W v · s * t + b v ),<label>(14)</label></formula><p>p(</p><formula xml:id="formula_15">x t+1 ) = g · p vocab + (1 − g) j:wj =xt a t j ,<label>(15)</label></formula><p>where σ is the sigmoid function to output a value between 0 and 1, the g is the gate to make a trade-off between copying and generating, the p(x t+1 ) is the final probability distribution of generated word. Moreover, the coverage loss is introduced to penalize repeatedly attending:</p><formula xml:id="formula_16">covloss t = j min(a t j , c t j ).<label>(16)</label></formula><p>Finally, the loss of the decoder is the weighted sum of negative log-likelihood and the coverage loss:</p><formula xml:id="formula_17">loss dec = i t − log p(x i t ) + λ covloss i t ,<label>(17)</label></formula><p>where i is corresponding to the index of token in original utterance, λ is the hyperparameter for adjusting the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Training</head><p>The model is optimized jointly. Once the loss of tagger and decoder are obtained, we sum and backward propagate the total loss as below:</p><formula xml:id="formula_18">loss = α loss tag + loss dec ,<label>(18)</label></formula><p>where α is also the hyperparameter for adjusting the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Realization</head><p>In the realization, we convert the predicted editing labels and the generated phrases to a complete utterance. In detail, we remain the KEEP denoted token and remove the DELETE token in the original utterance (involved dummy tokens), and replace the token, assigned by CHANGE A, with the generated phrase A. </p><formula xml:id="formula_19">train dev test Restoration-200k 194k 5k 5k CANARD 32k 4k 6k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we first detail the experimental settings and the compared methods; then the main results and ablation study are described; finally, we report the human evaluation results and additional analysis based on some cases. Our experiments are conducted on Restoration-200K <ref type="bibr" target="#b21">(Pan et al. 2019)</ref> and CANARD <ref type="bibr" target="#b9">(Elgohary, Peskov, and Boyd-Graber 2019)</ref>. The statistics of the datasets are shown in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>We initialize SARG with RoBERTa-wwm-ext <ref type="bibr" target="#b6">(Cui et al. 2019)</ref> for Restoration-200k and bert-base-uncased <ref type="bibr" target="#b7">(Devlin et al. 2018)</ref> for CANARD, the hidden size is set to 768, the number of attention heads to 12, the number of attention layers to 12. Adam optimizer is utilized, the loss of tagger weighted to α = 3, coverage loss weighted to λ = 1 and the initial learning rate is 5e-5. The above hyperparameters are all tuned on the standard validation data. The according automatic evaluation metrics are utilized as in previus works <ref type="bibr" target="#b21">(Pan et al. 2019;</ref><ref type="bibr" target="#b9">Elgohary, Peskov, and Boyd-Graber 2019)</ref>, which contain BLEU, ROUGE, and restoration score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>We compare the performance of our proposed SARG with the following methods:</p><p>• CopyNet: in this baseline, LSTM-based Seq2Seq model with attention and a copy mechanism is employed. • PAC <ref type="bibr" target="#b21">(Pan et al. 2019)</ref>: this model restores the incomplete utterance in a cascade way: firstly, select the remained words by fintuning BERT, and then roughly concatenate the selected words, history, original utterance and feed them into a standard pointer-generator network.</p><p>• T-Ptr-λ 4 <ref type="bibr" target="#b27">(Su et al. 2019)</ref>: this model solves such restoration task in an end-to-end way. It employs six layers of  transformer blocks as encoder and another six layers of transformer blocks as pointer decoder. Moreover, to emphasize the difference between history and utterance, it takes two individual channels in the encoder-decoder attention.</p><p>• Seq2Seq-Uni: we construct this baseline by employing the unified transformer blocks <ref type="bibr" target="#b8">(Dong et al. 2019)</ref> as the backbone of Seq2Seq, so that we can load the pretrained transformers easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>The main results on Restoration-200k are as shown in <ref type="table" target="#tab_3">Table 3</ref>. Focusing on the automatic metrics, we observe that SARG achieves the best results on 6 of 7 automatic metrics. The superiority of SARG is as expected, on one hand, the words of original utterance can be easily kept or deleted by sequence labeling, on the other hand, the rest of words can be easily copied from history or generated from vocabulary. And we also find PAC is 1.2 higher than SARG on restoration f 1 score but 3.0 and 6.1 lower on f 2 and f 3 separately. In fact, f 1 pays more attention to those tokens restored from history than others from the original utterance. In other words, though PAC can recall appropriate restored tokens from history, it may not place these restored tokens in their right positions well. We also exemplify such problem in the case study. Additionally, we compare the results of the beam-search with those of the greedy-earch, which we find that the beam-search brings pretty significant improvements on those complete autoregressive models, but less obvious on our model. It means that, SARG is less dependent on the beam-search and can be more time-efficient in the inference phase.   <ref type="table" target="#tab_8">Table 6</ref> shows the main results on CANARD dataset. As can be seen, SARG achieve the best BLEU score 5 on the development and test data. It is 5.56 higher than the previous best on the development data and 5.13 higher on the test data. Moreover, we also find that the result of our method is far from the level of human rewrites, which means that there is still a large room for the improvement of existing rewriting methods.</p><p>Inference Time T-Ptr-λ (n beam=1) 522 s T-Ptr-λ (n beam=5) 602 s Seq2Seq-Uni (n beam=1) 321 s Seq2Seq-Uni (n beam=5) 467 s SARG (n beam=1) 50 s SARG (n beam=5) 70 s  <ref type="table" target="#tab_9">Table 7</ref>, we can observe that, compared to those complete autoregressive methods, our semi autoregressive model takes less time for inference. SARG is near 10x times as fast as T-Ptr-λ and 6x times as fast as Seq2Seq-Uni. Beam-search increases the burden on inference. It needs more time and more memory for maintaining the candidate beams. Generally, the incomplete utterance restoration is required to be time-efficient as the intermediate subtask of multi-turn dialogue task, and it is unpractical to maintain plenty of beams in decoding. Therefore, our SARG may be a suitable choice with the less dependent on the beam-search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this subsection, we conduct a series of ablation studies to evaluate the effectiveness of different components in our proposed SARG, which includes pretrained weights (WEIGHT), copy mechanism (COPY), and generation from vocabulary (GEN), and the results are shown in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>As can be seen, GEN plays the least important role in our model. By contrast, the absence of COPY or WEIGHT may raise a substantial lack of performance. Following our previous experimental setting, the above two variant models both can not converge well. In fact, the model without COPY only selects words from the pre-defined overall vocabulary, and the decoder is more difficult to be trained well. Furthermore, without the WEIGHT, the model needs to update the overall weights from scratch, which incorporate the 768-dimensional embedding table and 12 transformer lay-  ers. Therefore, it is a considerable burden for the optimization, where the limited corpus is provided. And we also compare the output of tagger among the above listed models. An observation is that the tagger without WEIGHT is conservative on predicting the CHANGE operations; by contrast, the decoder without WEIGHT is less affected and has normal-appearing. Therefore, in some cases, even though the decoder produces the right restored words, the model still can not output the correct answers because the tagger does not produce the corresponding CHANGE operations.  In the phase of human evaluation, we employ three experienced workers to score the restoration quality and sentence fluency separately on 200 randomly selected samples. Specifically, each sample is scored by the three workers in turn, and the final quality or fluency scores are calculated by averaging the annotated results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>As shown in <ref type="table" target="#tab_13">Table 9</ref>, SARG obtains the highest score in restoration quality among the compared methods, which is consistent with the results of automatic evaluation. However, in the aspect of fluency score, Seq2Seq-Uni achieves the best performance. Seq2Seq-Uni takes a way of complete autoregression and benefits from the pretrained weights, which can complete the causal language modeling well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>In this subsection, we observe the prediction results among different models, and then select several representative examples to illustrate the superiority of our proposed model as <ref type="table" target="#tab_11">Table 8</ref> shows.</p><p>As can be seen in Example 1, the first three models can restore the action "cry out", and only SARG can restore the predicate "hire you", which is important to understand the direction of the action.</p><p>In Example 2, all four models restore the keyword "constellation" correctly. However, for T-Ptr-λ and Seq2Seq-Uni, undesired words "not believing" are also restored, which changes the intention of utterance. In PAC, we can find the keyword "constellation" is placed in a wrong position, which leads to the difficulty in understanding. Moreover, for the restoration scores, the wrong position problem has no effect on f 1 but is negative for f 2 and f 3 . That is a possible reason, compared with SARG, PAC has higher f 1 but lower f 2 and f 3 in the automatic evaluation.</p><p>Finally Example 3 demonstrates the ability of SARG to restore utterance from distant context. Specifically, the keyword "skin" appears in A 1 , and the model is required to restore it after three utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel semi autoregressive generator for multi-turn incomplete utterance restoration. The proposed model takes in the high efficiency of inference time from sequence labeling and the flexibility of generation from autoregressive modeling. Experimental results on two benchmarks demonstrate that the proposed model is significantly superior to other state-of-the-art methods and an appropriate model of utterance restoration for boosting the multi-turn dialogue system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Convert the target to label Input: S: the original utterance T : the restored utterance Output: L: the supervised label 1 Insert dummy tokens in S 2 L[i] = DELETE, ∀i = 1, 2, ..., 2n + 1 3 j = 0; k = 0; A = [ ] 4 Compute the longest common subsequence K between S and T 5 for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the average length between the added phrase and the restored utterance on </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The main results on Restoration-200k of our method and other SOTA methods. The models with " ‡" means that pretrained weights like BERT are utilized. Except to SARG, other models employ the 5-beam-search in their decoding procedure. SARG employs the greedy search in decoding step.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The count of conversations in different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of proposed model on the Restoration-200K. The beam size is fixed to 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The main results on CANARD of our method and other SOTA methods.Table shows the BLEU scores of the listed models on development and test data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: The inference time on Restoration-200k, which is</cell></row><row><cell>evaluated on the same blind test set (5104 examples) with</cell></row><row><cell>one Nvidia Tesla P40. We do not consider the inference</cell></row><row><cell>speed of PAC, because the cascade way takes lower effi-</cell></row><row><cell>ciency than other end-to-end methods</cell></row><row><cell>Through the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Examples for incomplete utterance restoration. A 1 to B 2 is the history of conversation, A 3 is the original utterance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Human evaluation of the restoration quality and language fluency on Restoration-200k. Both quality and fluency score adopt a 3-point scale.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">By convention, the bold letters represent the vectors, the capital letters represent the matrices and others represent the scalars.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It is a remarkable fact that there is a one-to-one correspondence between the hidden representation ui and the state s0, however, we omit the subscript i in s0 for the convenient expression.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We re-implement the transformer-based method and evaluate on the same blind test set for the fair comparison.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use multi-bleu-detok.perl<ref type="bibr" target="#b24">(Sennrich et al. 2017</ref>) as in<ref type="bibr" target="#b9">(Elgohary, Peskov, and Boyd-Graber 2019)</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Hongbo Zhang, Xiaolei Qin, Fuxiao Zhang and all the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing Community Interactions with Data-Driven Chatbots-The DBpedia Chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11080</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural quality estimation of grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2528" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-Training with Whole Word Masking for Chinese BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can You Unpack That? Learning to Rewrite Questionsin-Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1605</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1605" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5918" to="5924" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fluency boost learning and inference for neural grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1055" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Customer Service Automatic Answering System Based on Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Symposium on Signal Processing Systems</title>
		<meeting>the 2019 International Symposium on Signal Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Case Study of User Communication Styles with Customer Service Agents versus Intelligent Virtual Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special</title>
		<meeting>the 21th Annual Meeting of the Special</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="79" to="85" />
		</imprint>
	</monogr>
	<note>Interest Group on Discourse and Dialogue</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Challenges in building intelligent open-domain dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Analyzing sentence fusion in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muchovej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00203</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An Empirical Investigation of Pre-Trained Transformer Language Models for Open-Domain Dialogue Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Hierarchical Structured Multi-Head Attention Network for Multi-Turn Response Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="46802" to="46810" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mirylenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01187</idno>
		<title level="m">Encode, tag, realize: High-precision text editing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving open-domain dialogue systems via multi-turn incomplete utterance restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1824" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nematus: a Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nȃdejde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<ptr target="https://www.aclweb.org/anthology/E17-3017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Get to the point: Summarization with pointer-generator networks</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving multi-turn dialogue modelling with utterance ReWriter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07004</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised sentence fusion with single-stage inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ranking responses oriented to conversational relevance in chat-bots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="652" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sequential matching network: A new architecture for multiturn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01627</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Topic augmented neural response generation with a joint attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08340</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrieval-based humancomputer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sentence Simplification with Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="584" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Modeling multi-turn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09102</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00138</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01972</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Multi-view response selection for human-computer conversation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
