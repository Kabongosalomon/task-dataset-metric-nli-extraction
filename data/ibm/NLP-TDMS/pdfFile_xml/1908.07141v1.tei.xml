<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LogicENN: A Neural Based Knowledge Graphs Embedding Model with Logical Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Nayyeri</surname></persName>
							<email>nayyeri@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjin</forename><surname>Xu</surname></persName>
							<email>xu@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
							<email>jens.lehmann@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IAIS</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><forename type="middle">Shariat</forename><surname>Yazdi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LogicENN: A Neural Based Knowledge Graphs Embedding Model with Logical Rules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding models have gained significant attention in AI research. Recent works have shown that the inclusion of background knowledge, such as logical rules, can improve the performance of embeddings in downstream machine learning tasks. However, so far, most existing models do not allow the inclusion of rules. We address the challenge of including rules and present a new neural based embedding model (LogicENN). We prove that LogicENN can learn every ground truth of encoded rules in a knowledge graph. To the best of our knowledge, this has not been proved so far for the neural based family of embedding models. Moreover, we derive formulae for the inclusion of various rules, including (anti-)symmetric, inverse, irreflexive and transitive, implication, composition, equivalence and negation. Our formulation allows to avoid grounding for implication and equivalence relations. Our experiments show that LogicENN outperforms the state-of-the-art models in link prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) such as DBpedia, Freebase and Yago encode structured information in the form of a multirelational directed graph in which nodes represent entities and edges represent relations between nodes. In its simplest form, a KG is a collection of triples (h, r, t) where h and t are head and tail entities (nodes), respectively, and r is the relation (edge) between. For instance, (Albert Einstein, coauthor, Boris Podolsky) is a triple. Link prediction, entity resolution and linked-based clustering are among the most common tasks in KG analysis .</p><p>Knowledge Graph Embeddings (KGEs) have become one of the most promising approaches for KG analysis . The assumption is that there are global features which explain the existence of triples in a KG and embedding models try to capture those features using (typically low dimensional) vectors known as embeddings. Therefore, a KGE model assigns vectors <ref type="bibr">(h, r, t)</ref> to the symbolic entities and relations <ref type="bibr">(h, r, t)</ref>. The vectors are initialized randomly and updated by solving an optimization problem. To measure the degree of plausibility of a triple (h, r, t), a scoring function is defined. The function takes the embedding vectors of the triple and returns a value showing plausibility of the triple. KGEs have a wide range of downstream applications such as recommender systems, question answering, sentiment analysis etc.</p><p>Several KGE models have been proposed so far. Earlier works such as TransE , <ref type="bibr">RESCAL [Nickel et al., 2012]</ref> and E-MLP  focus just on existing triples as inputs for the link prediction task (predicting missing relations between entities). Due to the intrinsic incompleteness of KGs, relying only on triples may not deliver the best performance. Recent works such as ComplEx-NNE+AER and RUGE have invested the usage of background knowledge such as logical rules in order to enhance the performance .</p><p>To exploit rules, the inherent incapability of some existing models to encode rules is an obstacle. For instance, different variants of translational approaches such as TransE, FTransE, STransE, TransH and TransR have restrictions in encoding reflexive, symmetric and transitivity relations <ref type="bibr">[Kazemi and Poole, 2018]</ref>. Considering TransE as a concrete example, the main intuition is that one should derive the embedding vector of tail t when it is the sum of the embedding vectors of head and relation i.e. h + r = t. Once r is assumed to be symmetric, e.g. "co-author", we have t + r = h which results in r = 0. Therefore, TransE cannot capture symmetry and collapses all symmetrical relations into a null vector, resulting the same embedding vectors of all entities (i.e. h = t).</p><p>Due to incompleteness of KGs, even if one adds groundings of rules to a KG, there is still no guarantee that a capable embedding model learns the associated rules. That means, we need to properly inject rules into the learning process of a capable model. This issue has also been highlighted in the recent works , but not been investigated deeply in the literature. Therefore, the capability of a model to support rules as well as how rules are injected, i.e. encoding techniques, are the main challenges. Existing KGE models have solely addressed one of the mentioned challenges. This indeed causes two issues: a) Solely focusing on encoding techniques and disregard-  <ref type="bibr">et al., 2019]</ref> is proven to be capable of encoding inverse, symmetric (asymmetric), and composition rules without providing any rule injection mechanism. The authors  show their model properly encodes the rules. However, the results are obtained by generation of a lot of negative samples (e.g. 1000) together with using a very big embedding dimension (e.g. 1000). Such a big setting requires a very powerful computational infrastructure, adversely limit their applicability. Apart from lack of providing encoding technique, RotatE is not fully expressive i.e., the model is incapable of encoding some rules e.g., reflexive.</p><p>In contrast to the previous works, this paper addresses and contributes to the both previously highlighted points, i.e. the capability and the encoding technique, to avoid the mentioned issues. Regarding capability, our first contribution is that we propose a new neural embedding model (LogicENN) which is capable enough to encode rules, i.e. function free clauses with predicates of arity at most 2. Moreover, LogicENN avoids grounding for two logical rules: implication and equivalence relations. As the second contribution, we prove that Logi-cENN is fully expressive, i.e. for any ground truth of clauses of the above form, there exists a LogicENN model (with embedding vectors) that represents that ground truth. To the best of our knowledge, it is the first time that theoretical proofs are provided for the expressiveness of a neural network based embedding model. This proof indeed reassures us to inject different Horn rules in the model (encoding technique). Regarding the encoding technique, our third contribution is that we additionally derive formulae for enforcing the model to learn different relations including (anti-)symmetric, implication, equivalence, inverse, transitive, composition, negation as well as irreflexive. To our knowledge, our model is the first model that can encode these rules as well as provides practical solution for encoding them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>We investigate the related works in the light of two main issues we mentioned in the previous section, i.e. i) capability of a model to encode rules and, ii) the encoding techniques. Moreover, we briefly review the relevant neural based models and show, in contrast to LogicENN, they are not able to avoid grounding for the implication and equivalence relationships.</p><p>Considering the capability, <ref type="bibr">[Kazemi and Poole, 2018]</ref> reports that TransE, FTransE, STransE, TransH and TransR have restrictions in encoding rules. More concretely, TransE is incapable of encoding reflexive, symmetric and transitivity  and DistMult  cannot capture antisymmetric. The CP decomposition cannot encode both symmetric and antisymmetric relations . Wang et al. also investigate expressiveness of different bilinear models from a ranking perspective of their scoring matrix.</p><p>Despite the fact that score function of DistMult and Com-plEx are similar, ComplEx can encode symmetric and antisymmetric relations due to the algebraic properties of complex numbers . SimplE <ref type="bibr">[Kazemi and Poole, 2018]</ref> is one the recent embedding model which is proven to be fully expressive. Moreover, conditions for encoding symmetric, antisymmetric and inverse patterns are derived. Although SimplE is fully expressive, for each entity/relation, two vectors should be provided which doubles the space. <ref type="bibr">RotatE [Sun et al., 2019]</ref> is able to encode symmetric, antisymmetric and inverse and composition patterns. Although RotatE is shown to properly encode the patterns, a lot of negative samples should be generated together with a very big embedding dimension. It is indeed a big limitation when the model is trained on a large scale KG.</p><p>Regarding encoding techniques, various approaches are introduced in the literature, which we review the most relevant ones. As a preprocessing step,  iteratively infer new facts based on rules till no new facts can be inferred from a KG. Then, they regard both ground atoms and existing rules as the set of new rules to be learned. Accordingly, marginal probability of them are included in the training set and the loss function is minimized. <ref type="bibr">KALE [Guo et al., 2016]</ref> uses margin ranking loss over logical formulae as well as triple facts and jointly learnins triples and formulae. In order not to rely on propositionalization for implication,  proposes a lifted rule injection method. Minervini et al. derive formulae for inverse and equivalence rules according to the score functions of TransE, ComplEx and DistMult. The obtained formulae are added to the objective as a regularization terms. Other methods that consider relation paths, which is closely connected to rules, are well-studied in the literature e.g. .</p><p>There are also other ways of encoding rules, e.g. RUGE  presents a generic (model-independent) framework to inject rules with confidence scores into an embedding model. The rules are encoded as constraints for an optimization problem. One of the main disadvantages of RUGE is that the model needs grounding of all rules. For example, to inject the rule ∀h, t, if (h, BornIn, t) − → (h, Nationality, t), h, t should be replaced by all the entities that the triple (h, BornIn, t) exists in the KG. In contrast to RUGE, <ref type="bibr" target="#b7">[Ding et al., 2018]</ref> follows a model dependent approach for injection of rules. It encodes non-negativity and entailment as constraints in ComplEx. It is shown <ref type="bibr" target="#b7">[Ding et al., 2018]</ref> that the model dependent approach of <ref type="bibr" target="#b7">[Ding et al., 2018]</ref> outperforms the generic approach of  on the FB15k dataset. However, <ref type="bibr" target="#b7">[Ding et al., 2018]</ref> can only inject implication rule which is a limitation. As we mentioned, LogicENN, in contrast to other relevant neural based models, avoids grounding for the implication and equivalence relationships. E-MLP, ER-MLP, NTN, ConvE and ConvKB are among the most successful models in the literature <ref type="bibr" target="#b6">Dettmers et al., 2018;</ref>. The main common characteristics of all models is that h, r and t are treated as inputs or weights of hidden layers while in LogicENN h and t are inputs and r is the output of the network. Having relations encoded as inputs or hidden layer weights requires that all groundings of the rules be fed into the network. The detailed explanation of why Logi-cENN is capable of avoiding grounding is properly addressed in Section 3.</p><p>To sum up, many models, like translation based models are incapable of encoding some rules. The models which are reported to be capable, can either learn rules using existing triples in a KG or are enforced to learn by properly injecting the rules into their formulation. The former kinds of models have still the risk of not properly learning rules as data in KGs are known to be very incomplete. Therefore injecting rules enhance the learning performance of models. Regarding fully expressiveness <ref type="formula">(</ref> Figure 1: LogicENN: The hidden layer mapping, which is universal according to the theory of NN, is shared between entities and relations. One output node is associated to each relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The LogicENN Approach</head><p>In this section we introduce our model and contribute to both the capability and the encoding technique. We first present LogicENN as a neural embedding model which is capable of encoding rules and we prove that it is fully expressive. We then discuss how we can algebraically formulate the rules and inject them into the model. We then present our optimization approach in order to learn rules by LogicENN.</p><p>This work considers clauses of the form "premise ⇒ conclusion", in which "conclusion" is an atom and "premise" is a conjunction of several atoms. Atoms are triples of type (x, r, y) where x, y are variables and "r" is a known relation in the KG. We refer to such clauses as rules from now on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Proposed Neural Embedding Model</head><p>It is known that using the same embedding space to represent both entities and relations is less competitive compared to considering two separate spaces . This motivates to consider a neural network (NN) in which entities and relations are embedded in two different spaces. Another motivation is that the previously reviewed NN approaches encode relations into the input layer or consider them as input weights of a hidden layer which is restrictive for avoiding grounding when one considers implication relationship.</p><p>We consider entity pairs as input and relations as output. More precisely, we consider the embeddings of entity pairs, <ref type="bibr">[h, t]</ref>, as input which together with weights are randomly initialized in the beginning. During learning, LogicENN optimizes both weights and embeddings of the entities according to its loss function. The output weights of the network are embeddings of relations and the hidden layer weights are shared between all entities and relations as shown in <ref type="figure">Figure 1</ref>. Despite LogicENN takes embedding pairs of entities as input, it learns the embedding of each individual entity through a unique vector. This is in contrast to some matrix factorization approaches which loose information by binding embedding vectors in form of entity-entity or entity-relations .</p><p>We denote the score function of a given triple (h, r, t) by f r (h, t), or more compactly by f r h,t . Without loss of generality, we use a single hidden layer for the NN to show theoretical capabilities of LogicENN and we define its score as:</p><formula xml:id="formula_0">f r h,t = L i=1 φ( w i , [h, t] + b i )β r i = L i=1 φ h,t (w i , b i )β r i = Φ T h,t β r</formula><p>(1) where L is the number of nodes in hidden layer, w i ∈ R 2d and β r i ∈ R are input and output weights of the ith hidden node respectively. β r = [β r 1 , . . . , β r L ] T are the output weights of the network which are actually embedding of relations. That is because in the last layer a linear function acts as the activation function.</p><formula xml:id="formula_1">φ h,t (w i , b i ) = φ( w i , [h, t] + b i ) is the output of the ith hidden node and Φ h,t = [φ h,t (w 1 , b 1 ), . . . , φ h,t (w L , b L )]</formula><p>T is feature mapping of the hidden layer of the network which is shared between all relations. h, t ∈ R d are embedding vectors of head and tail respectively and d is the embedding dimension.</p><formula xml:id="formula_2">Therefore w i , [h, t] ∈ R 2d . Finally, φ(.)</formula><p>is an activation function and ., . is the inner product.</p><p>Due to having shared hidden layers in the design, Logi-cENN is efficient in space complexity. The space complexity of the proposed model is O(N e d + N r L) where N e , N r are number of entities and relations respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Capability of the Proposed Network</head><p>As mentioned in the section 1, if a model is not fully expressive, it might be wrongly expected to learn a rule which is incapable of. Therefore, investigation of the theories corresponding to the expressiveness of an embedding model is indeed important. Accordingly, we now prove that LogicENN Rule Definition ∀h, t, s ∈ E : . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulation based on score function</head><p>Formulation based on NN Equivalent regularization form (Denoted as R i in Equation <ref type="formula">(2)</ref> ) is fully expressive i.e., capable of representing every ground truth over entities and relations in a KG. Let F L be the set of all possible neural networks with L hidden nodes as defined by <ref type="formula">(1)</ref>. Therefore, the set of all possible networks with arbitrary number of hidden nodes will be F = ∞ l=1 F l . Let C(X) denote set of continuous functions over R. Let E be the set of entities and e ∈ E is an entity with embedding vector e ∈ Ω E ⊂ R d . We also assume that Ω E is a compact set. We have the following theorem.</p><formula xml:id="formula_3">Equivalence (h, r 1 , t) ⇔ (h, r 2 , t) f r1 h,t = f r2 h,t + ξ h,t Φ T h,t (β r1 − β r2 ) = ξ h,t max( β r1 − β r2 1 − ξ Eq , 0) Symmetric (h, r, t) ⇔ (t, r, h) f r h,t = f r t,h + ξ h,t (Φ h,t − Φ t,h ) T β r = ξ h,t max(|(Φ h,t − Φ t,h ) T β r | − ξ Sy , 0) Asymmetric (h, r, t) ⇒ ¬(t, r, h) f r h,t = f r t,h + M h,t (Φ h,t − Φ t,h ) T β r = M NC Negation (h, r 1 , t) ⇔ ¬(h, r 2 , t) f r1 h,t = M − f r2 h,t + ξ h,t Φ T h,t (β r1 +β r2 ) = M+ξ h,t NC Implication (h, r 1 , t) ⇒ (h, r 2 , t) f r1 h,t ≤ f r2 h,t Φ T h,t (β r1 − β r2 ) ≤ 0 max( i (β r1 i − β r2 i ) + ξ Im , 0) Inverse (h, r 1 , t) ⇒ (t, r 2 , h) f r1 h,t ≤ f r2 t,h Φ T h,t β r1 − Φ T t,h β r2 ≤ 0 max(Φ T h,t β r1 − Φ T t,h β r2 + ξ In , 0) Reflexivity (h, r, h) f r h,h = M − ξ h,h Φ T h,h β r = M − ξ h,h NC Irreflexive ¬(h, r, h) f r h,h = ξ h,h Φ T h,h β r = ξ h,h NC Transitivity (h, r, t) ∧ (t, r, s) ⇒ (h, r, s) σ(f r h,s ) ≥ σ(f r h,t ) × σ(f r t,s ) σ(Φ h,t β r ) × σ(Φ t,s β r ) − σ(Φ T h,s β r ) ≤ 0 max(σ(Φ h,t β r ) × σ(Φ t,s β r ) − σ(Φ T h,s β r ) + ξ Tr , 0) Composition (h, r 1 , t) ∧ (t, r 2 , s) ⇒ (h, r 3 , s) σ(f r1 h,s ) ≥ σ(f r2 h,t ) × σ(f r3 t,s ) σ(Φ h,t β r1 ) × σ(Φ t,s β r2 ) − σ(Φ T h,s β r3 ) ≤ 0 max(σ(Φ h,t β r1 ) × σ(Φ t,s β r2 ) − σ(Φ T h,s β r3 ) + ξ Co , 0)</formula><p>Theorem 1. Let F be the set of all possible networks defined as above, F be dense in C(R 2d ) where d ≥ 1 is arbitrary embedding dimension. Given any ground truth in a KG with τ true facts, there exists a LogicENN in F with embedding dimension d, that can represent the ground truth. The same holds when F is dense in C(Ω) where Ω = Ω E × Ω E is the Cartesian product of two compact sets.</p><p>The theorem proof as well as more detailed technical discussion are included in the supplementary materials of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Formulating Rules</head><p>Let a, b be two grounded atoms of a clause as defined in the beginning of Section 3, and let the truth values of a and b are denoted by P (a) and P (b) respectively. To model the truth values of negation, conjunction, disjunction and implication of a and b we define P (¬a), P (a ∧ b), P (a ∨ b) as in  but we define P (a ⇒ b) : P (a) ≤ P (b). These can be used to derive formulation of rules based both on score function as well as the NN, as shown in <ref type="table">Table 6</ref>.</p><p>As an example, consider the implication rule ∀h, t :</p><formula xml:id="formula_4">(h, r 1 , t) ⇒ (h, r 2 , t). Using P (a) ≤ P (b), we can infer f r1 h,t ≤ f r2 h,t . By (1), we get Φ T h,t (β r1 − β r2 ) ≤ 0.</formula><p>Provided that our activation function is positive, i.e. Φ T h,t ≥ 0, we will have (β r1 − β r2 ) ≤ 0. That latter formula is independent of h and t which means we do not need any grounding for implication. The same procedure shows that we can avoid grounding for equivalence. However for other rules this is not possible.</p><p>Using truth values defined as above, we can derive a formulation of a rule based on score function (e.g. f r1</p><p>h,t ≤ f r2 h,t for implication) in the 3rd column of <ref type="table">Table 6</ref>, and its equivalent formulation based on the NN of (1) in the 4th column. Assume M &gt; 0 indicates True and 0 indicates False. We now state the necessity and sufficiency conditions for LogicENN to infer various rules. For the proof we can do similar procedure to the one we did for implication. The detailed proof is provided in the supplementary materials of the paper.</p><p>Theorem 2. For all h, t, s ∈ E, set ξ h,t = 0 in column "Formulation based on NN" of <ref type="table">Table 6</ref>. For each relation type given in <ref type="table">Table 6</ref>, LogicENN can infer it if and only if the corresponding equation in column "Formulation based on NN" holds (ξ h,t is set to 0).</p><p>Since KGs may contain wrong data or facts with less truth confidence <ref type="bibr" target="#b7">[Ding et al., 2018]</ref>, the assumption of ξ h,t = 0 in Theorem 4 is too rigid in practice. Therefore, as shown in <ref type="table">Table 6</ref> we consider ξ h,t to be a slack variable that allows enough flexibility to deal with uncertainty in KG. This allows us to infer uncertainty as ξ through a validation step of learning. Although considering ξ h,t as slack variables improves flexibility, due to grounding we will have too many slack variables. Therefore, in the implementation level we decided to consider one slack variable for each relation type e.g. one ξ Eq was used for all the equivalence relations (see the last column of <ref type="table">Table 6</ref>). This enables the model to mitigate the negative effect of uncertainty of rules by considering average uncertainty per rule type. Experimental results show the effectiveness of inclusion of a slack variable per a rule type. During experiments, we obtained the hyper-parameters corresponding to each rule type sequentially through validation step. Therefore, instead of having n 1 × n 2 × . . . × n m combinations for hyper-parameter search corresponding to the rules injection, we have n 1 + n 2 + . . . + n m ,combinations where n i refers to the number of candidates for the slack variable of the i-th rule type. Experimental results confirms that this approach gets satisfactory performance as well as significant reduction in the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rule Injection and Optimization</head><p>To inject rules into embeddings and weights of (1), we define the following optimization:</p><formula xml:id="formula_5">min θ (h,r,t)∈S α r h,t log(1 + exp(−y r h,t f r h,t )) + λ l i=1 R i N i subject to h = 1 and t = 1 .</formula><p>(2) where S is the set of all positive or negative samples, α r h,t is set to 1 for positive samples. For negative samples if we get big scores, then the model should suppress it by enforcing a big value for α r h,t using formula No.5 in . R i refers to the ith rule, N i is the number of groundings, λ is a regularization term and y r h,t represents the label of (h, r, t) which is 1 for positive and -1 for negative samples.</p><p>In <ref type="formula">(2)</ref>, we use negative log-likelihood loss with regularizations over logical rules. Loss as the first term focuses on learning facts in KG while the second term, i.e. regularization, injects rules into the learning process. The regularization are provided as penalties in the last column of <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussions</head><p>To show the capability of LogicENN, we evaluated it on the link prediction task. The task is to complete a triple (h, r, t) when h or t missing, i.e. to predict h given (r, t) or t given (h, r). For evaluation, we will use Mean Rank (MR), Mean Reciprocal Rank (MRR) and Hit@10 in the raw and filtered settings as reported in . Datasets. We used FB15k and WN18 with the settings reported in . We used the rules reported in  for FB15k, and the rules in  for WN18. The confidence levels of rules are supposed to be no less than 0.8. Totally we used 454 rules for FB15k and 14 rules for WN18. As both data sets are reported <ref type="bibr" target="#b6">[Dettmers et al., 2018]</ref> to have the inverse of triples in their test sets, it is argued that the increase of performance of some models might be due to the fact that models have learned more from the inverses rather the graph itself. Using these data sets to compare just the rule-based models would be fine as all are using rules, e.g. experiments of RUGE . However when one wants to compare rule-based with non-rule-based models, it would be better to use a data set which has not much inverses. As FB15k-237 has already addressed this, we used it to compare LogicENN with other non-rule-based models.</p><p>To formulate rules, we categorized them according to their definitions in <ref type="table">Table 6</ref>. We did grounding for all rules except for those denoted by NC, as well as equivalence and implication since LogicENN does not need them by formulation (see Sec. 3). To maximize the utility of inferencing, like RUGE, we take as valid groundings whose premise triples are observed in the training set, while conclusion triples are not.</p><p>Experimental Setup. To select the structure of the model, we tried different settings for the number of neurons/layers and types of activation functions. Two of the best settings were LogicENN R and LogicENN S which both had 3 hidden layers with 1k, 2k and 200 neurons respectively. The 4th layer was the output layer where the number of neurons were equal to the number of relationships in the datasets.</p><p>In LogicENN R we used ReLU on all hidden layers and in LogicENN S we used Sigmoid as activation functions between layers and ReLU on the last hidden layer. Both of LogicENN R and LogicENN S were combined with each of rules we used. When models were armed with all existing rules for a dataset, we denote them by LogicENN * R and LogicENN * S respectively. Moreover, LogicENN * RR denotes our approach when we have added the reverse of triples in the target data set, as also done in .</p><p>We implemented the models in PyTorch and used the Adam optimizer for training. We select the optimal hyperparameters of our models by early validation stopping according to MRR on the validation set. We restricted the iterations to 2000.</p><p>For basic models of LogicENN R and LogicENN S which integrate no rules, we created 100 mini-batches on each dataset. We tuned the embedding dimensionality d in {100, 150, 200}, the learning rate γ in {0.0001, 0.0005, 0.001, 0.005, 0.01} and the ratio of negatives over positive training samples α in {1, 2, 3, 5, 8, 10}. The optimal configuration for both LogicENN R and LogicENN S are: d = 200, γ = 0.001, α = 8 on FB15k; and d = 200, γ = 0.001, α = 5 on WN18. Based on LogicENN R and LogicENN S with their optimal configurations, we further tuned the regularization coefficient λ in {0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1} and slack variables ξ i in {0, 0.1, 0.5, 1, 3, 5, 10} for different types of rules (see <ref type="table">Table 6</ref>) to obtain all optimal hyperparameters of LogicENN * R and LogicENN * S which integrate all rules in datasets. For LogicENN * R , we find the following hyperparameters are optimal: λ = 0.05, ξ Eq = 1, ξ Sy = 0.5, ξ Im = 5, ξ Co = 0.1, ξ In = 3 on FB15k; λ = 0.01, ξ In = 0.1 on WN18. The optimal configurations of LogicENN * S on FB15k are : λ = 0.05, ξ Eq = 0.5, ξ Sy = 0.1, ξ Im = 3, ξ Co = 0.1, ξ In = 3. For WN18 they are λ = 0.005, ξ In = 0.1.     <ref type="bibr" target="#b4">[Zhang et al., 2019]</ref> are obtained by running their code with embedding dimension 60 and 10 negative samples without using type constraint to have a fair comparison to our method. We set the embedding dimension to 60 (240 adjustable parameters) because QuatE provides 4 vectors for each entity. Other results in <ref type="table" target="#tab_2">Table 2</ref> are taken from the original papers. As we previously discussed, FB15k and WN18 have the inverse of triples in their test sets. To show the performance of LogicENN in comparison of non-rule-based methods we ran experiments on FB15k-237 which is reported to have not much reverse of triples. <ref type="table" target="#tab_5">Table 4</ref> shows the comparison of our method with other non-rule-based models in this regard. Discussion of Results. As shown in <ref type="table" target="#tab_2">Table 2</ref>, LogicENN * R outperformed all embedding models on FB15k in the raw setting using MR, Hit@10 and MRR. For the filtered setting it also performs better considering FHit@10 and very close to RUGE (the 2nd best performing model) using FMRR. Considering WN18, our model got the best performance in Raw-Hit@10 and Raw-MRR. In the terms of FHit@10, only ConvE and QuatE outperformed our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>To investigate whether inclusion of logical rules improve the performance of our model, we added each rule to the naked LogicENN separately. <ref type="table" target="#tab_4">Table 3</ref> shows the improvements by adding each rules separately. As shown, inclusion of each rule improves the performance of the naked model. For FB15k, the best improvement is obtained by the inverse rule which is the most common rule in FB15k. Two variants of model i.e. LogicENN S and LogicENN R performed better when rules added.</p><p>The two most recent methods of RUGE and ComplEx-NNE+AER use ComplEx score function to encode rules. As <ref type="table" target="#tab_2">Table 2</ref> shows, the performance of ComplEx on Raw-Hit@10 was 48.5% which encoding of rules by RUGE and ComplEx-NNE+AER improved it by less than 10% (to 55.5% and 57.3% respectively). In contrast, our method without any rule-encoding performed around 40% <ref type="table" target="#tab_4">(Table 3)</ref> which jumped to around 67% <ref type="table" target="#tab_2">(Table 2)</ref> when rules were encoded. It shows our method improved around 27% which is more than double of its competitors. Therefore we can conclude that the encoding techniques of <ref type="table">Table 6</ref> can properly encode rules. <ref type="figure" target="#fig_1">Figure 2</ref> shows that the model has properly learned the equivalence and implication relations. To better comprehend that, in Section 3.3 we already explained that we can avoid grounding for the implication rule and the resulting formula was ∆ Implication := (β r1 − β r2 ) and we had ∆ Implication ≤ 0. Similar argument implies that ∆ Equivalence = 0. Therefore, if the model has properly learned implication (equivalence) the differences of the embedding vectors of these two relations should contain negative (zero) elements. In <ref type="figure" target="#fig_1">Figure 2</ref>, the x-axis represents the means of the elements of the two ∆s and the y-axis represents their variances. As depicted, the points associated to the equivalence relations are accumulated around the origin and the points associated to the implication relations are negative. This shows that LogicENN has properly encoded these rules without using grounding for 30 implication and 68 equivalence relations in FB15k.</p><formula xml:id="formula_6">• Equivalence Relation ■ Implication Relation • • • • • •• •• • • • • • • • • • •• • • • • • •• • • • • • • • • • • • • • • • • • • • • • •• • • • • • • • • • • • • • • • • • • ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■</formula><p>As <ref type="table" target="#tab_5">Table 4</ref> show, LogicENN * R outperforms all state-of-theart in the terms of Raw Hits@10. We should note that the originally reported result of ComplEx-N3 used d=2k as the embedding dimension . To have a fair and equal comparison, we re-ran their code with the same setting we used for all of our experiments, i.e. we used embedding dimension of 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work we introduced a new neural embedding model (LogicENN) which is able to encode different rules. We proved that LogicENN is fully expressive and we derived algebraic formulae to enforce it to learn different relations. We also showed how rules can be properly injected into learning.</p><p>Our extensive experiments on different benchmarks show that LogicENN outperformed all embedding models on FB15k in the raw and performed very well in the filtered setting. For WN18, the model performed better than almost all others in the raw and very close to the best models in the filtered settings. On FB15k-237 the model was better than non-rule-based models on raw Hit@10.</p><p>The expressiveness of other kinds of neural models as well as the necessity and sufficiency conditions for injecting rules, are targets of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supplementary Materials for the Paper: LogicENN</head><p>This section contains supplementary materials for our paper called: "LogicENN: A Neural Based Knowledge Graphs Embedding Model with Logical Rules" In Section 6.1, we first review the relevant neural based models and describe their similarities and differences to Log-icENN. Then in Section 6.2, we will provide our proposed theorems with their detailed proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">LogicENN vs State-of-the-art Neural Based Models</head><p>This section describes the relevant neural network based embedding models. We divided them to models which do not consider logical rules into their embeddings and those which consider rules. We then compare their score functions with LogicENN and discuss how it is different from other state-ofthe-art models. Before progressing more, we first define the relevant notations. Vectors of are denoted by bold non-capital letters (e.g. h or 0, 1 as vectors of zeros and ones respectively), matrices by bold capital letters and tensors by underlined bold capital letters. W (.) are weight matrices which have no dependency on h, r or t, while W (.) r (W r ) means that the weight matrix (tensor) is associated to relation r. Moreover,h,r andt are 2D-reshape of h, r and t respectively. ER-MLP. <ref type="bibr" target="#b8">[Dong et al., 2014]</ref> In contrast to E-MLP which uses one neural network per each relation, ER-MLP shares its weights between all entities and relations. The relation embedding is placed as input of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTN.</head><p>[Socher et al., 2013] employs 3-way tensor in the hidden layer to better capture interactions between features of two entities. The score function of NTN is as follows:</p><formula xml:id="formula_7">f r u,v = w T 1r tanh(u T W 0htr v + W 0hr u + W 0tr v + b r )<label>(3)</label></formula><p>where W 0htr ∈ R d×d×L , b r ∈ R L are the 3-way relation specific tensor and bias of hidden layer respectively.</p><p>ConvE. <ref type="bibr" target="#b6">[Dettmers et al., 2018</ref>] is a multi-layer convolutional network for link prediction. The score function of ConvE is as follows:</p><formula xml:id="formula_8">f r u,v = g(Vec(g([ū;r] * ω))W )v<label>(4)</label></formula><p>where ω is filter and W is a linear transformation matrix. g is an activation function.</p><p>ConvKB.  is a multi-layer convolutional network for link prediction with the following score function:  <ref type="bibr">et al., 2018]</ref> is a triple branch neural network in which parallel branched layers are defined on the top of an interaction layer where each embedding of any element of a KG is specified by its multi restriction. The loss function is defined based on the score of three elements of each triple. ProjE. [Shi and Weninger, 2017] is a two layer neural network. The first layer is a combination layer which works on tail and relation and the second layer is a projection layer which projects the obtained vector from the last layer to the candidate-entity matrix. The candidate-entity matrix is a subset of entity matrix where entities can be sampled in different ways.</p><formula xml:id="formula_9">f r u,v = Concat(g([ū;r,v] * ω))W (5) Method Score Function (f r h,t ) E-MLP w T r tanh(W (1) r h + W (2) r t) ER-MLP w T tanh(W (1) h + W (2) r + W (3) t) NTN w T r tanh(h T W r t + W (1) r h + W (2) r t + br) ConvE g(Vec(g([h;r] * ω)) W ) t ConvKB Concat(g([h;r,t] * ω)) w</formula><p>In short, <ref type="table" target="#tab_6">Table 5</ref> specifies the score functions of different neural based embedding models.</p><p>KG Embedding Models with Logical Rules RUGE. provides a general framework to iteratively inject logical rules in KGE. Given a set of soft logical rules R = {(l p , λ p )} where l p is a rule and l p is its confidence value, rules are represented as mathematical constraints to obtain soft labels for unlabeled triples. Then, an optimization problem is solved to update embedding vectors based on hard and soft labeled triples. The framework is used to train ComplEx model as case study. ComplEx-NNE+AER. <ref type="bibr" target="#b7">[Ding et al., 2018]</ref>, which is a model dependent approach, derives formula for entailment rule to avoid grounding in ComplEx. The model outperforms RUGE on FB15K in the terms of Meanrank and Hit@k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of LogicENN with Other Models</head><p>LogicENN uses the scoring function (1). We formulate the score function to separate the entity and relation spaces. This enables the model to map entities by a universal hidden layer mapping Φ h,t . Since we prove that Φ h,t is universal, we can share it between all relations. Since Φ h,t is used by several relations, we have fewer parameters. Several neural embedding models such as NTN and E-MLP didn't share parameters of hidden layer. Therefore, for each relation, a separate neural network is used. ER-MLP feeds entity pairs as well as relation to the neural network. Inclusion of relation in the hidden layer disables the model to avoid grounding for implication rule. The same problem happens for ConvE and ConvKB. Moreover, full expressiveness of ConvE and ConvKB is not investigated yet.</p><p>Regarding encoding techniques, we derive formula for the proposed NN to encode function free Horn clause rules. For implication and equivalence rules, we approximate the original formula by avoiding grounding. Since we proved that our model is fully expressive, we can encode all Horn clause rules.</p><p>Regarding the last column of the <ref type="table" target="#tab_1">Table 1</ref>, we add slack variables to better handle uncertainty during injection of rules in the embeddings. The uncertainty is inherited from the fact that KG contain False positive triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Theorems and Proofs</head><p>In this section we state the theorems which prove the full expressiveness of our proposed model LogicENN.</p><p>Theorem 3. Let F be the set of all possible networks defined as above, F be dense in C(R 2d ) where d ≥ 1 is arbitrary embedding dimension. Given any ground truth in a KG with τ true facts, there exists a LogicENN in F with embedding dimension d, that can represent the ground truth. The same holds when F is dense in C(Ω) where Ω = Ω E × Ω E is the Cartesian product of two compact sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>FE), SimplE and RESCAL, HolE etc are FE under some conditions [Wang et al., 2018].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Statistics of difference vectors of equivalence relations (∆Equivalence) and implication relations (∆Implication).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Neural</head><label></label><figDesc>Network Based Embedding Models E-MLP. [Socher et al., 2013] which is standard Multi Layer Perceptron (MLP) for KGE. E-MLP uses one neural network per each relations in the KG which has high space complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ing capability of a model, has the risk that the model is expected to learn a rule which is not capable of. For example, RUGE proposes a general optimization framework to iteratively inject first order logical rules in an embedding model. ComplEx is used as the base model for rule injection. However, ComplEx is not capable of encoding composition pattern. The similar issue can be found in[Minervini et al., ]. Minervini et al.</figDesc><table><row><cell>use function-free Horn</cell></row><row><cell>clause rules to regularize an embedding model by includ-</cell></row><row><cell>ing inconsistency loss. The loss measures degree of viola-</cell></row><row><cell>tion from assumption on adversarially generated examples.</cell></row><row><cell>Although their framework nicely encodes Horn rules, they</cell></row><row><cell>use DistMult to inject rules which is not capable of encod-</cell></row><row><cell>ing asymmetric rule. For example, the authors [Minervini</cell></row><row><cell>et al., ] try to inject (h, Hypernym, t) → (t, Hyponym, h)</cell></row><row><cell>rule in DistMult. During injection of (h, Hypernym, t) →</cell></row><row><cell>(t, Hyponym, h), (h, Hypernym, t) → (h, Hyponym, t)</cell></row><row><cell>is also injected wrongly. Therefore, the model considers</cell></row><row><cell>many false triples as positive.</cell></row><row><cell>b) Solely focusing on capability of a model and disregard-</cell></row><row><cell>ing encoding techniques, has the risk that the model does not</cell></row><row><cell>properly encode rule due to incompleteness of KGs. For ex-</cell></row><row><cell>ample, RotatE [Sun</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Formulation and representation of rules (NC: Not considered for implementation).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows comparison of LogicENN with eight state-of-the-art embedding models as basic models which only use observed triples in KG and rely on no rules. We also took PTransE, KALE, RUGE, ComplEx-NNE+AER as additional baselines. They encode relation paths or logical rules like LogicENN * R/S . Among them, the first two are extension of TransE, while the rest are extensions of ComplEx.To compare both raw and filtered results of LogicENN * R/S and baselines, we take the results of the first five baseline models on FB15k reported by and use the code provided by for KALE, ComplEx, RUGE and ComplEx-NNE+AER to produce the raw results with the optimal configurations reported in the original papers. We also ran the code of RotatE to get its results. Because RotatE [Sun et al.,</figDesc><table><row><cell>2019] uses Complex vectors, we set its embedding dimension</cell></row><row><cell>to 130 (260 adjustable parameters per each entity) and gen-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="7">: Link prediction results. Rows 1-8: basic models with no rules. Rows 9-12: models which encode rules. Results on FB15k in rows</cell></row><row><cell cols="7">1-5 are taken from [Akrami et al., 2018] while the rest are taken from original papers/code. Dashes: results could not be obtained.</cell></row><row><cell></cell><cell>MR</cell><cell></cell><cell cols="2">Hit@10</cell><cell cols="2">MRR</cell></row><row><cell></cell><cell>ReLU</cell><cell>σ</cell><cell>ReLU</cell><cell>σ</cell><cell>ReLU</cell><cell>σ</cell></row><row><cell>With No Rule</cell><cell>320</cell><cell>430</cell><cell>42.2</cell><cell>36.6</cell><cell>18.1</cell><cell>15.2</cell></row><row><cell>Inverse</cell><cell>187</cell><cell>180</cell><cell>62.1</cell><cell>59.3</cell><cell>37.7</cell><cell>35.2</cell></row><row><cell>Implication</cell><cell>321</cell><cell>421</cell><cell>40.5</cell><cell>37.1</cell><cell>18.2</cell><cell>16.4</cell></row><row><cell>Symmetry</cell><cell>299</cell><cell>387</cell><cell>42.2</cell><cell>37.9</cell><cell>18.5</cell><cell>17.2</cell></row><row><cell>Equivalence</cell><cell>302</cell><cell>330</cell><cell>41.7</cell><cell>38.4</cell><cell>19</cell><cell>17.7</cell></row><row><cell>Composition</cell><cell>303</cell><cell>391</cell><cell>41</cell><cell>37.1</cell><cell>18.1</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Link prediction using different activation functions</cell></row><row><cell cols="5">and rules (FB15k). "ReLU", "σ" respectively correspond to</cell></row><row><cell cols="5">LogicENNR, LogicENNS. : the model outperformed all baselines.</cell></row><row><cell></cell><cell></cell><cell>Raw</cell><cell></cell><cell>Filtered</cell></row><row><cell></cell><cell cols="4">MR Hits@10 MR Hits@10</cell></row><row><cell>ComplEx</cell><cell>620</cell><cell>25.4</cell><cell>457</cell><cell>45.7</cell></row><row><cell>ComplEx-N3</cell><cell>553</cell><cell>29.7</cell><cell>421</cell><cell>50.0</cell></row><row><cell>ConvE</cell><cell>489</cell><cell>28.4</cell><cell>246</cell><cell>49.1</cell></row><row><cell cols="2">ASR-COMPLEX 570</cell><cell>26.3</cell><cell>420</cell><cell>46.1</cell></row><row><cell>RotatE</cell><cell>374</cell><cell>33.3</cell><cell>258</cell><cell>47.1</cell></row><row><cell>QuatE</cell><cell>354</cell><cell>32.2</cell><cell>161</cell><cell>48.3</cell></row><row><cell>LogicENN  *  R</cell><cell>454</cell><cell>34.7</cell><cell>424</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Link prediction results on FB15k-237.</figDesc><table><row><cell>erate 10 negative samples to have a fair comparison to our</cell></row><row><cell>method. Results of QuatE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of score functions of the state-of-the-art relevant models. SENN. [Guan et al., 2018] defines three multi-layer neural networks with Relu activation function for head, relation and tail prediction. Then, it integrates them into one loss function to train the model. TBNN. [Han</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* This work is the arxiv version of the paper uploaded in openreview</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The proof for relations Asymmetric, Negation, Inverse, Reflexive, Irreflexive and Composition are similarly done.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. Regarding the assumption of the theorem, Ω E is a compact set. Ω is a compact set, since the Cartesian product of two compact set is also compact . Regarding lemma 2.1 in , given N disjoint regions K 1 , . . . , K N ⊂ R 2d , there exists at least one continuous function f : R 2d → R such that f (x) = c i when x ∈ K i , i = 1, . . . , N . c i , i = 1, . . . , N are arbitrary distinct constant values. Therefore, dealing with ground truth, c i ∈ {0, 1}, there exists a continuous function f that represents the ground truth. Because F is dense in C(R 2d ) or C(Ω), there exists at least one neural network in F that approximates the function f . As a conclusion, there exists a LogicENN that can represent the ground truth.</p><p>Remark: The density assumption of F in Theorem 3 depends on the activation function of Equation (1) of the paper. When the activation function is continuous, bounded, and non-constant, then F is dense in C(Ω) for every compact set Ω. When it is unbounded and non-constant, then the set is dense in L p (µ) for all finite measure µ. In this case, the compactness condition can be removed. For non-polynomial activation functions which are locally essentially bounded, the set is dense in C(R d ).</p><p>Theorem 4. For all h, t, s ∈ E, set ξ h,t = 0 in column "Formulation based on NN" of <ref type="table">Table 6</ref>. For each relation type given in <ref type="table">Table 6</ref>, LogicENN can infer it if and only if the corresponding equation in column "Formulation based on NN" holds (ξ h,t is set to 0). Proof for the Equivalence Relation. Based on the theorem statement, we want to show that LogicENN can infer equivalence rule if and only if Φ T h,t (β r1 − β r2 ) = 0. If r is an equivalence relation, we have:</p><p>Without loss of generality, let M, 0 show scores of NN for positive and negative triples respectively. To be equivalence, both triples should be true or false simultaneously. Therefore,</p><p>From Equation <ref type="formula">(1)</ref> </p><p>Proof for the Symmetric Relation. Based on the theorem statement, we want to show that LogicENN can infer sym-</p><p>To be Symmetric relation, both triples should be true or false simultaneously. Therefore</p><p>From Equation <ref type="formula">(1)</ref> in the paper, we have Φ T h,t β r1 − Φ T h,t β r2 = 0. Therefore, we have:</p><p>Proof for the Implication Relation. Based on the theorem statement, we want to show that LogicENN can infer implication rule if and only if Φ T h,t (β r1 − β r2 ) ≤ 0. If r is implication rule, we have:</p><p>(h, r 1 , t) → (h, r 2 , t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To satisfy the implication rule</head><p>From Equation <ref type="formula">(1)</ref> in the paper, we have:</p><p>Proof for the Transitivity Relation. To prove transitivity, we can use the truth table of the rule. Considering Equation <ref type="formula">(1)</ref>, we already assume that f r (h, t) = M &gt; 0 denotes True and f r (h, t) = 0 denotes False. In the following conditions, the rule is True: <ref type="figure">If (h, r, t)</ref> is True, (t, r, s) is True then (h, r, s) is True.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule</head><p>Definition ∀h, t, s ∈ E : . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulation based on score function</head><p>Formulation based on NN Equivalent regularization form (Denoted as R i in Equation <ref type="formula">(2)</ref> )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Re-evaluating embedding-based knowledge graph completion methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akrami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08359</idno>
	</analytic>
	<monogr>
		<title level="m">Lifted rule injection for relation embeddings</title>
		<editor>T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification ability of single hidden layer feedforward neural networks</title>
		<idno type="arXiv">arXiv:1802.04868</idno>
	</analytic>
	<monogr>
		<title level="m">27th ACM-CIKM</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>IEEE TNN. Kazemi and Poole, 2018] S.M Kazemi and D. Poole. Simple embedding for link prediction in knowledge graphs</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacroix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07297</idno>
		<idno>arXiv:1504.06662</idno>
	</analytic>
	<monogr>
		<title level="m">Canonical tensor decomposition for knowledge base completion</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A translation-based knowledge graph embedding preserving logical property of relations</title>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quaternion knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10281</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Re-evaluating embedding-based knowledge graph completion methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akrami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08359</idno>
	</analytic>
	<monogr>
		<title level="m">Lifted rule injection for relation embeddings</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ACM-CIKM</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dettmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embedding using simple constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classification ability of single hidden layer feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04868</idno>
	</analytic>
	<monogr>
		<title level="m">27th ACM-CIKM</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>IEEE TNN. Kazemi and Poole, 2018] S.M Kazemi and D. Poole. Simple embedding for link prediction in knowledge graphs</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacroix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07297</idno>
		<idno>arXiv:1504.06662</idno>
	</analytic>
	<monogr>
		<title level="m">Canonical tensor decomposition for knowledge base completion</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A translation-based knowledge graph embedding preserving logical property of relations</title>
		<idno type="arXiv">arXiv:1904.10281</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Quaternion knowledge graph embedding</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
