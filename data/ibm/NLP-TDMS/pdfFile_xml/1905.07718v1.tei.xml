<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Pose Affordance: 3D Human Pose with Scene Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">UC Irvine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Chen</surname></persName>
							<email>liyanc@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">UC Irvine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Rathore</surname></persName>
							<email>rathores@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">UC Irvine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
							<email>daeyuns@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">UC Irvine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
							<email>fowlkes@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">UC Irvine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geometric Pose Affordance: 3D Human Pose with Scene Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Full 3D estimation of human pose from a single image remains a challenging task despite many recent advances. In this paper, we explore the hypothesis that strong prior information about scene geometry can be used to improve pose estimation accuracy. To tackle this question empirically, we have assembled a novel Geometric Pose Affordance dataset 1 , consisting of multi-view imagery of people interacting with a variety of rich 3D environments. We utilized a commercial motion capture system to collect goldstandard estimates of pose and construct accurate geometric 3D CAD models of the scene itself.</p><p>To inject prior knowledge of scene constraints into existing frameworks for pose estimation from images, we introduce a novel, view-based representation of scene geometry, a multi-layer depth map, which employs multi-hit ray tracing to concisely encode multiple surface entry and exit points along each camera view ray direction. We propose two different mechanisms for integrating multi-layer depth information pose estimation: input as encoded ray features used in lifting 2D pose to full 3D, and secondly as a differentiable loss that encourages learned models to favor geometrically consistent pose estimates. We show experimentally that these techniques can improve the accuracy of 3D pose estimates, particularly in the presence of occlusion and complex scene geometry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate estimation of human pose in 3D from image data would enable a wide range of interesting applications in emerging fields such as virtual and augmented reality, humanoid robotics, and monitoring mobility and fall prevention in aging populations. Interestingly, many such applications are set in relatively controlled environments (e.g., the home) where large parts of the scene geometry are relatively static (e.g., walls, doors, heavy furniture). We are interested in the following question, "Can strong knowledge <ref type="bibr">1</ref> The dataset is available online here: https://wangzheallen.github.io/GPA.html of the scene geometry improve our estimates of human pose from images?".</p><p>Consider the images in <ref type="figure" target="#fig_0">Figure 1</ref>. Intuitively, if we know the 3D locations of surfaces in the scene, this should constrain our estimates of pose. Hands and feet should not interpenetrate scene surfaces, and if we see someone sitting on a surface of known height we should have a good estimate of where their hips are even if large parts of the body are occluded. This general notion of scene affordance 2 has been explored as a tool for understanding functional and geometric properties of a scene <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b17">19]</ref>. However, the focus of such work has largely been on using estimated human pose to infer scene geometry and function.</p><p>Surprisingly, there has been little demonstration of how scene knowledge can improve pose estimation. Traditional 3D pose datasets have focused on people freely performing actions in large open spaces and have focused on kinematic and dynamic constraints which are scene agnostic. Recent examples include <ref type="bibr" target="#b21">[23]</ref> which proposes temporal filter and skeleton fitting with pictorial structure constraints, <ref type="bibr" target="#b38">[40]</ref> which utilizes constraint on bone-length ratios for 2D <ref type="bibr" target="#b0">2</ref> "the meaning or value of a thing consists of what it affords." -JJ <ref type="bibr" target="#b8">Gibson (1979)</ref> and 3D estimation with weak supervision, and <ref type="bibr" target="#b0">[2]</ref> explores to use of joint angle priors. We expect one reason that scene constraints have not been explored is lack of largescale datasets of 3D pose in rich environments. Methods have been developed on datasets like Human3.6M <ref type="bibr" target="#b14">[16]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b20">[22]</ref>, which do not include interesting geometric scene constraints (at most one chair or sofa) and are free from scene occlusion. Recent efforts have expanded to more precise 3D pose for in-the-wild environments <ref type="bibr" target="#b31">[33]</ref> but lack ground-truth scene descriptions.</p><p>Instead of tackling human pose estimation in isolation, we argue that systems should take into account available information about constraints imposed by complex environments. A complete solution must ultimately tackle two problems: (i) estimating the geometry and free space of the environment (even when much of that free space is occluded from view), (ii) integrating this information into pose estimation process. Tools for building 3D models of static environments are well developed and estimation of novel scene geometry from single-view imagery has also shown rapid progress. Thus, we focus on the second aspect under the assumption that high-quality geometric information is available as an input.</p><p>The question of how to represent geometry and incorporate the (hard) constraints it imposes into current learningbased approaches to modeling human pose is an open problem. There are several candidates for representing scene geometry: voxel representations of occupancy <ref type="bibr" target="#b23">[25]</ref> are straightforward but demand significant memory and computation to achieve reasonable resolution; Point cloud <ref type="bibr" target="#b2">[4]</ref> representations provide more compact representations of surfaces by sampling but lack topological information about which locations in a scene constitute free space. Instead, we propose to utilize multi-layer depth maps <ref type="bibr" target="#b27">[29]</ref> which provide a compact and nearly complete representation of geometric constraints that can be readily queried to verify pose-scene consistency.</p><p>We develop several approaches to utilize information contained in this representation. Since multi-layer depth is a view-centered representation of geometry, it can be readily incorporated as an additional input feature. We leverage estimates of 2D pose either as a heatmap or regressed coordinate and query the multi-layer depth map directly to extract features encoding local constraints on the z-coordinates of joints which can be used to predict geometry-aware 3D joint locations. Additionally, we introduce a differentiable loss that can encourage a model trained with such features to respect hard constraints imposed by scene geometry. We perform an extensive evaluation of our multi-layer depth map models on a range of scenes of varying complexity and occlusion. We provide both qualitative and quantitative evaluation on real data demonstrating that our pipeline improves upon scene-agnostic state-of-the-art methods for 3D pose estimation.</p><p>To summarize, our main contributions are: • We collect and curate a unique, large-scale 3D human pose estimation dataset with rich ground-truth scene geometry and a wide variety of pose-scene interactions (see e.g. <ref type="figure" target="#fig_0">Figure 1</ref>) • We propose a novel representation of scene geometry constraints: multi-layer depth map, and explore multiple ways to incorporate geometric constraints into contemporary learning-based methods for predicting 3D human pose.</p><p>• We experimentally demonstrate the effectiveness of integrating geometric constraints relative to two state-of-the-art scene-agnostic pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion capture for ground-truth 3D pose The work of <ref type="bibr" target="#b28">[30]</ref> offered one of the first large-scale 3D human pose estimation dataset with synchronized images and ground-truth 3D keypoint locations. <ref type="bibr" target="#b14">[16]</ref> scales their dataset to 3.6 million images covering a range of subjects and actions along with depth images an 3D scans of the human subjects. To overcome the limitations of marker-based data collection, several marker-less approaches have also been used. <ref type="bibr" target="#b29">[31]</ref> and <ref type="bibr" target="#b20">[22]</ref> propose marker-less capturing system (based on inertial measurement sensors and recording in green screen studio with more cameras) and also capture 3D human pose estimation in the wild. <ref type="bibr" target="#b15">[17]</ref> creates a panoptic studio and capture poses with 10 pre-calibrated RGBD cameras. <ref type="bibr" target="#b39">[41]</ref> also explores motion capture both indoor and outdoor using a Drone. However, since these datasets focus on pose and action recognition, they involve relatively simple environments with weak geometric constraints. In contrast, our dataset not only provides the 3D human joints ground truth but also rich geometry, offering a promising test-bed for the research in 3D human pose estimation with rich geometric affordance. We provide more details of comparison between recent 3d human pose estimation dataset in <ref type="table">Table  1</ref>.</p><p>Modeling of affordance The term "affordance" was coined by J Gibson <ref type="bibr" target="#b8">[10]</ref> in 1979 to capture the notion that meaning and relevance of many objects in the environment are largely defined in relation the ways in which an individual can functionally interact with them. For computer vision, this suggests scenarios in which the natural labels for some types of visual content may not be object categories or geometry but what human interactions they afford. <ref type="bibr" target="#b9">[11]</ref> presents a human-centric paradigm for scene understanding by modeling the physical interactions between the two. <ref type="bibr" target="#b7">[9]</ref> rely on pose estimation method to extract functional and geometric constraints about the scene and use the constraint to improve estimates of 3D scene geometry. <ref type="bibr" target="#b34">[36]</ref> Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frames</head><p>Scenes Year Property HumanEva <ref type="bibr" target="#b28">[30]</ref> 80k ground plane 2010 1st synchronized 3d pose and image dataset Human36M <ref type="bibr" target="#b14">[16]</ref> 3 <ref type="table" target="#tab_1">.6M  chairs  2014  with human scan and MR test set  MPI-INF-3DHP [22]</ref> 3k chairs, sofa 2017 indoor and outdoor, markerless Total Capture <ref type="bibr" target="#b29">[31]</ref> 1.9M ground plane 2017 IMU(inertial measurement unit) and 2d matte Surreal <ref type="bibr" target="#b30">[32]</ref> 6M ground plane 2017 Mosh+ SMPL body, depth, body parts.etc ski-pose PTZ camera <ref type="bibr" target="#b25">[27]</ref> 10k ski facility 2018 skiing 3DPW <ref type="bibr" target="#b31">[33]</ref> 51k in the wild 2018 IMUs and phone captured videos GPA 0.7M boxes, chairs, stairs 2019 affordance learning and full scene geometry <ref type="table">Table 1</ref>. Comparison of existing dataset for training and evaluating 3d human pose estimation. While other datasets mainly focus on the way to capture more rich information from human: both markers and IMUs. Our dataset focuses on providing full geometry mesh and designing different action subset for evaluation of human scene interaction.</p><p>collects a large-scale sitcom dataset with not only scenes but also the same scenes with humans. Leveraging stateof-the-art pose estimation and generative model <ref type="bibr" target="#b16">[18]</ref>, they can tell what kind of pose can each sitcom scene affords.</p><p>[19] build a fully automatic 3D pose synthesizer to predict semantically plausible and physically feasible human poses within a given scene. <ref type="bibr" target="#b22">[24]</ref> applies an energy-based model on synthetic videos to improve both scene and human motion mapping. Rather than labeling image content based on poses we observe, our approach is instead estimating scene affordance directly from physical principles and leverage those in constraining estimates of human pose and interactions with the scene. Our work is also closely related to earlier work on scene context for object detection. <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b12">14]</ref> used estimates of ground-plane geometry to reason about location and scales of objects in an image. More recent work such as <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b19">21]</ref> use more extensive 3D models of scenes as context to improve object detection performance in outdoor scenes. Our work on human pose differs in that humans are articulated objects which make incorporating such constraints more complicated as the resulting predictions should simultaneously satisfy both scene-geometric and kinematic constraints of the body.</p><p>Constraints in 3D human pose estimation Estimating 3D human pose from monocular image or video is an ill-posed problem which can benefit from prior constraints on prediction. <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b10">12]</ref> utilize ground-plane as prior to get better 3d human pose. <ref type="bibr" target="#b5">[7]</ref> models kinematics, symmetry and motor control using an RNN when predicting 3D human joints directly from 2D key point. <ref type="bibr" target="#b36">[38]</ref> proposes an anthropometrically adversarial network as a regularizer. <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b40">42]</ref> constructs a graphical model encoding priors to fit to 3D pose reconstruction. <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b3">5]</ref> first builds a large 3D human pose set and treats it as a matching or classification problem. <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b25">27]</ref> explores joint constraint in 3D and geometric consistency from multi-view images. <ref type="bibr" target="#b38">[40]</ref> improve the performance by adding constant bone ratio constraint. To the best of our knowledge, our work is the first to exploit strong geometric constraint provided by the scene to improve 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Geometric Pose Affordance Dataset</head><p>To collect a rich dataset for studying interaction of scene geometry and human pose, we designed a set of 3 action scripts performed by 13 subjects, each of which takes place in one of 8 scene arrangements. In this section, we describe the dataset components and the collection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Human Poses and Subjects</head><p>The three groups of designed poses each put emphasis on semantic actions, mechanical dynamics of skeletons, and pose-scene interactions. Thus, we name them by Action, Motion, and Interaction Set respectively. The semantic actions of Action Set are constructed from a subset of Human3.6M <ref type="bibr" target="#b14">[16]</ref>, namely, Direction, Discussion, Writing, Greeting, Phoning, Photo, Posing and Walk Dog to provide a connection for comparisons between our dataset and the de facto standard benchmark. Motion Set includes poses with more dynamic range of motion, such as running, sideto-side jumping, rotating, jumping over, and improvised poses from subjects. Interaction Set mainly consists of close interactions between poses and object boundaries to provide ground truth for modeling affordance in 3D. There are three main poses in this group: Sitting, Touching, Standing on, corresponding to typical affordance relations Walkable, Reachable, Sittable <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b9">11]</ref>. The 13 subjects include 9 males and 4 female with roughly the same age and medium variations in heights approximately from 155cm to 190cm, giving comparable subject diversity to Human3.6M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Recording and Motion Capture</head><p>As shown in <ref type="figure">Figure 2</ref>.a, there are two types of camera, RGBD and RGB, with 5 distinct locations in the capturing studio. All 5 cameras have a steady 30fps frame rate but their time stamps are only partially synchronized, and thus require an additional stage in processing that we de- <ref type="figure">Figure 2</ref>. a: Motion capture setup. We utilized multiple computers for capture, one acquires 3 KinectV2 data streams and the other one controls 2 HD cameras and the VICON mocap system. b: sample image of a human interacting with scene geometry. c: the corresponding 3D Maya model scene mesh with acquired human skeleton for visualization. d-f: corresponding first three layers of multi-layer depth map representation of scene geometry. d corresponds to a traditional depth map, recording the depth of the first surface of scene geometry from the same camera view of b. e is when the multi-hit ray leaves the first layer of objects (e.g. the backside of the boxes). f is the depth when the multi-hit view ray hits a third surface (e.g., floor behind the box). scribe below. The color sensor of 5 cameras have the same 1920x1080 resolution and the depth sensor of 3 kinect v2 cameras has a resolution at 640x480. Coexisting with the image capturing system, the motion capture system is a standard VICON [1] system with 28 pre-calibrated cameras covering the capture space to accurately estimate the 3D coordinates of IR-reflective tracking markers that we attach to the surface of subjects and subjects. This studio layout and recorded data is further illustrated in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometry Layouts and Arrangements</head><p>Unlike previous works that only focus on human poses without many objects present <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b20">22]</ref>, we introduce a variety of scene geometries with arrangements of 9 cuboid boxes in the scene. We capture the scene from 5 distinct viewpoints and two types of cameras, three of which include a depth sensor (KinectV2). The resulting images exhibit substantially more occlusion of subjects than existing datasets (as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>). We capture 1 or 2 subjects for each scene geometry and have a total of 8 scene geometries (see Appendix).</p><p>To record static scene geometry, we measure physical dimension of all the objects as well as scan with a Kinect sensor. We utilize additional motion-capture markers attached to the corners and center face of each surface so that we can easily align geometric models of the boxes with the global coordinate system of the motion capture system. We also use the location of these markers when visible in the 5 capture cameras in order to estimate extrinsic camera parameters in the same global coordinate system. This allowed us to quickly create geometric models of the scene which are well aligned to all calibrated camera views and the motion capture data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Geometry Representation</head><p>As mentioned previously, the scene geometry is represented as multi-layer depth maps computed by casting rays from a camera viewpoint and recording the depth of intersections with the scene mesh models. Mesh models are constructed with modeling software (Maya) with assistance from physical measurements. Multi-layer depth map: Multi-layer depth maps are defined as a map of ray entry and exit depths through all surfaces in a scene (as illustrated in <ref type="figure">Figure 2</ref> d − f ). Unlike standard depth-maps which only encode the geometry of visible surfaces in a scene (sometimes referred to as 2.5D), multi-layer depth provides a nearly 3 complete, view-based description of scene geometry. The multi-layer depth representation can be computed by performing multihit ray tracing from the camera. Specifically, the multi-hit ray tracing function g sends a ray from the camera cen- <ref type="figure">Figure 3</ref>. Overview of model architecture: we use ResNet-50 as our backbone to extract features from a human centered cropped image. The feature map is used to predict 2D joint location heatmaps and is also concatenated with encoded multi-layer depth map. The concatenated feature is used to regress the depth (z-coordinate) of each joint. The model is trained with a loss on joint location (joint regression loss) and scene affordance (geometric consistency loss). Argmax function is applied to the 2d joint heatmap to get the x,y location. The geometric consistency loss is described in more detail in ter towards a point on the image plane that corresponds to the pixel at (x, y) and outputs distance values g(x, y) = {t 1 , t 2 , t 3 , ..., t k } where k is the total number of polygon intersections at that pixel. Given a unit ray direction r and camera viewing direction v, the depth value at layer i is</p><formula xml:id="formula_0">D i (x, y) = t i r · v if i &lt;= k and D i (x, y) = ∅ if i &gt; k.</formula><p>In our case, the number of multi-layer depth maps is set to 15 which suffices to cover all scenes in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Processing Pipeline</head><p>The whole data processing pipeline includes validating motion capture pose estimates, camera calibration, joint temporal alignment of all data sources, and camera calibration. Unlike datasets such as <ref type="bibr" target="#b14">[16]</ref> which have few occlusion, many markers attached to the human body are occluded in the scene during our capture sessions. We spent 4 month on pre-processing with help of 6 annotators in total. There are three stages of generating ground truth joints from recorded VICON sessions: (a) recognizing and labeling recorded markers in each frame to 53 candidate labels which included three passes to minimize errors; (b) applying selective temporal interpolation for missing markers based on annotators' judgement. (c) removing clips with too few tracked markers. After the annotation pipeline, we compile recordings and annotations into 61 sessions recorded at 120fps by the VICON software. To temporally align these compiled ground-truth pose streams to image capture streams, (a) we first ask annotators to manually correspond 10-20 pose frames to image frames. (b) Then we estimate the scaling and offset parameters with RANSAC <ref type="bibr" target="#b6">[8]</ref>, linear regressing to convert all timestamps to a global timeline.</p><p>After we paired pose data and image frames, we generate the dataset by selecting non-redundant frames, estimating camera poses and rendering multi-layer depth maps at the calibrated camera positions. We use an adaptive approach to sampling frames. We consider frames with sufficient difference from adjacent ones as interesting frames. Here, the measure of difference between two skeletons is defined as the 75th percentile of L2 distances between corresponding joints (34 pairs per skeleton pair), since we want to both keep skeletons where few body parts moved significantly and be robust to the cases where skeletons have jerky movements caused by noise. With the measure of difference defined, we threshold the frames by choosing the change threshold as the 55th percentile, retaining 45% of total frames.</p><p>The RGB camera calibration is done manually using annotators' markings of corresponding image coordinates of visible markers (whose global 3D coordinates are known) and estimating camera parameters from those correspondences. With estimated camera parameters, specifically nonlinear ones, we correct the radial and lens distortions of the image so that they can be treated as projections from ideal pinhole cameras in later steps. Also, we performed visual inspection on all clips to check that the estimated camera parameters yield correct projects 3D markers to their corresponding locations in the image.</p><p>The last component of our dataset is the geometry of recorded scenes, which is represented as multi-layer depth maps. As mentioned before, we construct the scene geometry with 3D modeling software, Maya in this case. The scene geometry models are then rendered into multi-layer depth maps with given camera parameters from last step. We performed visual inspections to ensure the edges of objects in renderings overlay with images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Geometry-aware Pose Estimation</head><p>We now introduce two approaches for incorporating geometric affordance in CNN-based pose estimation, building on the baseline architecture of <ref type="bibr" target="#b38">[40]</ref>. Given an image I of a human subject, we aim to estimate the 3D human pose represented by a set of 3D joint coordinates of the human skeleton, P ∈ R J×3 where J is the number of joints. We follow the convention of representing each 3D coordinate in the local camera coordinate system associated with I, namely, the first two coordinates are given by image pixel coordinates and the third coordinate is the joint depth in metric coordinates, e.g., millimeters. We use P XY and P Z respectively as short-hand notations for the components of P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pose Estimation Baseline Model</head><p>We adopt the state-of-the-art ResNet-based network in <ref type="bibr" target="#b35">[37]</ref> as our 2D pose estimation module. The network output is a set of low-resolution heat-mapsŜ ∈ R H×W ×J , where each mapŜ[:, :, j] can be interpreted as a probability distribution over the j-th joint location. At test time, the 2D predictionP XY is given by the most probable (arg max) locations in S. This heat-map representation is convenient as it can be easily combined (concatenate or sum) with the other deep layer feature maps. To train this module, the loss function is</p><formula xml:id="formula_1">2D (Ŝ|P ) = Ŝ − G(P XY ) 2<label>(1)</label></formula><p>where G(P ) is a target distribution created from groundtruth P by placing a Gaussian with σ = 3 at each joint location.</p><p>To predict the depth of each joint, we follow the approach of <ref type="bibr" target="#b38">[40]</ref>, which combines the 2D joint heatmap and the intermediate feature representations in the 2D pose module as input to a joint depth regression module. These shared visual features provide additional cue for recovering full 3D pose. We train with a smooth 1 loss <ref type="bibr" target="#b24">[26]</ref> given by:</p><formula xml:id="formula_2">1smooth (P |P ) = 1 2 P Z − P Z 2 P Z − P Z ≤ 1 P Z − P Z − 1 2 o.w.<label>(2)</label></formula><p>Alternate baseline As a second baseline model we utilize the model of <ref type="bibr" target="#b18">[20]</ref>, which detects 2D joint locations and then trains a multi-layer perceptron to regress the 3D coordinates P from the vector of 2D coordinates P XY . We use the ResNet model <ref type="bibr" target="#b35">[37]</ref> to detect the 2d locations key points and also consider an upper-bound based on lifting the ground-truth 2d joint locations to 3d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Geometry Consistency Loss and Encoding</head><p>To inject knowledge of scene geometry we consider two approaches, geometric consistency loss which incorporates geometry during training and geometric encoding which makes scene geometry available as an input feature at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric consistency loss:</head><p>We design a geometric consistency loss (GCL) which specifically penalizes errors in pose estimation that violate scene geometry constraints. The intuition is illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. For a joint at 2D location (x, y), the estimated depth z should lie within one of a disjoint set of intervals defined by the multi-depth values at that location.</p><p>To penalize a joint prediction P j = (x, y, z) that falls inside a region bounded by front-back surfaces with depths D i (x, y) and D i+1 (x, y) we define a loss that increases linearly with the penetration distance inside the surface:</p><formula xml:id="formula_3">G(i) (P j |D) = min(max(0,P j Z − D i (P j XY )), max(0, D i+1 (P j XY ) −P j Z ))<label>(3)</label></formula><p>Our complete geometric consistency loss penalizes predictions which place any joint inside the occupied scene geometry</p><formula xml:id="formula_4">G (P |D) = j max i∈{0,2,4,...} G(i) (P j |D)<label>(4)</label></formula><p>Assuming {D i } is piecewise smooth, this loss is differentiable almost everywhere and hence amenable to optimization with SGD. The gradient of the loss "pushes" joint location predictions for a given example to the surface of occupied volumes in the scene.</p><p>Encoding local scene geometry: When scene geometry is available at test time (e.g., fixed cameras pointed at a known scene), it is reasonable to provide the model with an encoding of the scene geometry. Our view-centered multidepth representation of scene geometry can be naturally included as an additional feature channel in a CNN. We considered two different encodings of multi-layer depth.</p><p>(1) We crop the multi-layer depth map to the input frame, re-sample to the same resolution as the 2D heatmap using nearestneighbor interpolation, and offset by the depth of the skeleton root joint. (2) Instead of feeding the transformed multidepth values, we also consider a volumetric encoding of the scene geometry by sampling 64 depths centered around the root joint using a range based on the largest residual depth between the root and any other joint seen during training (approx. +/ − 1m). For each (x, y) location and depth, we evaluate the geometric consistency loss G at that point. This resulting encoding is of size H × W × 64 and encodes the local volume occupancy around the pose estimate.</p><p>In our experiments we found that the simple and efficient multi-layer depth encoding (1), performed the same or better than volumetric encoding (2) and utilized it exclusively for the results reported here.  Our multi-depth encoding of the scene geometry stores the depth to each surface intersection along this ray (i.e., the depth values Z0, Z1, Z2, Z3, Z4). Valid poses must satisfy the constraint that the joint depth falls in one of the intervals: ZJ &lt; Z0 or Z1 &lt; ZJ &lt; Z2 or Z3 &lt; ZJ &lt; Z4. The geometric consistency loss pushes the prediction ZJ towards the closest valid configuration, ZJ = Z2 .</p><p>Injection of scene geometry for alternate method: To inject the scene geometry into the second model <ref type="bibr" target="#b18">[20]</ref>, we use 2D joint locations as the query input to get the exact multi-depth values, and use values as encoded input or GCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Overall Training</head><p>Finally we combine the losses in Eq. 1, 2, and 4, the overall loss for each training image is</p><formula xml:id="formula_5">(P ,Ŝ|P, D) = 2D (Ŝ|P ) + 1smooth (P |P ) + geom (P |P, D)</formula><p>We follow <ref type="bibr" target="#b38">[40]</ref> and adopt a stage-wise training approach: Stage 1 initializes the 2D pose module using 2D annotated images (i.e., MPII dataset); Stage 2 trains the 3D pose estimation module, including the depth regression module as well as the 2D pose estimation module; Stage 3 training enables geometry-aware components (encoding input, geometric consistency loss) in addition to the modules in stage 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Dataset: Our GeometricPoseAffordance (GPA) dataset has 304,892 images along with scene geometry. We utilize  <ref type="table">Table 2</ref>. Numbers of frames in each test subset. We evaluate performance on different subsets of the test data split by the scripted behavior (Action/Motion/Interaction), subjects that were excluded from the training data (cross-subject) and novel actions (crossaction). Finally, we evaluate on a subset with significant occlusion (Occlusion) and a subset where many joints were near scene geometry (Close2Geometry).</p><p>82k images for held-out test evaluation. In addition to the three subsets -Action, Motion, and Interaction -that are inherited from the global split of the dataset based on script contents, we also report performance on 4 other subsets of the test data: cross-subject (CS), cross-action (CA), occlusion, close-to-geometry. Despite non-orthogonal splits for these four test subset, trait subsets give us finer characterizations of model performances in various scenarios: (a) CS subset includes clips from held-out subjects to evaluate generalization ability on unseen subjects; (b) CA subset includes clips of held-out actions from same subjects from the training set; (c) occlusion subset includes frames with significant occlusions (at least 10 out of 34 joints are occluded by objects); (d) close2geometry subset includes frames where subjects are close to objects (i.e. at least 8 joints have distance less than 175 mm to the nearest surface). Statistics of these testing subsets are shown in <ref type="table">Table  2</ref> Following <ref type="bibr" target="#b38">[40]</ref>, we also use the MPII <ref type="bibr" target="#b1">[3]</ref> dataset, a large scale in-the-wild human pose dataset for training the 2D pose module. It contains 25k training images and 2,957 validation images.</p><p>For the alternative baseline model we experiment on <ref type="bibr" target="#b18">[20]</ref>, we use the MPII pre-trained ResNet <ref type="bibr" target="#b35">[37]</ref> to detect the 2d human key point. We also evaluate performance when using the ground truth 2d human pose, which serve as an upper-bound for lifting <ref type="bibr" target="#b18">[20]</ref> based method.</p><p>Implementation details: We take a crop around the skeleton from the original 1920 × 1080 image and isotropically resize them to 256 × 256, so that projected skeletons have roughly the same size. Ground truth 2D joint location are adjusted accordingly. Following <ref type="bibr" target="#b38">[40]</ref>, the ground truth depth coordinates are normalized to [0, 1]. The backbone for all models is ResNet-50 <ref type="bibr" target="#b11">[13]</ref>  with RMSprop as the optimizer. For the <ref type="bibr" target="#b18">[20]</ref> based method we use the same process as above to detect 2d human joint and following the original paper by subtracting mean and dividing the variance for both 2d input and 3d ground truth. Evaluation metrics: Following standard protocols defined in <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b14">16]</ref>, we consider two evaluation metrics for experiments: MPJPE (mean per joint position error) and the 3D PCK (percent correctly localized keypoints) with a distance threshold of 150 mm. In computing the evaluation metrics, root-joint-relative joint locations are evaluated according to the standard evaluation protocol.</p><p>Ablative study: To demonstrate the performance of each component, we evaluate four variants of each model: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Result analysis</head><p>Analysis of baseline performance: <ref type="table" target="#tab_1">Table 3</ref> and 4 compares the proposed approaches against the baseline. For ResNet based method <ref type="bibr" target="#b38">[40]</ref> the baseline model achieves a MPJPE 96.5 mm, while the same method gives 64.9 mm on Human3.6M; For lifting-based method <ref type="bibr" target="#b18">[20]</ref>, baseline performs a MPJPE 68.2 mm with ground truth on GPA, while on Human36M it is 45.5 mm. This demonstrates the difficulty of our dataset and the necessity of including richer geometric constraints on understanding 3d human pose. The motion, occlusion and close2geometry subsets prove to be the most challenging as they involve large numbers of frames where subjects interact with the scene geometry.</p><p>Effectiveness of proposed methods: ResNet-E / SIM-P-E / SIM-G-E and ResNet-C / SIM-P-C / SIM-G-C models add geometric context as an input to the network or penalize predictions that violate constraints during training. We can see both methods yield improvement on all the test subsets. Not surprisingly, using geometry information during both training and test gives the best results, allowing the network to be geometry-aware and training with a loss that encourages predictions to respect those geometric constraints. We can see from table 3 that our full model, ResNet-F decreases the MPJPE by 2.1mm over the full test set. Among 4 subsets, the most significant improvement comes in the occlusion and close2geometry subsets. Our geometry-aware method decreases MPJPE in occlusion and close2geometry set by 5.4mm / 6.6mm and increase the PCK3d about 2% / 3%. <ref type="table">Table 4</ref> shows corresponding results for the SIM model. We can see the full model decreases the MPJPE for both predicted (SIM-P) and ground-truth based input (SIM-G) by 3 mm / 3.6 mm and improves 1.2% and 1.1% in terms of PCK3d. For subset of occlusion / close2geometry, they are decreased in terms of MPJPE by 8.6mm / 7 mm. And 4.2% / 2.8% in terms of PCK3d.</p><p>Errors by joint type: We partition the 16 human joints into the limb joints which are more likely to be interacting with scene geometry (out group) and the torso and hips (in group). The performance on these two subsets of joints is illustrated in <ref type="table">Table 5</ref>, which verifies our assumption that limb joint estimation (wrist,elbow,knees,ankles) benefits more from incorporating geometric scene affordance. One interesting conclusion we draw from table 5 of ResNet based method: good MPJPE does not guarantee good PCK3d. In group has better MPJPE than out group, however, the PCK3d is not comparable to out group.</p><p>Quantitative results: We show qualitative examples and its interaction with geometry in <ref type="figure" target="#fig_0">Fig 13.</ref> These examples show that ResNet-F has better accuracy in both xy localization and depth prediction and even resolves ambiguity under heavy occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this work, we introduce the first large scale 3D human pose estimate dataset with rich geometric pose affordance constraints. We propose multi-layer depth as a concise representation of scene geometry and explore two ef- <ref type="figure">Figure 5</ref>. MPJPE of predictions of ResNet-F and the baseline. Data points are from the close2geometry test set. We list the example performances by an increasing order of baseline MPJPE (red) with corresponding ResNet-F performances (GCL + encoding, in blue).</p><p>Among those examples, We also show 3 qualitative results, from left to right: (a) case shows ResNet-F improve over the baseline with respect to the depth prediction. (b,c) cases show ResNet-F improves over the baseline in all x, y, z axes. Furthermore, (b) case demonstrates ResNet-F can even resolve ambiguity under heavy occlusions with the aid of geometry information. We show the image with the estimated 2D pose(after cropping), 1st layer of multi-layer depth map and whether the joint is occluded or not. legend: hollow circles: occluded joints; solid dots: non-occluded joints; dotted lines: partially/completely occluded body parts; solid lines: non-occluded body parts.  <ref type="table">Table 4</ref>. MPJPE (in mm) and PCK3D on each subset and the full test set with SIM-P <ref type="bibr" target="#b18">[20]</ref>. The 2d prediction is based on the 2d prediction from MPII pre-trained ResNet (50 layers), while SIM-G <ref type="bibr" target="#b18">[20]</ref> is based on 2d ground truth as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE Action Set Motion Set Interaction Set Occlusion</head><p>fective ways to incorporate geometric constraints into training in an end-to-end fashion. There are many other alternatives for representing geometric scene constraints. We hope the availability of this dataset will inspire future work on geometry-aware feature design and affordance learning for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE</head><p>In Group Out Group Full Baseline <ref type="bibr" target="#b38">[40]</ref> 87.  <ref type="table">Table 5</ref>. Performance grouped by joints, with out group joints: rightfoot, rightleg, leftleg, leftfoot, righthand, rightforearm, leftforearm,lefthand, and in group joints: rightupleg, leftupleg, hips, spine1, head, neck, rightarm, leftarm. We can see that PCK3D and MPJPE are both important evaluation criteria which performs differently between in groups and out groups (in groups perform good on MPJPE while out groups perform good on PCK3d for ResNet <ref type="bibr" target="#b38">[40]</ref> based method). <ref type="figure" target="#fig_6">Figure 6</ref> shows sample images for different script segments when capturing: action/motion/interaction subset. We provide the illustration of all camera views from one scene in <ref type="figure">Figure 7</ref>, with corresponding multi-layer depth maps in <ref type="figure">Figure 8</ref>. In addition, all 8 scenes in our dataset are summarized in <ref type="figure" target="#fig_7">Figure 9</ref> with corresponding multi-layer depth map in <ref type="figure" target="#fig_0">Figure 10</ref>. Lastly, we illustrate all the scene geometry meshes in <ref type="figure" target="#fig_0">Figure 11</ref>. <ref type="figure" target="#fig_0">Figure 12</ref> summarizes statistics on the number of occluded joints as well as the distribution of which multi-depth layer is closest to a joint.</p><p>There are sample images of each subset included in <ref type="figure" target="#fig_6">Figure 6</ref> to show their different emphasis on pose, geometry, and interactions: action set focuses on the mimicking actions from Human3.6M <ref type="bibr" target="#b14">[16]</ref> and probing how state-ofthe-art algorithm performs on these actions with geometry added on. The motion set focuses on dynamic actions in which the subjects move quickly and take on poses which are not statically stable; The interaction set provides the test bed for studying affordance learning between human pose and environment geometry.</p><p>In addition, we attach a demo video <ref type="bibr" target="#b2">4</ref> showing the detail of full resolution video, cropped video with ground truth joints/markers overlay. We also show the subject id (here we use anonymous), take name, camera name, video time id, mocap time id, bone length (which is constant overtime), velocity, number of valid markers, invisible joints, and invisible markers (there are 53 markers and 34 joints for VI-CON system). The video is sampled from 10 clips from 'Action', 'Motion' set with the same subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Breakdown and Qualitative Evaluations</head><p>We provide more detailed quantitative performance comparisons between baseline <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b18">20]</ref> model prediction, and full model prediction for each joint on our GPA dataset in <ref type="table">Table 6</ref>, 7 and 8 (MPJPE) and <ref type="table" target="#tab_7">Table 9</ref>, 10 and 11 (PCK3D).</p><p>For the joint definition, we use joints from the human skeleton rig fit by the VICON motion capture system. To make it consistent with pre-defined set like MPII <ref type="bibr" target="#b1">[3]</ref>. We have our: 0: rightfoot, 1: rightleg, 2: rightupleg, 3: leftupleg, 4:leftleg, 5: leftfoot, 6: hips (root joint) , 7: spine1, 8: head, 9:site (neck), 10: righthand, 11: rightforearm, 12: rightarm, 13: leftarm, 14: leftforearm, 15: lefthand, corresponding to MPII: 0 -r ankle, 1 -r knee, 2 -r hip, 3 -l hip, 4 -l knee, 5 -l ankle, 6 -pelvis, 7 -thorax, 8 -upper neck, 9 -head top, 10 -r wrist, 11 -r elbow, 12 -r shoulder, 13 -l shoulder, 14 -l elbow, 15 -l wrist. To give qualitative evaluations, we include visualization of model prediction with scene geometry in <ref type="figure" target="#fig_0">Figure 13</ref> and <ref type="figure" target="#fig_0">Figure 14</ref>. The scene geometry is represented by the 1st layer of multi-layer depth map and the ground truth pose is included along with the predictions. The visualization and comparison is here to show how baseline models fail in some difficult scenarios while geometry priors effectively help our model to resolve to physically sound pose configurations. <ref type="figure">Figure 7</ref>. Illustration of 5 camera views of the same static scene without subjects performing actions. The Cam0, Cam1, Cam2 are from Kinect V2 cameras while Cam3 and Cam4 are from the other 2 HD cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Baseline <ref type="bibr" target="#b38">[40]</ref>   <ref type="table">Table 6</ref>. MPJPE (in mm, the lower the better) by each joint we use for evaluation. <ref type="figure">Figure 8</ref>. The same 5 cam views as in <ref type="figure">Figure 7</ref> and with the first 3 layers of corresponding multi-layer depth map (for visualization clarity, we plot 1/depth).  <ref type="table">Table 7</ref>. MPJPE (in mm, the lower the better) by each joint we use for evaluation.   <ref type="table">Table 8</ref>. MPJPE (in mm, the lower the better) by each joint we use for evaluation. <ref type="figure" target="#fig_0">Figure 10</ref>. All of our scenes from Cam2 view and the corresponding first 3 layers of multi-layer depth map which encodes the entry and exits of the objects. Here the depth value is displayed as disparity (1/depth) for visualization purposes.    <ref type="table">Table 10</ref>. PCK3d (in mm, the lower the better) by each joint we use for evaluation. <ref type="figure" target="#fig_0">Figure 13</ref>. Visualization for the input images, with the overlay of ground truth pose in the same view(GT)(blue corresponds right human skeletons while red represents left human skeletons), column 2-4 is the first 3 layer of multi-layer depth map. Column 5 is the baseline prediction overlay with the 1st layer multi-layer depth map while column 6 is our ResNet-F prediction. We can view from the figures that geometry input and constraint always help the model give better prediction. The red rectangles highlight where baseline model give prediction violating geometry or not as good as the full model.   <ref type="table">Table 11</ref>. PCK3d (in mm, the lower the better) by each joint we use for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Sample images from our training set with geometry afforded pose: stepping on the stairs, sitting on the tables and touching boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 4 (a) and Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>(a) is the illustration of the geometry consistency loss as a function of depth along a specific camera ray corresponding to a predicted 2D joint location. In (b) the green line indicates the ray corresponding to the 2D location of the right foot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the baseline[40] / SIM-P / SIM-G [20]; ResNet-E / SIM-P-E / SIM-G-E, the model with encoded scene geometry input; ResNet-C / SIM-P-C / SIM-G-C, the model with encoded geometry consistency loss (GCL); ResNet-F / SIM-P-F / SIM-G-F , our full model with both encoded geometry priors and GCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Sample images from different subset (the same across capturing to evaluation): action, motion, interaction. For the image in action set we can see the human action: Direction, Discussion, Posing, Walk Dog and Greeting. From the images in the interaction set we can see Touching, Standing, Climbing Stairs, Sitting. From the images in the motion set we can see side-to-side jumping, running, jumping over, improvising poses from subjects. Designing different subsets makes it possible to pose research on different concepts: 3d pose estimation, affordance learning, stability and action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Illustration of all the 8 scenes we capture from Cam2 view. The different scene object layout can be viewed from this figure. Method SIM-G [20] SIM-G-E SIM-G-C SIM-G-F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Illustration of all the 8 scene meshes, visualized in MeshLab.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Left: Percentile of number of joints occluded in training set and testing set. Right: Percentile of number of joints closest to each layer of multi-layer depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 .</head><label>14</label><figDesc>More visualization for the cases that ResNet-F model prediction get more reasonable pose prediction than baseline methods with the help of multi-layer depth map.MethodSIM-G [20] SIM-G-E SIM-G-C SIM-G-F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>(50-layer ResNet), 2d heat map/depth map spatial size is 64 × 64 with one output channel per joint. Models are implemented in Pytorch MPJPE (in mm) and PCK3D on each subset and the full test set. ResNet-E is the base model with encoded geometry input, ResNet-C is with geometric consistency loss (GCL). ResNet-F is our full model with both geometry input and GCL loss. CS stands for cross-subject test while CA stands for cross-action test.</figDesc><table><row><cell>MPJPE</cell><cell cols="4">Action Set Motion Set Interaction Set Occlusion</cell><cell>CS</cell><cell>CA</cell><cell cols="2">Close2Geometry Full</cell></row><row><cell>Baseline[40]</cell><cell>97.2</cell><cell>99.6</cell><cell>89.7</cell><cell>120.5</cell><cell cols="2">99.4 89.2</cell><cell>118.1</cell><cell>96.5</cell></row><row><cell>ResNet-E</cell><cell>95.8</cell><cell>97.0</cell><cell>87.5</cell><cell>116.1</cell><cell cols="2">98.1 85.8</cell><cell>113.2</cell><cell>94.6</cell></row><row><cell>ResNet-C</cell><cell>96.6</cell><cell>97.9</cell><cell>88.3</cell><cell>117.9</cell><cell cols="2">98.8 86.7</cell><cell>116.3</cell><cell>95.4</cell></row><row><cell>ResNet-F</cell><cell>95.1</cell><cell>96.5</cell><cell>87.4</cell><cell>115.1</cell><cell cols="2">97.8 85.6</cell><cell>111.5</cell><cell>94.1</cell></row><row><cell>PCK3D</cell><cell cols="4">Action Set Motion Set Interaction Set Occlusion</cell><cell>CS</cell><cell>CA</cell><cell cols="2">Close2Geometry Full</cell></row><row><cell>Baseline[40]</cell><cell>81.4</cell><cell>80.7</cell><cell>85.2</cell><cell>72.2</cell><cell cols="2">81.3 83.6</cell><cell>71.4</cell><cell>81.9</cell></row><row><cell>ResNet-E</cell><cell>81.8</cell><cell>81.5</cell><cell>86.0</cell><cell>73.9</cell><cell cols="2">81.7 84.7</cell><cell>73.7</cell><cell>82.5</cell></row><row><cell>ResNet-C</cell><cell>81.6</cell><cell>81.6</cell><cell>85.7</cell><cell>73.7</cell><cell cols="2">81.5 84.5</cell><cell>72.1</cell><cell>82.3</cell></row><row><cell>ResNet-F</cell><cell>82.0</cell><cell>82.0</cell><cell>86.1</cell><cell>74.2</cell><cell cols="2">82.0 84.8</cell><cell>74.7</cell><cell>82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>PCK3D (the higher the better, with threshold of 150 mm) by each joint we use for evaluation.</figDesc><table><row><cell>Method</cell><cell cols="4">Baseline [40] ResNet-E ResNet-C ResNet-F</cell></row><row><cell>rightfoot</cell><cell>77.3</cell><cell>80.7</cell><cell>80.3</cell><cell>81.5</cell></row><row><cell>rightleg</cell><cell>90.0</cell><cell>90.7</cell><cell>90.6</cell><cell>91.3</cell></row><row><cell>rightupleg</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>leftupleg</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>leftleg</cell><cell>89.5</cell><cell>90.3</cell><cell>89.8</cell><cell>90.6</cell></row><row><cell>leftfoot</cell><cell>76.8</cell><cell>79.8</cell><cell>79.0</cell><cell>80.5</cell></row><row><cell>spine1</cell><cell>20.1</cell><cell>20.7</cell><cell>20.5</cell><cell>21.0</cell></row><row><cell>head</cell><cell>94.7</cell><cell>94.7</cell><cell>94.5</cell><cell>94.7</cell></row><row><cell>neck</cell><cell>68.7</cell><cell>71.5</cell><cell>69.6</cell><cell>71.6</cell></row><row><cell>righthand</cell><cell>74.1</cell><cell>75.8</cell><cell>75.5</cell><cell>76.5</cell></row><row><cell>rightforearm</cell><cell>83.8</cell><cell>84.6</cell><cell>84.4</cell><cell>85.1</cell></row><row><cell>rightarm</cell><cell>93.7</cell><cell>94.1</cell><cell>94.0</cell><cell>94.2</cell></row><row><cell>leftarm</cell><cell>92.9</cell><cell>93.0</cell><cell>93.1</cell><cell>93.2</cell></row><row><cell>leftforearm</cell><cell>86.0</cell><cell>86.7</cell><cell>86.5</cell><cell>87.0</cell></row><row><cell>lefthand</cell><cell>77.5</cell><cell>78.7</cell><cell>78.4</cell><cell>79.0</cell></row><row><cell>Method</cell><cell cols="4">SIM-P [20] SIM-P-E SIM-P-C SIM-P-F</cell></row><row><cell>rightfoot</cell><cell>71.2</cell><cell>73.0</cell><cell>71.8</cell><cell>73.6</cell></row><row><cell>rightleg</cell><cell>87.1</cell><cell>89.0</cell><cell>87.2</cell><cell>89.2</cell></row><row><cell>rightupleg</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>leftupleg</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>leftleg</cell><cell>88.2</cell><cell>90.2</cell><cell>88.0</cell><cell>90.6</cell></row><row><cell>leftfoot</cell><cell>69.9</cell><cell>72.2</cell><cell>70.2</cell><cell>72.2</cell></row><row><cell>spine1</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>head</cell><cell>96.3</cell><cell>96.2</cell><cell>96.3</cell><cell>96.3</cell></row><row><cell>neck</cell><cell>85.8</cell><cell>87.0</cell><cell>85.7</cell><cell>87.2</cell></row><row><cell>righthand</cell><cell>65.0</cell><cell>66.3</cell><cell>65.5</cell><cell>67.2</cell></row><row><cell>rightforearm</cell><cell>80.4</cell><cell>81.9</cell><cell>80.7</cell><cell>82.9</cell></row><row><cell>rightarm</cell><cell>95.1</cell><cell>95.0</cell><cell>95.2</cell><cell>95.4</cell></row><row><cell>leftarm</cell><cell>92.6</cell><cell>93.8</cell><cell>92.7</cell><cell>94.1</cell></row><row><cell>leftforearm</cell><cell>82.2</cell><cell>82.3</cell><cell>81.9</cell><cell>82.9</cell></row><row><cell>lefthand</cell><cell>67.0</cell><cell>66.0</cell><cell>66.6</cell><cell>66.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Surfaces tangent to a camera view ray are not represented</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Video Link: https://youtu.be/ZRnCBySt2fk</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This project is supported by NSF grants IIS-1813785, IIS1618806, IIS-1253538, CNS-1730158 and a hardware donation from NVIDIA. Zhe Wang personally thanks Shu Kong and Minhaeng Lee for helpful discussion, John Crawford and Fabio Paolizzo for providing support on the motion capture studio, and all the UCI friends who contribute to the dataset collection.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Statistics and Visualizations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A 3d-point-cloud feature for human-pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lifting gis maps into strong geometric context for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">People watching: Human actions as a cue for single-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<pubPlace>Boston; Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From 3d scene geometry to human workspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Markerless motion capture with unsynchronized moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">J G</forename><surname>Thorsten Thormahle1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arxiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Putting humans in a scene: Learning affordance in 3d indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nyc3dcars: A dataset of 3d vehicles in geographic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">imapper: Interaction-guided joint scene and human motion mapping from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In arxiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multiperson 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-layer depth and epipolar feature transformers for 3d scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronizedvideo and motion capture dataset and baseline algorithm forevaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Holistic 3d scene understanding from a single geo-tagged image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Binge watching: Scaling affordance learning from sitcoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human motion capture using a drone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
