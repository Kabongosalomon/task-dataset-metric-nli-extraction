<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Physical Reasoning Using Dynamics-Aware Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eltayeb</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Physical Reasoning Using Dynamics-Aware Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A common approach to solving physical-reasoning tasks is to train a value learner on example tasks. A limitation of such an approach is it requires learning about object dynamics solely from reward values assigned to the final state of a rollout of the environment. This study aims to address this limitation by augmenting the reward value with additional supervisory signals about object dynamics. Specifically, we define a distance measure between the trajectory of two target objects, and use this distance measure to characterize the similarity of two environment rollouts. We train the model to correctly rank rollouts according to this measure in addition to predicting the correct reward. Empirically, we find that this approach leads to substantial performance improvements on the PHYRE benchmark for physical reasoning [2]: our approach obtains a new state-of-the-art on that benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many open problems in artificial intelligence require agents to reason about physical interactions between object. Spurred by the release of benchmarks such as Tools <ref type="bibr" target="#b0">[1]</ref> and PHYRE <ref type="bibr" target="#b1">[2]</ref>, such physical reasoning tasks have become a popular subject of study <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Specifically, the tasks define an initial state and a goal state of the world, and require selecting an action that comprises placing one or more additional objects in the world. After the action is performed, the world simulator is unrolled to determine whether or not the goal state is attained. Despite their simplicity, benchmarks like PHYRE are surprisingly difficult to solve due to the chaotic nature of the dynamics of physical objects. Current approaches for physical reasoning problems can be subdivided into two main types:</p><p>1. Dynamics-agnostic approaches treat the problem as a "standard" contextual bandit that tries to learn the value of taking a particular action given an initial state, without using the simulator rollout in any way <ref type="bibr" target="#b1">[2]</ref>. An advantage of such approaches is that they facilitate the use of popular learning algorithms for this setting, such as deep Q-networks (DQNs; <ref type="bibr" target="#b6">[7]</ref>) However, the approaches do not use information from the simulator rollout as learning signal, which limits their efficacy.</p><p>2. Dynamics-modeling approaches learn models that explicitly aim to capture the dynamics of objects in the world, and use those models to perform forward prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Such forward predictions can then be used, for example, in a search algorithm to find an action that is likely to be successful. An advantage of such approaches is that it uses learning signal obtained from the simulator rollout. However, despite recent progress <ref type="bibr" target="#b9">[10]</ref>, high-fidelity dynamics prediction in environments like PHYRE remains an unsolved problem <ref type="bibr" target="#b2">[3]</ref>. Moreover, current approaches do not use the uncertainty in the dynamics model to select actions that are most likely to solve the task.</p><p>In this paper, we develop a dynamics-aware approach for physical reasoning that is designed to combine the strengths of the current two approaches. Our approach incorporates information on simulator rollout into the learning signal used to train DQNs. We show that the resulting models outperform prior models on the PHYRE benchmark, achieving a new state-of-the-art score of 85.2 on the 1B, within-template tranche of that benchmark (compared to 80.0 in prior work <ref type="bibr" target="#b2">[3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dynamics-Aware Deep Q-Networks</head><p>The basis of the model we develop for physical reasoning is a standard deep Q-network (DQN; <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>). We augment the loss function used to train this model with a dynamics-aware loss function. This allows the model-free DQN learner to explicitly incorporate dynamics of the environment at training time, without having to do accurate dynamics prediction at inference time.</p><p>Our backbone model is a ResNet <ref type="bibr" target="#b3">[4]</ref> that takes an image depicting the initial scene for task s as input.</p><p>The action, a, is parameterized as a (x, y, r)-vector 1 that is processed by a multilayer perceptron with one hidden layer to construct an action embedding. The action embedding is fused with the output of the third ResNet block using FiLM modulation <ref type="bibr" target="#b7">[8]</ref>. This fused representation is input into the fourth block of the ResNet to obtain a scene-action embedding, e s,a . We score action a by applying a linear layer with weights w 1 and bias b 1 on e s,a . At training time, we evaluate this score using a logistic loss that compares it against a label, y s,a , that indicates whether or not action a solves task s:</p><formula xml:id="formula_0">y s,a = w 1 e s,a + b 1 ; L = − (y s,a log (ŷ s,a ) + (1 − y s,a ) log (1 −ŷ s,a )) .<label>(1)</label></formula><p>Dynamics-aware loss. We develop an auxiliary loss function that encourages the embeddings of actions that lead to similar rollouts in a given scene to be similar. Given a pair of actions (a, a ) for task s, we compute a joint embedding of the two actions j s,a,a for that task as follows: p s,a = MLP(e s,a ); p s,a = MLP(e s,a ); j s,a,a = p s,a p s,a .</p><p>Herein, refers to a combination function: we use the element-wise product by default but we also experiment with outer products and concatenation in Section 3.1. We pass j s,a,a through another linear layer to predict the similarity of the two actions in task s. The model is trained to minimize a loss that compares the predicted similarity to a "ground-truth" similarity. Specifically, we bin the ground-truth similarity into K bins and minimize the cross-entropy loss of predicting the right bin:</p><formula xml:id="formula_2">u s,a,a = W 2 j s,a,a + b 2 ; L aux = y s u s,a,a − log   y s exp y s u s,a,a   . (3)</formula><p>Herein, y s is a one-hot vector of length K indicating the bin in which the ground-truth similarity falls. The model is trained to minimize L + L aux , assigning equal weight to both losses.</p><p>Measuring action similarity. To measure the ground-truth similarity, v a,a , between two actions a and a on task s, we run the simulator on the two scenes obtained after applying the actions. We track all objects throughout the simulator roll-outs, and measure the Euclidean distance between each object in one roll-out and its counterpart in the other roll-out. This results in distance functions, d a,a (o, t), for all objects o ∈ O (where t represents time). We convert the distance function into a similarity functions and aggregate all similarities over time and over all objects:</p><formula xml:id="formula_3">q a,a (o, t) = 1 − min(d a,a (o, t), α) α ; v a,a = o∈O T t=1 q a,a (o, t) T |O| ,<label>(4)</label></formula><p>where α is a hyperparameter that clips the distance at a maximum value, and T is the number of time steps in the roll-out. The similarity v a,a is binned to construct y s . See Appendix A for details.</p><p>Training. We follow <ref type="bibr" target="#b1">[2]</ref> and train the model using mini-batch SGD. We balance the training batches to contain an equal number of positive and negative task-action pairs. To facilitate computation of L aux , we further constrain the batch composition. First, we sample t tasks uniformly at random in a batch. For each task, we sample n actions that solve the task and n actions that do not solve the task. We compute the similarity, v a,a , for all 4n 2 action pairs for a task. To evaluate L aux , we average over these 4tn 2 action pairs. Simultaneously, we average L over the 2tn task-action pairs. Additional details on our training procedure as well as hyperparameter settings are presented in the appendix.</p><p>Inference. At inference time, the agent scores a set of A randomly selected actions using the scoring functionŷ t,ai . The agent proposes the highest-scoring action as a solution. If that action does not solve the task, the agent submits the subsequent highest-scoring action until the task is solved or until the agent has exhausted its attempts (whichever happens first).  <ref type="table" target="#tab_0">Table 1</ref>. For all ablation studies, we use the 4 folds on the validation splits; results are in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Implementation Details. We train our models as described in Section 2, using both ResNet-18 and ResNet-50 backbones. We use 100,000 batches with 512 samples per batch. Each batch contains 64 unique tasks with 8 actions per task, such that half of them solve the task (positives) and half of them do not (negatives). Training is performed using Adam <ref type="bibr" target="#b4">[5]</ref> with an initial learning rate of 3 · 10 −4 and a cosine learning rate schedule <ref type="bibr" target="#b5">[6]</ref>. We set K = 5, we set the maximum possible distance in a PHYRE scene α = √ 2, and we set the dimensionality of p s,a to 256. We train and test all models in both the within-template and the cross-template generalization settings. Following <ref type="bibr" target="#b1">[2]</ref>, we also study the effect of online updates during inference time in the cross-template setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ablation Study</head><p>The results of our ablation study are presented in <ref type="table" target="#tab_1">Table 2</ref>. In each subtable, we vary only one component of the model and keep all the other components to their default value. The default values that we used to produce our final results in <ref type="table" target="#tab_0">Table 1</ref> are underlined in the subtables.</p><p>Effect of network depth. We evaluate the effect of backbone depth in <ref type="table" target="#tab_1">Table 2a</ref>. We observe that ResNet-50 performs a little better than ResNet-18 in the within-template setting, but not in the cross-template setting. These results hold for both "vanilla" DQNs and our dynamics-aware DQNs.</p><p>Effect of projection layer. In Equation 2, we described an MLP to project the embedding from the backbone network. In <ref type="table" target="#tab_1">Table 2b</ref>, we test replacing this module by a linear layer or directly using the backbone embeddings. We find a two-layer MLP works slightly better and adopt it in our final model.</p><p>Effect of number of bins, K. We evaluate the effect of the number of bins, K, used for classifying action similarity values in L aux . The results in <ref type="table" target="#tab_1">Table 2c</ref> show that K = 5 performs well but using fewer bins works fine, too. We also compare the bin-classification approach to a regression approach that minimizes mean-squared error (MSE) on the action similarities, and find it to perform worse.</p><p>Effect of combination function. We compare different combination functions for computing the representations j s,a,a . <ref type="table" target="#tab_1">Table 2d</ref> presents the results of this comparison. We find that elementwise multiplication works substantially better than concatenation and matches the performance of combination via a bilinear layer. We opt to use element-wise multiplication over bilinear combination for our final model, as it is computationally cheaper and uses less parameters.</p><p>Effect of frames considered in action similarity measure. We evaluate the effect of changing the frames of the simulator roll-out used to compute the action similarity, v a,a . The results of this  , we visualize all the ground truth (GT) and top 10 predicted actions' positions (x, y) that solves the above two tasks, with darker color representing higher confidence. On Task A, our method performs similarly to a dynamics-agnostic baseline. In Task B where the incline is slanted the other way, however, the baseline model is confused between two possible sets of positions of the action. By contrast, our dynamics-aware DQN model is able to solve this task correctly. Finally in (c), we visualize all actions with &gt; 0.9 cosine similarity to any action that solves the task, with denser color =⇒ higher similarity. The illustration suggests that our dynamics-aware DQN model is able to rule out incorrect actions much more effectively than the baseline DQN model. evaluation in <ref type="table" target="#tab_1">Table 2e</ref> show that using the first 10 frames or the last 5 frames works best. Although the differences are small, using only the first or the last frame is clearly worse. In our final model, we average the action similarity over the last five 5 frames of the roll-out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to SOTA and Qualitative Analysis</head><p>Table 1 presents the AUCCESS and success percentage of our best dynamics-aware DQNs, and compares it to results reported in prior work. The results show the strong performance of our models: in the within-template setting, our models improve the prior state-of-the-art AUCCESS by 5 to 8 points. In the cross-template setting, our dynamics-aware DQN also outperforms it dynamics-agnostic counterpart but does not outperform a model that makes full dynamics prediction <ref type="bibr" target="#b2">[3]</ref>. Nevertheless, the results suggest dynamics-aware DQNs have the potential to improve physical-reasoning models. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates these results by visualizing how our dynamics-aware DQN model more effectively rules out parts of the action space that cannot lead to a successful solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented a dynamics-aware DQN model for efficient physical reasoning that is better at capturing object dynamics than baseline models, without having to do explicit forward prediction. Our best models substantially outperform prior work on the challenging PHYRE benchmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Action Similarity Metric</head><p>The similarity between two actions is computed from the object feature representation of the actions' rollouts provided by the PHYRE API. For two rollouts of two actions (a, a ) we use the notation that (x(o, t), y(o, t)) and (x (o, t), y (o, t)) are the locations of the object o at the timestep t in the rollouts of a and a respectively then:</p><formula xml:id="formula_4">T = min(t 1 , t 2 ) (5) q a,a (o, t) = 1 − min(d a,a (o, t), α) α (6) v a,a = o∈O T t=1 q a,a (o, t) T |O| ,<label>(7)</label></formula><p>where O is the set of moving objects in the scene, t 1 and t 2 are the lengths of the first and second rollouts respectively and α is a hyperparameter that clips the distance at a maximum value. When computing the metric using only "last n" frames the frames we consider are the frames from time (T − n + 1) to T . The similarity v a,a is binned to construct y s as follows:</p><formula xml:id="formula_5">y s = v a,a (K − 1) ,<label>(8)</label></formula><p>where · is an operator that rounds continuous numbers to the nearest integer and K is the number of bins used.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we take α = √ 2. This value is suggested by the PHYRE environment since the coordinates of locations in the scene fall in the square limited by the corner points (0, 0) and (1, 1) with the maximum possible Euclidean distance between two objects being √ 2. In other environments we might not have easy access to the maximum possible Euclidean distance or it might not be finite.</p><p>To study the sensitivity of our method with respect to this parameter choice we train a group of models with arbitrary values for α &lt; √ 2. Using a value α &gt; √ 2 corresponds to disabling distance thresholding altogether. We show the results in <ref type="table" target="#tab_2">Table 3</ref> and find the effect using an arbitrary distance threshold to be negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Ablations</head><p>Here we conduct one further ablation to those that were carried out in Section 3.1. Here we examine the effect of our batch composition on the models In [2] the task-actions pairs are sampled uniformly at random with the sole imposed constraint that the number of negative examples are equal to the number of positive examples in the batch. This label balancing is in line with standard practices such as over-sampling which are beneficial when training on datasets with heavy label imbalances. In our method, we keep this label balancing and to facilitate using the auxiliary losses we impose an additional constraint such that in each batch we have multiple actions for each task as described in Section 2. We examine the effect of this modified batch composition in <ref type="table" target="#tab_3">Table 4</ref> and we find that this change alone does not lead to a significant performance improvement.  <ref type="figure">Figure 2</ref>: Here we break down the AUCCESS by template in 2a and number of moving objects in 2b. We see our agents biggest gains are on templates where the baseline performs worst, while the baseline marginally outperforms our models in the templates where baseline was already performing well. We aggregate the templates by the number of moving objects in where we see our model outperforming the baseline across all numbers of moving objects.  <ref type="figure" target="#fig_0">Figure 1 (c)</ref>, showing action space embeddings color coded by similarity to GT actions, at different similarity thresholds. We observe our method leads to actions is able to rule out actions incapable of solving the task at all thresholds, and at 0.98 the selected actions are almost indistinguishable from GT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Study</head><p>We break down the AUCCESS improvement by template in <ref type="figure">Figure 2</ref> and try to characterize the the templates in which our method improves most over the baseline and we find in <ref type="figure">Figure 3</ref> that our method introduces the highest gains in templates which the performance of the baseline was lower. This holds even when comparing across templates where the baseline has still not reached maximum performance. In <ref type="figure">Figure 4</ref>, we show all actions for the given task, color coded by their similarity to GT actions for ours and baseline model. We find our dynamics-aware model is able to rule out incorrect actions much more effectively than the baseline, at all different levels of similarity thresholds considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>In (a) and (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Here we show baseline AUCCESS vs ∆AUCCESS from our method and find a statistically significant correlation with a Pearson correlation factor of -0.55. This shows we get highest gains on templates where the baseline performs poorly. Here we show an extended version of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AUCCESS and success percentage @10 of our approach compared to state-of-the-art models on the PHYRE-1B and 2B tiers in the within-template and cross-template generalization settings. Results are averaged over 10 test folds. We also report the corresponding standard deviations. 7±0.5 13.0±5.0 3.6±0.6 2.6±1.5 7.7±0.<ref type="bibr" target="#b7">8</ref> 6.8±4.7 3.2±0.8 2.2±1.7 MEM [2] 2.4±0.3 18.5±5.1 3.2±0.2 3.7±2.3 2.7±0.5 15.2±5.6 3.4±0.3 1.9±1.5 DQN [2] 77.6±1.1 36.8±9.7 67.8±1.5 23.2±9.1 81.4±1.8 34.5±9.7 74.9±1.7 22.4±9.5 39.9±8.9 76.4±1.1 23.2±7.4 89.1±1.5 38.5±9.7 83.1±1.2 22.7±8.4</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AUCCESS</cell><cell></cell><cell cols="4">Success Percentage @10</cell></row><row><cell></cell><cell cols="2">PHYRE-1B</cell><cell cols="2">PHYRE-2B</cell><cell cols="2">PHYRE-1B</cell><cell cols="2">PHYRE-2B</cell></row><row><cell></cell><cell>Within</cell><cell>Cross</cell><cell>Within</cell><cell>Cross</cell><cell>Within</cell><cell>Cross</cell><cell>Within</cell><cell>Cross</cell></row><row><cell cols="3">RAND [2] 13.Dec[Joint]1f [3] 80.0±1.2 40.3±8.0</cell><cell>-</cell><cell>-</cell><cell cols="2">84.1±1.8 39.2±8.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ours 85.2±1.3 DQN (Online) [2] -</cell><cell>56.2±10.5</cell><cell>-</cell><cell>39.6±11.1</cell><cell>-</cell><cell>58.1±10.9</cell><cell>-</cell><cell>41.6±11.7</cell></row><row><cell>Ours (Online)</cell><cell>-</cell><cell>56.4±9.9</cell><cell>-</cell><cell>40.9±10.2</cell><cell>-</cell><cell>59.1±10.4</cell><cell>-</cell><cell>43.3±10.6</cell></row><row><cell>3 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Dataset. We test our dynamics-aware deep Q-network (DQN) on the PHYRE benchmark on both tiers (1B and 2B) and both generalization settings: within-template and cross-template. Following [2], we use all 10 folds and evaluate on the test splits in our final experiments; the results are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AUCCESS on PHYRE-1B (averaged over 4 validation folds) observed in our ablation studies. Unless otherwise noted, results are for the within-template setting.(a) ResNet backbone depth. Frames used in v a,a .</figDesc><table><row><cell cols="3">Model Depth Within</cell><cell>Cross</cell><cell cols="3">(c) Number of bins. k AUCCESS</cell><cell>(e) Frames</cell><cell>AUCCESS</cell></row><row><cell>DQN</cell><cell>18</cell><cell cols="2">81.2±0.3 44.4±7.4</cell><cell>2</cell><cell cols="2">82.9±0.4</cell><cell>First 1</cell><cell>81.7±0.5</cell></row><row><cell>DQN</cell><cell>50</cell><cell cols="2">82.4±0.4 43.5±8.9</cell><cell>5</cell><cell cols="2">83.3±0.5</cell><cell>First 3</cell><cell>82.2±0.3</cell></row><row><cell>Ours</cell><cell>18</cell><cell cols="2">83.2±0.6 44.4±7.8</cell><cell>10</cell><cell cols="2">83.2±0.6</cell><cell>First 5</cell><cell>82.7±0.4</cell></row><row><cell>Ours</cell><cell>50</cell><cell cols="2">83.5±0.8 42.3±8.8</cell><cell>20</cell><cell cols="2">82.8±0.8</cell><cell>First 10</cell><cell>83.2±1.0</cell></row><row><cell cols="4">(b) Projection layer. AUCCESS</cell><cell cols="3">MSE (d) Combination function. 82.8±0.8</cell><cell>Last 1 Last 3 Last 5</cell><cell>82.8±1.2 82.9±1.0 83.6±1.1</cell></row><row><cell>None</cell><cell></cell><cell>82.7±0.6</cell><cell></cell><cell></cell><cell></cell><cell>AUCCESS</cell><cell>Last 10</cell><cell>82.5±0.6</cell></row><row><cell cols="2">Linear</cell><cell>82.7±0.8</cell><cell></cell><cell cols="2">Multiplication</cell><cell>83.2±0.6</cell><cell cols="2">Entire Rollout</cell><cell>82.7±0.8</cell></row><row><cell cols="2">2-layer MLP</cell><cell>83.1±0.4</cell><cell></cell><cell cols="2">Concatenation</cell><cell>82.2±0.8</cell><cell></cell></row><row><cell cols="2">3-layer MLP</cell><cell>82.8±0.5</cell><cell></cell><cell>Bilinear</cell><cell></cell><cell>83.2±0.4</cell><cell></cell></row><row><cell>GT</cell><cell cols="2">Baseline</cell><cell>Ours</cell><cell>GT</cell><cell></cell><cell>Baseline</cell><cell>Ours</cell><cell>Baseline</cell><cell>Ours</cell></row><row><cell></cell><cell cols="2">(a) Task A</cell><cell></cell><cell></cell><cell></cell><cell>(b) Task B</cell><cell></cell><cell>(c) Task B top actions space</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Maximum distance at which distances are clipped. We find stable performance across different clipping distance, and use 0.1 for final results.</figDesc><table><row><cell>α</cell><cell>AUCCESS</cell></row><row><cell>0.1</cell><cell>83.2±0.6</cell></row><row><cell>0.2</cell><cell>82.7±0.9</cell></row><row><cell>0.5</cell><cell>83.3±0.4</cell></row><row><cell>0.7 √ 2</cell><cell>83.3±0.8 83.4±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>DQN trained with our modified batch composition approach used for training our dynamics-aware models. We find the batching by itself does not lead to any gains.</figDesc><table><row><cell cols="2">Batching approach AUCCESS</cell></row><row><cell>Standard [2]</cell><cell>81.1±0.4</cell></row><row><cell>Ours</cell><cell>81.6±0.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the PHYRE-2B tier, the action is parametrized via a (x1, y1, r1, x2, y2, r2)-vector.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Tools challenge: Rapid trial-and-error learning in physical problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">A</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PHYRE: A new benchmark for physical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10734</idno>
		<title level="m">Forward prediction for physical reasoning</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning long-term visual dynamics with region proposal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02265</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09405</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamics-aware embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
