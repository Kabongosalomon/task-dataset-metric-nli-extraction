<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Goldman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Open University of Israel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of self-supervised monocular depth estimation has seen huge advancements in recent years. Most methods assume stereo data is available during training but usually under-utilize it and only treat it as a reference signal. We propose a novel self-supervised approach which uses both left and right images equally during training, but can still be used with a single input image at test time, for monocular depth estimation. Our Siamese network architecture consists of two, twin networks, each learns to predict a disparity map from a single image. At test time, however, only one of these networks is used in order to infer depth. We show state-of-the-art results on the standard KITTI Eigen split benchmark as well as being the highest scoring selfsupervised method on the new KITTI single view benchmark. To demonstrate the ability of our method to generalize to new data sets, we further provide results on the Make3D benchmark, which was not used during training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single-view depth estimation is a fundamental problem in computer vision with numerous applications in autonomous driving, robotics, computational photography, scene understanding, and many others. Although single image depth estimation is an ill-posed problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>, humans are remarkably capable of adapting to estimate depth from a single view <ref type="bibr" target="#b21">[22]</ref>. Of course, humans can use stereo vision, but when restricted to monocular vision, we can still estimate depth fairly accurately by exploiting motion parallax, familiarity with known objects and their sizes, and perspectives cues.</p><p>There is a large body of work on monocular depth estimation using classical computer vision methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, including several recent approaches based on convolutional neural networks (CNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>. These methods, how- † Work done while at the University of Southern California. ever, are supervised and require large quantities of ground truth data. Obtaining ground truth depth data for realistic scenes, especially in unconstrained viewing settings, is a complicated task and typically involves special equipment such as light detection and ranging (LIDAR) sensors.</p><p>Several methods recently tried to overcome this limitation, by taking a self-supervised approach. These methods exploit intrinsic geometric properties of the problem to train monocular systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. All these cases, assume that both images are available during training, though only one training image is used as input to the network; the second image is only used as a reference. Godard et al. <ref type="bibr" target="#b14">[15]</ref> showed that predicting both the left and the right disparity maps vastly improves accuracy. While predicting the left disparity using the left image is intuitive and straightforward, they also estimate the right disparity using the left image. This process is prone to errors due to occlusions and information missing from the left viewpoint. By comparison, we fully utilize both images when learning to estimate disparity from a single image.</p><p>We propose a self-supervised approach similar to that of Godard et al. <ref type="bibr" target="#b14">[15]</ref>. Unlike them, however, we exploit the symmetry of the disparity problem in order to obtain effective deep models. We observe that a key problem of existing methods is that they try to train a single network to predict both left and right disparity maps using a single image. This does not work well in practice since crucial information available in the right image is often occluded from the left viewpoint due to parallax (and vice versa). Instead, we propose a simple yet effective alternative approach of flipping the images around the vertical axis (vertical mirroring) and using them for training. In this way, the network only learns a left disparity map; right disparity maps are simply obtained by mirroring the right image, estimating the disparity, and then mirroring the result back to get the correct right disparity.</p><p>Specifically, we use a deep Siamese <ref type="bibr" target="#b4">[5]</ref> network that learns to predict a disparity map both from the left image and the flipped right image. By using a Siamese architec- <ref type="figure">Figure 1</ref>. System overview. Our approach uses stereo data during training, but works on single image data during test time. Both images are treated equally by mirroring the right image. We use Siamese <ref type="bibr" target="#b4">[5]</ref> networks with weight sharing. This reduces computational cost and allows us to run the system on single image during test time.</p><p>ture, we learn to predict each disparity map using its corresponding image. By mirroring the right image, prediction of both left and right disparity maps becomes equivalent. We can therefore train both Siamese networks using shared weights. These shared weights have the dual advantage of reducing the computational cost of training and, as evident by our results, resulting in improved networks. A high level overview of our approach is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>We evaluate our proposed system on the KITTI <ref type="bibr" target="#b12">[13]</ref> and Make3D <ref type="bibr" target="#b42">[43]</ref> benchmarks and show that, remarkably, in some cases our self-supervised approach outperforms even supervised methods. Importantly, despite the simplicity of our proposed approach and the improved results it offers, we are unaware of previous reports of methods which exploit the symmetry of stereo training in the same manner as we propose to do.</p><p>To summarize we provide the following contributions:</p><p>• A novel approach for self-supervised learning of depth (disparity) estimation which trains on pairs of stereo images simultaneously and symmetrically.</p><p>• We show how a network trained on stereo images can naturally be used for monocular depth estimation at test time.</p><p>• We report state-of-the-art, monocular disparity estimation results which, in some cases, even outperform supervised systems.</p><p>Our code and models are available online from the following URL: https://github.com/mtngld/lsim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There is a long line of research on the problem of depth estimation. Much of this work assumed image pairs <ref type="bibr" target="#b45">[46]</ref> or sequences <ref type="bibr" target="#b23">[24]</ref> are available in order to infer depth. We focus on the related but different task of monocular depth estimation, where only a single image is used as input. Example based methods. Example based methods use reference images with corresponding, per-pixel, ground truth depth values as priors when estimating depth for a query image. An early example is the Make3D model of Saxena et al. <ref type="bibr" target="#b42">[43]</ref>, which transforms local image patches into a feature vectors and then uses a linear model trained offline to assign depth for each query patch. These estimates were then globally adjusted using a Gaussian Markov random field (MRF). Hassner et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> suggested an on-the-fly example generation scheme which was used to produce depth estimates using a global coordinate descent method. Example based methods explicitly assume familiarity with the object classes they are being applied to. Patch based methods further have difficulties ensuring that their solutions are globally consistent. Scene assumption methods. Shape-from-X methods make assumptions on the properties of the scene in order to infer depth. Some use shading in order to estimate 3D shape from a single image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b48">49]</ref>. Vanishing points and other perspective cues have also been used for monocular depth estimation <ref type="bibr" target="#b7">[8]</ref>. Ladicky et al. <ref type="bibr" target="#b30">[31]</ref> suggested incorporating object semantics into the model, thus requiring additional labeled data. When objects belong to a single class, class statistics are used, as in the 3D morphable models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Other scene assumptions include the use of texture <ref type="bibr" target="#b1">[2]</ref> and focus <ref type="bibr" target="#b39">[40]</ref>. In the absence of stereo images, all these methods use visual cues inspired by human perception. Whenever these cues are absent from the scene, these approaches fail.</p><p>Supervised, deep, monocular methods. Several deep learning-based methods were recently proposed for solving this problem. These methods formulated the problem using a regression function from an input image to its corresponding depth map <ref type="bibr" target="#b8">[9]</ref>. Xie et al. <ref type="bibr" target="#b51">[52]</ref> used a neural network to estimate a probabilistic disparity map, followed by a selection layer. Liu et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> combined the neural net approach with a conditional random field (CRF) in order to address the global nature of the problem. Roy et al. <ref type="bibr" target="#b41">[42]</ref> proposed neural regression forest (NRF), a random forest method where at each tree node a shallow CNN is used. Laina et al. <ref type="bibr" target="#b31">[32]</ref> trained an end-to-end fully convolutional network with residual connections and introduced the reversed Huber loss for this task. More recently, Fu et al. <ref type="bibr" target="#b9">[10]</ref> suggested using ordinal regression to model this problem.</p><p>Although deep supervised methods achieve accurate results, they require large amounts of image data with corresponding ground truth depth maps. Collecting such datasets at scale is very difficult and expensive.</p><p>Self-supervised, deep, monocular methods. Garg et al. <ref type="bibr" target="#b10">[11]</ref> were first to suggest a self-supervised method for this problem, relying on the geometrical structure of the scene. First, they estimate a disparity image for the left image. This disparity map is then used to inverse warp the right image and measure reconstruction loss ( <ref type="figure" target="#fig_0">Fig. 2 (left)</ref>).</p><p>Our approach is related to the one recently described by Godard et al. <ref type="bibr" target="#b14">[15]</ref>. Whereas they apply similar reasoning for data augmentation, we use a specially crafted Siamese network to better utilize the training data. Please see Sec. 3.5 for a detailed discussion on the differences between their approach and ours.</p><p>Our method is further related to the one proposed by Kuznietsov et al. <ref type="bibr" target="#b29">[30]</ref> who also use two networks. There are some important differences between our work and theirs. First, we use two networks with weight sharing, which reduces model size and allows applying the network at test time in monocular settings. Second, they use depth information as a semi-supervisory signal. We do not use any depth information or any other labels. We report results that nearly match theirs despite the fact that our method is completely self-supervised.</p><p>Some methods suggested incorporating both depth and pose estimation <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b56">57]</ref>. We focus solely on depth estimation and show our results to outperform the ones reported by these recent methods. There is also a line of work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref> where for using self-supervision for extracting depth from monocular video, here we do not assume sequential data is at hand.</p><p>Siamese networks. Siamese networks were first suggested by Bromley et al. <ref type="bibr" target="#b4">[5]</ref> and have since been used for a wide range of tasks, including metric learning <ref type="bibr" target="#b5">[6]</ref> and recognition <ref type="bibr" target="#b26">[27]</ref>. Some recently applied Siamese networks to depth estimation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref>. These methods were all supervised and assume stereo vision during both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>We use pairs of RGB rectified images for training and assume the images were acquired in a controlled setup where the baseline between the cameras is known. Later on, this assumption will allow us to easily convert from disparities to depth. We believe it is reasonable to assume availability of rectified stereo pairs, even at scale, and there are several datasets containing data of this type <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>We aim to learn a mappingd l = f (I l ), from an RGB image to a depth map and similarlyd r = f (I r ). Compare to <ref type="bibr" target="#b14">[15]</ref> in which the problem during training could be formulated to (d l ,d r ) = f (I l ).</p><p>The two functions, f () and f () cannot be the same: inferring a left disparity map is a different problem than inferring a right disparity map, if only because of the different relative positions of the two images and hence the different disparities that are assigned to their pixels. Clearly, we can train two separate networks, one for each function, but that would prevent weight sharing between the two networks or allow us to exploit the inherent symmetry of the problem. We propose an alternative method which utilizes both images in an equivalent manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Siamese architecture with mirroring</head><p>To make equivalent and symmetric use of available training data, we exploit the symmetry of the problem and note that by mirroring (horizontal flipping) I r we get a new image m(I r ) which can be considered as being sampled from the distribution of left images, that means we can apply our f () function on such image, but now, in order to return to right disparity another mirroring is required, to summarize f (·) = m(f (m(·))). We hence change the architecture used to train and infer depth to exploit the symmetry. These changes are presented in <ref type="figure" target="#fig_0">Figure 2</ref> as a detailed block diagram of our method, compared to the designs of previous approaches. As can be seen, both Garg et al. <ref type="bibr" target="#b10">[11]</ref> and Godard et al. <ref type="bibr" target="#b14">[15]</ref> propose an architecture with a single input used as input during training. Garg et al. are further limited by using the right image only as a supervisory signal. We use a Siamese architecture which takes both images simultaneously as input during training, treating both views equally. Our approach therefore not only saves memory, it also shares information between the networks.</p><p>Specifically, both previous methods under-utilize the right view <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>: Neither feeds the right image as input to the encoder-decoder architecture. The right image is only used as reference signal to the reconstructed imagê I l (d r ) =Î r . Of course, data augmentation can be used to flip both images and present each one, separately. In doing so, however, the network cannot see regions occluded in one view but visible in the other. We discuss these limitations in detail, in Sec. 3.5.</p><p>Note that while Siamese networks require double the training time, the actual net throughput is the same as that of a single network trained separately on both images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, because two training images are viewed and processed in each step. Also note that because of weight sharing the memory consumption is also unaffected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>We use a network architecture based on DispNet <ref type="bibr" target="#b40">[41]</ref>, applying modifications similar to those described by Godard et al. <ref type="bibr" target="#b14">[15]</ref>. We use both ResNet <ref type="bibr" target="#b19">[20]</ref> and VGG <ref type="bibr" target="#b46">[47]</ref> architecture variants. The network is composed of an encoder-decoder pair with skip connections, allowing the network to overcome data lost during down-sampling steps while still using the advantages of a deep network.</p><p>The network produces multi-scale disparity maps: d 1 view , ..., d 4 view for the four scales considered by our network and view representing either l or r for the left/right images of a stereo pair. Lower resolution disparity predictions are concatenated with previous decoder layer output and with the corresponding encoder output using the skip connections. The concatenated results are then fed into the next (higher) scale of the network <ref type="bibr" target="#b40">[41]</ref>. In order to warp each disparity map and image onto its counterpart, we use a bilinear sampler as in <ref type="bibr" target="#b22">[23]</ref> which allows for end-to-end back-propagation and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>We define a multi-scale loss, somewhat related to one proposed by others <ref type="bibr" target="#b14">[15]</ref>. The single scale loss is defined by:</p><formula xml:id="formula_0">L s = α im (L l im + L r im ) + α tv (L l tv + L r tv ) + α lr (L l lr + L r lr ).</formula><p>(1) The components of Eq. (1) are defined below. Note that this loss averages prediction errors from both left and right views. This should be compared with Garg et al. <ref type="bibr" target="#b10">[11]</ref>, who consider single view predictions, and Godard et al. <ref type="bibr" target="#b14">[15]</ref> who average two predictions, but unlike us, their predictions are not equivalent (See also <ref type="figure" target="#fig_0">Fig. 2)</ref>.</p><p>The total loss is then a sum over the four scales:</p><formula xml:id="formula_1">L = 4 s=1 L s .<label>(2)</label></formula><p>We tried using only the loss defined for the most detailed (high resolution) scale but found that combining multiple scales leads to better accuracy.</p><p>An additional modification of our loss, Eq. (2) compared with previous work <ref type="bibr" target="#b14">[15]</ref> is that we use a total variation component, described below, instead of their disparity smoothness term. We found this change to improve disparity results. We next detail the terms included in Eq. (1). Image loss. Zhao et al. <ref type="bibr" target="#b55">[56]</ref> compared multiple loss functions for the task of image restoration and showed that combining L 1 loss with the structural similarity (SSIM) loss <ref type="bibr" target="#b50">[51]</ref> leads to better results. It was later shown by others <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54]</ref> that this loss function is very suitable for the task of depth estimation. We follow their steps and use this as our loss function. Unlike previous work <ref type="bibr" target="#b14">[15]</ref>, however, where only an average pooling version of SSIM is applied, we use the original SSIM with a Gaussian kernel as we find it to improve the localization of the metric.</p><p>Specifically, SSIM is defined as:</p><formula xml:id="formula_2">SSIM(x, y) = (2µ x µ y + c 1 )(2σ xy + c 2 ) (µ 2 x + µ 2 y + c 1 )(σ 2 x + σ 2 y + c 2 ) ,<label>(3)</label></formula><p>where x, y are two equal sized windows in the two compared images. Scalars µ x , µ y , σ x , and σ y are the mean and variance of x and y respectively, and σ xy is the covariance of x, y. To summarize, the image loss is therefore measured as follows:</p><formula xml:id="formula_3">L l im = α N i,j 1 − SSIM(I l ij ,Î l ij ) 2 + (1 − α) I l ij −Î l ij .</formula><p>(4) Left-right consistency loss. As demonstrated by others <ref type="bibr" target="#b14">[15]</ref>, adding a constraint on the left-right consistency of the estimated disparity images leads to improved results. Because the task we are trying to solve is self-supervised, it is reasonable to use any geometric property that can be used as feedback to the model performance. To this end, left-right consistency is introduced to the loss and defined as follows:</p><formula xml:id="formula_4">L l lr = 1 N i,j |d l i,j − d r i,j+d l i,j |.<label>(5)</label></formula><p>Total variation loss. In order to promote smoothness of the estimated disparity maps we use a total variation loss that serves as a regularization term</p><formula xml:id="formula_5">L tv (d) = i,j |d i+1,j − d i,j | + |d i,j+1 − d i,j |. (6)</formula><p>We have also tried weighting this loss with the gradients of the original images, as suggested by others <ref type="bibr" target="#b14">[15]</ref>. We found, however, that this also emphasizes disparity gradients in unnecessary places in objects like windows and walls. These objects should have the same depth but have different disparities in the weighted version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Post-processing</head><p>Due to occlusions, the left side of the disparity map is usually missing important data. To overcome this, we follow a post-processing method based on the one suggested by Godard et al. <ref type="bibr" target="#b14">[15]</ref>. Given the image I, at test time, we also infer the depth of the horizontally mirrored image, m(I). The two disparity images are later blended together using a weighting function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion: Comparison with Godard et al. [15]</head><p>It is instructional to consider the significance of the differences in the design of our approach and the related work of Godard et al. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Similar loss, different components.</head><p>As mentioned in Sec. 3.3, the loss used by Godard et al. averages predictions for two views, similarly to ours. However, unlike in our approach, their predictions are not equivalent: both were produced from the left view, while the right view is used only as a supervisory signal (see also <ref type="figure" target="#fig_0">Figure  2</ref>). We provide the model inputs from two views, simultaneously, treating them equally, thus the network is given more data as input and each predicted disparity map is created independently from it's corresponding image. Siamese Network = Data Augmentation. Instead of training a Siamese network, as proposed here, a single input network can be trained on the left image, with the right image used for supervision, and, separately, on the two images flipped and their roles switched <ref type="bibr" target="#b14">[15]</ref>. This approach, however, is different than the dual-input Siamese network approach proposed here.</p><p>First, using both images allows the network to backpropagate information from one branch into the other simultaneously. This information is unavailable when training with a single view. Second, including both left and right images as input adds information which would otherwise be unavailable due to occlusions and limited field-of-view. <ref type="figure">Fig. 3</ref>, compares the right disparity map produced by Godard et al. to ours. Their disparity is blurry and missing important details and contours. These errors can be intuitively explained by their uncertainty of the right image. This uncertainty creates an asymmetry between d l and d r . Notice that in our prediction (bottom row) both left and right disparities are fine grained. Put differently, the left-right consistency of our loss relies on accurate predictions of both left and right disparities. The network must therefore learn to predict the right disparity map as accurately as possible in order to minimize its loss. Why does flipping work? Can we just reverse the directions of the disparities? It is possible to reverse the disparity directions, since: d l = x l − x r and d r = x r − x l = −d l , <ref type="figure">Figure 3</ref>. Qualitative comparison of disparity maps. Top row contains the input pair of images, the two rows below contains the left and right disparity maps predicted by Godard et al. <ref type="bibr" target="#b14">[15]</ref> and by our method. As evident from the zoomed-in views, our results are crisper, containing more high-gradient information. This is particularly evident in depth discontinuities, such as the edge of the bushes. Also note the boundary effects, these are modeled differently for left and right disparities, hence the flipping is needed. where x l and x r are two corresponding points in the left and right image respectively. This approach, however, does not take into account boundary effects, as seen in <ref type="figure">Fig. 3</ref>. We expect the left (right) disparity to include some boundary artifact in the left (right) side, due to missing data. Another potential limitation of this approach is that the information is distributed differently for the left and right images, I l ∼ I r , due to the different positions of the left and right cameras. We design our network with bias towards left images, but by exploiting the symmetry and flipping right images we can assume the flipped distribution is the same I l ∼ m(I r ). This allows us to avoid bias and use the same network for both images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We tested our approach on two standard benchmark for monocular depth estimation: the KITTI Eigen split <ref type="bibr" target="#b12">[13]</ref> and the KITTI single image depth prediction challenge <ref type="bibr" target="#b49">[50]</ref>. In addition, to show that our method generalizes well to new data, we provide results on the Make3D benchmark <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. Importantly, Make3D has only 400 images and so training is impossibly on this set, which has appearance biases different from those of KITTI images. Our results were therefore obtained without training on Make3D images. These results are reported next. Implementation details. Similarly to previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54]</ref>, we first train our model on the high resolution Cityscapes dataset <ref type="bibr" target="#b6">[7]</ref> and later fine-tune for 30 epochs on KITTI training images <ref type="bibr" target="#b12">[13]</ref>, in order to provide our net-work with as much training data as possible while domainshifting to KITTI data.</p><p>For optimization we use Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We use a constant learning rate of λ = 10 −4 . Our loss parameters of Eq. (2) are set as: α im = 1.0, α lr = 1.0 and α tv = 0.001.</p><p>We use a batch size of eight for training. We also augment the data by applying on-the-fly color, gamma, and brightness transformations. Training uses the TensorFlow package [1] on a Titan X GPU. The average test time for each image is 73ms. This includes processing both the image and its mirrored version. KITTI Eigen split. The KITTI dataset <ref type="bibr" target="#b12">[13]</ref> contains 42, 382 rectified stereo pairs from 61 scenes. Most of the images are 1, 242 × 375 pixels in size. For easy comparison with previous work, we use the metrics and proposed train/test splits defined by others <ref type="bibr" target="#b8">[9]</ref>. KITTI Eigen split contains 697 test images taken from 29 scenes. Additional 32 scenes are provided for training and evaluation. Ground truth depth data is created by reprojecting 3D points acquired by the Velodyne laser onto the left image. It should be noted that depth data is available only for a sparse subset of the pixels; only 5% of the pixels include ground truth depth data. This ground truth data also contains measurement noise due to sensor rotation and movement of the carrying vehicle.</p><p>We use the same image crop defined by others <ref type="bibr" target="#b10">[11]</ref>, as the same crop was used by all the baseline methods we compared with. Predictions are rescaled using bilinear interpolation in order to match the original image size. While this is the most common evaluation for the task, some concerns were recently raised regarding this methodology <ref type="bibr" target="#b13">[14]</ref>. We provide results for this protocol for completeness but emphasize that a more appropriate evaluation may be the KITTI single image prediction challenge <ref type="bibr" target="#b49">[50]</ref>, which we have also tested and for which we offer results below. <ref type="table" target="#tab_0">Table 1</ref> reports results on this data set. As can be seen, our method achieves state-of-the-art accuracy in nearly all accuracy measures, with the exception of RMSE and RMSE log, where it trails the best results by a very narrow margin. Importantly, these metrics are often considered less stable. KITTI Single image depth benchmark. We also evaluate our method using the recently released KITTI single image depth prediction challenge <ref type="bibr" target="#b49">[50]</ref>. This benchmark contains 500 RGB test images that are provided for evaluation but the ground truth is only accessible to the dataset creators. We do not use the ground truth depth maps provided with the train/validation datasets. Our results are compared with existing public results in <ref type="table" target="#tab_2">Table 2</ref>, with qualitative examples of our estimates provided in <ref type="figure">Fig. 5</ref>.</p><p>As this is a fairly new challenge published by the KITTI team, there is a limited number of published results on this benchmark, all of which were obtained by supervised meth-   <ref type="bibr" target="#b14">[15]</ref> (column b and zoomed-in version in column d) and our method results (column c and zoomed-in version in column e). Our method improves depth estimation for small objects and overcomes texture-less regions. For Godard et al. <ref type="bibr" target="#b14">[15]</ref> we used a publicly available model <ref type="bibr" target="#b15">[16]</ref> . ods. While our method does not always achieve the best results it is the only one which is self-supervised. Still, our method achieves comparable accuracy with those supervised methods as well as outperforming the supervised baselines provided for this benchmark. In addition, our method is faster than any of these previous methods.</p><p>Make3D. In order to test the generalization of the proposed method we also evaluate it on the Make3D <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> dataset. Similarly to <ref type="bibr" target="#b14">[15]</ref> we use a model trained only on Cityscapes data, as it is of higher resolution and contains similar scenes. We also take a central crop of the images in order to match Cityscapes aspect ratio.</p><p>The Make3D test set contains 134 pairs of singleview RGB and depth images. As common for evaluating Make3D <ref type="bibr" target="#b35">[36]</ref>, we use the C1 error measures listed below, ignoring pixels where depth is larger than 70 meters:</p><p>• Squared relative error (Sq Rel): 1</p><formula xml:id="formula_6">T T i (d gt i −d p i ) 2 d gt i • Absolute relative error (Abs Rel): 1 |T | T i d gt i −d p i d gt i • Root-mean squared error (RMSE): 1 |T | T i (d gt i − d p i ) 2</formula><p>• log 10 error: 1 |T | T i log 10 (d gt i ) − log 10 (d p i ) In all of the measures listed above, d gt i and d p i are the ground truth depth data and the predicted depth data, respectively.</p><p>We report results in <ref type="table">Table 3</ref> with some qualitative results <ref type="figure">Figure 5</ref>. Qualitative disparity results on the KITTI single image depth prediction test set <ref type="bibr" target="#b49">[50]</ref>. Left: RGB images. Right: Disparity maps produced by our model. Note that ground truth data is not available for these images.   <ref type="table">Table 3</ref>. Comparison on the Make3D dataset: Our method generalizes well to the unseen Make3D dataset. Visually, our results are plausible and consistent. Please see <ref type="figure" target="#fig_2">figure 6</ref> for examples. Bold numbers are best scoring for supervised and self-supervised methods respectively. provided in <ref type="figure" target="#fig_2">Fig. 6</ref>. The strength of the proposed method is shown in its ability to perform well even when applied to a totally different domain and scene, where it outperforms other self-supervised methods and achieves comparable results to some of the supervised methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a self-supervised method for monocular depth estimation. Our method trains on stereo image pairs but applied to to single images at test time. There is no need to provide depth information during training or any other supervisory data or labels: our system is fully selfsupervised. We achieve state-of-the-art results on challenging datasets by making better use of the stereo input. Our key contribution is showing how left and right images can be symmetrically handled by mirroring the right image. Despite the simplicity of this approach, we are unaware of previous reports of similar approaches.</p><p>In addition, we provide technical contributions, including the use of a Siamese network with weight sharing for this task. As a result, we cut model size in half, using only one branch of the network at run time to process a single view input. we further define a loss function which better represents the novel design of our model.</p><p>An obvious extension of this approach is to test our method in stereo rather than monocular settings: There is nothing prohibiting our approach from being applied to stereo pairs. This ability to process monocular and stereo views is reminiscent of the human visual system which is likewise capable of generalizing from stereo to monocular settings and back. An additional direction for future work will explore the use of video and pose estimation in our suggested framework. Another technical matter that should be tackled is integrating the post-processing step into the network training architecture to achieve a better end-to-end learning. Finally, compared to other similar systems, our approach requires relatively small networks. This small size makes it appropriate for deployment on mobile platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of system architectures. Left: The method of Garg et al. [11] uses the right image only as a supervisory signal. Center: The method of Godard et al. [15] favors the left image over the right image. Both methods use a single image as input during training. Right: Our Siamese network trains on pairs of images, treating them both equally, by flipping the right image. Hence, our loss combines errors from two separate predictions, equally treating both views and their predictions. At test time, only the area bounded by the dashed line is used; the rest of the blocks are used only for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison on KITTI data. Comparing Godard et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results on the Make3D dataset. (Left) Single view images used as inputs. (Center) the provided ground truth depth maps. (Right) our depth predictions as produced by a model trained on the Cityscapes dataset. As can be seen, while the quantitative results are not as good as supervised methods, the qualitative results are visually plausible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>MethodDataset Abs Rel ↓ Sq Rel ↓ RMSE ↓ RMSE log ↓ δ &lt; 1.25 ↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑ Results for KITTI 2015<ref type="bibr" target="#b12">[13]</ref>. Our method achieves state-of-the-art accuracy on some of the metrics and comparable results on others. Results in the top part of the table represent scenes of up to 80 meters; the bottom part of the table provides results of up to 50 meters. Our results follow post-processing, described in Sec. 3.4. Bold numbers are best.</figDesc><table><row><cell>Train set mean</cell><cell>K</cell><cell>0.361</cell><cell>4.826</cell><cell>8.102</cell><cell>0.377</cell><cell>0.638</cell><cell>0.804</cell><cell>0.894</cell></row><row><cell>Eigen et al. [9] -Coarse</cell><cell>K</cell><cell>0.214</cell><cell>1.605</cell><cell>6.563</cell><cell>0.292</cell><cell>0.673</cell><cell>0.884</cell><cell>0.957</cell></row><row><cell>Eigen et al. [9] -Fine</cell><cell>K</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell><cell>0.702</cell><cell>0.890</cell><cell>0.958</cell></row><row><cell>Liu et al. [35]</cell><cell>K</cell><cell>0.202</cell><cell>1.614</cell><cell>6.523</cell><cell>0.275</cell><cell>0.678</cell><cell>0.895</cell><cell>0.965</cell></row><row><cell>Godard et al. [15]</cell><cell cols="2">CS + K 0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Zhou et al. [57]</cell><cell cols="2">CS + K 0.198</cell><cell>1.836</cell><cell>6.565</cell><cell>0.275</cell><cell>0.718</cell><cell>0.901</cell><cell>0.960</cell></row><row><cell>Yin et al. [54]</cell><cell cols="2">CS + K 0.153</cell><cell>1.328</cell><cell>5.737</cell><cell>0.232</cell><cell>0.802</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Ours, VGG</cell><cell cols="2">CS + K 0.121</cell><cell>0.9643</cell><cell>5.137</cell><cell>0.213</cell><cell>0.846</cell><cell>0.944</cell><cell>0.976</cell></row><row><cell>Ours, Resnet</cell><cell cols="2">CS + K 0.113</cell><cell>0.898</cell><cell>5.048</cell><cell>0.208</cell><cell>0.853</cell><cell>0.948</cell><cell>0.976</cell></row><row><cell>Garg et al. cap 50m [9]</cell><cell>K</cell><cell>0.169</cell><cell>1.080</cell><cell>5.104</cell><cell>0.273</cell><cell>0.740</cell><cell>0.904</cell><cell>0.962</cell></row><row><cell>Yin et al. [54] cap 50m</cell><cell>K</cell><cell>0.147</cell><cell>0.936</cell><cell>4.348</cell><cell>0.218</cell><cell>0.810</cell><cell>0.941</cell><cell>0.977</cell></row><row><cell cols="3">Godard et al. [15] cap 50m CS + K 0.108</cell><cell>0.657</cell><cell>3.729</cell><cell>0.194</cell><cell>0.873</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>Ours, VGG, cap 50m</cell><cell cols="2">CS + K 0.1155</cell><cell>0.7152</cell><cell>3.922</cell><cell>0.201</cell><cell>0.859</cell><cell>0.951</cell><cell>0.979</cell></row><row><cell>Ours, Resnet, cap 50m</cell><cell cols="2">CS + K 0.1069</cell><cell>0.6531</cell><cell>3.790</cell><cell>0.195</cell><cell>0.867</cell><cell>0.954</cell><cell>0.979</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results for KITTI single image depth prediction challenge. While the other methods are supervised our method is self-supervised yet is able to achieve comparable results. In addition, our runtime is much faster than the other listed methods. Results reported here are for the Resnet variant of our method, trained on both Cityscapes and KITTI. We note that the challenge also lists multiple unpublished methods; we report only published, non-anonymous results.</figDesc><table><row><cell>Method</cell><cell cols="3">Supervision? Sq Rel Abs Rel RMSE log 10</cell></row><row><cell>Train set mean</cell><cell>Full</cell><cell cols="2">15.517 0.893 11.542 0.223</cell></row><row><cell>Karsch et al. [24]</cell><cell>Full</cell><cell cols="2">4.894 0.417 8.172 0.144</cell></row><row><cell>Liu et al. [36]</cell><cell>Full</cell><cell cols="2">6.625 0.462 9.972 0.161</cell></row><row><cell>Laina et al. [32]</cell><cell>Full</cell><cell cols="2">1.665 0.198 5.461 0.082</cell></row><row><cell>Kuznietsov et al. [30]</cell><cell>Semi</cell><cell>-</cell><cell>0.421 8.237 0.190</cell></row><row><cell>Godard et al. [15]</cell><cell>Self</cell><cell cols="2">7.112 0.443 11.513 0.156</cell></row><row><cell>Our method (Resnet)</cell><cell>Self</cell><cell cols="2">4.766 0.406 8.789 0.183</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape from texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="345" to="360" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photometric stereo with general, unknown lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. conf. on Computer graphics and interactive techniques</title>
		<meeting>conf. on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="737" to="744" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2366" to="2374" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01260</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<ptr target="http://visual.cs.ucl.ac.uk/pubs/monoDepth/models/city2eigen_resnet.zip" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Example based 3d reconstruction from single 2d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Single view depth estimation from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.3915</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Shape from shading: A method for obtaining the shape of a smooth opaque object from one view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Basic Mechanisms</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01556</idno>
		<title level="m">Pixel-wise attentional gating for parsimonious pixel labeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depthnet: A recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02287</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object Scene Flow for Autonomous Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shape from focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="824" to="831" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Depth estimation using monocular and stereo cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. J. Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5163" to="5172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extreme 3d face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tun Trn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3935" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06500</idno>
		<title level="m">Sparsity invariant cnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="835" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Geonet: Unsupervised learning of dense depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02276</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="430" to="442" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
