<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated-SCNN: Gated Shape CNNs for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
							<email>ttakikaw@edu.uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
							<email>davidj@cs.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
							<email>vjampani@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>sfidler@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gated-SCNN: Gated Shape CNNs for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Project Website: https://nv-tlabs.github.io/GSCNN/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-ofthe-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic image segmentation is one of the most widely studied problems in computer vision with applications in autonomous driving <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57]</ref>, 3D reconstruction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref> and image generation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref> to name a few. In recent years, Convolutional Neural Networks (CNNs) have led to dramatic improvements in accuracy in almost all the major segmentation benchmarks. A standard practice is to adapt an image classification CNN architecture for the task of semantic segmentation by converting fully-connected layers into convolutional layers <ref type="bibr" target="#b36">[37]</ref>. However, using classification architectures for dense pixel prediction has several drawbacks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b10">11]</ref>. One eminent drawback is the loss in spatial resolution of the output due to the use of pool- <ref type="figure">Figure 1</ref>: We introduce Gated-SCNN (GSCNN), a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing stream. GSCNN uses a new gating mechanism to connect the intermediate layers. Fusion of information between streams is done at the very end through a fusion module. To predict highquality boundaries, we exploit a new loss function that encourages the predicted semantic segmentation masks to align with ground-truth boundaries.</p><p>ing layers. This prompted several works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> to propose specialized CNN modules that help restore the spatial resolution of the network output.</p><p>We argue here that there is also an inherent inefficacy in the architecture design since color, shape and texture information are all processed together inside one deep CNN. Note that these likely contain very different amounts of information that are relevant for recognition. For example, one may need to look at the complete and detailed object boundary to get a discriminative encoding of shape <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>, while color and texture contain fairly low-level information. This may also provide an insight of why residual <ref type="bibr" target="#b18">[19]</ref>, skip <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b51">52]</ref> or even dense connections <ref type="bibr" target="#b20">[21]</ref> lead to the most prominent performance gains. Incorporating additional connectivity helps the different types of information to flow across different scales of network depth. Disentangling these representations by design may, however, lead to a more natural and effective recognition pipeline.</p><p>In this work, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch. In particular, we keep the classical CNN in one stream, and add a so-called shape stream that processes information in par-allel. We explicitly do not allow fusion of information between the two streams until the very top layers.</p><p>Key to our architecture are a new type of gates that allow the two branches to interact. In particular, we exploit the higher-level information contained in the classical stream to denoise activations in the shape stream in its very early stages of processing. By doing so, the shape stream focuses on processing only the relevant information. This allows the shape stream to adopt a very effective shallow architecture that operates on the full image resolution. To achieve that the shape information gets directed to the desired stream, we supervise it with a semantic boundary loss. We further exploit a new loss function that encourages the predicted semantic segmentation to correctly align with the groundtruth semantic boundaries, which further encourages the fusion layer to exploit information coming from the shape stream. We call our new architecture GSCNN.</p><p>We perform extensive evaluation on the Cityscapes benchmark <ref type="bibr" target="#b12">[13]</ref>. Note that our GSCNN can be used as plugand-play on top of any classical CNN backbone. In our experiments, we explore ResNet-50 <ref type="bibr" target="#b18">[19]</ref>, ResNet-101 <ref type="bibr" target="#b18">[19]</ref> and WideResnet <ref type="bibr" target="#b55">[56]</ref> and show significant improvements in all. We outperform the state-of-the-art DeepLab-v3+ <ref type="bibr" target="#b10">[11]</ref> by more than 1.5 % in terms of mIoU and 4% in F-boundary score. Our gains are particularly significant for the thinner and smaller objects (i.e. poles, traffic light, traffic signs), where we get up to 7% improvement in terms of IoU.</p><p>We further evaluate performance at varying distances from the camera, using a prior as proxy for distance. Experiments show that we consistently outperform the state-ofthe-art baseline achieving up to 6% improvement in terms of mIoU at the largest distance (i.e. further away objects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation. State-of-the-art approaches for semantic segmentation are predominantly based on CNNs. Earlier approaches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b7">8]</ref> convert classification networks into fully convolutional networks (FCNs) for efficient end-to-end training for semantic segmentation. Several works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5]</ref> propose to use structured prediction modules such as conditional random fields (CRFs) on network output for improving the segmentation performance, especially around object boundaries. To avoid costly DenseCRF <ref type="bibr" target="#b28">[29]</ref>, the work of <ref type="bibr" target="#b5">[6]</ref> uses fast domain transform <ref type="bibr" target="#b15">[16]</ref> filtering on network output while also predicting edge maps from intermediate CNN layers. We also predict boundary maps to improve segmentation performance. Contrary to <ref type="bibr" target="#b5">[6]</ref>, which uses edge information to refine network output, we inject the learned boundary information into intermediate CNN layers. Moreover, we propose specialized network architecture and a dual-task regularizer to obtain high-quality boundaries.</p><p>More recently, dramatic improvements in performance and inference speed have been driven by new architectural designs. For example, PSPNet <ref type="bibr" target="#b57">[58]</ref> and DeepLab <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> proposed a feature pyramid pooling module that incorporates multiscale context by aggregating features at multiples scales. Similar to us, <ref type="bibr" target="#b41">[42]</ref> proposed a two stream network, however, in their case, the main purpose of the second stream is to recover high-resolution features that are lost with pooling layers. Here, we explicitly specialize the second stream to process shape related information. Some works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48]</ref> propose modules that use learned pixel affinities for structured information propagation across intermediate CNN representations. Instead of learning specialized information propagation modules, we propose to learn high-quality shape information through carefully designed network and loss functions. Since we simply concatenate shape information with segmentation CNN features, our approach can be easily incorporated into existing networks for performance improvements.</p><p>Multitask Learning. Several works have also explored the idea of combining networks for complementary tasks to improve learning efficiency, prediction accuracy and generalization across computer vision tasks. For example, the works of <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>, proposed unified architectures that learn a shared representation using multi-task losses.</p><p>Our main goal is not to train a multi-task network, but to enforce a structured representation that exploits the duality between the segmentation and boundary prediction tasks. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref> simultaneously learned segmentation and boundary detection network, while <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40]</ref> learned boundaries as an intermediate representation to aid segmentation. Contrary to these works, where semantics and boundary information interact only at the loss functions, we explicitly inject boundary information into segmentation CNN and also propose a dual-task loss function that refines both semantic masks and boundary predictions.</p><p>Gated Convolutions. Recent work on language modeling have also proposed the idea of using gating mechanisms in convolutions. For instance, <ref type="bibr" target="#b13">[14]</ref> proposed to replace the recurrent connections typically used in recurrent networks with gated temporal convolutions. <ref type="bibr" target="#b52">[53]</ref>, on the other hand, proposed the use of convolutions with a soft-gating mechanism for Free-Form Image Inpainting and <ref type="bibr" target="#b45">[46]</ref> proposed Gated PixelCNN for conditional image generation. In our case, we use a gated convolution operator for the task of semantic segmentation and to define the information flow between the shape and regular streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Gated Shape CNN</head><p>In this section, we present our Gated-Shape CNN architecture for semantic segmentation. As depicted in <ref type="figure">Fig. 2</ref>, our network consists of two streams of networks followed by a fusion module. The first stream of the network ("reg-ASPP Residual Block Gated Conv Layer ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; conv 1x1 conv 1x1 conv 1x1 ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;  <ref type="figure">Figure 2</ref>: GSCNN architecture. Our architecture constitutes of two main streams. The regular stream and the shape stream. The regular stream can be any backbone architecture. The shape stream focuses on shape processing through a set of residual blocks, Gated Convolutional Layers (GCL) and supervision.</p><p>A fusion module later combines information from the two streams in a multi-scale fashion using an Atrous Spatial Pyramid Pooling module (ASPP). High quality boundaries on the segmentation masks are ensured through a Dual Task Regularizer . ular stream") is a standard segmentation CNN, and the second stream ("shape stream") processes shape information in the form of semantic boundaries. We enforce shape stream to only process boundary-related information by our carefully designed Gated Convolution Layer (GCL) and local supervision. We then fuse semantic-region features from the regular stream and boundary features from the shape stream to produce a refined segmentation result, especially around boundaries. Next, we describe, in detail, each of the modules in our framework followed by our novel GCL.</p><p>Regular Stream. This stream, denoted as R θ (I), with parameters θ, takes image I ∈ R 3×H×W with height H and width W as input and produces dense pixel features. The regular stream can be any feedforward fully-convoutional network such as ResNet <ref type="bibr" target="#b18">[19]</ref> based or VGG <ref type="bibr" target="#b43">[44]</ref> based semantic segmentation network. Since ResNets are the recent state-of-the-art for semantic segmentation, we make use of ResNet-like architecture such as ResNet-101 <ref type="bibr" target="#b18">[19]</ref> and WideResNet <ref type="bibr" target="#b55">[56]</ref> for the regular stream. We denote the output feature representation of the regular stream as</p><formula xml:id="formula_0">r ∈ R C× H m × W m</formula><p>where m is the stride of the regular stream.</p><p>Shape Stream. This stream, denoted as S φ , with parameters φ, takes image gradients ∇I as well as output of the first convolutional layer of the regular stream as input and produces semantic boundaries as output. The network architecture is composed of a few residual blocks interleaved with gated convolution layers (GCL). GCL, explained below, ensures that the shape stream only processes boundaryrelevant information. We denote the output boundary map of the shape stream as s ∈ R H×W . Since we can obtain ground-truth (GT) binary edges from GT semantic segmentation masks, we use supervised binary cross entropy loss on output boundaries to supervise the shape stream.</p><p>Fusion Module. This module, denoted as F γ , with parameters γ, takes as input the dense feature representation r com-ing from the regular branch and fuses it with the boundary map s output by the shape branch in a way that multi-scale contextual information is preserved. It combines region features with boundary features and outputs a refined semantic segmentation output. More formally, for a segmentation prediction of K semantic classes, it outputs a categorical distribution f = p(y|s, r) = F γ (s, r) ∈ R K×H×W , which represents the probability that pixels belong to each of the K classes. Specifically, we merge the boundary map s with r using an Atrous Spatial Pyramid Pooling <ref type="bibr" target="#b10">[11]</ref>. This allows us to preserve the multi-scale contextual information and is proven to be an essential component in state-of-theart semantic segmentation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gated Convolutional Layer</head><p>Since the tasks of estimating semantic segmentation and semantic boundaries are closely related, we devise a novel GCL layer that facilitates flow of information from the regular stream to the shape stream. GCL is a core component of our architecture and helps the shape stream to only process relevant information by filtering out the rest. Note that the shape stream does not incorporate features from the regular stream. Rather, it uses GCL to deactivate its own activations that are not deemed relevant by the higher-level information contained in the regular stream. One can think of this as a collaboration between two streams, where the more powerful one, which has formed a higher-level semantic understanding of the scene, helps the other stream to focus only on the relevant parts since start. This enables the shape stream to adopt an effective shallow architecture that processes the image at a very high resolution.</p><p>We use GCL in multiple locations between the two streams. Let m denote the number of locations, and let apply GCL, we first obtain an attention map α t ∈ R H×W by concatenating r t and s t followed by a normalized 1 × 1 convolutional layer C 1×1 which in turn is followed by a sigmoid function σ :</p><formula xml:id="formula_1">α t = σ(C 1×1 (s t ||r t )),<label>(1)</label></formula><p>where || denotes concatenation of feature maps. Given the attention map α t , GCL is applied on s t as an element-wise product with attention map α followed by a residual connection and channel-wise weighting with kernel w t . At each pixel (i, j), GCL * is computed aŝ s</p><formula xml:id="formula_2">(i,j) t = (s t * w t ) (i,j) = ((s t (i,j) α t (i,j) ) + s t (i,j) ) T w t .<label>(2)</label></formula><p>s t is then passed on to the next layer in the shape stream for further processing. Note that both the attention map computation and gated convolution are differentiable and therefore backpropagation can be performed end-to-end. Intuitively, α can also be seen as an attention map that weights more heavily areas with important boundary information. In our experiments, we use three GCLs and connect them to the third, fourth and last layer of the regular stream. Bilinear interpolation, if needed, is used to upsample the feature maps coming from the regular stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Multi-Task Learning</head><p>We jointly learn the regular and shape streams together with the fusion module in an end-to-end fashion. We jointly supervise segmentation and boundary map prediction during training. Here, the boundary map is a binary representation of all the outlines of objects and stuff classes in the scene <ref type="figure" target="#fig_2">(Fig 6)</ref>. We use standard binary cross-entropy (BCE) loss on predicted boundary maps s and use standard crossentropy (CE) loss on predicted semantic segmentation f :</p><formula xml:id="formula_3">L θ φ,γ = λ 1 L θ,φ BCE (s,ŝ) + λ 2 L θ φ,γ CE (ŷ, f )<label>(3)</label></formula><p>whereŝ ∈ R H×W denotes GT boundaries andŷ ∈ R H×W denotes GT semantic labels. Here, λ 1 , λ 2 are two hyperparameters that control the weighting between the losses. As depicted in <ref type="figure">Fig. 2</ref>, the BCE supervision on boundary maps s is performed before feeding them into the fusion module. Thus the BCE loss L θ,φ BCE updates the parameters of both the regular and shape streams. The final categorical distribution f of semantic classes is supervised with CE loss L θ φ,γ CE at the end as in standard semantic segmentation networks, updating all the network parameters. In the case of BCE on boundaries, we follow <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b53">54]</ref> and use a coefficient β to account for the high imbalance between boundary/non-boundary pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dual Task Regularizer</head><p>As mentioned above, p(y|r, s) ∈ R K×H×W denotes a categorical distribution output of the fusion module. Let ζ ∈ R H×W be a potential that represents whether a particular pixel belongs to a semantic boundary in the input image I. It is computed by taking a spatial derivative on segmentation output as follows:</p><formula xml:id="formula_4">ζ = 1 √ 2 ||∇(G * arg max k p(y k |r, s))||<label>(4)</label></formula><p>where G denotes Gaussian filter. If we assumeζ is a GT binary mask computed in the same way from the GT semantic labelsf , we can write the following loss function:</p><formula xml:id="formula_5">L θ φ,γ reg→ = λ 3 p + |ζ(p + ) −ζ(p + )|<label>(5)</label></formula><p>where p + contains the set of all non-zero pixel coordinates in both ζ andζ. Intuitively, we want to ensure that boundary pixels are penalized when there is a mismatch with GT boundaries, and to avoid non-boundary pixels to dominate the loss function. Note that the above regularization loss function exploits the duality between boundary prediction and semantic segmentation in the boundary space. Similarly, we can use the boundary prediction from the shape stream s ∈ R H×W to ensure consistency between the binary boundary prediction s and the predicted semantics p(y|r, s):</p><formula xml:id="formula_6">L θ φ,γ reg← = λ 4 k,p 1 sp [ŷ k p log p(y k p |r, s)],<label>(6)</label></formula><p>where p and k runs over all image pixels and semantic classes, respectively. 1 s = 1 : s &gt; thrs corresponds to the indicator function and thrs is a confidence threshold, we use 0.8 in our experiments. The total dual task regularizer loss function can be written as:</p><formula xml:id="formula_7">L θ φ,γ = L θ φ,γ reg→ + L θ φ,γ reg←<label>(7)</label></formula><p>Here, λ 3 and λ 4 are two hyper-parameters that control the weighting of the regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Gradient Propagation during Training</head><p>In order to back-propagate through Eq 7, we need to compute the gradients of Eq 4. Letting g = ||.||, the partial derivatives with respect to a given parameter η can be computed as follows:</p><formula xml:id="formula_8">∂L ∂η i = j,l ∇G * ∂L ∂ζ j ∂ζ j ∂g l ∂ arg max k p(y k ) l ∂η i<label>(8)</label></formula><p>Since arg max is not a differentiable function we use the Gumbel softmax trick <ref type="bibr" target="#b23">[24]</ref>. During the backward pass, we approximate the argmax operator with a softmax with temperature τ :</p><p>∂ arg max k p(y k ) ∂η i = ∇ ηi exp((log p(y k ) + g k )/τ ) j exp((log p(y j ) + g j )/τ )    where g j ∼ Gumbel(0,I) and τ a hyper-parameter. The operator ∇G * can be computed by filtering with Sobel kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we provide an extensive evaluation of each component of our framework on the challenging Cityscapes dataset <ref type="bibr" target="#b12">[13]</ref>. We further show the effectiveness of our approach for several backbone architectures and provide qualitative results of our method.</p><p>Baselines. We use DeepLabV3+ <ref type="bibr" target="#b10">[11]</ref>, as our main baseline. This consitutes the state-of-the-art architecture for semantic segmentation and pretrained models are available. In most of our experiments, we use our own PyTorch implementation of DeeplabV3+ which differs from <ref type="bibr" target="#b10">[11]</ref> in the choice of the backbone architecture. Specifically, we use ResNet-50, ResNet-101 and WideResNet as the backbone architecture for our version of DeeplabV3+. For a fair comparison, when applicable, we refer to this as Baseline in our tables. Additionally, we also compare against published state-of the-art-methods on the validation set and in the Cityscapes benchmark (test set).</p><p>Dataset. All of our experiments are conducted on the Cityscapes dataset. This dataset contains images from 27 cities in Germany and neighboring countries. It contains 2975 training, 500 validation and 1525 test images. Cityscapes additionally includes 20,000 additional coarse annotations (i.e., coarse polygons covering individual objects). Notice that we supervise our shape stream with boundary ground-truth, and thus the coarse subset is not ideal for our setting. We thus do not use coarse data in our experiments. The dense pixel annotations include 30 classes which frequently occur in urban street scenes, out of which 19 are used for the actual training and evaluation. We follow <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b0">1]</ref> to generate the ground truth boundaries and supervise our shape stream.</p><p>Evaluation Metrics. We use three quantitative measures to evaluate the performance of our approach. 1) We use the widely used intersection over union (IoU) to evaluate whether the network accurately predicts regions. 2) Since our method aims to predict high-quality boundaries, we include another metric for evaluation. Specifically, we follow the boundary metric proposed in <ref type="bibr" target="#b40">[41]</ref> to evaluate the quality of our semantics boundaries. This metric computes the F-score along the boundary of the predicted mask, given a small slack in distance. In our experiments, we use thresholds 0.00088, 0.001875, 0.00375, and 0.005 which correspond to 3, 5, 9, and 12 pixels respectively. Similarly to the IoU calculation, we also remove void areas during the computation of the F-score. Since boundaries are not provided for the test-set, we use the validation set to compute F-Scores as a metric for boundary alignment. 3) We use distance-based evaluation in terms of IoU, explained below, in order to evaluate the performance of the segmentation models at varying distances from the camera. <ref type="bibr">ResNet</ref>    Distance-based Evaluation. We argue that high accuracy is also important for small (distant) objects, where however, the global IoU metric does not well reflect this. Thus, we take crops of varying size around an approximate (fixed) vanishing point as a proxy for distance. In our case, this is performed by cropping 100 pixels along each image side except for the top, and the center of the resulting crop is our approximate vanishing point. Then, given a predefined cropping factor c , crops are applied such that: we crop c from the top and bottom and c × 2 from the left and right. Intuitively, a smaller centered crop puts a larger weighting on the smaller objects farther away from the camera. An illustration of the procedure is shown in <ref type="figure" target="#fig_0">Fig 3. Fig 4 shows</ref> example predictions in each of the crops, illustrating how the metrics can focus on evaluating object at different sizes.</p><p>Implementation Details. In most of our experiments, we follow the methodology of Deeplab v3+ <ref type="bibr" target="#b10">[11]</ref> but use simpler encoders as described in the experiments. All our networks are implemented in PyTorch. We use 800×800 as the training resolution and synchronized batch norm. Training is done on an NVIDIA DGX Station using 8 GPUs with a total batch size of 16. For Cityscapes, we use a learning rate of 1e-2 with a polynomial decay policy. We run the training for 100 epochs for the ablation purposes, and showcase our best results in <ref type="table" target="#tab_1">Table 1</ref> at 230 epochs. For our joint loss, we set λ 1 = 20, λ 2 = 1, λ 3 = 1 and λ 4 = 1. We set τ = 1 for the Gumbel softmax. All our experiments are conducted in the Cityscapes fine set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we compare the performance of our GSCNN against the baselines in terms of region accuracy (measured by mIoU). The numbers are reported on the validation set, and computed on the full image (no cropping). In this metric, we achieve a 2% improvement, which is a significant result at this level of performance. In particular, we notice that we obtain significant improvements for small objects: motorcycles, traffic signs, traffic lights, and poles. <ref type="table" target="#tab_2">Table 2</ref>, on the other hand, compares the performance of our method against the baseline in terms of boundary accuracy (measured by F score). Similarly, our model performs considerably better, outperforming the baseline by close to 4% in the strictest regime. Note that, for fair comparison, we only report models trained on the Cityscapes fine set. Inference for all models is done on a single-scale.</p><p>In <ref type="figure">Fig 5,</ref> we show the performance of our method vs baseline following the proposed distance-based evaluation method. Here, we find that GSCNN performs increasingly better compared to DeeplabV3+ as the crop factor increases. The gap in performance between GSCNN and DeeplabV3+ increases from 2% at crop factor 0 (i.e. no cropping) to close to 6% at crop factor 400. This confirms that our network achieves significant improvements for smaller objects located further away from the camera.</p><p>Cityscapes Benchmark. To get optimal performance on the test set, we use our best model (i.e., regular stream is WideResNet). Training is done on an NVIDIA DGX Station using 8 GPUs with a total batch size of 16. We train this network with GCL and dual task loss for 175 epochs with a learning rate of 1e-2 with a polynomial decay policy. We also use a uniform sampling scheme to retrieve a 800 × 800 crop that uniformly samples from all classes. Additionally, we use a multi-scale inference scheme using scales 0.5, 1.0 and 2.0. We do not use coarse data during training, due to our boundary loss which requires fine boundary annotation.</p><p>In <ref type="table" target="#tab_8">Table 6</ref>, we compare against published state-of-the-art methods on the Cityscapes benchmark, evaluated on the test set. It is important to stress that our model is not trained on coarse data. Impressively, we can see that our model consistently outperforms very strong baselines, some of which   also use extra coarse training data. At the time of this writing, our approach is also ranked as first among the published methods that do not use coarse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we evaluate the effectiveness of each component of our method using different encoder networks for the regular stream. For fairness, comparison in this table is performed with respect to our own implementation of the baseline (i.e DeepLabV3+ with different backbone architectures), trained from scratch using the same set of hyperparameters and ImageNet initialization. Specifically, we use ResNet-50, ResNet-101 and Wide-ResNet for the backbone architectures. Here, GCL denotes a network trained with the shape stream with dual task loss, and Gradients de-notes the network that also adds image gradients before the fusion layer. In our network, we use a Canny edge detector to retrieve such gradients. We see from the table that we achieve between 1 to 2 % improvement in performance in terms of mIoU, and around 3 % for boundary alignment. <ref type="table" target="#tab_5">Table 4</ref>, on the other hand, showcases the effect of the Dual Task loss in terms of F-score for boundary alignment. We illustrate its effect at three different thresholds. Here, GCL denotes the network with the GCL shape stream trained without Dual Task Loss. With respect to the baseline, we can observe that the dual loss significantly improves the performance of the model in terms of boundary accuracy. Concretely, by adding the Dual-Task loss, we see up to 3% improvement at the strictest regime.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head><p>In <ref type="figure" target="#fig_3">Figure 7</ref>, we provide qualitative results of our method on the Cityscapes test set. We compare our method to the baseline by highlighting typical cases where our methods excels in <ref type="figure">Figure 8</ref>. Specifically, we visualize the prediction errors for both methods. In these zoomed images, we can see a group of people standing around an area densely populated by poles. Here, Deeplab v3+ fails to capture the poles and naively classifies them as humans. Conversely, we can see that in our model poles are properly classified, and the error boundaries for pedestrians also thin out. Additionally, objects such as traffic lights, which are typically predicted as an over compromising blob in Deeplab v3+ (especially at higher distances) retain their shape and structure in the output of our model. <ref type="figure" target="#fig_5">Fig 10 provides</ref> a visualization of the alpha channels from the GCL. We can notice how the gates help to emphasize the difference between the boundary/region areas in the incoming feature map. For example, the first gate empha-sized very low-level edges while the second and third focus on object-level boundaries. As the result of gating, we obtain a final boundary map in the shape stream which accurately outlines objects and stuff classes. This stream learns to produce high quality class-agnostic boundaries which are then fed to the fusion module. Qualitative results of the output of the shape stream are shown in <ref type="figure" target="#fig_2">Fig 6.</ref> In <ref type="figure" target="#fig_4">Figure 9</ref>, on the other hand, we show the boundaries obtained from the final segmentation masks. Notice their accuracy on the thinner and smaller objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed Gated-SCNN (GSCNN), a new two-stream CNN architecture for semantic segmentation that wires shape into a separate parallel stream. We used a new gating mechanism to connect the intermediate layers and a new loss function that exploits the duality between the tasks of semantic segmentation and semantic boundary prediction. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our architecture achieves state-of-the-art results on the challenging Cityscapes dataset, significantly improving over strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>0Figure 3 :</head><label>3</label><figDesc>Illustration of the crops used for the distance-based evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Predictions at diff. crop factors. Distance-based evaluation: Comparison of mIoU at different crop factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Example output of shape stream fed into the fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of our method on the Cityscapes test set. Figure shows the predicted segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results on the Cityscapes test set showing the high-quality boundaries of our predicted segmentation masks. Boundaries are obtained by finding the edges of the predicted segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of the alpha channels from the GCLs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison in terms of IoU vs state-of-the-art baselines on the Cityscapes val set. 52.2 46.2 72.0 62.8 67.7 71.8 52.0 80.9 61.5 66.4 78.8 78.2 83.9 91.7 77.9 60.9 69.7 Ours 85.0 68.8 74.1 53.3 47.0 79.6 74.3 76.2 75.3 53.1 83.5 69.8 73.1 83.4 75.8 88.0 93.9 75.1 68.5 73.6</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">road s.walk build. wall fence pole t-light t-sign veg terrain sky person rider car truck bus train motor bike mean</cell></row><row><cell>LRR [18]</cell><cell></cell><cell cols="2">97.7 79.9</cell><cell>90.7 44.4 48.6 58.6 68.2 72.0 92.5 69.3 94.7 81.6 60.0 94.0 43.6 56.8 47.2 54.8 69.7 69.7</cell></row><row><cell cols="2">DeepLabV2 [9]</cell><cell cols="2">97.9 81.3</cell><cell>90.3 48.8 47.4 49.6 57.9 67.3 91.9 69.4 94.2 79.8 59.8 93.7 56.5 67.5 57.5 57.7 68.8 70.4</cell></row><row><cell cols="2">Piecewise [32]</cell><cell cols="2">98.0 82.6</cell><cell>90.6 44.0 50.7 51.1 65.0 71.7 92.0 72.0 94.1 81.5 61.1 94.3 61.1 65.1 53.8 61.6 70.6 71.6</cell></row><row><cell>PSP-Net [58]</cell><cell></cell><cell cols="2">98.2 85.8</cell><cell>92.8 57.5 65.9 62.6 71.8 80.7 92.4 64.5 94.8 82.1 61.5 95.1 78.6 88.3 77.9 68.1 78.0 78.8</cell></row><row><cell cols="4">DeepLabV3+ [11] 98.2 84.9</cell><cell>92.7 57.3 62.1 65.2 68.6 78.9 92.7 63.5 95.3 82.3 62.8 95.4 85.3 89.1 80.9 64.6 77.3 78.8</cell></row><row><cell cols="2">Ours (GSCNN)</cell><cell cols="2">98.3 86.3</cell><cell>93.3 55.8 64.0 70.8 75.9 83.1 93.0 65.1 95.2 85.3 67.9 96.0 80.8 91.2 83.3 69.6 80.4 80.8</cell></row><row><cell>Thrs</cell><cell cols="2">Method</cell><cell cols="2">road s.walk build. wall fence pole t-light t-sign veg terrain sky person rider car truck bus train motor bike mean</cell></row><row><cell></cell><cell cols="4">DeepLabV3+ 92.3 80.4</cell><cell>87.2 59.6 53.7 83.8 75.2 81.2 90.2 60.8 90.4 76.6 78.7 91.6 81.0 87.1 92.6 81.8 78.0 80.1</cell></row><row><cell>12px</cell><cell>Ours</cell><cell></cell><cell cols="2">92.2 81.7</cell><cell>87.9 59.6 54.3 87.1 82.3 84.4 90.9 61.1 91.9 80.4 82.8 92.6 78.5 90.0 94.6 79.1 82.2 81.8</cell></row><row><cell></cell><cell cols="4">DeepLabV3+ 91.2 78.3</cell><cell>84.8 58.1 52.4 82.1 73.7 79.5 87.9 59.4 89.5 74.7 76.8 90.0 80.5 86.6 92.5 81.0 75.4 78.7</cell></row><row><cell>9px</cell><cell>Ours</cell><cell></cell><cell cols="2">91.3 80.1</cell><cell>86.0 58.5 52.9 86.1 81.5 83.3 89.0 59.8 91.1 79.1 81.5 91.5 78.1 89.7 94.4 78.5 80.4 80.7</cell></row><row><cell></cell><cell cols="4">DeepLabV3+ 88.1 72.6</cell><cell>78.1 55.0 49.1 77.9 69.0 74.7 81.0 55.8 86.4 69.0 71.9 85.4 79.4 85.4 92.1 79.4 68.4 74.7</cell></row><row><cell>5px</cell><cell>Ours</cell><cell></cell><cell cols="2">88.7 75.3</cell><cell>80.9 55.9 49.9 83.6 78.6 80.4 83.4 56.6 88.4 75.4 77.8 88.3 77.0 88.9 94.2 76.9 75.1 77.6</cell></row><row><cell></cell><cell cols="4">DeepLabV3+ 83.7 65.1</cell><cell>69.7</cell></row><row><cell>3px</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison vs baselines at different thresholds in terms of boundary F-score on the Cityscapes val set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Comparison of the shape stream, GCL, and additional im-</cell></row><row><cell cols="5">age gradient features (Canny) for different regular streams. Score on</cell></row><row><cell cols="5">Cityscapes val (%) represents mean over all classes and F-Score rep-</cell></row><row><cell cols="3">resents boundary alignment (th=5px).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">th=3px th=5px th=9px th=12px</cell></row><row><cell>Baseline</cell><cell>64.1</cell><cell>69.8</cell><cell>74.8</cell><cell>76.7</cell></row><row><cell>GCL</cell><cell>65.0</cell><cell>70.8</cell><cell>75.9</cell><cell>77.8</cell></row><row><cell>+ Dual Task</cell><cell>68.0</cell><cell>73.0</cell><cell>77.2</cell><cell>78.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of the Dual Task Loss at difference thresholds in terms of boundary quality (F-score). ResNet-101 used in regular stream.</figDesc><table><row><cell cols="4">Base Network Param ∆ (%) Perf ∆ (mIoU) Perf ∆ (mF)</cell></row><row><cell>ResNet-50</cell><cell>+0.43</cell><cell>+1.7</cell><cell>+3.2</cell></row><row><cell>ResNet-101</cell><cell>+0.29</cell><cell>+2.0</cell><cell>+3.5</cell></row><row><cell>WideResNet38</cell><cell>+0.13</cell><cell>+0.9</cell><cell>+2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance improvements and the percentage increase in the number of parameters due to the shape stream on different base networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Qualitative comparison in terms of errors in predictions. Notice that our method produces more precise boundaries, particularly for smaller and thiner objects such as poles. Boundaries around people are also sharper.</figDesc><table><row><cell>image</cell><cell>ground-truth</cell><cell>Deeplab-v3+</cell><cell>ours</cell><cell>image</cell><cell>ground-truth</cell><cell>Deeplab-v3+</cell><cell>ours</cell></row><row><cell>Figure 8: Method</cell><cell cols="7">Coarse road s.walk build. wall fence pole t-light t-sign veg terrain sky person rider car truck bus train motor bike mean</cell></row><row><cell>PSP-Net [58]</cell><cell>98.7 86.9</cell><cell cols="6">93.5 58.4 63.7 67.7 76.1 80.5 93.6 72.2 95.3 86.8 71.9 96.2 77.7 91.5 83.6 70.8 77.5 81.2</cell></row><row><cell>DeepLabV3 [10]</cell><cell>98.6 86.2</cell><cell cols="6">93.5 55.2 63.2 70.0 77.1 81.3 93.8 72.3 95.9 87.6 73.4 96.3 75.1 90.4 85.1 72.1 78.3 81.3</cell></row><row><cell>DeepLabV3+ [11]</cell><cell>98.7 87.0</cell><cell cols="6">93.9 59.5 63.7 71.4 78.2 82.2 94.0 73.0 95.8 88.0 73.3 96.4 78.0 90.9 83.9 73.8 78.9 81.9</cell></row><row><cell>AutoDeepLab-L [34]</cell><cell>98.8 87.6</cell><cell cols="6">93.8 61.4 64.4 71.2 77.6 80.9 94.1 72.7 96.0 87.8 72.8 96.5 78.2 90.9 88.4 69.0 77.6 82.1</cell></row><row><cell>DPC [7]</cell><cell>98.7 87.1</cell><cell cols="6">93.8 57.7 63.5 71.0 78.0 82.1 94.0 73.3 95.4 88.2 74.5 96.5 81.2 93.3 89.0 74.1 79.0 82.7</cell></row><row><cell>AAF-PSP [25]</cell><cell>98.5 85.6</cell><cell cols="6">93.0 53.8 59.0 65.9 75.0 78.4 93.7 72.4 95.6 86.4 70.5 95.9 73.9 82.7 76.9 68.7 76.4 79.1</cell></row><row><cell>TKCN [49]</cell><cell>98.4 85.8</cell><cell cols="6">93.0 51.7 61.7 67.6 75.8 80.0 93.6 72.7 95.4 86.9 70.9 95.9 64.5 86.9 81.8 79.6 77.6 79.5</cell></row><row><cell>Ours (GSCNN)</cell><cell>98.7 87.4</cell><cell cols="6">94.2 61.9 64.6 72.9 79.6 82.5 94.3 74.3 96.2 88.3 74.2 96.0 77.2 90.1 87.7 72.6 79.4 82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison vs state-of-the-art methods (with/without coarse training) on the Cityscapes test set. We only include published methods.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ∈ 0, 1, · · · , m be a running index where r t and s t denote intermediate representations of the corresponding regular and shape streams that we process using a GCL. To</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Karan Sapra for sharing their DeepLabV3+ implementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Devil is in the edges: Learning semantic boundaries from noisy annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bottom-up instance segmentation using deep higher-order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3602" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="402" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8713" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fusionnet: Edge aware deep convolutional networks for semantic segmentation of remote sensing harbor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5769" to="5783" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain transform for edgeaware image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Exemplar-based CRF for Multiinstance Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast interactive object annotation with curve-gcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02985</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recovering three-dimensional shape from a single image of curved objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Maydan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="566" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully Connected Deep Structured Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tree-structured kronecker convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04945</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">CASENet: Deep category-aware semantic edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Simultaneous edge alignment and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
