<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-localization with Category-Consistent Features and Geodesic Distance Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Ping</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Zelinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Co-localization with Category-Consistent Features and Geodesic Distance Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Co-localization is the problem of localizing categorical objects using only positive set of example images, without any form of further supervision. This is a challenging task as there is no pixel-level annotations. Motivated by human visual learning, we find the common features of an object category from convolutional kernels of a pretrained convolutional neural network (CNN). We call these category-consistent CNN features. Then, we co-propagate their activated spatial regions using superpixel geodesic distances for localization. In our first set of experiments, we show that the proposed method achieves state-of-the-art performance on three related benchmarks: PASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that our method is able to detect and localize truly unseen categories, using six held-out ImagNet subset of categories with state-of-the-art accuracies. Our intuitive approach achieves this success without any region proposals or object detectors, and can be based on a CNN that was pre-trained purely on image classification tasks without further fine-tuning. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have been widely applied in the general problem of object localization and detection. The task is to detect a target object's location and its spatial coverage in an image in the form of bounding boxes <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b22">24]</ref>. To localize objects from images, typically a model is given images of category exemplars for training. Critically, these training samples have precise object-level annotations, such as segmentations or bounding boxes. The models can be fine-tuned from a pre-trained <ref type="figure">Figure 1</ref>. Object co-localization with CCFs and geodesic distance co-propagation. From a set of images containing a common object, first we find the CCFs -the group of features that consistently have high responses to the object images of the same class. The CCFs then are used to form an activation map for each image, followed by geodesic distance co-propagation to highlight the exact regions of the objects. network and utilize region proposals for candidate object locations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b2">4]</ref>, or trained end-to-end <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b22">24]</ref>. These models have demonstrated high performance in localizing objects from learned categories, and further fine-tuning is required in order to accommodate novel object categories <ref type="bibr" target="#b20">[22]</ref> .</p><p>Co-localization is the more challenging problem of localizing objects from only the set of positive image examples of the category without any object-level annotations. The lack of negative examples and detailed annotations hinders the use of supervised methods for the co-localization task. Recent methods typically utilize existing region proposal methods for generating a number of candidate regions for objects and object parts, followed by matching or selecting the region with the highest confidence score <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b19">21]</ref>. However, region and object proposals are part of a research problem of its own, and have drawbacks such as lack of repeatability, reduced detection performance with a large number of proposals, and lead to difficult balance in precision and recall <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Our method, however, does not require any object proposals or object detectors to perform co-localization. The main idea in our work is that objects of the same class share common features or parts. Moreover, these commonalities are central to both, the category representation and the detection and localization of the object. By finding those object categorical features, their joint locations can act as a single-shot object detector. This idea is also grounded in human visual learning, where it is suggested that people detect common features from examples of the category, as part of the object-learning process <ref type="bibr" target="#b37">[38]</ref>. We do this by obtaining the CNN features of the provided set of positive images, in order to select the ones that are highly and consistently activated, which we denote as Category-Consistent CNN Features (CCFs) We then use these CCFs to discover the rough object locations, and demonstrate an effective way to co-propagate the feature activations into a stable object for precise co-localization. <ref type="figure">Figure 1</ref> illustrates the pipeline of our proposed framework.</p><p>In more detail, our approach begins with a CNN that has been pre-trained for image classification on ImageNet. Then, the images of the target category are passed through the network. We identify the last-layer convolutional filters that have highly and consistently activated feature maps as the CCFs. The CCFs' feature maps are combined into a single normalized activation probability map, where the highly activated region directly implies the rough object location, since the CCFs represent object parts or object-associated features. The CCF step allows us to bypass the need for region proposals. Then, the activation map is partitioned into superpixels and weighted by the superpixel geodesic distance into an object-likelihood map such that the responses of the object-associated features propagate over the region of the entire object. Finally, the precise object location can be obtained by placing a tight bounding box around the thresholded object-likelihood map.</p><p>The three main contributions of this work are: 1. We pro-pose a novel CCF extraction method that can automatically highlight the rough initial object regions, which acts as a single-shot detector. 2. We introduce an effective method of feature co-propagation for generating a stable object region using superpixel geodesic distances on the original images.</p><p>3. Our method achieves state-of-the-art performance for object co-localization on the VOC 2007 and 2012 datasets <ref type="bibr" target="#b9">[11]</ref>, the Object Discovery dataset <ref type="bibr" target="#b26">[28]</ref>, and the six held-out ImageNet subset categories. Furthermore, our framework is fully unsupervised, objects are discovered using just positive image exemplars. We are able to accurately localize objects without needing any region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Co-localization is related to work on weakly supervised object localization (WSOL) <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b35">37]</ref> since both share the same objective: to localize objects from an image. However, since WSOL allows the use of negative examples, designing the objective function to discover the information of the object-of-interest is less challenging: WSOL-based methods achieve higher performance on the same datasets as compared to co-localization methods, due to the allowed supervised training. For instance, Wang et al. <ref type="bibr" target="#b34">[36]</ref> uses image labels to evaluate the discrimination of discovered categories in order to localize the objects. Ren et al. <ref type="bibr" target="#b24">[26]</ref> adopts a discriminative multiple instance learning scheme to compensate the lack of object-level annotations to localize the objects based on the most discriminative instances. Because of the supervision that is required by those methods, it is not trivial for WSOL approaches to be directly applied to the co-localization scenarios.</p><p>One challenge of co-localization is to define the criteria for discovering the objects without any negative examples. To fill the gap, state-of-the-art co-localization methods such as <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b16">18]</ref> employ object proposals as part of their object discovery and co-localization pipelines. Tang et al. <ref type="bibr" target="#b32">[34]</ref> use the measure of objectness <ref type="bibr" target="#b0">[2]</ref> to generate multiple bounding boxes for each image, followed by an objective function to simultaneously optimize the image-level labels and box-level labels. Such settings allow the use of discriminative cost function <ref type="bibr" target="#b14">[16]</ref>. This is also used in the work of co-localization on video frames <ref type="bibr" target="#b16">[18]</ref>. Cho et al. <ref type="bibr" target="#b5">[7]</ref> also starts from object proposals, their method shares the same spirit with the deformable part model <ref type="bibr" target="#b10">[12]</ref> where the objects are discovered and localized by matching common object parts. Most recently, Li et al. <ref type="bibr" target="#b19">[21]</ref> study the confidence score distribution of a supervised object detector over the set of object proposals to define an objective function, that learns a common object detector with similar confidence score distribution. All the aforementioned methods heavily depend on the quality of object proposals.</p><p>Our work approaches the problem from a different perspective. Instead of trying to fill in the gap of the negative data and annotations that are unavailable, we find the common features shared by the objects from the positive images. Then, we use the joint locations of those features as our single-shot object detector. This allows us to bypass the need for utilizing a region or object proposal algorithm as a fist step. Our subsequent step refines the detected object features into a stable object by co-propagating their activations together. We describe the details of our 2-step approach in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extracting Category-Consistent CNN Features</head><p>Our proposed method consists of two main steps. The first step is to find the CCFs of a category, and obtain their combined feature map that contains aggregated CCF activations over the rough object region. Then, the CCF activations are co-propagated into a stable object using superpixel geodesic distances on the original images.</p><p>Given a set of n object images from the same class and a CNN that has been pre-trained to contain sufficient visual features, we first compute the m feature maps from the k last-layer convolutional kernels over the n images. Then, we obtain an m × n activation matrix with each row being the activation vector a of a kernel containing the maximum values of the kernel's feature maps.</p><p>Specifically, for each kernel:</p><formula xml:id="formula_0">A i,j = max(F (i, j)), where F (i, j)</formula><p>is the feature map of kernel i, given image j that has been forward-passed through the CNN. The activation matrix A therefore describes the max-response distributions of all kernels to all category images.</p><p>Our goal in this step is to identify a subset of representative kernels from the global set of candidate kernels, that contain common features from the positive images of the same class. This implies that the activation vectors of the kernels should have high values over all vector elements, since there is at least one instance of the object on every image. Conceptually, the kernels that we seek correspond to object parts, or some object-associated features. To find the CCFs, we compute the pair-wise similarities between all pairs of kernels' activation vectors, and cluster them using k-means. The kernels from the cluster with the highest mean activation correspond to the CCFs. The similarity s i,j between two CNN kernels i and j, can be defined as the L p distance between two activation vectors A i and A j .</p><p>The sets of positive images are only required for the identification of the CCF kernels. The CCF kernels can then be used to generate the rough object location in an image in a single-shot: given an image from the target category, the feature maps of the CCFs are combined to form a single activation map. Since each CCF corresponds to an object part of object-associate features, the densely activated area of the activation map indicates the rough location of the target object. The final activation map is normalized into a probability map that sums to 1. In <ref type="figure" target="#fig_0">Figure 2</ref> we show the identified CCFs for the bus category, where the activated regions describe bus-related features and all fall within the spatial extent of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stable Object Completion via Co-Propagating CCF Activations</head><p>The activation probability map from the CCFs automatically points out only the rough location of the object. It does not ensure a reliable object localization due to: 1.The higher layer of a CNN does not guarantee a kernel's receptive-field size to cover the area of an entire object. 2.While the feature maps contain spatial information, they have unprecise object locations due to previous max-pooling layers. 3.The CNN was trained discriminatively. Hence, only discriminative features of each object may be localized rather than the the whole object.</p><p>In order to obtain the complete region that corresponds to the object, we compute geodesic distances between superpixels on the original image. In essence, the geodesic distance compactly encodes the similarity relationship between the two superpixels' image contents. The similarity is computed via object boundary detection algorithm <ref type="bibr" target="#b18">[20]</ref>. Therefore, the smaller geodesic distance between the two superpixels, the more likely that they belong to the same object. Based on this characteristic, we propose a simple and effective method to highlight the object region from the activation probability map, that is both low-resolution and contains non-smooth feature activations.</p><p>Given an input image, we oversegment it into superpixels. The geodesic distance d i,j between a pair of superpixels i, j is computed based on the graph built from the boundary probability map, which is done similarly as the method proposed by Krähenbühl and Koltun <ref type="bibr" target="#b18">[20]</ref>. We take the combined activation map that was obtained in the CCF identification step, and assign an energy value to each su- <ref type="figure">Figure 3</ref>. Geodesic distance propagation. The first column is the original image and the boundary map extracted by <ref type="bibr" target="#b21">[23]</ref>. The second column marks the position and initial activation of two selected superpixels, for illustrating the effect of our activation propagation for a single superpixel. The top superpixel is located in the background while the bottom one in located in the region of the category object (bird). The next columns illustrate the propagated activations from the two superpixels to all other superpixels, with the controlling parameter µ set at 0.5,1 and 2, respectively. perpixel by averaging its corresponding pixel values found in the activation map. For k superpixels, we denote the resulting flattened k × 1 superpixel activation vector as E. Vector E can be considered as the initial likelihood of each superpixel being within the object.</p><p>Next, we perform the geodesic distance propagation to localize the object. The main idea is that if two superpixels are likely to belong to the same object, then they should have similar geodesic distances and activations. This concept has been similarly adopted by various works in terms of interactive image segmentation and matting <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b11">13]</ref>, and we find it to suit specially well for our purpose. Therefore, we seek to obtain a global activation map that has regions of highly boosted or co-propagated activations by superpixels of similar geodesic distances, mediated by some level of consistency by formulating this co-propagating mechanism into a k × k co-propagation matrix W, such that W i,j is the normalized amount of co-propagation between superpixel i and j, with a parameter µ for controlling the amount of activation diffusion:</p><formula xml:id="formula_1">W i,j = exp(−d i,j × µ −1 ) N k=1 exp(−d i,k × µ −1 )</formula><p>.</p><p>(1)</p><p>Finally, we apply W to the activation vector E directly:</p><formula xml:id="formula_2">E = WE,<label>(2)</label></formula><p>where E is the co-propagated activation vector of the image, containing the globally boosted activations of the superpixels based on their pair-wise geodesic distances to all other superpixels. This allows us to fill in each superpixel on the image with their respective values from E , and normalize the co-propagated superpixel map by dividing every pixel by the max value of the map. The result is an objectlikelihood map, on which we apply a global threshold to obtain the region as our final object co-localization result. Finally, a tight bounding box is placed around the maximum coverage of the thresholded regions within an image. <ref type="figure">Figure 3</ref> illustrates the effects of activation propagation for two selected superpixels. The top row corresponds to the case of a background superpixel. This superpixel has small geodesic distances to a large set of other superpixels. Hence, the activation values propagated from this superpixel to others are relatively small. In the second row, we consider a superpixel corresponding to the bird. In this case, high activation values are only propagated from this superpixel to other superpixels that also correspond to the bird. Hence, we filter out the undesirable high activation of the background regions surrounding the object, while also balancing the activation of all superpixels that reside within the same object. The first column of <ref type="figure">figure 3</ref> shows the original image and its boundary map. The second column marks the position and initial activation of the two selected superpixels. The next columns illustrate the activation propagating from the selected superpixel to all other superpixels with a varying degree of controlling parameter µ set at 0.5,1 and 2, where warmer values indicate higher activations. It can be seen from the third column that since the background superpixel has small geodesic distances to many other superpixels, the activations being propagated from this superpixel to others are equally small; in contrast, the activations propagated from the bottom superpixel mostly fall into its close neighboring superpixels and ones that belong to the object's region. As µ increases, the amount of propagation is more evenly and widely spread, but with a lower overall magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our proposed 2-step framework with different parameter settings to illustrate different characteristics of our method. We also evaluate our method on multiple benchmarks, with intermediate and final results to show the localization effects of our proposed method. In all of our experiments, we used the last convolutional layer of a VGG-19 network <ref type="bibr" target="#b30">[32]</ref> that was pre-trained on ImageNet <ref type="bibr" target="#b27">[29]</ref> as our CCF kernel pool. We used k = 5 for kmeans clustering in the CCF identification step. The geodesic distances are computed using the Structured Forests soft boundary <ref type="bibr" target="#b21">[23]</ref>. The control parameter µ for activation co-propagation was set at 0.5. The final global threshold for obtaining the object region from the object-likelihood map was set at 0.25 for all images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation metric and datasets</head><p>We use the conventional CorLoc metric <ref type="bibr" target="#b8">[10]</ref> to evaluate our co-localization results. The metric measures the percentage of images that contain correctly localized results. An image is considered correctly localized if there is at least one ground truth bounding box of the objectof-interest having more than 50% Intersection-over-Union (IoU) score with the predicted bounding box. To benchmark our method performance, we evaluate our method on three commonly used datasets for the problem of co-localization. These are VOC 2007 and 2012 <ref type="bibr" target="#b9">[11]</ref>, and Object Discovery dataset <ref type="bibr" target="#b26">[28]</ref>. For experiments on VOC datasets, we followed previous works <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b19">21]</ref> that used all images on the trainval set excluding the images that only contain the object instances annotated as difficult or truncated. For experiments on the Object Discovery dataset, we used the 100image subset following <ref type="bibr" target="#b25">[27]</ref> in order to make an appropriate comparison with related methods. The ground truth bounding box for each image in the Object Discovery dataset is defined as the smallest bounding box covering all the segmentation ground truth of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to state-of-the-art co-localization methods</head><p>We first evaluate our method on the 100-image subset of Object Discovery dataset which contains objects of three classes, namely airplane, car, and Horse. There are 18, 11, and 7 noisy images in each class, respectively. <ref type="table">Table  4</ref> reports the co-localization performance of our approach in comparison with the state-of-the-art methods on image co-localization <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b19">21]</ref>. In this small scale setting, our method outperformed other methods in both in individual object classes and overall.</p><p>Three examples of our co-localization approach on the Object Discovery dataset are illustrated in <ref type="figure">figure 4</ref>. The second row shows the combined activation map from the <ref type="figure">Figure 4</ref>. Object co-localization results on Object Discovery dataset. From the top row to the bottom row: input image, combined activation map from the identified CCFs, propagated objectlikelihood map, and resulting bounding boxes. We also depict in green the pixels with the object-region that's predicted by our method. Our predicted bounding boxes are colored as white while the ground truth bounding boxes are green. set of identified CCFs, that acted as our single-shot object detector. It is apparent that the combined activation maps already provided object estimates that were quite accurate to the location of the actual object in the images, with different parts of each object getting high values such as the tail of the airplane, the wheel of the car, or head and tail of the horse. All these values were co-propagated based on the superpixel geodesic distances, resulted in the images shown in the third row. The co-propagated object-likelihood maps on the third row show that the sporadic activations on the background have been smoothed out evenly, and that the non-smooth object parts and their associated activations have been boosted and completed into complete and stable objects with significantly higher activation magnitudes. This shows that our two-step framework was able to generate informative single-shot object detection using the CCFs, and the subsequent stable object region via activation copropagation.</p><p>The PASCAL VOC 2007 and 2012 datasetes both contains realistic images of 20 object classes with significantly larger numbers of images per class. These datasets are more challenging than the Object Discovery dataset due to the diversity of viewpoints, and the complexity of the objects.  Our approach outperforms the state-of-the-art method <ref type="bibr" target="#b19">[21]</ref> by 1.2% on average, and acquires the highest scores for 11 out of 20 classes. Results on VOC 2012 dataset, that has twice the number of images than the VOC 2007 dataset, are shown in table 2. Our method also achieves significantly better results than state-of-the-art methods with an 3.65% increase on average, acquiring the highest scores for 10 out of 20 classes. We also compare our method with state-of-the-art weekly-supervised object localization methods <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b35">37]</ref>, which is summarized in table 3. While our results did not achieve the overall state-of-the-art, our approach was able to outperform the methods in 3 of the 20 classes, without using any negative examples or any form of supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Category-consistent CNN features selection analysis</head><p>In this section, we provide an analysis to justify our CCF selection method, where we conducted additional experi-ments with the same configurations but using different subsets of CNN features for the initial object detection step. After clustering the last-layer CNN kernels based on their image-level activations, these clusters were sorted in a decreasing order by the clusters' average activations. We then obtained the rough object locations using individual clusters and thresholded on those maps directly to obtain the object locations. Their respective average CorLoc scores on the VOC 2007 and 2012 dataset are reported in table 5.</p><p>The table shows that the co-localization performance followed in the exact order of the clusters based on their average activations, suggesting that the most representative features were indeed members of the top cluster. The visualized examples are shown in <ref type="figure">figure 5</ref>, with an image from the dog and motorbike category, respectively. For each image, the first row is the results of our method when using the first cluster (ranked by their average activation) and the second row shows the results of our method when using the third cluster. It is clear that the combined activation maps from the third cluster failed to detect and estimate the object locations, and ultimately lead to incorrect object localization results. This indicates that the selection of the top cluster is  <ref type="table">Table 6</ref>. The number of kernels (%) per cluster. The percentage of kernels in each cluster based on the activation vectors described in Section 3. The clusters were formed by using K-mean with k = 5, and the kernels were taken from the last convolutional layer of VGG-19, on each class of the VOC 2012 dataset. <ref type="figure">Figure 5</ref>. Two examples illustrating the effect of our feature selection method. For each image, the first row is the results of our method when using the first cluster and the second row is the results of our method when using the third cluster. The green bounding box is the ground truth and the predicted bounding boxes are colored as white, the predicted object regions are masked as green. essential, and the CCFs could not be chosen arbitrarily. We furthermore validate our feature selection method by evaluating the performance of our approach when the CCFs were identified from more than one cluster, and the results are shown in table 7. This experiment shows that large amount of kernels do not provide enough object specificity, and therefore resulting in a similar performance decline as in table 5, such that performance decreases when more clusters were added.  <ref type="table">Table 7</ref>. Co-localization performance of our methods on the VOC 2007 and 2012 dataset. Each column indicates how many top clusters were taken as the CCF clusters (out of 5 total clusters), and the corresponding average corLoc score (%) by using the selected clusters of features for co-localization. For example, the last column indicates that all available features were used for co-localization.</p><p>We also report the number of kernels per cluster in table 6. Looking at table 6 and table 2 together, they suggest that there are multiple classes that require just a small amount of kernels in order to be localized decently. For example, the cat class used less than 20 kernels (3.7% of the 512 total kernels) to achieve the state-of-the-art CorLoc score of 74.9% in VOC 2012 dataset. In general, the table shows that the first cluster only contains a small number of kernels, and the results suggest that these small number of specific CCFs are sufficient for the co-localization performed on the three benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Geodesic distance co-propagation analysis</head><p>Geodesic distance acts as a refinement step in our pipeline to get rid of the background as well as boosting the activation within the object region. To evaluate the effect of geodesic distance co-propagation, we simply compared the performances of our method on VOC 2007 and 2012 datasets with and without geodesic distance copropagation, and the results are reported in table 8. The results show that geodesic distance co-propagation significantly improved the co-localization accuracy by more than 6% in absolute CorLoc score for both dataset, which means it is an important process subsequent to our initial CCF object detection step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Unseen classes from ImageNet subset</head><p>Our VGG-19 model was pre-trained on ImageNet's 1000 classes. While VOC 2007 and 2012 are different datasets from ImageNet, there are significant overlaps between the object categories in VOC and ImageNet. For example, the "motorbike" class of VOC datasets is equivalent to the "moped" class of ILSVRC 2012 dataset. outperformed <ref type="bibr" target="#b19">[21]</ref> and <ref type="bibr" target="#b5">[7]</ref> by a large margin on average, and all but one individual classes. The six subsets of the ImageNet dataset, chosen by Li et al. <ref type="bibr" target="#b19">[21]</ref>, are held-out categories from the 1000-label classification task, which means they do not overlap with the 1000 classes used to train VGG-19. We show that our method is generalizable to truly novel object categories with the six held-out ImageNet subset classes. The images and the corresponding bounding box annotations were downloaded from ImageNet website [1], and <ref type="table">Table 9</ref> shows the numbers of images in each class with available bounding box annotations when we accessed the ImageNet dataset website. It is noticeable that paper <ref type="bibr" target="#b19">[21]</ref> used an even smaller set with less images <ref type="table">(table 9)</ref>. To compare with the methods of <ref type="bibr" target="#b19">[21]</ref> and <ref type="bibr" target="#b5">[7]</ref>, we first conduct our experiment on the same smaller sets of images that was used in <ref type="bibr" target="#b19">[21]</ref>. Then, We test our method on the full set of ImageNet held-out categories that we downloaded from the ImageNet dataset. <ref type="table">Table 10</ref> reports the co-localization of our method compares to <ref type="bibr" target="#b19">[21]</ref> on the smaller ImageNet subset, and table 11 reports the performance of our method on the ImageNet full subsets. The results show that our method significantly outperforms the competing methods in the smaller subset, with even higher accuracies for the full subset in <ref type="table">Table 11</ref>. This result demonstrates that our method is robust to detect and localize truly unseen categories using previously learned CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative results</head><p>We show some examples of our co-localization results in figure 7 and 8. The results show that the bounding boxes generated by our proposed framework accurately match the ground truth bounding boxes. It is apparent that our results generate well-covered object regions, that has the potential to well delineate the objects in majority of the cases. The figures also show that the objects were able to be accurately co-localized with various sizes and locations. <ref type="figure">Figure 6</ref>. Some failed examples of our approach. While these examples did not have significant coverage with the ground truth boudning box in terms of CorLoc, the mistakes were still accurate, in the sense that the areas were detected but not the spatial extent. <ref type="figure">Figure 6</ref> illustrates three failure scenarios of our approach. While these three examples did not cover the ground truth bounding box sufficiently, but they were not far off. Some analysis suggests that these failures were due to some CCFs that are shared by multiple categories, and that the object boundaries may not have been strong enough (i.e. bottle and boat).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed a fully unsupervised 2-step approach for the problem of co-localization, that uses only positive images, and without any region or object proposals. Our method is motivated by human vision, people implicitly detect the common features of category examples to learn the representation of the class. We show that the identified category-consistent features can also act as an effective first-pass object detector. This idea is implemented by finding the group of CNN features that are highly and consistently activated by a given positive set of images. The result of this first step generates a rough but reliable object <ref type="bibr">Figure 7</ref>. Some example results of object co-localization with our CCFs and geodesic distance co-propagation. The results show the bounding boxes that we generate match the ground truth bounding boxes very well, even when objects are not located centrally in the image. In addition, our co-localized object pixel-level regions (pixels colored in green) show well delineated shape in most cases. <ref type="figure">Figure 8</ref>. Some example results of our object co-localization on class "chipmunk" from the six held-out ImageNet subset categories. The results show co-localized object pixel-level regions (pixels colored in green), the bounding boxes from our method (white), and ground truth bounding boxes (green). location, and acts as a single-shot object detector. Then, we aggregate the activations of the identified CCFs, and copropagate their activations so that the activations over the true object region are boosted, while the activations over the background region are smoothed out. This effective activation refinement step allowed us to obtain accurately co-localized objects in terms of the standard CorLoc score with bounding boxes. We achieved new state-of-the-art performance on the three commonly used benchmarks. In the future, we plan to extend our method to generate unsupervised object co-segmentations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples of our CCFs for bus category. Each row is a different example image, and each column is the activation (in violet) of a single CNN feature in the set of our CCFs. The last column shows the final co-localization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Table 1 .Table 2 .</head><label>112</label><figDesc>reports our performance on the VOC 2007 dataset. CorLoc scores of our approach and state-of-the-art co-localization methods on Pascal VOC 2007 dataset. 41.20 36.00 26.90 5.00 81.10 54.60 50.90 18.20 54.00 31.20 44.90 61.80 48.00 13.00 11.70 51.40 45.30 64.60 39.20 41.80 [21] 65.70 57.80 47.90 28.90 6.00 74.90 48.40 48.40 14.60 54.40 23.90 50.20 69.90 68.40 24.00 14.20 52.70 30.90 72.40 21.60 43.80 ours 63.06 67.79 50.39 41.06 19.73 79.20 44.50 74.79 15.27 40.39 27.42 68.40 69.36 74.34 21.17 14.29 53.37 39.69 69.41 15.46 47.45 CorLoc scores of our approach and state-of-the-art co-localization methods on Pascal VOC 2012 dataset. 58.30 28.40 20.70 6.80 54.90 69.10 20.80 9.20 50.50 10.20 29.00 58.00 64.90 36.70 18.70 56.50 13.20 54.90 59.40 38.84 [37] 37.70 58.80 39.00 4.70 4.00 48.40 70.00 63.70 9.00 54.20 33.30 37.40 61.60 57.60 30.10 31.70 32.40 52.80 49.00 27.80 40.16 [5] 66.40 59.30 42.70 20.40 21.30 63.40 74.30 59.60 21.10 58.20 14.00 38.50 49.50 60.00 19.80 39.20 41.70 30.10 50.20 44.10 43.69 [26] 79.20 56.90 46.00 12.20 15.70 58.40 71.40 48.60 7.20 69.90 16.70 47.40 44.20 75.50 41.20 39.60 47.40 32.20 49.80 18.60 43.91 [36] 80.10 63.90 51.50 14.90 21.00 55.70 74.20 43.50 26.20 53.40 16.30 56.70 58.30 69.50 14.10 38.30 58.80 47.20 49.10 60.90 47.68</figDesc><table><row><cell cols="3">VOC aero bike</cell><cell cols="4">bird boat bottle bus</cell><cell>car</cell><cell cols="12">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mean</cell></row><row><cell cols="5">[18] 32.8 17.3 20.9 18.2</cell><cell>4.5</cell><cell cols="3">26.9 32.7 41.0</cell><cell>5.8</cell><cell cols="4">29.1 34.5 31.6 26.1</cell><cell>40.4</cell><cell>17.9</cell><cell>11.8</cell><cell cols="3">25.0 27.5 35.6 12.1 24.6</cell></row><row><cell>[7]</cell><cell cols="4">50.3 42.8 30.0 18.5</cell><cell>4.0</cell><cell cols="3">62.3 64.5 42.5</cell><cell>8.6</cell><cell cols="4">49.0 12.2 44.0 64.1</cell><cell>57.2</cell><cell>15.3</cell><cell>9.4</cell><cell cols="3">30.9 34.0 61.6 31.5 36.6</cell></row><row><cell cols="5">[21] 73.1 45.0 43.4 27.7</cell><cell>6.8</cell><cell cols="3">53.3 58.3 45.0</cell><cell>6.2</cell><cell cols="4">48.0 14.3 47.3 69.4</cell><cell>66.8</cell><cell>24.3</cell><cell>12.8</cell><cell cols="3">51.5 25.5 65.2 16.8 40.0</cell></row><row><cell cols="5">ours 56.3 46.75 43.8 29.6</cell><cell>8.7</cell><cell cols="3">62.8 54.6 74.9</cell><cell>8.8</cell><cell cols="4">42.1 27.7 50.5 59.2</cell><cell>71.0</cell><cell>19.3</cell><cell>13.9</cell><cell cols="3">45.4 31.0 68.9 9.6</cell><cell>41.2</cell></row><row><cell cols="2">VOC aero</cell><cell>bike</cell><cell>bird</cell><cell cols="2">boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell cols="6">horse mbike person plant sheep sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row><row><cell cols="2">[7] 57.00 VOC aero</cell><cell>bike</cell><cell>bird</cell><cell cols="2">boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell cols="6">horse mbike person plant sheep sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row><row><cell cols="5">[33] 42.40 46.50 18.20 8.80</cell><cell cols="10">2.90 40.90 73.20 44.80 5.40 30.50 19.00 34.00 48.80 65.30</cell><cell>8.20</cell><cell cols="4">9.40 16.70 32.30 54.80 5.50 30.38</cell></row><row><cell cols="15">[31] 67.30 54.40 34.30 17.80 1.30 46.60 60.70 68.90 2.50 32.40 16.20 58.90 51.50 64.60</cell><cell>18.20</cell><cell cols="4">3.10 20.90 34.70 63.40 5.90 36.18</cell></row><row><cell cols="4">[8] 56.60 ours 56.3 46.75 43.8</cell><cell>29.6</cell><cell>8.7</cell><cell>62.8</cell><cell>54.6</cell><cell>74.9</cell><cell>8.8</cell><cell>42.1</cell><cell>27.7</cell><cell>50.5</cell><cell>59.2</cell><cell>71.0</cell><cell>19.3</cell><cell>13.9</cell><cell>45.4</cell><cell>31.0</cell><cell>68.9</cell><cell>9.6</cell><cell>41.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>CorLoc scores of our approach and state-of-the-art weekly-supervised-object-localization methods on Pascal VOC 2007 dataset. Experiment on Object Discovery Dataset. Highest performances are labeled in bold.</figDesc><table><row><cell cols="2">Methods Airplane</cell><cell>Car</cell><cell>Horse Mean</cell></row><row><cell>[19]</cell><cell>21.95</cell><cell>0.00</cell><cell>16.13 12.69</cell></row><row><cell>[16]</cell><cell>32.93</cell><cell cols="2">66.29 54.84 51.35</cell></row><row><cell>[17]</cell><cell>57.32</cell><cell cols="2">64.04 52.69 58.02</cell></row><row><cell>[27]</cell><cell>74.39</cell><cell cols="2">87.64 63.44 75.16</cell></row><row><cell>[18]</cell><cell>71.95</cell><cell cols="2">93.26 64.52 76.58</cell></row><row><cell>[7]</cell><cell>82.93</cell><cell cols="2">94.38 75.27 84.19</cell></row><row><cell>Ours</cell><cell>84.15</cell><cell cols="2">94.38 78.49 85.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Co-localization performance of our method on the VOC 2007 and 2012 dataset. Each column indicates which top cluster was taken as the CCF cluster (out of 5 total clusters), and the corresponding average CorLoc score (%) by using the selected cluster of features for co-localization. 43.0 40.6 24.6 37.5 28.7 28.7 23.6 20.1 34.4 32.8 38.1 35.2 29.7 30.5 28.5 38.3 26.0 25.2 28.5 5th 15.0 26.8 33.0 23.6 15.0 18.8 14.3 31.6 11.5 21.7 14.3 27.3 29.5</figDesc><table><row><cell>Dataset</cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th</cell><cell>5th</cell></row><row><cell cols="6">VOC07 41.24 38.85 31.73 29.62 23.12</cell></row><row><cell cols="6">VOC12 47.45 42.35 37.70 33.91 25.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>VOC07 41.24 41.00 38.22 34.80 33.25 VOC12 47.45 46.68 44.14 40.65 39.33</figDesc><table><row><cell>Dataset</cell><cell>1</cell><cell>1-2</cell><cell>1-3</cell><cell>1-4</cell><cell>1-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10 Table 9 .</head><label>109</label><figDesc>reports the co-localization performance of our method on the 6 smaller subsets in comparison with the two state-of-the-art methods. The table shows that our method Number of images in each of the 6 subsets selected from ImageNet dataset, collected by us directly from the ImageNet website (Full), and the smaller set by Li et al.<ref type="bibr" target="#b19">[21]</ref> (Smaller from<ref type="bibr" target="#b19">[21]</ref>).</figDesc><table><row><cell cols="2">ImageNet</cell><cell></cell><cell cols="5">chipmunk rhino stoat racoon rake wheelchair</cell></row><row><cell cols="2"># of images (Full)</cell><cell></cell><cell>307</cell><cell>213</cell><cell>237</cell><cell>1301</cell><cell>466</cell><cell>420</cell></row><row><cell cols="3"># of images (Smaller from [21])</cell><cell>159</cell><cell>89</cell><cell>159</cell><cell>103</cell><cell>146</cell><cell>174</cell></row><row><cell cols="8">ImageNet chipmunk rhino stoat racoon rake wheelchair mean</cell></row><row><cell>[7]</cell><cell>26.60</cell><cell cols="3">81.80 44.20 30.10</cell><cell>8.30</cell><cell cols="2">35.30</cell><cell>37.72</cell></row><row><cell>[21]</cell><cell>44.00</cell><cell cols="4">81.80 67.30 41.80 14.50</cell><cell cols="2">39.30</cell><cell>48.12</cell></row><row><cell>Ours</cell><cell>72.33</cell><cell cols="4">88.76 75.59 83.16 11.64</cell><cell cols="2">49.53</cell><cell>63.50</cell></row><row><cell cols="8">Table 10. CorLoc scores [10] (%) of our approach and state-of-the-art co-localization methods on the 6 smaller subsets of ImageNet</cell></row><row><cell>collected by Li et al. [21].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">ImageNet chipmunk rhino stoat racoon rake wheelchair mean</cell></row><row><cell>Ours</cell><cell>76.87</cell><cell cols="4">87.79 80.59 74.10 50.64</cell><cell cols="2">54.05</cell><cell>70.67</cell></row></table><note>Table 11. CorLoc scores [10] (%) of our approach and state-of-the-art co-localization methods on the 6 full subsets of ImageNet.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.28</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2012.28" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geodesic matting: a framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active object localization with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-fold MIL Training for Weakly Supervised Object Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-00975746" />
	</analytic>
	<monogr>
		<title level="m">CVPR 2014 -IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multifold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-012-0538-3</idno>
		<idno>1573-1405. doi: 10.1007/ s11263-012-0538-3</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-012-0538-3" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with frank-wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>doi: 10.1109/ ICCV.2011.6126239</idno>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Geodesic Object Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_47</idno>
		<idno>978-3-319-10602-1. doi: 10.1007/ 978-3-319-10602-1 47</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-10602-1_47" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="725" to="739" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image colocalization by mimicking a good detector&apos;s confidence score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is object localization for free? weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/structured-forests-for-fast-edge-detection/" />
	</analytic>
	<monogr>
		<title level="m">ICCV. International Conference on Computer Vision, December 2013</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised large scale object localization with multiple instance learning and bag splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2456908</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2015.2456908" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OverFeat:integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian joint topic modelling for weakly supervised object localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126261</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2011.6126261" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-4_28</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-10599-4_28" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relaxed multiple-instance svm with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno>978-1-4673-8391-2. doi: 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1224" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2015.145</idno>
		<idno>2015.145</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2015.145" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Searching for category-consistent features: a computational approach to understanding visual category representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maxfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient video segmentation using parametric graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
