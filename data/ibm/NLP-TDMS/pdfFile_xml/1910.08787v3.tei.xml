<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY SpatialFlow: Bridging All Tasks for Panoptic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anda</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY SpatialFlow: Bridging All Tasks for Panoptic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Panoptic segmentation</term>
					<term>Scene understanding</term>
					<term>Location-aware</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object location is fundamental to panoptic segmentation as it is related to all things and stuff in the image scene. Knowing the locations of objects in the image provides clues for segmenting and helps the network better understand the scene. How to integrate object location in both thing and stuff segmentation is a crucial problem. In this paper, we propose spatial information flows to achieve this objective. The flows can bridge all sub-tasks in panoptic segmentation by delivering the object's spatial context from the box regression task to others. More importantly, we design four parallel sub-networks to get a preferable adaptation of object spatial information in sub-tasks. Upon the sub-networks and the flows, we present a location-aware and unified framework for panoptic segmentation, denoted as SpatialFlow. We perform a detailed ablation study on each component and conduct extensive experiments to prove the effectiveness of SpatialFlow. Furthermore, we achieve state-ofthe-art results, which are 47.9 PQ and 62.5 PQ respectively on MS-COCO and Cityscapes panoptic benchmarks. Code will be available at https://github.com/chensnathan/SpatialFlow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. An illustration of the panoptic segmentation task. We also provide the bouding box for each object in the image and add process to integrate box location to both thing and stuff segmentation.</p><p>Panoptic segmentation aims to assign all pixels in an image with a semantic and an instance id, which is a challenging task as it requires a global view of segmentation. In <ref type="bibr" target="#b2">[3]</ref>, the authors tried to solve the task by adopting two independent models, Mask R-CNN <ref type="bibr" target="#b3">[4]</ref> and PSPNet <ref type="bibr" target="#b4">[5]</ref>, for thing and stuff segmentation 1 respectively. Then, they applied a heuristic post-processing method to merge the segmentation outputs of two tasks, as illustrated on the right side of <ref type="figure">Figure 1</ref>. These two independent models ignore the underlying relationship between things and stuff and bring computation budder into the framework. Recently, several works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> follow <ref type="bibr" target="#b2">[3]</ref> and try to build a unified pipeline for panoptic segmentation via sharing the backbone between two segmentation tasks.</p><p>However, most of the recent works focus on how to combine the outputs of segmentation tasks properly, failing to highlight the significance of object location when training networks. As demonstrated in the literature, the spatial information of objects can boost the performance of algorithms in object detection <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, instance segmentation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and semantic segmentation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Our key insight is that, as a combination of these tasks, panoptic segmentation can benefit from delivering spatial information of objects among its sub-tasks. We illustrate the process of performing panoptic segmentation with box locations in <ref type="figure">Figure 1</ref>.</p><p>A crucial question then arises: how to integrate spatial information into the segmentation tasks seamlessly? To fulfill this goal, we propose to combine object location by explicitly delivering the spatial context from the box regression task to others. Based on this, we introduce a new unified framework for panoptic segmentation by fully leveraging the reciprocal relationship among detection, thing segmentation, and stuff segmentation. Two primary principles are considered as follows.</p><p>First, keep the spatial context in pixel-level before segmenting things and stuff. Although thing and stuff segmentation can complement one another, the format of dominated features in these two segmentation tasks may be inconsistent. The instance-level features control the thing segmentation, while the pixel-level features guide the stuff segmentation. The instance-level spatial context may not be suitable for stuff segmentation, given the format of its dominant feature. Besides, instances can be overlapping, which makes it hard to map them back to pixel-level. Based on this principle, we resort to one-stage detector RetinaNet <ref type="bibr" target="#b17">[18]</ref> instead of two-stage detector Faster R-CNN <ref type="bibr" target="#b18">[19]</ref>. It prevents the spatial context of objects from being in the format of instance-level before performing segmentation tasks. Then, we extend RetinaNet with taskspecific heads -thing head <ref type="bibr" target="#b19">[20]</ref> and stuff head <ref type="bibr" target="#b7">[8]</ref> to perform thing and stuff segmentation. In task-specific heads, the spatial context can be instance-level for things and be pixel-level for stuff.</p><p>Second, integrate the spatial context into segmentation by fully leveraging feature interweaving among tasks. The spatial context plays a significant role in improving the quality of segmentation. It is plentiful in the box regression sub-task but insufficient in others. To make other sub-tasks locationaware, we propose the information flows to deliver the spatial context from the box regression task to others and integrate it by feature interweaving. However, the absence of multi-stage features in thing and stuff segmentation makes it inconvenient to absorb the spatial context. To solve this dilemma, we design four parallel sub-networks for four sub-tasks in the framework, enabling the model to leverage feature interweaving among tasks.</p><p>The overall design fully leverages the spatial context, bridges all the tasks in panoptic segmentation by integrating features among them, and builds a global view for the image scene, leading to better refinement of features, more robust representations for image segmentation, and higher prediction results.</p><p>Our contributions are three-fold:</p><p>• In this paper, we present a new unified framework for panoptic segmentation. Our framework is built on the one-stage detector RetinaNet, which facilitates feature interweaving in pixel-level. • Based on the proposed framework, we design four parallel sub-networks to refine sub-task features. Among the sub-networks, we propose the spatial information flows to bridge all sub-tasks by making them location-aware. Our framework is denoted as SpatialFlow. <ref type="bibr">•</ref> We perform a detailed ablation study on various components of SpatialFlow. Extensive experimental results show that SpatialFlow achieves state-of-the-art results, which are 47.9 PQ and 62.5 PQ on COCO <ref type="bibr" target="#b20">[21]</ref> and Cityscapes <ref type="bibr" target="#b21">[22]</ref> panoptic benchmarks. The rest of our paper is organized as follow: In Section II, we briefly revisit recent progress related to this paper; in Section III, we first present the proposed unified framework for panoptic segmentation based on RetinaNet, then we illustrate all the details of the designed parallel sub-networks and the spatial information flows; in Section IV, V, VI, we present all details and results of the experiments, analyze the effect of each component, and make further discussions; finally, we conclude the paper in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>After the pioneering application of AlexNet <ref type="bibr" target="#b22">[23]</ref> on the ImageNet datasets <ref type="bibr" target="#b23">[24]</ref>, deep learning methods have come to dominate computer vision. These methods have dramatically improved the state-of-the-art in many vision tasks, including image recognition <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, image retrieval <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, metric learning <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, object detection <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b18">[19]</ref>, image segmentation <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b3">[4]</ref>, human pose estimation <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and many other tasks. Our work builds on prior works in object detection and image segmentation. We apply multi-task learning <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> in our model, which makes things and stuff segmentation tasks benefit each other and builds a global view for the image scene. Next, we review some works that are closest to our work as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Detection</head><p>Our community has witnessed remarkable progress in object detection. Works, such as <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b43">[44]</ref>, tackled the detection problem by a two-stage approach. They first generated a number of object proposals as candidates, followed by a classification head and a regression head on each RoI. Numerous recent breakthroughs has been made, such as adjusting network structures <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> and searching for better training strategies <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Another type of detector followed the single-stage pipeline, such as <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b17">[18]</ref>. They directly predict object categories and regress bounding box locations based on pre-defined anchors. Recently, researchers focus on improving the localization quality of one-stage detectors and propose anchor-free algorithms <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, the authors designed two parallel sub-networks for classification and regression, respectively. In this paper, SpatialFlow extends RetineNet by adopting the design of parallel sub-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Instance Segmentation</head><p>Instance segmentation is a task that requires a pixel-level mask for each instance. Existing methods can be divided into two main categories, segmentation-based and regionbased methods. Segmentation-based approaches, such as <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, first generate a pixel-level segmentation map over the image and then perform grouping to identify the instance mask of each object. While region-based methods, such as <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b13">[14]</ref>, are closely related to object detection algorithms. They predict the instance masks within the bounding boxes generated by detectors. Region-based methods can achieve higher performance than their segmentation-based counterparts, which motivates us to resort to the region-based methods. In SpatialFlow, we adopt a thing branch upon RetinaNet for thing segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Segmentation</head><p>Fully convolutional networks are essential to semantic segmentation <ref type="bibr" target="#b58">[59]</ref>, and its variants achieve state-of-the-art results on various segmentation benchmarks. It has been proven that contextual information plays a vital role in segmentation <ref type="bibr" target="#b59">[60]</ref>. A bunch of works followed this idea: dilated convolution <ref type="bibr" target="#b37">[38]</ref> was invented to keep feature resolution and maintain contextual details; Deeplab series <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref> proposed Atrous Spatial Pyramid Pooling (ASPP) to capture global and multi-scale contextual information; PSPNet <ref type="bibr" target="#b4">[5]</ref> used spatial pyramid pooling to collect contextual priors; the encoderdecoder networks <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b62">[63]</ref> are designed to capture contextual information in encoder and gradually recover the details in decoder. Our SpatialFlow, built upon FPN <ref type="bibr" target="#b44">[45]</ref>, uses an encoder-decoder architecture for stuff segmentation to capture the contextual information. We take the spatial context of object detection into consideration and build a connection for thing and stuff segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Panoptic Segmentation</head><p>The panoptic segmentation task was proposed in <ref type="bibr" target="#b2">[3]</ref>, where the authors provided a baseline method with two separate networks, then used a heuristic post-processing method to merge two outputs. Later, Li et al. <ref type="bibr" target="#b63">[64]</ref> followed this task and introduced a weakly-and semi-supervised panoptic segmentation method. Recently, several unified frameworks have been proposed. De Geus et al. <ref type="bibr" target="#b5">[6]</ref> used a shared backbone for both things and stuff segmentation, while Li et al. <ref type="bibr" target="#b6">[7]</ref> took a step further by considering things and stuff consistency and proposed a unified network named TASCNet. Kirillov et al. <ref type="bibr" target="#b7">[8]</ref> introduced PanopticFPN by endowing Mask R-CNN <ref type="bibr" target="#b3">[4]</ref> with a stuff branch, which ignores the connection between things and stuff. Li et al. <ref type="bibr" target="#b8">[9]</ref> aimed to capture the connection by utilizing the attention module. To solve the conflicts in the result merging process, Liu et al. <ref type="bibr" target="#b10">[11]</ref> designed a spatial ranking module. Also, Xiong et al. <ref type="bibr" target="#b9">[10]</ref> proposed a parameter-free panoptic head to resolve the conflicts. Thinking differently, Yang et al. presented a single-shot approach for panoptic segmentation. However, most of these methods ignored to highlight the significance of the spatial features. Our SpatialFlow proposes the information flows to enable all tasks to be location-aware, which helps build a panoptic view for image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SPATIALFLOW</head><p>Object location is one of the key factors when building a global view for panoptic segmentation. While recent works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> for panoptic segmentation focus on how to combine the outputs of segmentation tasks properly but ignore to highlight the significance of the object location in the training phase. In this work, we propose a new unified framework, SpatialFlow, which enables all sub-tasks to be locationaware. Our SpatialFlow is conceptually simple: RetinaNet <ref type="bibr" target="#b17">[18]</ref> with two added sub-networks and two extra heads for thing and stuff segmentation. More importantly, we add multi-stage spatial information flows among the sub-networks.</p><p>We begin by reviewing the RetinaNet detector. RetinaNet is one of the most successful fully convolutional one-stage detectors. It consists of three parts: backbone with FPN <ref type="bibr" target="#b44">[45]</ref>, two parallel sub-networks, and two task-specific heads for box classification and regression. In SpatialFlow, we adopt the main network structure of RetinaNet. We illustrate the sketch of our framework in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Naive Implementation</head><p>As we discussed in Section I, RetinaNet shows its merits in pixel-level feature integration, which is beneficial for segmentation tasks. To develop a unified framework for panoptic segmentation based on RetinaNet, the most naive way is to add one thing head and one stuff head upon FPN features to enable thing and stuff segmentation. In this section, we introduce the naive implementation of the unified framework, which ignores the task feature refinement and the integration of box locations   like previous methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref> but built on RetinaNet.</p><p>Next, we will introduce the detailed design of each element in the naive implementation. 1) Backbone: We adopt the same backbone structure as RetinaNet. The backbone contains FPN, whose outputs are five levels of features named {P 3 , P 4 , P 5 , P 6 , P 7 } with a downsample rate of 8, 16, 32, 64, 128 respectively. In FPN, all features have 256 channels. We show the details in <ref type="figure" target="#fig_3">Figure 3</ref> (a). Following <ref type="bibr" target="#b19">[20]</ref>, we treat these features differently against various tasks: we use all the five levels to predict the bounding boxes in detection but only send {P 3 , P 4 , P 5 } to thing and stuff segmentation.</p><p>2) RetinaNet-based sub-networks: We present the parallel sub-networks in RetinaNet -classification sub-network (cls sub-net for short) and regression sub-network (reg sub-net for short). The operations in these sub-networks, which transform the output features of FPN to the inputs of downstream heads, can be formulated as follows:</p><formula xml:id="formula_0">P regi,j = φ(P regi,j−1 ), P clsi,j = φ(P clsi,j−1 ).<label>(1)</label></formula><p>Here, i represents the level index of FPN levels, j is the layer stage index in sub-networks, and φ denotes to a network block that contains a 3 × 3 convolution layer and a ReLU layer. In the cls and reg sub-networks, i ∈ {3, 4, 5, 6, 7}, j ∈ {1, 2, 3, 4}, and we have P clsi,0 = P regi,0 = P i , while i ∈ {3, 4, 5} for thing and stuff segmentation.</p><p>3) Task-specific heads: As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (c), we apply four heads for box classification, box regression, thing segmentation, and stuff segmentation, respectively. In the classification and the regression head, the final outputs of the detection can be obtained by O clsi = ψ(P clsi,4 ), O regi = φ(P regi,4 ), where O clsi and O regi represent the outputs of the classification head and the regression head in the FPN level i. We implement one 3 × 3 convolution layer φ on the outputs of classification and regression sub-nets. For the thing head, we apply it to each predicted box and adopt the same design as Mask R-CNN <ref type="bibr" target="#b3">[4]</ref>. For each RoI feature,</p><formula xml:id="formula_1">O RoI k = ψ(ζ(φ(P RoI k )),</formula><p>where O RoI k is the output of the k-th predicted box, φ represents for four 3 × 3 convolution layers with ReLU, one 2 × 2 stride 2 deconvolution layer with ReLU is denoted as ζ, and ψ is a 1 × 1 output convolution layer. After the stuff sub-net, we obtain three levels of feature maps with scales of 1/8, 1/16, 1/32 of the original image. We perform upsampling on each feature map gradually by blocks, each of which contains a 3 × 3 convolution layer, a group norm <ref type="bibr" target="#b64">[65]</ref> layer, a ReLU layer, and a 2× bilinear upsampling operation. All the features are upsampled to the scale of 1/4, which are then element-wise summed. A final 1 × 1 convolution layer, a 4× bilinear upsampling operation, and a softmax are applied to get the segmentation result. The stuff head is shown in <ref type="figure" target="#fig_3">Figure 3</ref> (b) with details. To generate the final output of SpatialFlow, we first perform a heuristic post-processing method <ref type="bibr" target="#b7">[8]</ref> to merge the results of thing and stuff segmentation, then fill the unassigned area in the merged map with the predicted boxes' locations and categories.</p><p>We show the key components of the proposed unified framework. The adaptation of RetinaNet <ref type="bibr" target="#b17">[18]</ref> enables the feature in pixel-level before performing segmentation tasks.</p><p>There remain obstacles preventing the unified framework from building a global view for the image scene, e.g., lack of feature intersection between things and stuff. The naive implementation also has practicality problems regarding the refinement of the FPN feature for things and stuff segmentation. To further improve the quality of learned features for things and stuff segmentation and strengthen the intersection between things and stuff in the image, we propose two techniques: adding things and stuff parallel sub-networks and proposing spatial information flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Thing and stuff parallel sub-networks</head><p>In RetinaNet, the parallel sub-networks refine the FPN features with multi-stage convolution layers, which transform the FPN features to task-specific features and lead to better performance. But there is no refinement for the input features in thing and stuff segmentation in the naive implementation. In this section, we apply the same mechanism to these two segmentation tasks. Moreover, the created multi-stage features facilitate the delivery of the spatial context from box regression task to others. We show the details of this part in <ref type="figure" target="#fig_3">Figure 3</ref> (c).</p><p>In this section, we propose to add two additional subnetworks -thing sub-network and stuff sub-network. We adopt the similar structure as in cls and reg sub-networks. Until now, there are four parallel sub-networks between the FPN and the task-specific heads. We present the modifications in thing and stuff sub-networks below: P thingi,j = φ(P thingi,j−1 ); P stuf fi,j = φ(P stuf fi,j−1 ).</p><p>Where P thingi,0 = P stuf fi,0 = P i . As the dominated features in thing and stuff segmentation are different, the number of stages required by the sub-networks depends on tasks.</p><p>More stages are needed in stuff segmentation than in thing segmentation to do feature refinement. We conjecture that the reason for the phenomenon is that pixel-level features are more sensitive to the details than instance-level features. In the final version, we adopt four stages in stuff sub-network and keep only one in thing sub-network. This setting gives the best performance of segmentation. In each stage, we implement a 3 × 3 convolutional layer and a ReLU layer. We illustrate the overall structure of sub-networks in <ref type="figure" target="#fig_3">Figure 3</ref> (c). And the experimental results for the number of stages in thing and stuff sub-networks can be found in <ref type="table" target="#tab_2">Table VII and Table VIII</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial information flows</head><p>As illustrated in <ref type="figure">Figure 1</ref>, all sub-tasks in our proposed panoptic segmentation framework are related to the locations of objects. The box location information is implied in the multi-stage feature representations of the box regression subnetwork. We propose the spatial information flows to support feature refinement in sub-networks. The spatial information flows can make other sub-tasks aware of box locations. Furthermore, adding the semantic feature in the thing segmentation has been proved to be effective in HTC <ref type="bibr" target="#b13">[14]</ref>. We also add a semantic flow that adopts a 3 × 3 convolution layer to transform the stuff feature to the thing feature. It brings slight improvements in our SpatialFlow, as shown in <ref type="table" target="#tab_2">Table IX and  Table X</ref>. Then we display the detailed structure of the spatial flows in <ref type="figure" target="#fig_3">Figure 3</ref> (c). They can be implemented as follows: P regi,j = φ(P regi,j−1 ); P clsi,j = φ(P clsi,j−1 + ψ(P regi,j )); P stuf fi,j = φ(P stuf fi,j−1 + ψ(P regi,j )); P thingi,1 = φ(P i + ζ(P stuf fi,4 ), ψ of f set (P regi,4 + P i )); P regi,0 = P clsi,0 = P thingi,0 = P stuf fi,0 = P i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3)</head><p>Here, P regi,0 = P clsi,0 = P thingi,0 = P stuf fi,0 = P i and ψ denotes an adaptation convolution from box regression task to others; ζ denotes an adaptation convolution from stuff sub-net to thing sub-net. We use a 3 × 3 convolution layer for both ψ and ζ. All features have 256 channels in this part.</p><p>Moreover, to make a fair comparison with UPSNet [10] on COCO, we introduce deformable convolutions <ref type="bibr" target="#b65">[66]</ref> layers to the sub-networks. We further adopt a method to incorporate the spatial context into deformable convolution more appropriately. We first combine the spatial information flow and the task-specific feature, then use the combined feature to generate the offsets for the deformable convolution on the task-specific sub-networks. The process can be formulated as follow:</p><p>P regi,j = φ(P regi,j−1 ); P clsi,j = φ dcn (P clsi,j−1 , ψ of f set (P regi,j + P clsi,j−1 )); P stuf fi,j = φ dcn (P stuf fi,j−1 , ψ of f set (P regi,j + P stuf fi,j−1 )); P thingi,1 = φ(P i + ζ(P stuf fi,4 ), ψ of f set (P regi,4 + P i )); P regi,0 = P clsi,0 = P thingi,0 = P stuf fi,0 = P i .</p><p>In the equation, φ dcn represents a deformable convolution layer, ψ of f set means an adaptation convolution layer, which generates offsets for the deformable convolution. Unless specified, we do not adopt the setting with deformable convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset and Evaluation metric 1) Dataset:</head><p>We evaluate our model on both COCO <ref type="bibr" target="#b20">[21]</ref> and Cityscapes <ref type="bibr" target="#b21">[22]</ref>. COCO consists of 80 things and 53 stuff classes. We use the 2017 data splits with 118k/5k/20k train/val/test images. We use train split for training, and report leision and sensitive studies by evaluating on val split. For our main results, we report our panoptic performance on the test-dev split. Cityscapes has 5k high-resolution images with fine pixel-accurate annotations: 2975 train, 500 val, and 1525 test. There are 19 classes on Cityscapes, 8 with instance-level masks. For all experiments on Cityscapes, we report our performance on val split with 11 stuff classes and 8 things classes.</p><p>2) Evaluation metric: We adopt the panoptic quality (PQ) as the metric. As proposed in <ref type="bibr" target="#b2">[3]</ref>, PQ can be formulated as follow:</p><formula xml:id="formula_4">P Q = (p,g)∈T P IoU (p, g) |T P | segmentation quality (SQ) × |T P | |T P | + 1 2 |F P | + 1 2 |F N | recognition quality (RQ)<label>(5)</label></formula><p>where p and g are predicted and ground truth segments, TP (true positives), FP (false positives), and FN (false negatives) represent matched pairs of segments (IoU (p, g) &gt; 0.5), unmatched predicted segments, and unmatched ground truth segments, respectively. Besides, PQ can be explained as the multiplication of a segmentation quality (SQ) and a recognition quality (RQ). We also use SQ and RQ to measure the performance in our experiments.</p><p>B. Implementation Details 1) Training: As a unified framework for panoptic segmentation, there are four different losses for SpatialFlow to optimize during the training stage. The loss function can be formulated as follow:</p><formula xml:id="formula_5">L = (L cls + L reg + L thing ) + λ · L stuf f<label>(6)</label></formula><p>where L cls , L reg and L thing belong to the thing segmentation task, and L stuf f is the loss of the stuff segmentation. We add a hyper-parameter λ to balance the losses between thing and stuff segmentation. We implement our SpatialFlow with a toolbox <ref type="bibr" target="#b66">[67]</ref> based on PyTorch <ref type="bibr" target="#b67">[68]</ref>. We inherit all the hyper-parameters from RetinaNet except that we set the threshold of NMS to 0.4 when generating proposals during training. For thing prediction, we add the ground truth boxes to the proposals set and run the thing head for all proposals. For training strategies, we fix the batch norm layer in the backbone and train all models over 4 GPUs with a total of 8 images per minibatch. On MS-COCO <ref type="bibr" target="#b20">[21]</ref>, we use the training strategy of training longer that adopted by RetinaNet(1.5×) <ref type="bibr" target="#b17">[18]</ref> and RetinaMask(2×) <ref type="bibr" target="#b19">[20]</ref>. All models are trained for 20 epochs with an initial learning rate of 5 × 10 −3 , which is decreased by 10 after 16 and 19 epochs; on Cityscapes <ref type="bibr" target="#b21">[22]</ref>, we set the initial learning rate as 1.25 × 10 −2 and borrow the number of iterations from <ref type="bibr" target="#b7">[8]</ref>. Unless specified, we resize the shorter edge of the image to 800 pixels on COCO, while on Cityscapes, we adopt 512 × 1024 image crops after scaling each image by 0.5 to 2.0×. As Kirillov et al. <ref type="bibr" target="#b7">[8]</ref> did, we also predict a particular 'other' class for all things categories in stuff head on COCO benchmark.</p><p>2) Inference: Our model follows a pipeline in the inference stage: (1) generate the detection results; (2) obtain the maps of thing and stuff segmentation; (3) merge the two outputs to form a panoptic segmentation map; (4) fill the unassigned area in the result with the detected boxes and its categories. In detection, we set the threshold of NMS to 0.4 for each class separately, and choose the top-100 scoring bounding boxes to send to thing head. During merging, we first ignore the stuff regions labeled 'other'; then we resolve the overlap problem between instances based on their scores, and merge the thing and stuff map in favor of things; at last, we fill the unassigned area with detection boxes in the result segmentation map to form the final output. For the hyper-parameters of SpatialFlow on in the inference stage, we fixed the confidence score threshold for the instance masks as 0.37, set the overlap threshold of instance masks as 0.37, and set the area limit threshold of stuff regions as 4900. When performing integration with detection box results, we propose a new hyper-parameter, which is the overlap between the detection box and the unassigned area in the segmentation map. We fix the threshold of the box overlap as 0.6. For the hyper-parameters on Cityscapes, we modify the overlap threshold of the instance masks to 0.25 and change the area limit threshold of stuff regions to 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Main Results</head><p>In this section, we compare our SpatialFlow with the stateof-the-art methods in panoptic segmentation. We show all the main results in <ref type="table" target="#tab_2">Table I, Table II, and Table III</ref>. Spa-tialFlow achieves state-of-the-art results on both COCO <ref type="bibr" target="#b20">[21]</ref> and Cityscapes <ref type="bibr" target="#b21">[22]</ref> panoptic benchmark.</p><p>1) MS COCO: To make a fair comparison, we report the results in <ref type="table" target="#tab_2">Table I and Table II</ref> PQ, which outperforms other models by a large margin (3.3 PQ and 5.3 PQ respectively). The results demonstrate the effectiveness of integrating the spatial features in pixel-level, which significantly impact stuff segmentation. However, Spa-tialFlow is lagging behind OANet <ref type="bibr" target="#b10">[11]</ref> in PQ T h . In OANet, the authors focus on solving the overlapping problem of instances when rendering the thing results to the final panoptic result. SpatialFlow applies a simple method to this problem, causing the inferior performance in PQ th .</p><p>Then we apply deformable convolution <ref type="bibr" target="#b65">[66]</ref> to both backbone and sub-networks and report its results with the multiscale strategy in <ref type="table" target="#tab_2">Table II</ref>. When training, the scales of short edges are randomly sampled from [400, 1400], and the scales of long edges are fixed as 1600. For inference, we feed multi-scale images to SpatialFlow, and the scales are (1500, 1000), (1800, 1200), and (2100, 1400) with horizontal flip. We achieve 47.9 PQ, which is the state-of-the-art result on COCO panoptic benchmark. As shown in <ref type="table" target="#tab_2">Table II</ref>, our method outperforms the runner-up of the COCO 2018 challenge by 1.1 PQ with a single model, demonstrating the effectiveness of SpatialFlow. Although AUNet outperforms our method in PQ th , they use ResNeXt-152-DCN as their backbone. In fact, with a stronger backbone (ResNeXt-101-DCN) and model ensemble, our method can achieve 50.2 PQ on COCO testdev split.</p><p>2) Cityscapes: We also report the results under different experiment settings in <ref type="table" target="#tab_2">Table III</ref>. Without the COCO pretrained model, SpatialFlow can achieve 59.6 PQ on Cityscapes val split, which is 1.5 PQ and 0.6 PQ higher than Panop-ticFPN <ref type="bibr" target="#b7">[8]</ref> and AUNet <ref type="bibr" target="#b8">[9]</ref> respectively. With the COCO pretrained model, SpatialFlow can achieve 62.5 PQ on Cityscapes with multi-scale testing, which is 0.7 PQ higher than UP-SNet <ref type="bibr" target="#b9">[10]</ref> under the same setting. SpatialFlow outperforms all other methods in terms of PQ st while getting inferior performance on PQ th comparing to UPSNet and AdaptIS. We conjecture that the phenomenon is caused by the inferior detection performance of RetinaNet on Cityscapes. To obtain the result of 62.5 PQ on Cityscapes val split, we first replace the convolution layers in stuff with deformable convolutions as UPSNet does, then we follow the steps below: (1) Finetune the COCO pre-trained model. As the number of things and stuff classes in Cityscapes is smaller than the number in COCO, 11/8 vs. 80/53, we have to finetune the layers that related to the number of classes. We freeze the rest layers and use a learning rate of 2.5 × 10 −3 to train for 2 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Experiments</head><p>We run a number of ablations to analyze the SpatialFlow. Unless specified, we use the naive implementation of Spa- tialFlow presented in Section III-A as our baseline model for all experiments in this section. We discuss the details below.</p><p>Loss Balance. We first investigate the best value of the hyperparameter λ. We adopt the baseline model in this section. <ref type="table" target="#tab_2">Table IV</ref> shows the model results of using various λ on COCO. We demonstrate the power of λ and discover that the best value to balance the losses on COCO is 0.25, with which the baseline model achieves 39.3 PQ with an image size of 600px and earns a 1.8 PQ gain compared with λ = 1.0. While for Cityscapes, we set λ = 1.0 by following <ref type="bibr" target="#b7">[8]</ref>.</p><p>Contribution of Components. In this section, we evaluate the sub-networks and the spatial flows of SpatialFlow on both COCO and Cityscapes. The results are shown in <ref type="table" target="#tab_2">Table V and  Table VI</ref> respectively. From the experiment results, we can see that both the sub-networks and the spatial flows demonstrate their contribution. The sub-networks improve PQ by 0.6 points and 0.2 points on COCO and Cityscapes. In particular, we obtain a significant gain on stuff (1.2 PQ on COCO) with the sub-networks as in which we refine the pixel-level feature before sending it to stuff head. For the spatial flows, they can improve the performance of things and stuff simultaneously  by improving 0.5 PQ and 0.7 PQ on COCO and Cityscapes. Moreover, the spatial flows can bring further gains with the sub-networks compared with the obtained benefits on the baseline model. The results indicate that the integration of the spatial context can benefit from the feature refinement in sub-networks.</p><p>Design of Sub-networks. We search the best number of stages for thing and stuff sub-networks. We conduct experiments on the COCO dataset with ResNet-50 based on the baseline model. The results are shown in <ref type="table" target="#tab_2">Table VII and Table VIII.</ref> According to the results, we choose to add only one stage in thing sub-network and add four stages in stuff sub-network. We obtain 0.2 PQ and 0.6 PQ improvements with thing subnetwork and stuff sub-network. The different number of blocks in sub-networks are related to the difference of the dominated feature in thing and stuff segmentation.</p><p>Spatial Flows. We conduct experiments to highlight the significance of the proposed spatial information flows between tasks. The baseline model marked with '-' in <ref type="table" target="#tab_2">Table IX and  Table X</ref> is the one with all sub-networks. There are three paths to deliver the spatial context from the box regression task to others: the path from the reg sub-net to the cls sub-net (reg-cls flow), the path to the stuff sub-net (reg-stuff flow), and the path to the thing sub-net (reg-thing flow). The results are reported in <ref type="table" target="#tab_2">Table IX and Table X</ref>. At first, we add the reg-cls path, and we obtain a 0.4 PQ T h improvement on COCO and a 1.5 PQ T h gain on Cityscapes, which are brought by better detection results. Adding spatial context helps cls sub-net to extract discriminative features, which is essential for detection. Then we build a spatial path for stuff sub-net, as shown in the 4th row of <ref type="table" target="#tab_2">Table IX and Table X</ref>, we earn a 0.6 PQ St gain on COCO and a 0.8 PQ St gain on Cityscapes compared with the former model, which indicates that the spatial context begets a positive effect on stuff segmentation. The reg-thing path and the semantic path also show their effectiveness on both things and stuff segmentation. Comparing with the original model, SpatialFlow can achieve a consistent gain in both thing and stuff segmentation. The results prove the significance of the spatial context in panoptic segmentation to some extent. It is worth noting that we only apply the element-wise sum operation to integrate the spatial context in this work. We believe further improvement could be achieved with a more deliberate design like attention modules.</p><p>V. FURTHER DISCUSSION In this section, we provide further discussions about the spatial flows and give an overview of how the spatial flows work, how fast the SpatialFlow is, and how to apply the spatial flows to other vision tasks.</p><p>Spatial Flows vs. Trivial Feature Fusion: Our main idea is to integrate spatial information into all sub-tasks and make them (a) PQ score (b) PQ score for things (b) PQ score for stuff aware of the locations of the objects, which is different from trivial feature integration among sub-networks. To prove the effectiveness of the spatial flows, we design an experiment on COCO by delivering the feature of cls sub-networks to other three sub-tasks, denoted as the cls flows model in <ref type="figure">Figure 4</ref>. We conduct an experiment on it with the image input size of 600px. As shown in <ref type="figure">Figure 4</ref>, our method outperform the cls-based feature integration method by 0.7 PQ, 0.8 PQ T h , and 0.5 PQ St respectively. The results suggest that trivial feature integration can not bring consistent improvements to the baseline model as our method does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Spatial Flows:</head><p>We choose to study the effects of spatial flows using two models, which are the models with or without spatial flows. We visualize the last feature map in the cls-head and the stuff-head of both models via CAM <ref type="bibr" target="#b73">[74]</ref> in <ref type="figure">Figure 5</ref>. The visualized heatmaps illustrate that the spatial flows can help the thing branch focus on objects and make the stuff branch aware of the precise boundary of things and stuff. The spatial flows bridge all tasks and help build a global view of the image in panoptic segmentation.</p><p>Accuracy vs. Speed: In <ref type="table" target="#tab_2">Table XI</ref>, we compare our method with the state-of-the-art methods in terms of accuracy and speed balance on COCO val split. The FPS is measured on a single Tesla V100 GPU. We show the results of different image sizes and different inference speed. Although SpatialFlow is not the fastest among all the methods, the results show good accuracy and speed balance of SpatialFlow. Larger image size yield higher accuracy, in slower inference speeds. Also, we find that thing segmentation benefits from large image sizes, while stuff segmentation is robust to the image size. Thanks to this, SpatialFlow can achieve 19.6 FPS and remain 37.4 PQ when we set the image size to 400px.</p><p>Detection Results: We also conduct experiments on Reti-naNet <ref type="bibr" target="#b17">[18]</ref> to investigate the generalization of the spatial flows. We deliver the spatial context from reg sub-network to cls subnetwork. The detection result is shown in <ref type="table" target="#tab_2">Table XII</ref>. With the help of the spatial context, the multi-stage features in sub-networks can be more discriminative, which boosted the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth SpatialFlow Image Ground Truth SpatialFlow </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work, we focus on the box locations in panoptic segmentation and propose a new location-aware and unified framework, denoted as SpatialFlow. We emphasize the importance of the spatial context and bridge all the tasks by building spatial information flows, then achieve state-of-the-art performance on both COCO test-dev split and Cityscapes val split, which prove the effectiveness of our model. Moreover, we find that the spatial flows can improve the performance of detection models, indicating the importance of spatial information. We expect that SpatialFlow can provide valuable insights on how to integrate spatial information in vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth SpatialFlow </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of the overall architecture. The SpatialFlow consists of three parts: (a) Backbone with FPN. (b) Four parallel sub-networks: We propose the spatial information flow and feature fusion among tasks in this part. The spatial flows are illustrated as orange dashed arrows, and the feature fusion is not shown in this figure for an elegant presentation; (c) Four heads for specific tasks: The classification head and regression head predict detection box together for thing head. The final result of SpatialFlow is a combination of the detected boxes and the outputs of thing head and stuff head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The designs for each part in SpatialFlow. In the dashed rectangle (a), we show the output features of FPN, which are the features named {P 3 , P 4 , P 5 , P 6 , P 7 }. In the dashed rectangle (b), we present the architecture of the stuff head. More importantly, all the information flows in sub-networks are illustrated in the dashed box (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 )</head><label>2</label><figDesc>Train the finetuned model as the standard SpatialFlow does. (3) Apply the multi-scale testing trick. The scales that we use in Cityscapes are (2304, 1152), (2432, 1216), (2560, 1280), and (2688, 1344) with horizontal flip.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>The PQ, PQ T h , and PQ St results of the base model, the cls flows model, and the spatial flows model with image size of 600px on COCO val split. An illustration of the cls-head heatmap and the stuff-head heatmap. We provide a comparison between the model with and without spatial flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>An illustration of visualization examples of SpatialFlow on COCO val split using a single ResNet-101 network. VI. VISUALIZATION We show some visualization examples of SpatialFlow on COCO and Cityscapes in Figure 6 and Figure 7 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>An illustration of visualization examples of SpatialFlow on Cityscapes val split using a single ResNet-101 network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, where the experiment settings are different. InTable I, we present the prediction results without bells and whistles. With a single ResNet-101-FPN backbone, our SpatialFlow can achieve 42.9 PQ on COCO test-dev split, which outperforms PanopticFPN [8] by 2.0 PQ and OANet [11] by 1.6 PQ. More importantly, SpatialFlow achieves a new state-of-the-art performance on PQ St , 33.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH THE STATE-OF-THE-ART METHODS ON COCO 2017 test-dev SPLIT. WE ONLY COMPARE WITH THE STATE-OF-THE-ART METHODS THAT WITHOUT DEFORMABLE CONVOLUTIONS HERE.TABLE II COMPARISON WITH THE STATE-OF-THE-ART METHODS ON COCO 2017 test-dev SPLIT. IN THIS TABLE, WE REPORT OUR RESULTS WITH DEFORMABLE CONVOLUTION AND MULTI-SCALE STRATEGY. THE TOP 3 ROWS CONTAIN RESULTS OF TOP 3 MODELS TAKEN FROM THE OFFICIAL LEADERBOARD OF COCO 2018 PANOPTIC SEGMENTATION CHALLENGE.TABLE III COMPARISON WITH THE STATE-OF-THE-ART METHODS ON CITYSCAPES val SPLIT. IN THIS TABLE, '-R101' REPRESENTS THAT THE BACKBONE IS RESNET-101 AND '-X101' FOR RESNEXT-101 [73]; '-COCO' MEANS USING COCO PRETRAINED MODEL; '-M' IS THE MULTI-SCALE TESTING.</figDesc><table><row><cell></cell><cell>model</cell><cell></cell><cell>backbone</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell>SQ</cell><cell>RQ</cell></row><row><cell></cell><cell cols="2">JSIS-Net [6]</cell><cell>ResNet-50</cell><cell>27.2</cell><cell>29.6</cell><cell>23.4</cell><cell>71.9 35.9</cell></row><row><cell cols="3">DeeperLab [69]</cell><cell>Xception-71</cell><cell>34.3</cell><cell>37.5</cell><cell>29.6</cell><cell>77.1 43.1</cell></row><row><cell cols="5">PanopticFPN [8] ResNet-101-FPN 40.9</cell><cell>48.3</cell><cell>29.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">OANet [11]</cell><cell cols="2">ResNet-101-FPN 41.3</cell><cell>50.4</cell><cell>27.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">SSAP [70]</cell><cell cols="2">ResNet-101-FPN 36.9</cell><cell>40.1</cell><cell>32.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">SpatialFlow</cell><cell>ResNet-101-FPN</cell><cell>42.9</cell><cell>49.5</cell><cell>33.0</cell><cell>78.8 52.3</cell></row><row><cell></cell><cell>model</cell><cell></cell><cell>backbone</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell>SQ</cell><cell>RQ</cell></row><row><cell cols="3">Megvii (Face++)</cell><cell>ensemble model</cell><cell>53.2</cell><cell>62.2</cell><cell>39.5</cell><cell>83.2 62.9</cell></row><row><cell></cell><cell>Caribbean</cell><cell></cell><cell>ensemble model</cell><cell>46.8</cell><cell>54.3</cell><cell>35.5</cell><cell>80.5 57.1</cell></row><row><cell></cell><cell>PKU 360</cell><cell></cell><cell cols="2">ResNeXt-152-DCN 46.3</cell><cell>58.6</cell><cell>27.6</cell><cell>79.6 56.1</cell></row><row><cell cols="3">AdaptIS [71]</cell><cell>ResNeXt-101</cell><cell>42.8</cell><cell>50.1</cell><cell>31.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>AUNet [9]</cell><cell></cell><cell cols="2">ResNeXt-152-DCN 46.5</cell><cell>55.9</cell><cell>32.5</cell><cell>81.0 56.1</cell></row><row><cell cols="3">UPSNet [10]</cell><cell>ResNet-101-DCN</cell><cell>46.6</cell><cell>53.2</cell><cell>36.7</cell><cell>80.5 56.9</cell></row><row><cell cols="3">SOGNet [72]</cell><cell>ResNet-101-DCN</cell><cell>47.8</cell><cell>-</cell><cell>-</cell><cell>80.7 57.6</cell></row><row><cell cols="3">SpatialFlow</cell><cell>ResNet-101-DCN</cell><cell>47.9</cell><cell>54.5</cell><cell>38.0</cell><cell>81.7 57.6</cell></row><row><cell>model</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PanopticFPN-R101 [8]</cell><cell>58.1</cell><cell>52.0</cell><cell>62.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUNet-R101 [9]</cell><cell>59.0</cell><cell>54.8</cell><cell>62.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TASCNet-R101-COCO [7]</cell><cell>59.2</cell><cell>56.0</cell><cell>61.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UPSNet-R101-COCO-M [10]</cell><cell>61.8</cell><cell>57.6</cell><cell>64.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSAP-R101-M [70]</cell><cell>61.1</cell><cell>55.0</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdaptIS-X101-M [71]</cell><cell>62.0</cell><cell>58.7</cell><cell>64.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpatialFlow-R101</cell><cell>59.6</cell><cell>55.0</cell><cell>63.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SpatialFlow-R101-COCO-M 62.5</cell><cell>56.6</cell><cell>66.8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV LOSS</head><label>IV</label><figDesc>BALANCE: THE RESULTS OF THE BASELINE MODEL ON COCO val FOR DIFFERENT VALUES OF λ BASED ON RESNET-50 WITH IMAGE SIZE OF 600PX. THE PROPER λ BRINGS A LARGE GAIN.</figDesc><table><row><cell>λ</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.3</cell><cell>0.25</cell><cell>0.2</cell></row><row><cell>PQ</cell><cell cols="5">37.5 38.2 38.8 39.1 39.3</cell><cell>39.0</cell></row><row><cell>PQ T h</cell><cell cols="5">41.8 43.0 44.0 44.5 45.1</cell><cell>44.9</cell></row><row><cell>PQ St</cell><cell cols="5">30.9 31.1 31.0 30.9 30.5</cell><cell>30.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>CONTRIBUTION OF COMPONENTS: ABLATION RESULTS ON COCO val SPLIT WITH RESNET-50. BOTH SUB-NETWORKS AND SPATIAL FLOWS BRING SIGNIFICANT GAINS BASED ON THE BASELINE MODEL.</figDesc><table><row><cell>sub-nets spatial-flows</cell><cell>PQ</cell><cell cols="2">PQ T h PQ St</cell></row><row><cell></cell><cell>39.7</cell><cell>46.0</cell><cell>30.2</cell></row><row><cell></cell><cell>40.3</cell><cell>46.2</cell><cell>31.4</cell></row><row><cell></cell><cell>40.2</cell><cell>46.5</cell><cell>30.7</cell></row><row><cell></cell><cell>40.9</cell><cell>46.8</cell><cell>31.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI CONTRIBUTION</head><label>VI</label><figDesc>OF COMPONENTS: ABLATION RESULTS ON CITYSCAPES val SPLIT WITH RESNET-50. SIMILAR GAINS CAN BE OBTAINED ON CITYSCAPES.TABLE VII DESIGN OF SUB-NETWORKS: ABLATION RESULTS ON NUMBER OF STAGES IN THING SUB-NETWORK. ONLY ONE STAGE IS NEEDED TO REFINE THE FPN FEATURE FOR THE INPUT OF THE THING HEAD. RESULTS ON NUMBER OF STAGES IN STUFF SUB-NETWORK. MORE STAGES BRING MORE GAINS. INPUT FEATURE OF THE STUFF HEAD NEED TO BE FULLY REFINED BY SUB-NETWORK. THE RESULTS OF THE SPATIAL FLOWS ON COCO. EACH ROW ADDS AN EXTRA COMPONENT TO THE ABOVE. THE RESULTS OF THE SPATIAL FLOWS ON CITYSCAPES. EACH ROW ADDS AN EXTRA COMPONENT TO THE ABOVE.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sub-nets spatial-flows</cell><cell>PQ</cell><cell cols="2">PQ T h PQ St</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.3</cell><cell>53.5</cell><cell>60.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.5</cell><cell>53.6</cell><cell>60.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58.0</cell><cell>54.3</cell><cell>60.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58.6</cell><cell>54.9</cell><cell>61.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell></row><row><cell>num stages</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell>DESIGN OF SUB-NETWORKS: num stages</cell><cell>PQ</cell><cell cols="2">PQ T h</cell><cell>PQ St</cell></row><row><cell>0</cell><cell>39.7</cell><cell>46.0</cell><cell>30.2</cell><cell>0</cell><cell>39.7</cell><cell>46.0</cell><cell></cell><cell>30.2</cell></row><row><cell>1</cell><cell>39.9</cell><cell>46.3</cell><cell>30.3</cell><cell>1</cell><cell>39.9</cell><cell>45.9</cell><cell></cell><cell>30.9</cell></row><row><cell>2</cell><cell>39.7</cell><cell>46.0</cell><cell>30.2</cell><cell>2</cell><cell>40.1</cell><cell>46.0</cell><cell></cell><cell>31.1</cell></row><row><cell>3</cell><cell>39.9</cell><cell>46.2</cell><cell>30.3</cell><cell>3</cell><cell>40.2</cell><cell>46.1</cell><cell></cell><cell>31.4</cell></row><row><cell>4</cell><cell>39.7</cell><cell>45.9</cell><cell>30.3</cell><cell>4</cell><cell>40.3</cell><cell>46.2</cell><cell></cell><cell>31.5</cell></row><row><cell></cell><cell cols="2">TABLE IX</cell><cell></cell><cell></cell><cell cols="2">TABLE X</cell><cell></cell></row><row><cell>SPATIAL FLOWS: flows</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell>SPATIAL FLOWS: flows</cell><cell>PQ</cell><cell cols="2">PQ T h</cell><cell>PQ St</cell></row><row><cell>-</cell><cell>40.3</cell><cell>46.2</cell><cell>31.4</cell><cell>-</cell><cell>57.5</cell><cell cols="2">53.6</cell><cell>60.3</cell></row><row><cell>+ reg-cls</cell><cell>40.5</cell><cell>46.6</cell><cell>31.4</cell><cell>+ reg-cls</cell><cell>58.0</cell><cell cols="2">55.1</cell><cell>60.1</cell></row><row><cell>+reg-stuff</cell><cell>40.7</cell><cell>46.3</cell><cell>32.0</cell><cell>+reg-stuff</cell><cell>58.3</cell><cell cols="2">54.6</cell><cell>60.9</cell></row><row><cell>+reg-thing</cell><cell>40.7</cell><cell>46.4</cell><cell>31.8</cell><cell>+reg-thing</cell><cell>58.5</cell><cell cols="2">54.7</cell><cell>61.3</cell></row><row><cell>+stuff-thing</cell><cell>40.9</cell><cell>46.8</cell><cell>31.9</cell><cell>+stuff-thing</cell><cell>58.6</cell><cell cols="2">54.9</cell><cell>61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE XI</head><label>XI</label><figDesc>ACCURACY vs. SPEED: COMPARISON WITH THE STATE-OF-THE-ART METHODS ON ACCURACY AND SPEED BALANCE. WE ILLUSTRATE SPATIALFLOW PERFORMANCE WITH DIFFERENT IMAGE SCALES. * INDICATES THAT UPSNET APPLY DEFORMABLE CONVOLUTION ON THE STUFF HEAD. THE RESULTS OF RETINANET WITH OR WITHOUT SPATIAL FLOWS ON COCO val SPLIT WITH RESNET-50 AS THE BACKBONE. THE SHORTED EDGES OF IMAGES ARE 800PX.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Scale</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell>FPS</cell></row><row><cell>PanopticFPN [8]</cell><cell>ResNet-50</cell><cell>800</cell><cell>39.0</cell><cell>45.9</cell><cell>27.9</cell><cell>18.9</cell></row><row><cell>UPSNet* [10]</cell><cell>ResNet-50</cell><cell>800</cell><cell>42.5</cell><cell>48.5</cell><cell>33.4</cell><cell>9.1</cell></row><row><cell>DeeperLab [69]</cell><cell>Xception-71</cell><cell>641</cell><cell>34.3</cell><cell>37.5</cell><cell>29.6</cell><cell>10.6</cell></row><row><cell>SpatialFlow</cell><cell>ResNet-50</cell><cell>800</cell><cell>40.9</cell><cell>46.8</cell><cell>31.9</cell><cell>10.3</cell></row><row><cell>SpatialFlow</cell><cell>ResNet-50</cell><cell>600</cell><cell>40.3</cell><cell>45.6</cell><cell>32.2</cell><cell>13.0</cell></row><row><cell>SpatialFlow</cell><cell>ResNet-50</cell><cell>400</cell><cell>37.4</cell><cell>41.5</cell><cell>31.4</cell><cell>19.6</cell></row><row><cell></cell><cell cols="2">TABLE XII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DETECTION RESULTS: Detectors</cell><cell cols="2">mAP AP 50</cell><cell>AP 75</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RetinaNet</cell><cell>35.6</cell><cell>55.5</cell><cell>37.7</cell><cell></cell><cell></cell></row><row><cell cols="2">RetinaNet w/ flows</cell><cell>36.7</cell><cell>57.1</cell><cell>39.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Refer to as instance and semantic segmentation, in this paper, we use the thing and the stuff to emphasize the tasks in panoptic segmentation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What, where and how many? combining object detectors and crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="424" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR. Citeseer</title>
		<meeting>CVPR. Citeseer</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to fuse things and stuff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01192</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02446</idno>
		<title level="m">Panoptic feature pyramid networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03904</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03784</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An end-to-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05027</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Singleshot object detection with enriched semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5813" to="5821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Box-driven classwise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Retinamask: Learning to predict masks improves state-of-the-art single-shot detection for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03353</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-order distance-based multiview stochastic learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2431" to="2442" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-grained image classification by exploring bipartite-graph labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1124" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical deep click feature prediction for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to rank using user clicks and visual features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning for image retrieval: What works and what doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Data Mining Workshop (ICDMW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep multimodal distance metric learning using click constraints for image ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4014" to="4024" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal deep autoencoder for human pose recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5659" to="5670" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6054" to="6063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deeperlab: Single-shot image parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7355" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Sognet: Scene overlap graph network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07527</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
