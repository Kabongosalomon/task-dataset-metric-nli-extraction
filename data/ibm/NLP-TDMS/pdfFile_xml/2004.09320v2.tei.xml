<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantization Guided JPEG Artifact Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Ehrlich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
							<email>sernamlim@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantization Guided JPEG Artifact Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>JPEG</term>
					<term>Discrete Cosine Transform</term>
					<term>Artifact Correction</term>
					<term>Quantization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The JPEG image compression algorithm is the most popular method of image compression because of its ability for large compression ratios. However, to achieve such high compression, information is lost. For aggressive quantization settings, this leads to a noticeable reduction in image quality. Artifact correction has been studied in the context of deep neural networks for some time, but the current methods delivering state-of-the-art results require a different model to be trained for each quality setting, greatly limiting their practical application. We solve this problem by creating a novel architecture which is parameterized by the JPEG files quantization matrix. This allows our single model to achieve state-of-the-art performance over models trained for specific quality settings. . . .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The JPEG image compression algorithm <ref type="bibr" target="#b42">[43]</ref> is ubiquitous in modern computing. Thanks to its high compression ratios, it is extremely popular in bandwidth constrained applications. The JPEG algorithm is a lossy compression algorithm, so by using it, some information is lost for a corresponding gain in saved space. This is most noticable for low quality settings For highly space-constrained scenarios, it may be desirable to use aggressive compression. Therefore, algorithmic restoration of the lost information, referred to as artifact correction, has been well studied both in classical literature and in the context of deep neural networks.</p><p>While these methods have enjoyed academic success, their practical application is limited by a single architectural defect: they train a single model per JPEG quality level. The JPEG quality level is an integer between 0 and 100, where 100 indicates very little loss of information and 0 indicates the maximum loss of information. Not only is this expensive to train and deploy, but the quality setting is not known at inference time (it is not stored with the JPEG image <ref type="bibr" target="#b42">[43]</ref>) making it impossible to use these models in practical applications. Only recently have methods begun considering the "blind" restoration scenario <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref> with a single network, with mixed results compared to non-blind methods.</p><p>We solve this problem by creating a single model that uses quantization data, which is stored in the JPEG file. Our CNN model processes the image entirely in the DCT <ref type="bibr" target="#b1">[2]</ref> arXiv:2004.09320v2 [eess.IV] 16 Jul 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y Channel Correction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Channel</head><p>Correction GAN domain. While previous works have recognized that the DCT domain is less likely to spread quantization errors <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>, DCT domain-based models alone have historically not been successful unless combined with pixel domain models (so-called "dual domain" models). Inspired by recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, we formulate fully DCT domain regression. This allows our model to be parameterized by the quantization matrix, an 8 × 8 matrix that directly determines the quantization applied to each DCT coefficient. We develop a novel method for parameterizing our network called Convolution Filter Manifolds, an extension of the Filter Manifold technique <ref type="bibr" target="#b21">[22]</ref>. By adapting our network weights to the input quantization matrix, our single network is able to handle a wide range of quality settings. Finally, since JPEG images are stored in the YCbCr color space, with the Y channel containing more information than the subsampled color channels, we use the reconstructed Y channel to guide the color channel reconstructions. As in <ref type="bibr" target="#b52">[53]</ref>, we observe that using the Y channel in this way achieves good color correction results. Finally, since regression results for artifact correction are often blurry, as a result of lost texture information, we fine-tune our model using a GAN loss specifically designed to restore texture. This allows us to generate highly realistic reconstructions. See <ref type="figure" target="#fig_0">Figure 1</ref> for an overview of the correction flow. To summarize, our contributions are:</p><p>1. A single model for artifact correction of JPEG images at any quality, parameterized by the quantization matrix, which is state-of-the-art in color JPEG restoration. 2. A formulation for fully DCT domain image-to-image regression. 3. Convolutional Filter Manifolds for parameterizing CNNs with spatial side-channel information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Pointwise Shape-Adaptive DCT <ref type="bibr" target="#b9">[10]</ref> is a standard classical technique which uses thresholded DCT coefficients reconstruct local estimates of the input signal. Yang et al. <ref type="bibr" target="#b46">[47]</ref> use a lapped transform to approximate the inverse DCT on the quantized coefficients. More recent techniques use convolutional neural networks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>. ARCNN <ref type="bibr" target="#b7">[8]</ref> is a regression model inspired by superresolution techniques; L4/L8 <ref type="bibr" target="#b39">[40]</ref> continues this work. CAS-CNN <ref type="bibr" target="#b4">[5]</ref> adds hierarchical skip connections and a multi-scale loss function. Liu et al. <ref type="bibr" target="#b26">[27]</ref> use a wavelet-based network for general denoising and artifact correction, which is extended by Chen et al. <ref type="bibr" target="#b5">[6]</ref>. Galteri et al. <ref type="bibr" target="#b11">[12]</ref> use a GAN formulation to achieve more visually appealing results. S-Net <ref type="bibr" target="#b51">[52]</ref> introduces a scalable architecture that can produce different quality outputs based on the desired computation complexity. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> use a dense residual formulation for image enhancement. Tai et al. <ref type="bibr" target="#b41">[42]</ref> use persistent memory in their restoration network.</p><p>Liu et al. <ref type="bibr" target="#b27">[28]</ref> introduce the dual domain idea in the sparse coding setting. Guo and Chao <ref type="bibr" target="#b16">[17]</ref> use convolutional autoencoders for both domains. DMCNN <ref type="bibr" target="#b48">[49]</ref> extends this with DCT rectifier to constrain errors. Zheng et al. <ref type="bibr" target="#b50">[51]</ref> target color images and use an implicit DCT layer to compute DCT domain loss using pixel information. D3 <ref type="bibr" target="#b44">[45]</ref> extends Liu et al. <ref type="bibr" target="#b27">[28]</ref> by using a feed-forward formulation for parameters which were assumed in <ref type="bibr" target="#b27">[28]</ref>. Jin et al. <ref type="bibr" target="#b19">[20]</ref> extend the dual domain concept to separate streams processing low and high frequencies, allowing them to achieve competitive results with a fraction of the parameters.</p><p>The latest works examine the "blind" scenario that we consider here. Zhang et al. <ref type="bibr" target="#b47">[48]</ref> formulate general image denoising and apply it to JPEG artifact correction with a single network. DCSC uses convolution features in their sparse coding scheme <ref type="bibr" target="#b10">[11]</ref> with a single network. Galteri et al. <ref type="bibr" target="#b12">[13]</ref> extend their GAN work with an ensemble of GANs where each GAN in the ensemble is trained to correct artifacts of a specific quality level. They train an auxiliary network to classify the image into the quality level that it was compressed with. The resulting quality level is used to pick a GAN from the ensemble to use for the final artifact correction. Kim et al. <ref type="bibr" target="#b23">[24]</ref> also use an ensemble method based on quality factor estimation. AGARNET <ref type="bibr" target="#b22">[23]</ref> uses a single network by learning a per-pixel quality factor extending the concept <ref type="bibr" target="#b12">[13]</ref> from a single quality factor to a per-pixl map. This allows them to avoid the ensemble method and using a single network with two inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Our goal is to design a single model capable of JPEG artifact correction at any quality. Towards this, we formulate an architecture that is parameterized by the quantization matrix.</p><p>Recall that a JPEG quantization matrix captures the amount of rounding applied to DCT coefficients and is indicative of information lost during compression. A key contribution of our approach is utilizing this quantization matrix directly to guide the restoration process using a fully DCT domain image-to-image regression network. JPEG stores color data in the YCbCr colorspace. The compressed Y channel is much higher quality compared to CbCr channels since human perception is less sensitive to fine color details than to brightness details. Therefore, we follow a staged approach: first restoring artifacts in the Y channel and then using the restored Y channel as guidance to restore the CbCr channels.</p><p>An illustrative overview of our approach is presented in <ref type="figure" target="#fig_1">Figure 2</ref>. Next, we present building blocks utilized in our architecture in §3.1, that allow us to parameterize our model using the quantization matrix and operate entirely in the DCT domain. Our Y channel and color artifact correction networks are described in §3.2 and §3.3 respectively, and finally the training details in §3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Building Blocks</head><p>By creating a single model capable of JPEG artifact correction at any quality, our model solves a significantly harder problem than previous works. To solve it, we parameterize our network using the 8 × 8 quantization matrix available with every JPEG file. We first describe Convolutional Filter Manifolds (CFM), our solution for adaptable convolutional kernels parameterized by the quantization matrix. Since the quantization matrix encodes the amount of rounding per each DCT coefficient, this parameterization is most effective in the DCT domain, a domain where CNNs have previously struggled. Therefore, we also formulate artifact correction as fully DCT domain image-to-image regression and describe critical frequency-relationships-preserving operations.</p><p>Convolutional Filter Manifold (CFM). Filter Manifolds <ref type="bibr" target="#b21">[22]</ref> were introduced as a way to parameterize a deep CNN using side-channel scalar data. The method learns a manifold of convolutional kernels, which is a function of a scalar input. The manifold is modeled as a three-layer multilayer perceptron. The input to this network is the scalar side-channel data, and the output vector is reshaped to the shape of the desired convolutional kernel and then convolved with the input feature map for that layer.</p><p>Recall that in the JPEG compression algorithm, a quantization matrix is derived from a scalar quality setting to determine the amount of rounding to apply, and therefore the amount of information removed from the original image. This quantization matrix is then stored in the JPEG file to allow for correct scaling of the DCT coefficients at decompression time. This quantization matrix is then a strong signal for the amount of information lost. However, the quantization matrix is an 8×8 matrix with spatial structure, applying the Filter Manifold technique to it has the same drawbacks as processing images with multilayer perceptrons, e.g., a large number of parameters and a lack of spatial relationships.</p><p>To solve this, we propose an extension to create Convolutional Filter Manifolds (CFM), replacing the multilayer perceptron by a lightweight three-layer CNN. The input to the CNN is our quantization matrix, and the output is reshaped to the desired convolutional kernel shape and convolved with the input feature map as in the Filter Manifold method. For our problem, we follow the network structure in <ref type="figure" target="#fig_2">Figure 3</ref> for each CFM layer. However, this is a general technique and can be used with a different architecture when spatially arranged side-channel data is available. Coherent Frequency Operations. In prior works, DCT information has been used in dual-domain models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>. These models used standard 3 × 3 convolutional kernels with U-Net <ref type="bibr" target="#b34">[35]</ref> structures to process the coefficients. Although the DCT is a linear map on image pixels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b8">9]</ref>, ablation studies in prior work show that the DCT network alone is not able to surpass even classical artifact correction techniques.</p><p>Although the DCT coefficients are arranged in a grid structure of the same shape as the input image, that spatial structure does not have the same meaning as pixels. Image pixels are samples of a continuous signal in two dimensions. DCT coefficients, however, are samples from different, orthogonal functions and the two-dimensional arrangement indexes them. This means that a 3 × 3 convolutional kernel is trying to learn a relationship not between spatially related samples of the same function as it was designed to do, but rather between samples from completely unrelated functions. Moreover, it must maintain this structure throughout the network to produce a valid DCT as output. This is the root cause of CNN's poor performance on DCT coefficients for image-to-image regression, semantic segmentation, and object detection (Note that this should not affect whole image classification performance as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>).</p><p>A class of recent techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, which we call Coherent Frequency Operations for their preservation of frequency relationships, are used as the building block for our regression network. The first layer is an 8 × 8 stride-8 layer <ref type="bibr" target="#b6">[7]</ref>, which computes a representation for each block (recall that JPEG blocks are non-overlapping 8 × 8 DCT coefficients). This block representation, which is one eighth the size of the input, can then be processed with a standard CNN.</p><p>The next layer is designed to process each frequency in isolation. Since each of the 64 coefficients in an 8 × 8 JPEG block corresponds to a different frequency, the input DCT coefficients are first rearranged so that the coefficients corresponding to different frequencies are stored channelwise (see <ref type="figure">Figure 4</ref>). This gives an input, which is again one eighth the size of the original image, but this time with 64 channels (one for each frequency). This was referred to as Frequency Component Rearrangement in <ref type="bibr" target="#b28">[29]</ref>. We then use convolutions with 64 groups to learn per-frequency convolutional weights. Combining these two operations (block representation using 8 × 8 8-stride and frequency component rearrangement) allows us to match state-of-the-art pixel and dualdomain results using only DCT coefficients as input and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Y Channel Correction Network</head><p>Our primary goal is artifact correction of full color images, and we again leverage the JPEG algorithm to do this. JPEG stores color data in the YCbCr colorspace. The color channels, which contribute less to the human visual response, are both subsampled and more heavily quantized. Therefore, we employ a larger network to correct only the Y channel, and a smaller network which uses the restored Y channel to more effectively correct the Cb and Cr color channels. Subnetworks. Utilizing the building blocks developed earlier, our network design proceeds in two phases: block enhancement, which learns a quantization invariant representations for each JPEG block, and frequency enhancement, which tries to match each frequency reconstruction to the regression target. These phases are fused to produce the final residual for restoring the Y channel. We employ two purpose-built subnetworks: the block network (BlockNet) and the frequency network (FrequencyNet). Both of these networks can be thought of as separate image-to-image regression models with a structure inspired by ESRGAN <ref type="bibr" target="#b43">[44]</ref>, which allows sufficient low-level information to be preserved as well as allowing sufficient gradient flow to train these very deep networks. Following recent techniques <ref type="bibr" target="#b43">[44]</ref>, we remove batch normalization layers. While recent works have largely replaced PReLU <ref type="bibr" target="#b18">[19]</ref> with LeakyReLU <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, we find that PReLU activations give much higher accuracy. BlockNet. This network processes JPEG blocks to restore the Y channel (refer to <ref type="figure">Figure 5</ref>). We use the 8 × 8 stride-8 coherent frequency operations to create a block representation. Since this layer is computing a block representation from all the input DCT coefficients, we use a Convolutional Filter Manifold (CFM) for this layer so that it has access to quantization information. This allows the layer to learn the quantization table entry to DCT coefficient correspondence with the goal to output a quantization-invariant block representation. Since there is a one to one correspondence between the quantization table entry and rounding applied to a DCT coefficient, this motivates our choice to operate entirely in the DCT domain. We then process these quantization-invariant block representations with Residual-in-Residual Dense Blocks (RRDB) from <ref type="bibr" target="#b43">[44]</ref>. RRDB layers are an extension of the commonly used residual block <ref type="bibr" target="#b17">[18]</ref> and define several recursive and highly residual layers. Each RRDB has 15 convolution layers, and we use a single RRDB for the block network with 256 channels. The network terminates with another 8 × 8 stride-8 CFM, this time transposed, to reverse the block representation back to its original form so that it can be used for later tasks. FrequencyNet. This network, shown in <ref type="figure">Figure 6</ref>, processes the individual frequency coefficients using the Frequency Component Rearrangement technique ( <ref type="figure">Figure 4</ref>). The architecture of this network is similar to BlockNet. We use a single 3 × 3 convolution to change the number of channels from the 64 input channels to the 256 channels used by the RRDB layer. The single RRDB layers processes feature maps with 256 channels and 64 groups yielding 4 channels per frequency. An output 3 × 3 convolution transforms the 4 channel output to the 64 output channels, and the coefficients are rearranged back into blocks for later tasks. Final Network. The final Y channel artifact correction network is shown in <ref type="figure" target="#fig_5">Figure 8</ref>. We observe that since the FrequencyNet processes frequency coefficients in isolation, if those coefficients were zeroed out by the compression process, then it can make no attempt at restoring them (since they are zero valued they would be set to the layer bias). This is common with high frequencies by design, since they have larger quanitzation table entries and they contribute less to the human visual response. We, therefore, lead with the BlockNet to restore high frequencies.</p><p>We then pass the result to the FrequencyNet, and its result is then processed by a second block network to restore more information. Finally, a three-layer fusion network (see <ref type="figure">Figure 7</ref> and 8) fuses the output of all three subnetworks into a final result. Having all three subnetworks contribute to the final result in this way allows for better gradient flow. The effect of fusion, as well as the three subnetworks, is tested in our ablation study. The fusion output is treated as a residual and added to the input to produce the final corrected coefficients for the Y channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Color Correction Network</head><p>The color channel network <ref type="figure">(Figure 9</ref>) processes the Cb and Cr DCT coefficients. Since the color channels are subsampled with respect to the Y channel by half, they incur a much higher loss of information and lose the structural information which is preserved in the Y channel. We first compute the block representation of the downsampled color channel coefficients using a CFM layer, then process them with a single RRDB layer. The block representation is then upsampled using a 4 × 4 stride-2 convolutional layer. We compute the block representation of the restored Y channel, again using a CFM layer. The block representations are concatenated channel-wise and processed using a single RRDB layer before being transformed back into coefficient space using a transposed 8 × 8 stride-8 CFM. By concatenating the Y channel restoration, we give the network structural information that may be completely missing in the color channels. The result of this network is the color channel residual. This process is repeated individually for each color channel with a single network learned on Cb and Cr. The output residual is added to nearest-neighbor upsampled input coefficients to give the final restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>Objective. We use two separate objective functions to train, an error loss and a GAN loss. Our error loss is based on prior works which minimize the l 1 error of the result and the target image. We additionally maximize the Structural Similarity (SSIM) <ref type="bibr" target="#b45">[46]</ref> of the result since SSIM is generally regarded as a closer metric to human perception than PSNR. This gives our final objective function as</p><formula xml:id="formula_0">L JPEG (x, y) = y − x 1 − λSSIM(x, y)<label>(1)</label></formula><p>where x is the network output, y is the target image, and λ is a balancing hyperparameter. A common phenomenon in JPEG artifact correction and superresolution is the production of a blurry or textureless result. To correct for this, we fine tune our fully trained regression network with a GAN loss. For this objective, we use the relativistic average GAN loss L Ra G <ref type="bibr" target="#b20">[21]</ref>, we use l 1 error to prevent the image from moving too far away from the regression result, and we use preactivation network-based loss <ref type="bibr" target="#b43">[44]</ref>. Instead of a perceptual loss that tries to keep the outputs close in ImageNet-trained VGG feature space used in prior works, we use a network trained on the MINC dataset <ref type="bibr" target="#b3">[4]</ref>, for material classification. This texture loss provided only marginal benefit in ESRGAN <ref type="bibr" target="#b43">[44]</ref> for super-resolution. We find it to be critical in our task for restoring texture to blurred regions, since JPEG compression destroys these fine details. The texture loss is defined as where MINC <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3</ref> indicates that the output is from layer 5 convolution 3. The final GAN loss is</p><formula xml:id="formula_1">L texture (x, y) = MINC 5,3 (y) − MINC 5,3 (x) 1<label>(2)</label></formula><formula xml:id="formula_2">L GAN (x, y) = L texture (x, y) + γL Ra G (x, y) + ν x − y 1<label>(3)</label></formula><p>with γ and ν balancing hyperparameters. We note that the texture restored using the GAN model is, in general, not reflective of the regression target at inference time and actually produces worse numerical results than the regression model despite the images looking more realistic.</p><p>Staged Training. Analogous to our staged restoration, Y channel followed by color channels, we follow a staged training approach. We first train the Y channel correction network using L JPEG . We then train the color correction network using L JPEG keeping the Y channel network weights frozen. Finally, we train the entire network (Y and color correction) with L GAN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validate the theoretical discussion in the previous sections with experimental results. We first describe the datasets we used along with the training procedure we followed. We then show artifact correction results and compare them with previous state-of-the-art methods. Finally, we perform an ablation study. Please see our supplementary material for further results and details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets and Metrics. For training, we use the DIV2k and Flickr2k <ref type="bibr" target="#b0">[1]</ref> datasets. DIV2k consists of 900 images, and the Flickr2k dataset contains 2650 images. We preextract 256 × 256 patches from these images taking 30 random patches from each image and compress them using quality in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">100]</ref> in steps of 10. This gives a total training set of 1,065,000 patches. For evaluation, we use the Live1 <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, Classic-5 <ref type="bibr" target="#b9">[10]</ref>, BSDS500 <ref type="bibr" target="#b2">[3]</ref>, and ICB datasets <ref type="bibr" target="#b33">[34]</ref>. ICB is a new dataset which provides 15 highquality lossless images designed specifically to measure compression quality. It is our hope that the community will gradually begin including ICB dataset results. Where previous works have provided code and models, we reevaluate their methods and provide results here for comparison. As with all prior works, we report PSNR, PSNR-B <ref type="bibr" target="#b40">[41]</ref>, and SSIM <ref type="bibr" target="#b45">[46]</ref>. Implementation Details. All training uses the Adam <ref type="bibr" target="#b24">[25]</ref> optimizer with a batch size of 32 patches. Our network is implemented using the PyTorch <ref type="bibr" target="#b31">[32]</ref> library. We normalize the DCT coefficients using per-frequency and per-channel mean and standard deviations.</p><p>Since the DCT coefficients are measurements of different signals, by computing the statistics per-frequency we normalize the distributions so that they are all roughly the same magnitude. We find that this greatly speeds up the convergence of the network. Quantization table entries are normalized to [0, 1], with 1 being the most quantization and 0 the least. We use libjpeg <ref type="bibr" target="#b14">[15]</ref> for compression with the baseline quantization setting. Training Procedure. As described in Section 3.4, we follow a staged training approach by first training the Y channel or grayscale artifact correction network, then training the color (CbCr) channel network, and finally training both networks using the GAN loss. For the first stage, the Y channel artifact correction network, the learning rate starts at 1 × 10 −3 and decays by a factor of 2 every 100,000 batches. We stop training after 400,000 batches. We set λ in Equation 1 to 0.05.</p><p>For the next stage, all color channels are restored. The weights for the Y channel network are initialized from the previous stage and frozen during training. The color channel network weights are trained using a cosine annealing learning rate schedule <ref type="bibr" target="#b29">[30]</ref> decaying from 1 × 10 −3 to 1 × 10 −6 over 100,000 batches.</p><p>Finally, we train both Y and color channel artifact correction networks (jointly referred to as the generator model) using a GAN loss to improve qualitative textures. The generator model weights are initialized to the pre-trained models from the previous stages. We use the DCGAN <ref type="bibr" target="#b32">[33]</ref> discriminator. The model is trained for 100,000 iterations using cosine annealing <ref type="bibr" target="#b29">[30]</ref> with the learning rate starting from 1 × 10 −4 and ending at 1 × 10 −6 . We set γ and ν in Equation 3 to 5 × 10 −3 and 1 × 10 −2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results: Artifact Correction</head><p>Color Artifact Correction. We report the main results of our approach, color artifact correction, on Live1, BSDS500, and ICB in <ref type="table" target="#tab_4">Table 1</ref>. Our model consistently outperforms recent baselines on all datasets. Note that of all the approaches, only ours and IDCN <ref type="bibr" target="#b50">[51]</ref> include native processing of color channels. For the other models, we convert input images to YCbCr and process the channels independently.</p><p>For quantitative comparisons to more methods on Live-1 dataset, at compression quality 10, refer to <ref type="figure" target="#fig_0">Figure 12</ref>. We present qualitative results from a mix of all three datasets in <ref type="figure" target="#fig_0">Figure 13</ref> ("Ours"). Since our model is not restricted by which quality settings it can be run on, we also show the increase in PSNR for qualities 10-100 in <ref type="figure" target="#fig_0">Figure 11</ref>. Intermediate Results on Y Channel Artifact Correction. Since the first stage of our approach trains for grayscale or Y channel artifact correction, we can also compare the intermediate results from this stage with other approaches. We report results in <ref type="table" target="#tab_5">Table 2</ref> for Live1, Classic-5, BSDS500, and ICB. As the table shows, intermediate results from our model can match or outperform previous state-of-the-art models in many cases, consistently providing high SSIM results using a single model for all quality factors. GAN Correction Finally, we show results from our model trained using GAN correction. We use model interpolation <ref type="bibr" target="#b43">[44]</ref> and show qualitative results for the interpolation parameter (α) set to 0.7 in <ref type="figure" target="#fig_0">Figure 13</ref>. ("Ours-GAN") Notice that the GAN loss is able to restore texture to blurred, flat regions and sharpen edges, yielding a more visually pleasing result. We provide additional qualitative results in the supplementary material. Note that we do not show error metrics using the GAN model as it produces higher quality images, at the expense of quantitative metrics, by adding texture details that are not present in the original images. We instead show FID scores for the GAN model compared to our regression model in <ref type="table" target="#tab_7">Table 4</ref>, indicating that the GAN model generates significantly more realistic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results: Generalization Capabilities</head><p>The major advantage of our method is that it uses a single model to correct JPEG images at any quality, while prior works train a model for each quality factor. Therefore, we explore if other methods are capable of generalizing or if they really require this ensemble of quality-specific models. To evaluate this, we use our closest competitor and prior state-of-the-art, IDCN <ref type="bibr" target="#b50">[51]</ref>. IDCN does not provide a model for quality higher than 20, we explore if their model generalizes by using their quality 10 and quality 20 models to correct quality 50 Live-1 images. We also use the quality 20 model to correct quality  <ref type="figure" target="#fig_0">Fig. 12</ref>: Comparison for Live-1 quality 10.</p><p>Where code was available we reevaluated, otherwise we used published numbers. 10 images and use the quality 10 model to correct quality 20 images. These results are shown in <ref type="table" target="#tab_6">Table 3</ref> along with our result.</p><p>As the table shows, the choice of model is critical for IDCN, and there is a significant quality drop when choosing the wrong model. Neither their quality 10 nor their quality 20 model is able to effectively correct images that it was not trained on, scoring significantly lower than if the correct model were used. At quality 50, the quality 10 model produces a result worse than the input JPEG, and the quality 20 model makes only a slight improvement. In comparison, our single model provides consistently better results across image quality factors. We stress that the quality setting is not stored in the JPEG file, so a deployed system has no way to pick the correct model. We show an example of a quality 50 image and artifact correction results in <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Design and Ablation Analysis</head><p>Here we ablate many of our design decisions and observe their effect on network accuracy. The results are reported in <ref type="table" target="#tab_8">Table 5</ref>, we report metrics on quality 10 classic-5. Implementation details:For all ablation experiments, we keep the number of parameters approximately the same between tested models to alleviate the concern that a network performs better simply because it has a higher capacity. All models are trained for 100,000 batches on the grayscale training patch set using cosine annealing <ref type="bibr" target="#b29">[30]</ref> from a learning rate of 1 × 10 −3 to 1 × 10 −6 .</p><p>Original JPEG IDCN Ours Ours-GAN <ref type="figure" target="#fig_0">Fig. 13</ref>: Qualitative Results. All images were compressed at Quality 10. Please zoom in to view details.  Importance of CFM layers. We emphasized the importance of adaptable weights in the CFM layers, which can be adapted using the quantization matrix. However, there are other simpler methods of using side-channel information. We could simply concatenate the quantization matrix channelwise with the input, or we could ignore the quantization matrix altogether. As shown in the "CFM" experiment in <ref type="table" target="#tab_8">Table 5</ref>, the CFM unit performs better than both of these alternatives by a considerable margin. We further visualize the filters learned by the CFM layers and the underlying embeddings in the supplementary material which validate that the learned filters follow a manifold structure.</p><p>BlockNet vs. FrequencyNet. We noted that the FrequencyNet should not be able to perform without a preceding BlockNet because high-frequency information will be zeroed out from the compression process. To test this claim, we train individual BlockNet and FrequencyNet in isolation and report the results in <ref type="table" target="#tab_8">Table 5</ref> ("Subnetworks"). We can see that BlockNet alone attains significantly higher performance than FrequencyNet alone. Importance of the fusion layer. Finally, we study the necessity of the fusion layer presented. We posited that the fusion layer was necessary for gradient flow to the early layers of our network. As demonstrated in <ref type="table" target="#tab_8">Table 5</ref> ("Fusion"), the network without fusion fails to learn, matching the input PSNR of classic-5 after full training, whereas the network with fusion makes considerable progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We showed a design for a quantization guided JPEG artifact correction network. Our single network is able to achieve state-of-the-art results, beating methods which train a different network for each quality level. Our network relies only on information that is available at inference time, and solves a major practical problem for the deployment of such methods in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Evaluation Details</head><p>In this section we elaborate on the evaluation procedure for prior works as well as discuss a number of hyperparameters critical to correct evaluation. In our results section, three of the four prior works did not have native handeling of color channels. To evaluate them on color images, we applied their Y channel network to both Y, Cb, and Cr channels separately as well as R, G, and B channels separately. In all cases, using the Y, Cb, and Cr channels performed the best, so these are the results we report (e.g., we report the scheme that gives prior works the best numbers). Note that we do not modify the published network structure to take a three channel input as was done in IDCN. We do this to remain as faithful to the published methods as possible, and we note that by examining the numbers reported in IDCN, the ranking of the methods does not change.</p><p>Altering the network structures to take a three channel input does, however, improve their results on color images even if it is a small improvement. Next, we note important evaluation hyperparameters. We defer to the ARCNN evaluation code for these settings, although they are not objectively correct. SSIM evaluation in particular uses an 8 × 8 window with uniform weighting in contrast to the default 11 × 11 gaussian window. Setting this correctly is critical to producing a fair comparison and we have found prior works are not uniform in correctly setting it. ARCNN uses a strict definition of the Y channel giving an output in the range <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">240]</ref>, this was intended to match the YCbCr transform used in the JPEG standard, however it is incorrect and stems from the default MATLAB settings. JPEG uses the full-frame Y channel conversion giving outputs in [0, 255]. We would like to see this corrected in future works, however it seems unlikely as it changes the comparisons quite a bit. Finally, we note that PSNR-B is an assymetric measure, e.g., the blocking effect factor (BEF) is only computed on the degraded image, so the order of the arguments is critical. We have seen at least one prior work that passes these arguments in reverse order resulting in nearly perfect PSNR-B (defined as PSNR-B very close to PSNR).</p><p>We have made our model and evaluation code as well as pretrained weights avaible at https://gitlab.com/Queuecumber/quantization-guided-ac. The evaluation code is reimplemented in PyTorch using ARCNN MATLAB code as a reference and checked for accuracy. We invite future work to use this framework for correct evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Analysis</head><p>In this section we provide futher analysis of our model. We start by examining the Convolution Filter Manifold layers in more detail, providing visualizations of what they learn in order to better understand their contribution to our result. Next, we examine model interpolation in more detail by showing qualitative comparisons for varying interpolation strengths between the regression and GAN model. We then conduct a study that shows how much space can be saved by storing low quality JPEG images and using our method to restore them. We then examine the frequency domain qualitative results and show that our GAN model is capabile of generating images that have more high frequency content than the regression model alone. We conclude by examining the runtime throughput of our model compared to the other methods we tested against.  CFM layers are both our largest departure from a vanilla CNN and also quite important to learning quality invariant features, so it is a natural result to try to visualize their operation. In <ref type="figure" target="#fig_0">Figure 1</ref>, we compute the final 8 × 8 convolution weight for different quality levels. The quality levels, on the vertical axis, are 10, 50, and 100. The horizontal axis shows three different channels from the weight. What we see makes intuitive sense: the filters in different channels have different patterns, but for the same channel, the pattern is roughly the same as the quality increases. Furthermore, the filter response becomes smaller as the quality increases since the filters have to do less "work" to correct a high quality JPEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Understanding Convolutional Filter Manifolds</head><p>Next we visualize compression artifacts learned by the weight. To do this we find the image that maximally activates a single channel of the CFM weight. The result of this is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Again the horizontal axis shows different channels of the weight The qualtiy 100 images are almost untouched, leaving only the input noise pattern. It makes sense that quality 100 filters are only minmally activated since there is not much correction to do on a quality 100 JPEG. Note that we only show Y channel response for this figure and that <ref type="figure" target="#fig_0">Figures 1 and 2</ref> use the same channels from the same layer.</p><p>Finally we examine the manifold structure of the CFM. We claim in Section 3.1 (and the name implies) that the CFM learns a smooth manifold of filters through quantization space. If this is true, then a quality 25 quantization matrix should generate a weight halfway inbetween a qualty 20 and a quality 30 one. To show that this happens, we generate weights for all 101 quanitzation matrices (0 to 100 inclusive) and then compute t-SNE embeddings to reduce the dimensionality to 2. We plot 3 channels from the weight embeddings with the quality level that was used to generate the weight given as the color of the point. This plot is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. What see is a smooth line through the space starting from dark (low quality) to bright (high quality) showing that the CFM has not only separated the different quality levels but has ordered them as well. Futhermore we see that the low quality filters are separated in space, indicating that they are quite different (and perform different functions), a property that is important for effective neural networks. As the quality increases and the problem becomes easier, the filters tend to converge on a single point where they are all doing very little to correct the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Model Interpolation</head><p>Here we show more model interpolation results. Model interpolation creates a new model by linearly interpolating the GAN and regresion model parameters as follows</p><formula xml:id="formula_3">Θ I = (1 − α)Θ R + αΘ G<label>(1)</label></formula><p>where Θ I are the interpolated parameters, Θ R are the regression model parameters and Θ G are the GAN model parameters with α ∈ [0, 1] being the interpolation parameter. The new model blends the result of the GAN and regression results. We observe that using the GAN model alone can introduce artifacts (see <ref type="figure">Figure 4</ref>), blending the models in this way helps surpress those artifacts. Note that in this scheme, α = 0 gives the regression model and α = 1 gives the GAN model. Model interpolation has been shown to produce cleaner results than image interpolation, and has the added benefit of not needing to run two models to produce a result. In <ref type="figure">Figure 4</ref> we show the model interpolation results for α ∈ {0.0, 0.7, 0.9, 1.0} for several images from the Live-1 dataset. This figure also serves as additional qualitative results for our method. These results were generated from quality 10 JPEGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Equivalent Quality</head><p>One major motivation for JPEG artifact correction is that space or bandwidth can be saved by transmitting a small low quality JPEG and algorithmically correcting it before display. We explore how effective our model is at this by computing the equivalent quality JPEG file for a restored image. Our argument is that a system can get the storage space savings of the lower quality JPEG and the visual fidelity of a higher quality JPEG by using our model. To show this we use the Live-1 dataset. For qualities in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b49">50]</ref> in steps of 10, we compute the average increase in JPEG quality incurred by our model. We do this by compressing the input image at higher and higher qualities until we find the first quality with SSIM greater than or equal to our restoration's SSIM. We then save the low quality JPEG and the equivalent quality JPEG and measure the size difference in kilobytes. We average the quality increase and space savings over the entire dataset, to show the amount of space saved by using our method over using the higher quality JPEG directly. This result is shown in <ref type="figure">Figure 5</ref>. We also show qualitative examples for several images in <ref type="figure">Figure 6</ref>. Note that because the SSIM measure is not perfect, often our model outputs images that look better than the equivalent quality JPEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Frequency Domain Analysis</head><p>In this section we show results in the DCT frequency domain. A well known phenomenon of JPEG compression is the removal of high frequency information. To check how well our model restores this information, we take the Y channel from several images and show the colormapped DCT of the original image, the JPEG at quality 10, the image as restored by our regression model, and the image restored by our GAN model. Next, for each image, we plot the probability that each of the 15 spatial frequencies in a DCT block are set (e.g., has a magnitude greater than 0). This is shown in <ref type="figure">Figure 7</ref>. While our regression model is able to fill in high frequencies, our GAN model nearly matches the original images in terms of frequency saturation. Additionally since our network operates in the DCT domain, these outputs serve as an interesting qualitative result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Runtime analysis</head><p>We show the runtime inference performance of our network compared to the other networks we ran against. We measure FPS on our NVIDIA Pascal GPU for 100 720p   (1280 × 720) frames and plot frames per second vs SSIM increase for quality 10 Live-1 images in <ref type="figure" target="#fig_5">Figure 8</ref>. We do not include ARCNN in this figure as the authors do not provide GPU accelerated inference code. For grayscale only models we only use single channel test images (we not not run the model three times as would be required to produce an RGB output). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Results</head><p>In this section we show qualitative results on Quality 10 and 20 images for our regression network. These results are in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D JPEG Compression Algorithm</head><p>Since the JPEG algorithm is core to the operation of our method, we describe it here in detail. Where the JPEG standard is ambiguous or lacking in guidance, we defer to the Independent JPEG Group's libjpeg software.</p><p>Compression JPEG compression starts with an input image in RGB color space (for grayscale images the procedure is the same using only the Y channel equations) where each pixel uses the 8-bit unsigned integer represenation (e.g., the pixel value is an integer in [0, 255]). The image is then converted to the YCbCr color space using the full <ref type="bibr" target="#b7">8</ref> represenation (pixel values again in [0, 255], this is in contrast to the more common ITU-R BT.601 standard YCbCr color conversion) using the equations:</p><formula xml:id="formula_4">Y = 2.99R + 0.587B + 0.114G (2) Cb = 128 − 0.168736R − 0.331264B + 0.5G Cr = 128 + 0.5R − 0.418688B − 0.081312G</formula><p>Since the DCT will be taken on non-overlapping 8 × 8 blocks, the image is then padded in both dimensions to a multiple of 8. Note that if the color channels will be chroma subsampled, as is usually the case, then the image must be padded to the scale factor of the smallest channel times 8 or the subsampled channel will not be an even number of blocks. In most cases, chroma subsampling will be by half, so the image must be padded to a multiple of 16, this size is referred to as the minimum coded unit (MCU), or macroblock size. The padding is always done by repeating the last pixel value on the right and bottom edges. The chroma channels can now be subsampled.</p><p>Next the channels are centered around zero by subtracing 128 from each pixel, yielding pixel values in <ref type="bibr">[-128, 127]</ref>. Then the 2D Discrete type 2 DCT is take on each non-overlapping 8 × 8 block as follows:</p><formula xml:id="formula_5">D i,j = 1 4 C(i)C(j) 7 x=0 7</formula><p>y=0 P x,y cos (2x + 1)iπ 16 cos (2y + 1)jπ 16</p><p>(3)</p><formula xml:id="formula_6">C(u) = 1 √ 2 u = 0 1 otherwise</formula><p>Where D i,j gives the coefficient for frequency i, j, and P x,y gives the pixel value for image plane P at position pixel position x, y. Note that C(u) is a scale factor that ensures the basis is orthonormal.</p><p>The DCT coefficients can now be quantized. This follows the same procedure for the Y and color channels but with different quanitzation tables. We encourage readers to refer to the libjpeg software for details on how the quantization tables are computed given the scalar quality factor, an integer in [0, 100] (this is not a standardized process). Given the quantization tables Q Y and Q C , the quanized coeffcients of each block are computed as:</p><formula xml:id="formula_7">Y i,j = truncate Y i,j Q Yi,j<label>(4)</label></formula><p>Cb i,j = truncate Cb i,j Q Ci,j Cr i,j = truncate Cr i,j Q Ci,j</p><p>The quantized coefficients for each block are then vectorized (flattened) using a zig-zag ordering (see <ref type="figure" target="#fig_0">Figure 10</ref>) that is designed to place high frequencies further towards the end of the vectors. Given that high frequencies have lower magnitude and are more heavily quanitized, this usually creates a run of zeros at the end of each vector. The vectors are then compressed using run-length encoding on this final run of zeros (information prior to the final run is not run-length encoded.). The run-length encoded vectors are then entropy coded using either huffman coding or arithmetic coding and then written to the JPEG file along with associated metadata (EXIF tags), quantization tables, and huffman coding tables. Decompression The decompression algorithm largely follows the reverse procedure of the compression algorithm. After reading the raw array data, huffman tables, and quantization tables, the entropy coding, run-length coding, and zig-zag ordering is reversed. We reiterate here that the JPEG file does not store a scalar quality from which the decompressor is expected to derive a quanitzation table, the decompressor reads the quanitzation table from the JPEG file and uses it directly, allowing any software to correctly decode JPEG files that were not written by it. Next, the 8 × 8 blocks are scaled using the quantization table:</p><formula xml:id="formula_8">Y i,j = Y i,j Q Yi,j<label>(5)</label></formula><p>Cb i,j = Cb i,j Q Ci,j Cr i,j = Cr i,j Q Ci,j</p><p>There are a few things to note here. First, if dividing by the quantization table entry during compression (Equation 5) resulted in a fractional part (the result was not an integer), that fractional part was lost during truncation and the scaling here will recover an integer near to the true coefficient (how close it gets depends on the magnitude quantization table entry). Next, if the division in Equation 5 resulted in a number in [0, 1), then that coeffient would be truncated to zero and is lost forever (it remains zero after this scaling process). This is the only source of loss in JPEG compression, however it allows for the result to fit into integers instead of floating point numbers, and it creates larger runs of zeros which leads to significantly larger compression ratios.</p><p>Next, the DCT process for each block is reversed using the 2D Discrete type 3 DCT: and cropped to remove any block padding that was added during compression. The image is now ready for display.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>CFM Weight Visualization. Horizontal axis shows different channels of the weight, vertical axis shows quality. Quality levels shown are Top: 10, Middle: 50, Bottom: 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Images Which Maximally Activate CFM Weights. Horizontal axis shows different channels from the weight, vertical axis shows quality. Quality levels shown are Top: 10, Middle: 50, Bottom: 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Embeddings for Different CFM Layers. 3 channels are taken from each embedding, color shows JPEG quality setting that produced the input quantization matrix. Circled points indicate quantization matrices that were seen during training. and the vertical axis shows quality levels 10, 50, and 100. The result shows clear images of JPEG artifacts. At quality 10, the local blocking artifacts are extremely prominant. By quality 50, the blocking artifacts are suppressed, while structural artifacts remain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 : 2 RegressionFig. 5 :</head><label>425</label><figDesc>Model interpolation results 1/Equivalent quality and space savings for Live-1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :Fig. 7 :Fig. 7 :</head><label>677</label><figDesc>Equivalent quality visualizations. For each image we show the input JPEG, the JPEG with equivalent SSIM to our model output, and our model output. Frequency domain results 1/2. Frequency domain results 2/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Increase in SSIM vs FPS. Our result is highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>are arranged in their correct spatial positions. The pixel values are uncentered (adding 128 to each pixel value), and the color channels are interpolated to their original size. Finally, the image is converted from YCbCr color space to RGB color space: R = Y + 1.402(Cr − 128) (7) G = Y − 0.344136(Cb − 128) − 0.714136(Cr − 128) B = Y + 1.772(Cb − 128)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overview. We first restore the Y channel of the input image, then use the restored Y channel to correct the color channels which have much worse input quality.</figDesc><table><row><cell>Degraded Y channel, H x W, 1 channel</cell><cell cols="2">Y channel Quantization Matrix, 8 x 8, 1 channel</cell><cell>Degraded Color Image, (Cb-or Cr-channel), H x W, 1 channel</cell><cell>Color Quantization Matrix, 8 x 8, 1 channel</cell><cell>Legend: Input Our Networks Intermediate Output Final Output</cell></row><row><cell cols="2">Y Channel Correction Network</cell><cell cols="2">Intermediate Output Restored Y channel Image, H x W, 1 channel</cell><cell>Color Channel Correction Network</cell><cell>Final Output Restored Color channel Image, H x W, 1 channel</cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Color Artifact Correction Results. PSNR / PSNR-B / SSIM format. Best result in bold, second best underlined. JPEG column gives input error. For ICB, we used the RGB 8bit dataset. / 23.53 / 0.755 26.66 / 26.54 / 0.792 27.21 / 27.02 / 0.805 27.62 / 27.32 / 0.816 27.18 / 27.03 / 0.810 27.65 / 27.40 / 0.819 20 27.96 / 25.77 / 0.837 28.97 / 28.65 / 0.860 29.54 / 29.23 / 0.873 30.01 / 29.49 / 0.881 29.45 / 29.08 / 0.874 29.92 / 29.51 / 0.882 30 29.25 / 27.10 / 0.872 30.29 / 29.97 / 0.891 30.82 / 30.45 / 0.901 --31.21 / 30.71 / 0.908 BSDS500 10 25.72 / 23.44 / 0.748 26.83 / 26.65 / 0.783 27.18 / 26.93 / 0.794 27.61 / 27.22 / 0.805 27.16 / 26.95 / 0.799 27.69 / 27.36 / 0.810 20 28.01 / 25.57 / 0.833 29.00 / 28.53 / 0.853 29.45 / 28.96 / 0.866 29.90 / 29.20 / 0.873 29.35 / 28.84 / 0.866 29.89 / 29.29 / 0.876 30 29.31 / 26.85 / 0.869 30.31 / 29.85 / 0.887 30.71 / 30.09 / 0.895 --31.15 / 30.37 / 0.903 ICB 10 29.31 / 28.07 / 0.749 30.06 / 30.38 / 0.744 30.76 / 31.21 / 0.779 31.71 / 32.02 / 0.809 30.85 / 31.31 / 0.796 32.11 / 32.47 / 0.815 20 31.84 / 30.63 / 0.804 32.24 / 32.53 / 0.778 32.79 / 33.32 / 0.812 33.99 / 34.37 / 0.838 32.77 / 33.26 / 0.830 34.23 / 34.67 / 0.845 30 33.02 / 31.87 / 0.830 33.31 / 33.72 / 0.807 34.11 / 34.69 / 0.845 --35.20 / 35.67 / 0.860</figDesc><table><row><cell>Dataset</cell><cell>Quality</cell><cell>JPEG</cell><cell>ARCNN[8]</cell><cell>MWCNN [27]</cell><cell>IDCN [51]</cell><cell>DMCNN [49]</cell><cell>Ours</cell></row><row><cell></cell><cell>10</cell><cell>25.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Live-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Y Channel Correction Results. PSNR / PSNR-B / SSIM format, the best result is highlighted in bold, second best is underlined. The JPEG column gives with input error of the images. For ICB, we used the Grayscale 8bit dataset. We add Classic-5, a grayscale only dataset./  25.32 / 0.790 28.96 / 28.68 / 0.821 29.68 / 29.30 / 0.839 29.68 / 29.32 / 0.838 29.73 / 29.43 / 0.839 29.53 / 29.15 / 0.840 20 30.05 / 27.55 / 0.868 31.26 / 30.73 / 0.887 32.00 / 31.47 / 0.901 32.05 / 31.46 / 0.900 32.07 / 31.49 / 0.901 31.86 / 31.27 / 0.901 30 31.37 / 28.90 / 0.900 32.64 / 32.11 / 0.916 33.40 / 32.76 / 0.926 .780 29.03 / 28.76 / 0.811 30.01 / 29.59 / 0.837 29.83 / 29.48 / 0.833 29.98 / 29.65 / 0.836 29.84 / 29.43 / 0.837 20 30.12 / 27.50 / 0.854 31.15 / 30.59 / 0.869 32.16 / 31.52 / 0.886 31.99 / 31.46 / 0.884 32.11 / 31.48 / 0.885 31.98 / 31.</figDesc><table><row><cell>Dataset</cell><cell>Quality</cell><cell>JPEG</cell><cell>ARCNN[8]</cell><cell>MWCNN [27]</cell><cell>IDCN [51]</cell><cell>DMCNN [49]</cell><cell>Ours</cell></row><row><cell>Live-1</cell><cell>10</cell><cell cols="4">27.76 -</cell><cell>-</cell><cell>33.23 / 32.50 / 0.925</cell></row><row><cell>Classic-5</cell><cell>10</cell><cell cols="6">27.82 / 25.21 / 037 / 0.885</cell></row><row><cell></cell><cell>30</cell><cell cols="3">31.48 / 28.94 / 0.884 32.51 / 31.98 / 0.896 33.43 / 32.62 / 0.907</cell><cell>-</cell><cell>-</cell><cell>33.22 / 32.42 / 0.907</cell></row><row><cell></cell><cell>10</cell><cell cols="6">27.86 / 25.18 / 0.785 29.14 / 28.76 / 0.816 29.63 / 29.16 / 0.831 29.60 / 29.13 / 0.829 29.66 / 29.27 / 0.831 29.54 / 29.04 / 0.833</cell></row><row><cell>BSDS500</cell><cell>20</cell><cell cols="6">30.08 / 27.28 / 0.864 31.27 / 30.52 / 0.881 31.88 / 31.12 / 0.894 31.88 / 31.05 / 0.893 31.91 / 31.13 / 0.894 31.79 / 30.96 / 0.894</cell></row><row><cell></cell><cell>30</cell><cell cols="3">31.37 / 28.56 / 0.896 32.64 / 31.90 / 0.912 33.23 / 32.29 / 0.920</cell><cell>-</cell><cell>-</cell><cell>33.12 / 32.07 / 0.920</cell></row><row><cell></cell><cell>10</cell><cell cols="6">32.08 / 29.92 / 0.856 31.13 / 30.97 / 0.794 34.12 / 34.06 / 0.884 32.50 / 32.42 / 0.826 34.18 / 34.15 / 0.874 34.73 / 34.58 / 0.896</cell></row><row><cell>ICB</cell><cell>20</cell><cell cols="6">35.04 / 32.72 / 0.905 32.62 / 32.31 / 0.821 36.56 / 36.44 / 0.902 34.30 / 34.18 / 0.851 35.93 / 35.79 / 0.918 37.12 / 36.88 / 0.924</cell></row><row><cell></cell><cell>30</cell><cell cols="3">36.66 / 34.22 / 0.927 33.79 / 33.52 / 0.841 38.20 / 37.96 / 0.927</cell><cell>-</cell><cell>-</cell><cell>38.43 / 38.05 / 0.938</cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell>JPEG</cell><cell>IDCN Q=10</cell><cell>IDCN Q=20</cell><cell>Ours</cell><cell></cell></row></table><note>Fig. 10: Generalization Example. Input was compressed at quality 50. Please zoom in to view details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Generalization Capabilities. Live-1 dataset (PSNR / PSNR-B / SSIM). / 28.94 / 0.905 30.19 / 30.14 / 0.889 32.78 / 32.19 / 0.932 20 30.91 / 28.94 / 0.905 31.91 / 31.65 / 0.916 10 20 27.96 / 25.77 / 0.837 29.25 / 29.08 / 0.863 29.92 / 29.51 / 0.882 20 10 25.60 / 23.53 / 0.755 26.95 / 26.24 / 0.804 27.65 / 27.40 / 0.819</figDesc><table><row><cell></cell><cell>Model Quality Image Quality</cell><cell>JPEG</cell><cell cols="3">IDCN [51]</cell><cell>Ours</cell><cell></cell></row><row><cell>Increase in PSNR (dB)</cell><cell cols="2">10 10 20 30 40 50 60 70 80 90 100 50 Quality 30.91 0 1 2 3 BSDS500 ICB Live-1</cell><cell>Increase in SSIM</cell><cell>0.03 0.07 0.04 0.05 0.06</cell><cell>1</cell><cell>Increase in PSNR (dB) 1.25 1.5 1.75 ARCNN DPW-SDNet RDN MemNet MWCNN S-Net</cell><cell>2</cell><cell>2.25 IDCN OURS OURS</cell></row></table><note>Fig. 11: Increase in PSNR on color datasets. For all three datasets we show the average im- provement in PSNR values on qualities 10-100. Improvement drops off steeply at quality 90.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>GAN FID Scores.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Quality Ours Ours-GAN</cell></row><row><cell></cell><cell>10</cell><cell>69.57</cell><cell>35.86</cell></row><row><cell>Live-1</cell><cell>20</cell><cell>36.32</cell><cell>16.99</cell></row><row><cell></cell><cell>30</cell><cell>24.72</cell><cell>12.20</cell></row><row><cell></cell><cell>10</cell><cell>75.15</cell><cell>34.80</cell></row><row><cell>BSDS500</cell><cell>20</cell><cell>42.46</cell><cell>18.74</cell></row><row><cell></cell><cell>30</cell><cell>29.04</cell><cell>13.03</cell></row><row><cell></cell><cell>10</cell><cell>33.37</cell><cell>26.08</cell></row><row><cell>ICB</cell><cell>20</cell><cell>17.23</cell><cell>13.53</cell></row><row><cell></cell><cell>30</cell><cell>11.66</cell><cell>10.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation Results. (refer to Section 4.4 for details).</figDesc><table><row><cell>Experiment</cell><cell>Model</cell><cell cols="3">PSNR PSNR-B SSIM</cell></row><row><cell></cell><cell>None</cell><cell>29.38</cell><cell>28.9</cell><cell>0.825</cell></row><row><cell>CFM</cell><cell>Concat</cell><cell cols="3">29.32 28.94 0.823</cell></row><row><cell></cell><cell>CFM</cell><cell cols="3">29.46 29.05 0.827</cell></row><row><cell>Subnetworks</cell><cell cols="4">FrequencyNet 28.03 25.58 0.787 BlockNet 29.45 29.04 0.827</cell></row><row><cell>Fusion</cell><cell>No Fusion Fusion</cell><cell cols="3">27.82 25.21 29.22 28.76 0.822 0.78</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement This project was partially supported by Facebook AI and Defense Advanced Research Projects Agency (DARPA) MediFor program (FA87501620191).</head><p>There is no collaboration between Facebook and DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamisetty R</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="90" to="93" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Material recognition in the wild with the materials in context database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3479" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CAS-CNN: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DPW-SDNet: Dual pixel-wavelet domain deep CNNs for soft decoding of JPEG-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast object detection in compressed JPEG Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Deguerre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Gasso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08408</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning in the jpeg transform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3484" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive DCT for high-quality deblocking of compressed color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th European Signal Processing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">JPEG Artifacts Reduction via Deep Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2501" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep generative adversarial compression artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4826" to="4835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep universal generative adversarial compression artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep feature extraction in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthita</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3536" to="3541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jpeg</forename><surname>Independant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Group</surname></persName>
		</author>
		<ptr target="http://libjpeg.sourceforge.net" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster neural networks straight from JPEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Gueguen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3933" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual-stream Multi-path Recursive Residual Network for JPEG Image Compression Artifacts Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Crowd counting by adapting convolutional neural networks with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debarun</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06748</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AGARNet: Adaptively Gated JPEG Compression Artifacts Removal Network for a Wide Range Quality Factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woong</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="20160" to="20170" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A pseudo-blind convolutional neural network for the reduction of compression artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1121" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-CNN for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data-driven sparsity-based restoration of JPEG-compressed images in dual transform-pixel domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5171" to="5178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring Semantic Segmentation on the DCT Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ming</forename><surname>Shao-Yuan Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Asia on ZZZ</title>
		<meeting>the ACM Multimedia Asia on ZZZ</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rawzor</surname></persName>
		</author>
		<ptr target="http://imagecompression.info/" />
	</analytic>
	<monogr>
		<title level="j">Image Compression Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hr Sheikh</surname></persName>
		</author>
		<ptr target="http://live.ece.utexas.edu/research/quality" />
		<title level="m">LIVE image quality assessment database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast software processing of motion JPEG video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second ACM international conference on Multimedia</title>
		<meeting>the second ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Compression artifacts removal using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Svoboda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00366</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A Novel PSNR-B Approach for Evaluating the Quality of De-blocked Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trinadh</forename><surname>Tadala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sri E Venkata</forename><surname>Narayana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The JPEG still picture compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on consumer electronics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="xviii" to="xxxiv" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">D3: Deep dual-domain based fast restoration of jpegcompressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blocking artifact free inverse discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjoon</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 International Conference on Image Processing</title>
		<meeting>2000 International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DMCNN: Dual-domain multi-scale convolutional neural network for compression artifacts removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="390" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Implicit dual-domain convolutional network for robust color image compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">S-Net: a scalable convolutional neural network for JPEG compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">43037</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep Residual Autoencoder for quality independent JPEG restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Zini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06117</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
