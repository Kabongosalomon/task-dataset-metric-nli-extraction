<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LJK</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">École normale supérieure</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">PSL Research Univ</orgName>
								<address>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LJK</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Recognition</term>
					<term>Few-shot Learning</term>
					<term>Feature Selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Popular approaches for few-shot classification consist of first learning a generic data representation based on a large annotated dataset, before adapting the representation to new classes given only a few labeled samples. In this work, we propose a new strategy based on feature selection, which is both simpler and more effective than previous feature adaptation approaches. First, we obtain a multi-domain representation by training a set of semantically different feature extractors. Then, given a few-shot learning task, we use our multi-domain feature bank to automatically select the most relevant representations. We show that a simple nonparametric classifier built on top of such features produces high accuracy and generalizes to domains never seen during training, leading to state-ofthe-art results on MetaDataset and improved accuracy on mini-ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks <ref type="bibr" target="#b20">[21]</ref> (CNNs) have become a classical tool for modeling visual data and are commonly used in many computer vision tasks such as image classification <ref type="bibr" target="#b18">[19]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>, or semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref>. One key of the success of these approaches relies on massively labeled datasets such as ImageNet <ref type="bibr" target="#b38">[39]</ref> or COCO <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, annotating data at this scale is expensive and not always feasible, depending on the task at hand. Improving the generalization capabilities of deep neural networks and removing the need for huge sets of annotations is thus of utmost importance.</p><p>This ambitious challenge may be addressed from different perspectives, such as large-scale unsupervised learning <ref type="bibr" target="#b2">[3]</ref>, self-supervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, or by developing regularization techniques dedicated to deep networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b50">51]</ref>. An alternative solution is to use data that has been previously annotated for a different task than the one considered, for which only a few annotated samples may be available. This approach is particularly useful if the additional data is related to the new task <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>, which is unfortunately not known beforehand. How to use effectively this additional data is then an important subject of ongoing research <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54]</ref>. In this paper, we propose to use a multi-domain image representation, i.e., an exhaustive set of semantically different features. Then, by automatically selecting <ref type="figure">Fig. 1</ref>: Illustration of our approach. (Left) First, we obtain a multi-domain feature representation, consisting of feature blocks with different semantics. (Right) Given a few-shot task, we select only the relevant feature blocks from the multi-domain representation, by optimizing masking parameters λ on the support set.</p><p>only relevant feature subsets from the multi-domain representation, we show how to successfully solve a large variety of target tasks.</p><p>Specifically, we are interested in few-shot classification, where a visual model is first trained from scratch, i.e. starting from randomly initialized weights, using a large annotated corpus. Then, we evaluate its ability to transfer the knowledge to new classes, for which only very few annotated samples are provided. Simply fine-tuning a convolutional neural network on a new classification task has been shown to perform poorly <ref type="bibr" target="#b8">[9]</ref>. This has motivated the community to develop dedicated techniques, allowing effective adaptation with few samples.</p><p>Few-shot classification methods typically operate in two stages, consisting of first pre-training a general feature extractor and then building an adaptation mechanism. A common way to proceed is based on meta-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, which is a principle to learn how to adapt to new learning problems. That is, a parametric adaptation module (typically, a neural network) is trained to produce a classifier for new categories, given only a few annotated samples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. To achieve this goal, the large training corpus is split into smaller few-shot classification problems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref>, that are used to train the adaptation mechanism. Training in such episodic fashion is advocated to alleviate overfitting and improve generalization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref>. However, a more recent work <ref type="bibr" target="#b3">[4]</ref> demonstrates that using large training set to train the adapter is not necessary, i.e. a linear classifier with similar accuracy can be trained directly from new samples, on top of a fixed feature extractor. Finally, it has been shown that adaptation is not necessary at all <ref type="bibr" target="#b6">[7]</ref>; using a non-parametric prototypical classifier <ref type="bibr" target="#b27">[28]</ref> combined with a properly regularized feature extractor can achieve better accuracy than recent meta-learning baselines <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref>. These results suggest that on standard few-shot learning benchmarks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, the little amount of samples in a few-shot task is not enough to learn a meaningful adaptation strategy.</p><p>To address the shortcomings of existing few-shot benchmarks, the authors of <ref type="bibr" target="#b45">[46]</ref> have proposed MetaDataset, which evaluates the ability to learn across different visual domains and to generalize to new data distributions at test time, given few annotated samples. While methods based solely on pre-trained feature extractors <ref type="bibr" target="#b40">[41]</ref> can achieve good results only on test datasets that are similar to the training ones, the adaptation technique <ref type="bibr" target="#b36">[37]</ref> performs well across test domains. The method not only predicts a new few-shot classifier but also adapts the filters of a feature extractor, given an input task. The results thus suggest that feature adaptation may be in fact useful to achieve better generalization.</p><p>In contrast to these earlier approaches, we show that feature adaptation can be replaced by a simple feature selection mechanism, leading to better results in the cross-domain setup of <ref type="bibr" target="#b45">[46]</ref>. More precisely, we propose to leverage a multi-domain representation -a large set of semantically different features that captures different modalities of a training set. Rather than adapting existing features to a new few-shot task, we propose to select features from the multidomain representation. We call our approach SUR which stands for Selecting from Universal Representations. To be more clear, we say universal because SUR could be applied not only to multi-domain representations, but to any set of representations that are semantically different. In contrast to standard adaptation modules <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref> learned on the training set, selection is performed directly on new few-shot tasks using gradient descent. Approaching few-shot learning with SUR has several advantages over classical adaptation techniques. First, it is simple by nature, i.e. selecting features from a fixed set is an easier problem than learning a feature transformation, especially when few annotated images are available. Second, learning an adaptation module on the meta-training set is likely to generalize only to similar domains. In contrast, the selection step in our approach is decoupled from meta-training, thus, it works equally well for any new domain. Finally, we show that our approach achieves better results than current state-of-the-art methods on popular few-shot learning benchmarks. In summary, this work makes the following contributions:</p><p>-We propose to tackle few-shot classification by selecting relevant features from a multi-domain representation. While multi-domain representations can be built by training several feature extractors or using a single neural network, the selection procedure is implemented with gradient descent. -We show that our method outperforms existing approaches in in-domain and cross-domain few-shot learning and sets new state-of-the-art result on MetaDataset <ref type="bibr" target="#b45">[46]</ref> and improves accuracy on mini -ImageNet <ref type="bibr" target="#b32">[33]</ref>.</p><p>Our implementation is available at https://github.com/dvornikita/SUR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we now present previous work on few-shot classification and multi-domain representations, which is a term first introduced in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Few-shot classification. Typical few-shot classification problems consist of two parts called meta-training and meta-testing <ref type="bibr" target="#b3">[4]</ref>. During the meta-training stage, one is given a large-enough annotated dataset, which is used to train a predictive model. During meta-testing, novel categories are provided along with few annotated examples. The goal is to evaluate the ability of the predictive model to adapt and perform well on these new classes. Typical few-shot learning algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> first pre-train the feature extractor by supervised learning on the meta-training set. Then, they use metalearning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref> to train an adaptation mechanism. For example, in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>, adaptation consists of predicting the weights of a classifier for new categories, given a small few-shot training set. The work of <ref type="bibr" target="#b29">[30]</ref> goes beyond the adaptation of a single layer on top of a fixed feature extractor, and additionally generates FiLM <ref type="bibr" target="#b30">[31]</ref> layers that modify convolutional layers. Alternatively, the work of <ref type="bibr" target="#b3">[4]</ref> proposes to train a linear classifier on top of the features directly from few samples from new categories. In the same line of work, <ref type="bibr" target="#b21">[22]</ref> performs implanting, i.e. learning new convolutional filters within the existing CNN layers.</p><p>Other methods do not perform adaptation at all. It has been shown in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41]</ref> that training a regularized CNN for classification on the meta-training set and using these features directly with a nearest centroid classifier produces state-ofthe-art few-shot accuracy. To obtain a robust feature extractor, <ref type="bibr" target="#b6">[7]</ref> distills an ensemble of networks into a single extractor to obtain low-variance features.</p><p>Finally, the methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref> are the most relevant to our work as they also tackle the problem of cross-domain few-shot classification <ref type="bibr" target="#b45">[46]</ref>. In <ref type="bibr" target="#b36">[37]</ref>, the authors propose to adapt each hidden layer of a feature extractor for a new task. They first obtain a task embedding and use conditional neural process <ref type="bibr" target="#b9">[10]</ref> to generate parameters of modulation FiLM <ref type="bibr" target="#b30">[31]</ref> layers, as well as weights of a classifier for new categories. An adaptation-free method <ref type="bibr" target="#b40">[41]</ref> instead trains a CNN on ImageNet, while optimizing for high validation accuracy on other datasets, using hyper-parameter search. When tested on domains similar to ImageNet, the method demonstrates the highest accuracy, however, it is outperformed by adaptation-based methods when tested on other data distributions <ref type="bibr" target="#b40">[41]</ref>.</p><p>Multi-domain Representations. A multi-domain representation (introduced as "universal representation" in <ref type="bibr" target="#b1">[2]</ref>) refers to an image representation that works equally well for a large number of visual domains. The simplest way to obtain a multi-domain representation is to train a separate feature extractor for each visual domain and use only the appropriate one at test time. To reduce the computational footprint, <ref type="bibr" target="#b1">[2]</ref> investigates if a single CNN can be useful to perform image classification on very different domains. To achieve this goal, the authors propose to share most of the parameters between domains during training and have a small set of parameters that are domain-specific. Such adaptive feature sharing is implemented using conditional batch normalization <ref type="bibr" target="#b15">[16]</ref>, i.e. there is a separate set of batch-norm parameters for every domain. The work of <ref type="bibr" target="#b33">[34]</ref> extends the idea of domain-specific computations in a single network and proposes universal parametric network families, which consist of two parts: 1) a CNN feature extractor with universal parameters shared across all domains, and 2) domain-specific modules trained on top of universal weights to maximize the performance on that domain. It has been found important <ref type="bibr" target="#b33">[34]</ref> to adapt both shallow and deep layers in a neural network in order to successfully solve multiple visual domains. We use the method of this paper in our work when training a parametric network family to produce a multi-domain representation. In contrast, instead of parallel adapters, in this work, we use much simpler FiLM layers for domain-specific computations. Importantly, parametric networks families <ref type="bibr" target="#b33">[34]</ref> and FiLM <ref type="bibr" target="#b30">[31]</ref> adapters only provide a way to efficiently compute multi-domain representation; they are not directly useful for few-shot learning. However, using our SUR strategy on this representation produces a useful set of features leading to state-of-the-art results in few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>We now present our approach for few-shot learning, starting with preliminaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Few-Shot Classification with Nearest Centroid Classifier</head><p>The goal of few-shot classification is to produce a model which, given a new learning task and a few labeled examples, is able to generalize to unseen examples for that task. In other words, the model learns from a small training set S =</p><formula xml:id="formula_0">{(x i , y i )} n S i=1</formula><p>, called a support set, and is evaluated on a held-out test set Q = {(x * j , y * j )} n Q j=1 , called a query set. The (x i , y i )'s represent image-label pairs while the pair (S, Q) represents the few-shot task. To fulfill this objective, the problem is addressed in two steps. During the meta-training stage, a learning algorithm receives a large dataset D b , where it must learn a general feature extractor f (·). During the meta-testing stage, one is given a target dataset D t , used to repeatedly sample few-shot tasks (S, Q). Importantly, meta-training (D b ) and meta-testing (D t ) datasets have no categories in common.</p><p>During the meta-testing stage, we use feature representation f (·) to build a nearest centroid classifier (NCC), similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44]</ref>. Given a support set S, for each category present in this set, we build a class centroid by averaging image representations belonging to this category:</p><formula xml:id="formula_1">c j = 1 |S j | i∈Sj f (x i ), S j = {k : y k = j}, j = 1, ..., C.<label>(1)</label></formula><p>To classify a sample x, we choose a distance function d(f (x), c j ) to be negative cosine similarity, as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, and assign the sample to the closest centroid c j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>With the NCC classifier defined above, we may now formally define the concept of multi-domain representation and the procedure for selecting relevant features.</p><p>Typically, the meta-training set is used to train a set of K feature extractors {f i (·)} K i=1 that form a multi-domain set of features. Each feature extractor maps an input image x into a d-dimensional representation f i (x) ∈ R d . These features should capture different types of semantics and can be obtained in various manners, as detailed in Section 3.3.</p><p>Parametric Multi-domain Representations. One way to transform a multidomain set of features into a vectorized multi-domain representation f (x) is, by concatenating all image representations from this set (with or without l2normalization). As we show in the experimental section, directly using such f (x) for classification with NCC does not work well as many irrelevant features for a new task are present in the representation. Therefore, we are interested in implementing a selection mechanism. In order to do so, we define a selection operation as follows, given a vector λ in R K :</p><formula xml:id="formula_2">λ f (x) = λ ·   f 1 (x)</formula><p>. . .</p><formula xml:id="formula_3">f K (x)    =    λ 1 ·f 1 (x) . . . λ K ·f K (x)    = f λ (x),<label>(2)</label></formula><formula xml:id="formula_4">wheref i (x) is simply f i (x) after 2 -normalization. We call f λ (x) ∈ R K·d a</formula><p>parametrized multi-domain representation, as it contains information from the whole multi-domain set but the exact representation depends on the selection parameters λ. Using this mechanism, it is possible to select various combinations of features from the multi-domain representation by setting more than one λ i to non-zero values.</p><p>Finding optimal selection parameters. Feature selection is performed during meta-testing by optimizing a probabilistic model, leading to optimal parameters λ, given a support set S = {(x i , y i )} n S i=1 of a new task. Specifically, we consider the NCC classifier from Section 3.1, using f λ (x) instead of f (x), and introduce the likelihood function</p><formula xml:id="formula_5">p(y = l|x) = exp(−d(f λ (x), c l )) n S j=1 exp(−d(f λ (x), c j )) .<label>(3)</label></formula><p>Our goal is then to find optimal parameters λ that maximize the likelihood on the support set, which is equivalent to minimizing the negative log-likelihood:</p><formula xml:id="formula_6">L(λ) = 1 n S n S i=1 [− log(p(y = y i |x i ))]<label>(4)</label></formula><formula xml:id="formula_7">= 1 n S n S i=1   log C j=1 exp(cos(f λ (x i ), c j )) − cos(f λ (x i ), c yi )   .<label>(5)</label></formula><p>This objective is similar to the one of <ref type="bibr" target="#b48">[49]</ref> and encourages large lambda values to be assigned to representations where intra-class similarity is high while the inter-class similarity is low. In practice, we optimize the objective by performing several steps of gradient descent. The proposed procedure is what we call SUR. It is worth noting that nearest centroid classifier is a simple non-parametric model with limited capacity, which only stores a single vector to describe a class. Such limited capacity becomes an advantage when only a few annotated samples are available, as it effectively prevents overfitting. When training and testing across similar domains, SUR is able to select from the multi-domain representation the features optimized for each visual domain. When the target domain distribution does not match any of the train distribution, it is nevertheless able to adapt a few parameters λ to the target distribution. In such a sense, our method performs a limited form of adaptation, with few parameters only, which is reasonable given that the target task has only a small number of annotated samples.</p><p>Sparsity in selection. As said above, our selection algorithm is a form of weak adaptation, where parameters λ are adjusted to optimize image representation, given an input task. Selection parameters λ i are constrained to be in [0, 1]. However, as we show in the experimental section, the resulting λ vector is sparse in practice, with many entries equal to 0 or 1. This empirical behavior suggests that our algorithm indeed performs selection of relevant features -a simple and robust form of adaptation. To promote further sparsity, one may use an additional sparsity-inducing penalty such as 1 during the optimization <ref type="bibr" target="#b25">[26]</ref>; however, our experiments show that doing so is not necessary to achieve good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Obtaining Multi-Domain Representations</head><p>In this section, we explain how to obtain a multi-Domain set of feature extractors {f i (·)} K i when one or multiple domains are available for training. Three variants are used in this paper, which are illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Multiple training domains. In the case of multiple training domains, we assume that K different datasets (including ImageNet <ref type="bibr" target="#b38">[39]</ref>) are available for building a multi-domain representation. We start with a straightforward solution and train a feature extractor f i (·) for each visual domain independently. That results in a desired multi-domain set {f i (·)} K i . A multi-domain representation is then computed by concatenating the output set, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(a).</p><p>To compute multi-domain representations with a single network, we use parametric network families proposed in <ref type="bibr" target="#b33">[34]</ref>. We follow the original paper and first train a general feature extractor -ResNet -using our training split of ImageNet, and then freeze the network's weights. For each of the K − 1 remaining datasets, task-specific parameters are learned by minimizing the corresponding training loss. We use FiLM <ref type="bibr" target="#b30">[31]</ref> layers as domain-specific modules and insert them after each batch-norm layer in the ResNet. We choose to use FiLM layers over originally proposed parallel adapters <ref type="bibr" target="#b33">[34]</ref> because FiLM is much simpler, i.e. performs channel-wise affine transformation, and contain much fewer parameters. This helps to avoid overfitting on small datasets. In summary, each of the K datasets has its own set of FiLM layers and the base ResNet with a set of domainspecific FiLM layers constitutes the multi-domain set of extractors {f i (·)} K i . To compute the features of i-th domain f i (x), we forward the input image x trough the ResNet where all the intermediate activations are modulated with the FiLM layers trained on this domain, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(b). Using parametric network families instead of K separate networks to obtain a multidomain representation reduces the number of stored parameters roughly in K times. However, to be actually useful for few-shot learning, such representation must be processed by our SUR approach, as described in Section 3.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now present the experiments to analyze the performance of our selection strategy, starting with implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experiments Details</head><p>Datasets. We use mini-ImageNet <ref type="bibr" target="#b32">[33]</ref> and Meta-Dataset <ref type="bibr" target="#b45">[46]</ref> to evaluate the proposed approach. The mini -ImageNet <ref type="bibr" target="#b32">[33]</ref> dataset consists of 100 categories (64 for training, 16 for validation, 20 for testing) from the original ImageNet <ref type="bibr" target="#b38">[39]</ref> dataset, with 600 images per class. Since all the categories come from the same dataset, we use mini -ImageNet to evaluate our feature selection strategy in single-domain few-shot learning. During testing on mini -ImageNet, we measure performance over tasks where only 1 or 5 images (shots) per category are given for adaptation and the number of classes in a task is fixed to 5, i.e. 5-way classification. All images are resized to 84 × 84, as suggested originally by <ref type="bibr" target="#b32">[33]</ref>.</p><p>Meta-Dataset <ref type="bibr" target="#b45">[46]</ref> is much larger than previous few-shot learning benchmarks and it is actually a collection of multiple datasets with different data distributions. It includes ImageNet <ref type="bibr" target="#b38">[39]</ref>, Omniglot <ref type="bibr" target="#b19">[20]</ref>, Aircraft <ref type="bibr" target="#b26">[27]</ref>, CU-Birds <ref type="bibr" target="#b47">[48]</ref>, Describable Textures <ref type="bibr" target="#b4">[5]</ref>, Quick Draw <ref type="bibr" target="#b16">[17]</ref>, Fungi <ref type="bibr" target="#b42">[43]</ref>, VGG-Flower <ref type="bibr" target="#b28">[29]</ref>, Traffic Sign <ref type="bibr" target="#b14">[15]</ref> and MSCOCO <ref type="bibr" target="#b22">[23]</ref>. A short description of each dataset is contained in Appendix. Traffic Sign and MSCOCO datasets are reserved for testing only, while all other datasets have their corresponding train, val and test splits. To better study out-of-training-domain behavior, we follow <ref type="bibr" target="#b36">[37]</ref> and add 3 more testing datasets, namely MNIST [50] CIFAR10 <ref type="bibr" target="#b17">[18]</ref>, and CIFAR100 <ref type="bibr" target="#b17">[18]</ref>. Here, the number of shots and ways is not fixed and varies from one few-shot task to another. As originally suggested <ref type="bibr" target="#b45">[46]</ref>, all images are resized to 84 × 84 resolution.</p><p>Implementation Details. When experimenting with Meta-Dataset, we follow <ref type="bibr" target="#b36">[37]</ref> and use ResNet18 <ref type="bibr" target="#b13">[14]</ref> as feature extractor. The training detail for each dataset are described in Appendix. To report test results on Meta-Dataset, we perform an independent evaluation for each of the 10 provided datasets, plus for 3 extra datasets as suggested by <ref type="bibr" target="#b36">[37]</ref>. We follow <ref type="bibr" target="#b45">[46]</ref> and sample 600 tasks for evaluation on each dataset within Meta-Dataset.</p><p>When experimenting with mini -ImageNet, we follow popular works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>  Feature Selection. To perform feature selection from the multi-domain representation we optimize the selection parameter λ (defined in Eq. 2) to minimize NCC classification loss (Eq. 4) on the support set. Each individual scalar weight λ i is kept between 0 and 1 using sigmoid function, i.e. λ i = sigmoid(α i ). All α i are initialized with zeros. We optimize the parameters α i using gradient descent for 40 iterations. At each iteration, we use the whole support set to build nearest centroid classifier, and then we use the same set of examples to compute the loss, given by Eq. 4. Then, we compute gradients w.r.t [α 1 , ..., α K ] and use Adadelta <ref type="bibr" target="#b52">[53]</ref> optimizer with learning rate 10 2 to perform parameter updates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Domain Few-shot Classification</head><p>In this section, we evaluate the ability of SUR to handle different visual domains in MetaDataset <ref type="bibr" target="#b45">[46]</ref>. First, we motivate the use of multi-domain representations and show the importance of feature selection. Then, we evaluate the proposed strategy against important baselines and state-of-the-art few-shot algorithms. is obtained from a multi-domain feature set by concatenation. It is then multiplied by the selection parameters λ, that are being optimized for each new few-shot task, following Section 4.1. We ran this procedure on all 13 testing datasets and report the results in <ref type="figure">Figure 3</ref> (a). We compare our method with the following baselines: a) using a single feature extractor pre-trained on ImageNet split of MetaDataset (denoted "ImageNet-F"), b) using a single feature extractor pretrained on the union of 8 training splits in MetaDataset (denoted "Union-F") and c) manually setting all λ i = 1, which corresponds to simple concatenation and (denoted "Concat-F"). It is clear that the features provided by SUR have much better performance than any of the baselines on seen and unseen domains.</p><p>Comparison to other approaches. We now compare SUR against state-ofthe-art few-shot methods and report the results in <ref type="table" target="#tab_4">Table 2</ref>. The results on MNIST, CIFAR 10 and CIFAR 100 datasets are missing for most of the approaches because those numbers were not reported in the corresponding original papers.</p><p>Comparison to the best-performing methods on common datasets is summarized in <ref type="figure">Figure 3 (b)</ref>. We see that SUR demonstrated state-of-the-art results on 9 out of 13 datasets. BOHNB-E <ref type="bibr" target="#b40">[41]</ref> outperforms our approach on Birds, Textures and VGG Flowers datasets. This is not surprising since these are the only datasets that benefit from ImageNet features more than from their own (see <ref type="table" target="#tab_1">Table 1</ref>  Multi-domain representations with parametric network family. While it is clear that SUR outperforms other approaches, one may raise a concern that the improvement is due to the increased number of parameters, that is, we use 8 times more parameters than in a single ResNet18. To address this concern, we use a parametric network family <ref type="bibr" target="#b33">[34]</ref> that has only 0.5% more parameters than a single ResNet18. As described in Section 3.3, the parametric network family uses ResNet18 as a base feature extractor and FiLM <ref type="bibr" target="#b30">[31]</ref> layers for feature modulation. The total number of additional parameters, represented by all domain-specific FiLM layers is approximately 0.5% of ResNet18 parameters. For comparison, CNAPs adaptation mechanism is larger than ResNet18 itself. To train the parametric network family, we first train a base CNN feature extractor on ImageNet. Then, for each remaining training dataset, we learn a set of FiLM layers, as detailed in Section 3.3. To obtain a multi-domain feature set for an image, we run inference 8 times, each time with a set of FiLM layers, corresponding to a different domain, as described in 3.3. Once the multi-domain feature is built, our selection mechanism is applied to it as described before (see Section 4.1). The results of using SUR with a parametric network family are presented in <ref type="table" target="#tab_4">Table 2</ref> as "SUR-pf". The table suggests that the accuracy on datasets similar to ImageNet is improved suggesting that parameter sharing is beneficial in this case and confirms the original findings of <ref type="bibr" target="#b33">[34]</ref>. However, the opposite is true for different visual domains such as Fungi and QuickDraw. It implies that to do well on very different domains, the base CNN filters must be learned on those datasets from scratch, and feature modulation is not competitive.</p><p>Reducing the number of training domains. Another way to increase SUR's efficiency is to use fewer domains. This means training fewer domain-specific feature extractors, faster inference and selection. To achieve this goal, we merge similar datasets together into separate visual domains, and train a single feature extractor for each such domain. Here, we take 8 datasets within MetaDataset and use them to form 3 following visual domains: [ImageNet, Aircraft, CU-Birds, Textures, VGG-Flower], [Omniglot, Quick Draw], [Fungi, ImageNet, Aircraft]. Then, we train 3 feature extractors, one for each such domain. These 3 feature extractors constitute the multi-domain feature set that is then used for selection with SUR, as described in Section 4.1. Doing so leads to a 2x speed-up in training and inference. Moreover, as <ref type="table" target="#tab_4">Table 2</ref> suggests, using 'SUR-merge' allows us to achieve better performance. We attribute this to learning better representation thanks to the increased number of relevant training samples per feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Feature Selection</head><p>In this section, we analyze optimized selection parameters λ when applying SUR on MetaDataset. Specifically, we perform the experiments from Section 4.2, where we select appropriate representations from a set, generated by 8 independent networks. For each test dataset, we then average selection vectors λ (after being optimized for 40 SGD steps) over 600 test tasks and present them in <ref type="figure" target="#fig_1">Figure 4</ref>. First, we can see that the resulting λ are sparse, confirming that most of the time SUR actually select a few relevant features rather than takes all features with similar weights. Second, for a given test domain, SUR tends to select feature  <ref type="table">Table 3</ref>: Comparison to other methods on 1-and 5-shot mini -ImageNet. The first column specifies a way of training a feature extractor, while the second column reflects how the final image representation is constructed. The two last columns display the accuracy on 1-and 5-shot learning tasks. The average is reported over 1 000 independent experiments with 95% confidence interval. The best accuracy is in bold.</p><p>extractors trained on similar visual domains. Interestingly, for datasets coming from exactly the same distribution, i.e. CIFAR 10 and CIFAR 100, the averaged selection parameters are almost identical. All of the above suggests that the selection parameters could be interpreted as encoding the importance of features visual domains for the test domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Single-domain Few-shot Classification</head><p>In this section, we show that our feature selection strategy can be effective for various problems, not just for multi-domain few-shot learning. As an example, we demonstrate the benefits of SUR on few-shot classification, when training and testing classes come from the same dataset. Specifically, we show how to use our selection strategy in order to improve existing adaptation-free methods.</p><p>To test SUR in the single-domain scenario we use mini -ImageNet benchmark and solve 1-shot and 5-shot classification, as described in Section 4.1. When only one domain is available, obtaining a truly multi-domain set of features is not possible. Instead, we construct a proxy for such a set by using activations of network's intermediate layers. In this approximation, each intermediate layer provides features corresponding to some domain. Since different layers extract different features, selecting the relevant features should help on new tasks. We experiment with 3 adaptation-free methods. They all use the last layer of ResNet12 as image features and build a NCC on top, however, they differ in a way the feature extractor is trained. The method we call "Cls", simply trains ResNet12 for classification on the meta-training set. The work of <ref type="bibr" target="#b21">[22]</ref> performs dense classification instead (dubbed "DenseCls"). Finally, the "Robust20-dist" feature extractor <ref type="bibr" target="#b6">[7]</ref> is obtained by ensemble distillation. For any method, the "multi-domain" feature set is formed from activations of the last 6 layers of the network. This is because the remaining intermediate layers do not contain useful for the final task information, as we show in Appendix.</p><p>Here, we explore different ways of exploiting such "multi-domain" set of features for few-shot classification and report the results in <ref type="table">Table 3</ref>. We can see that using SUR to select appropriate for classification layers usually works better than using only the penultimate layer (dubbed "last") or concatenating all the features together (denoted as "concat"). For Robust20-dist, we observe only incremental improvements for 5-shot classification and negative improvement in 1-shot scenario. The penultimate layer in this network is probably the most useful for new problems and, if not selected, may hinder the final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>This work was funded in part by the French government under management of Agence Nationale de la Recherche as part of the "Investissements davenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute) and reference ANR-19-P3IA-0003 (3IA MIAI@Grenoble Alpes), and was supported by the ERC grant number 714381 (SOLARIS) and a gift from Intel.  <ref type="bibr" target="#b17">[18]</ref> (100 classes of common objects). <ref type="figure" target="#fig_2">Figure 5</ref> illustrated random samples drawn from each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation and Datasets details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MetaDataset training details</head><p>When using multiple ResNet18 on MetaDataset (a single ResNet per dataset) to build a multi-domain representation, we train the networks according to the following procedure. For optimization, we use SGD with momentum and adjust the learning rate using cosine annealing <ref type="bibr">[?]</ref>. The starting learning rate, the maximum number of training iterations ("Max iter.") and annealing frequency ("annealing freq.") are set individually for each dataset. To regularize training, we use data augmentation, such as random crops and random color augmentations, and set a constant weight decay of 7 × 10 −4 . For each dataset, we run a grid search over batch size in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64]</ref> and pick the one that maximizes accuracy on the validation set. The hyper-parameters maximizing the validation accuracy are given in <ref type="table" target="#tab_8">Table 4</ref>.</p><p>When training a parametric network family for building multi-domain representations, we start by adopting a ResNet18 already trained on ImageNet, that we keep fixed for the rest of the training procedure. For each new dataset, we then train a set of domain-specific FiLM layers, modulating intermediate ResNet layers, as described in Section 3.3 of the original paper. Here, we also use cosine annealing as learning rate policy, employ weight decay and data augmentation, as specified above. In <ref type="table" target="#tab_9">Table 5</ref>, we report the training hyper-parameters for each of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 mini-ImageNet training details</head><p>All the methods we evaluate on mini -ImageNet use ResNet12 <ref type="bibr" target="#b29">[30]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments and Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Additional results on MetaDataset</head><p>Here we elaborate on using SUR with a multi-domain set of representations obtained from independent feature extractors (see Section 3.2), report an ablation study on varying the number of extractors in the multi-domain set, and report detailed results, corresponding to <ref type="figure">Figure 3</ref> (a) of the original paper. Specifically, we use 8 domain-specific ResNet18 feature extractors to build a multi-domain representation and evaluate SUR against the baselines. The results are reported in <ref type="table" target="#tab_10">Table 6</ref>, which corresponds to <ref type="figure">Figure 3</ref> (a) of the original paper. In the following experiment, we remove feature extractors trained on Birds, Textures and VGG Flower from the multi-domain feature set and test the performance of SUR on the set of remaining 5 feature extractors. We chose to remove these feature extractors as none of them gives the best performance on any of the test sets. Hence, they probably do not add new knowledge to the multi-domain set of features. The results are reported in  can see, selecting from the truncated set of features may be beneficial for some out-of-domain categories, which suggests that even the samples form of adaptation -selection -may overfit when very few samples are available. On the other hand, for a new dataset Traffic Sign, selecting from all features is beneficial. This result is not surprising, as one generally does not know what features will be useful for tasks not known beforehand, and thus removing seemingly useless features may result in a performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Analysis of Feature Selection on MetaDataset</head><p>Here, we repeat the experiment from Section 4.3, i.e. studying average values of selection parameters λ depending on the test dataset. <ref type="figure" target="#fig_3">Figure 6</ref> reports the average selection parameters with corresponding confidence intervals. This is in contrast to <ref type="figure" target="#fig_1">Figure 4</ref> of the original paper that reports the average values only, without confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Importance of Intermediate Layers on mini-ImageNet</head><p>We clarify the findings in Section 4.4 of the original paper and provide an ablation study on the importance of intermediate layers activations for the meta-testing performance. For all experiments on mini -ImageNet, we use ResNet12 as a feature extractor and construct a multi-domain feature set from activations of intermediate layers. In <ref type="table">Table 7</ref>, we experiment with adding different layers outputs to the multi-domain set. The multi-domain set is then used to construct the final image representation either through concatenation "concat" or using <ref type="table">Table 7</ref>: Comparison to other methods on 1-and 5-shot mini -ImageNet.</p><p>The first column gives the name of the feature extractor. Columns 2-5 indicate if corresponding layers of ResNet12 were added to the multi-domain set of representations.</p><p>Column "Aggregation" specifies how the multi-domain set was used to obtain a vector image representation. The two last columns display the accuracy on 1-and 5-shot learning tasks. To evaluate our methods we performed 1 000 independent experiments on mini-ImageNet-test and report the average and 95% confidence interval. The best accuracy is in bold.</p><p>SUR. The table suggests that adding the first 6 layers negatively influences the performance of the target task. While our SUR approach can still select relevant features from the full set of layers, the negative impact is especially pronounced for the "concat" baseline. This suggests that the first 6 layers do not contain useful for the test task information. For this reason, we do not include them in the multi-domain feature set, when reporting the results in Section 4.4.</p><p>We further provide analysis of selection coefficients assigned to different layers in <ref type="figure" target="#fig_4">Figure 7</ref>. We can see that for all methods, SUR picks from the last 6 layers most of the time. However, it can happen that some of the earlier layers are selected too. According to <ref type="table">Table 7</ref>, these cases lead to a decrease in performance and suggest the SUR may overfit, when the number of samples if very low.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Different ways of obtaining a multi-domain representation. (a) A single image is embedded with multiple domain-specific networks. (b) Using a parametric network family<ref type="bibr" target="#b33">[34]</ref> to obtain multi-domain representations. Here, the gray blocks correspond to shared computations and colored blocks correspond to domain-specific FiLM<ref type="bibr" target="#b30">[31]</ref> layers. Gray blocks and arrows indicate shared layers and computation flow respectively, while domain specific ones are shown in corresponding colors. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Frequency of selected features depending on the test domain in Meta-Dataset. The top row indicates a testing dataset. The leftmost column a dataset the feature extractor has been trained on. A cells at location i, j reflects the average value of selection parameter λi assigned to the i-th feature extractor when tested on j-th dataset. The values are averaged over 600 few-shot test tasks for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Samples from all MetaDataset datasets Each line gives 8 random samples from a dataset specified above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Frequency of selected features depending on the test domain in Meta-Dataset. The top row indicates a testing dataset. The leftmost column presents a dataset the feature extractor has been trained on. A cells at location i, j reflects the average value of selection parameter λi assigned to the i-th feature extractor when tested on j-th dataset with corresponding 95% confidence intervals. The values are averaged over 600 few-shot test tasks for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Frequency of selecting intermediate layer's activation's on mini-ImageNet for 5-shot classification. The top row indicates intemediate layer. The leftmost column gives the name of a method used to pre-train the feature extractor. Each cells reflects the average value of selection parameter λi assigned to the i-th intemediate layer with corresponding 95% confidence intervals. The values are averaged over 1000 few-shot test tasks for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and use ResNet12 [30] as a feature extractor. Corresponding training details are reported in Appendix. During testing, we use mini -ImageNet's test set to sample 1000 5-way classification tasks. We evaluate scenarios where only 1 or 5 examples (shots) of each category is provided for training and 15 for evaluation. On both datasets, during meta-training, we use cosine classifier with learnable softmax temperature [4]. During testing, classes and corresponding train/test examples are sampled at random. For all our experiments, we report the mean accuracy (in %) over all test tasks with 95% confidence interval.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Features trained on:</cell></row></table><note>Performance of feature extractors trained with different datasets on Meta-Dataset. The first column indicates the dataset in Meta-Dataset, the first row gives the dataset, used to pre-train the feature extractor. The body of the table shows feature extractors' accuracy on few-shot classification, when applied with NCC. The average accuracy and 95% confidence intervals computed over 600 few-shot tasks. The numbers in bold indicate that a method has the best accuracy per dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Among the 8 datasets seen during training, 5 datasets are better solved with their own features, while 3 other datasets benefit more from ImageNet features. In general, ImageNet features suits 8 out of 13 test datasets best, while 5 others require a different feature extractor for better accuracy. Such results suggest that none of the domain-specific feature extractors alone can perform equally well on all the datasets simultaneously. However, using the whole multi-domain feature set to select appropriate representations would lead to superior accuracy.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell></row><row><cell>Fig.</cell><cell></cell></row><row><cell cols="2">Evaluating feature selection. We now employ SUR -our strategy for feature</cell></row><row><cell cols="2">selection -as described in Sec. 3.2. A parametrized multi-domain representation</cell></row></table><note>Evaluating domain-specific feature extractors. MetaDataset includes 8 datasets for training, i.e. ImageNet, Omniglot, Aircraft, CU-Birds, Textures, Quick Draw, Fungi, VGG-Flower. We treat each dataset as a separate visual domain and obtain a multi-domain set of features by training 8 domain-specific feature extractors, i.e. a separate ResNet18 for each dataset. Each extractor is trained independently, with its own training schedule specified in Appendix. We test the performance of each feature extractor (with NCC) on every test dataset specified in Section 4.1, and report the results in Table 1.3: Performance of different few-shot methods on MetaDataset. (a) Com- parison of our selection strategy to baselines. The chart is generated from the table in Appendix. (b) Comparing our selection strategy to state-of-the-art few-shot learning methods. The chart is generated from Table 2. The axes indicate the accuracy of methods on a particular dataset. Each color corresponds to a different method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>); and BOHNB-E<ref type="bibr" target="#b40">[41]</ref> is essentially an ensemble of multiple ImageNet-pretrained networks. When tested outside the training domain, SUR consistently outperforms CNAPs<ref type="bibr" target="#b36">[37]</ref> -the state-of-the-art adaptation-based method. Moreover, SUR shows the best results on all 5 datasets never seen during training.</figDesc><table><row><cell>Test Dataset</cell><cell>ProtoNet [44]</cell><cell>MAML [9]</cell><cell>Proto-MAML [46]</cell><cell>CNAPs [37]</cell><cell>BOHB-E [41]</cell><cell>SUR (ours)</cell><cell>SUR-pf (ours)</cell><cell>SUR-merge (ours)</cell></row><row><cell>ImageNet</cell><cell cols="2">44.5±1.1 32.4±1.0</cell><cell>47.9±1.1</cell><cell cols="4">52.3±1.0 55.4±1.1 56.3±1.1 56.4±1.2</cell><cell>57.2±1.1</cell></row><row><cell>Omniglot</cell><cell cols="2">79.6±1.1 71.9±1.2</cell><cell>82.9±0.9</cell><cell cols="4">88.4±0.7 77.5±1.1 93.1±0.5 88.5±0.8</cell><cell>93.2±0.8</cell></row><row><cell>Aircraft</cell><cell cols="2">71.1±0.9 52.8±0.9</cell><cell>74.2±0.8</cell><cell cols="4">80.5±0.6 60.9±0.9 85.4±0.7 79.5±0.8</cell><cell>90.1±0.8</cell></row><row><cell>Birds</cell><cell cols="2">67.0±1.0 47.2±1.1</cell><cell>70.0±1.0</cell><cell cols="4">72.2±0.9 73.6±0.8 71.4±1.0 76.4±0.9</cell><cell>82.3±0.8</cell></row><row><cell>Textures</cell><cell cols="2">65.2±0.8 56.7±0.7</cell><cell>67.9±0.8</cell><cell cols="4">58.3±0.7 72.8±0.7 71.5±0.8 73.1±0.7</cell><cell>73.5±0.7</cell></row><row><cell>Quick Draw</cell><cell cols="2">65.9±0.9 50.5±1.2</cell><cell>66.6±0.9</cell><cell cols="4">72.5±0.8 61.2±0.9 81.3±0.6 75.7±0.7</cell><cell>81.9±1.0</cell></row><row><cell>Fungi</cell><cell cols="2">40.3±1.1 21.0±1.0</cell><cell>42.0±1.1</cell><cell cols="4">47.4±1.0 44.5±1.1 63.1±1.0 48.2±0.9</cell><cell>67.9±0.9</cell></row><row><cell cols="3">VGG Flower 86.9±0.7 70.9±1.0</cell><cell>88.5±1.0</cell><cell cols="4">86.0±0.5 90.6±0.6 82.8±0.7 90.6±0.5</cell><cell>88.4±0.9</cell></row><row><cell>Traffic Sign</cell><cell cols="2">46.5±1.0 34.2±1.3</cell><cell>34.2±1.3</cell><cell cols="4">60.2±0.9 57.5±1.0 70.4±0.8 65.1±0.8</cell><cell>67.4±0.8</cell></row><row><cell>MSCOCO</cell><cell cols="2">39.9±1.1 24.1±1.1</cell><cell>24.1±1.1</cell><cell cols="4">42.6±1.1 51.9±1.0 52.4±1.1 52.1±1.0</cell><cell>51.3±1.0</cell></row><row><cell>MNIST</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.7±0.4</cell><cell>-</cell><cell cols="2">94.3±0.4 93.2±0.4</cell><cell>90.8±0.5</cell></row><row><cell>CIFAR 10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.5±0.7</cell><cell>-</cell><cell cols="2">66.8±0.9 66.4±0.8</cell><cell>66.6±0.8</cell></row><row><cell>CIFAR 100</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.1±1.0</cell><cell>-</cell><cell cols="2">56.6±1.0 57.1±1.0</cell><cell>58.3±1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison to existing methods on Meta-Dataset. The first column indicates the of a dataset used for testing. The first row gives a name of a few-shot algorithm. The body of the table contains average accuracy and 95% confidence intervals computed over 600 few-shot tasks. The numbers in bold have intersecting confidence intervals with the most accurate method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>±0.41 60.09 ±0.61 concat 75.67 ±0.41 57.15 ±0.61 SUR 79.25 ±0.41 60.79 ±0.62 DenseCls last 78.25 ±0.43 62.61 ±0.61 concat 79.59 ±0.42 62.74 ±0.61 SUR 80.04 ±0.41 63.13 ±0.62 ±0.41 64.14 ±0.62 concat 80.79 ±0.41 63.22 ±0.63 SUR 81.19 ±0.41 63.93 ±0.63</figDesc><table><row><cell>Method</cell><cell>Aggregation</cell><cell>5-shot</cell><cell>1-shot</cell></row><row><cell cols="3">Cls 76.28 Robust20-dist last last 81.06</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>as a feature extractor. It is trained with batch size 200 for 48 epochs. For optimization, we use Adam optimizer [?] with initial learning rate 0.1 which is kept constant for the first 36 epochs. Between epochs 36 and 48, the learning rate was exponentially decreased from 0.1 to 10 −5 , i.e. by dividing the learning rate by 10 1 3 after each epoch. As regularization, we use weight decay with 5 × 10 −4 multiplier and data augmentation such as random crops, flips and color transformations. Dataset learning rate weight decay Max iter. annealing freq. batch size</figDesc><table><row><cell>Test ImageNet</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>480,000</cell><cell>48,000</cell><cell>64</cell></row><row><cell>Omniglot</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>50,000</cell><cell>3,000</cell><cell>16</cell></row><row><cell>Aircraft</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>50,000</cell><cell>3,000</cell><cell>8</cell></row><row><cell>Birds</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>50,000</cell><cell>3,000</cell><cell>16</cell></row><row><cell>Textures</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>50,000</cell><cell>1,500</cell><cell>32</cell></row><row><cell>Quick Draw</cell><cell>1 × 10 −2</cell><cell>7 × 10 −4</cell><cell>480,000</cell><cell>48,000</cell><cell>64</cell></row><row><cell>Fungi</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>480,000</cell><cell>15,000</cell><cell>32</cell></row><row><cell>VGG Flower</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>50,000</cell><cell>1,500</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Training hyper-parameters of individual feature networks on Meta-Dataset. The first column indicates the dataset used for training. The first row gives the name of he hyper-parameter. The body of the table contains hyper-parameters that produced the most accurate model on the validation set.</figDesc><table><row><cell cols="6">Test Dataset learning rate weight decay Max iter. annealing freq. batch size</cell></row><row><cell>Omniglot</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>40,000</cell><cell>3,000</cell><cell>16</cell></row><row><cell>Aircraft</cell><cell>1 × 10 −2</cell><cell>7 × 10 −4</cell><cell>30,000</cell><cell>1,500</cell><cell>32</cell></row><row><cell>Birds</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>30,000</cell><cell>1,500</cell><cell>16</cell></row><row><cell>Textures</cell><cell>3 × 10 −2</cell><cell>7 × 10 −4</cell><cell>40,000</cell><cell>1,500</cell><cell>16</cell></row><row><cell>Quick Draw</cell><cell>1 × 10 −2</cell><cell>7 × 10 −4</cell><cell>400,000</cell><cell>15,000</cell><cell>32</cell></row><row><cell>Fungi</cell><cell>1 × 10 −2</cell><cell>7 × 10 −4</cell><cell>400,000</cell><cell>15,000</cell><cell>32</cell></row><row><cell>VGG Flower</cell><cell>1 × 10 −2</cell><cell>7 × 10 −4</cell><cell>30,000</cell><cell>3,000</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Training hyper-parameters of the parametric network family on MetaDataset. The first column indicates the dataset used for training. The first row gives the name of he hyper-parameter. The body of the table contains hyper-parameters that produced the most accurate model on the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>(a) as "SUR (5/8)". As we</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Motivation for feature selection. The table shows accuracy of different feature combinations on the Meta-Detaset test splits. The first column indicates the dataset the algorithms are tested on, the first row gives a name of a few-shot algorithm. The body of the table contains average accuracy and 95% confidence intervals computed over 600 few-shot tasks. The numbers in bold lie have intersecting confidence intervals with the most accurate method.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A kernel perspective for regularizing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<title level="m">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01613</idno>
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>InternationalConferenceonMachineLearning(ICML</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K J K</forename><surname>Fox-Gieg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<ptr target="quickdraw.withgoogle.com" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse modeling for image and vision processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Computer Graphics and Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<title level="m">Meta-learning for semi-supervised few-shot classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Optimized generic feature learning for few-shot classification across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shifting inductive bias with successstory algorithm, adaptive levin search, and incremental self-improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fgvcx fungi classification challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="github.com/visipedia/fgvcx_fungi_comp" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Yann Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10941</idno>
		<title level="m">Spectral norm regularization for improving the generalizability of deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In: Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<title level="m">The visual task adaptation benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
