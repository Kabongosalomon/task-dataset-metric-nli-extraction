<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Multi-Task Learning for Face Recognition with Facial Expression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuheng</forename><surname>Ming</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">L3i</orgName>
								<orgName type="institution" key="instit2">University of La Rochelle</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Xia</surname></persName>
							<email>jushi.xia@riken.jp</email>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project (AIP)</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<postCode>103-0027</postCode>
									<region>Tokyo</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Muzzamil</forename><surname>Luqman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">L3i</orgName>
								<orgName type="institution" key="instit2">University of La Rochelle</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Burie</surname></persName>
							<email>jcburie@univ.lr-fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">L3i</orgName>
								<orgName type="institution" key="instit2">University of La Rochelle</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixing</forename><surname>Zhao</surname></persName>
							<email>kaixing.zhao@irit.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">IRIT</orgName>
								<orgName type="institution" key="instit2">University of Toulouse</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Multi-Task Learning for Face Recognition with Facial Expression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Benefiting from the joint learning of the multiple tasks in the deep multi-task networks, many applications have shown the promising performance comparing to single-task learning. However, the performance of multi-task learning framework is highly dependant on the relative weights of the tasks. How to assign the weight of each task is a critical issue in the multi-task learning. Instead of tuning the weights manually which is exhausted and time-consuming, in this paper we propose an approach which can dynamically adapt the weights of the tasks according to the difficulty for training the task. Specifically, the proposed method does not introduce the hyperparameters and the simple structure allows the other multi-task deep learning networks can easily realize or reproduce this method. We demonstrate our approach for face recognition with facial expression and facial expression recognition from a single input image based on a deep multi-task learning Conventional Neural Networks (CNNs). Both the theoretical analysis and the experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning method. This multi-task learning with dynamic weights also boosts of the performance on the different tasks comparing to the state-of-art methods with single-task learning. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-task learning has been used successfully across many areas of machine learning <ref type="bibr" target="#b26">[26]</ref>, from natural language processing and speech recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> to computer vision <ref type="bibr" target="#b9">[10]</ref>. By joint learning in multiple tasks in the related domains with different information, especially from information-rich tasks to information-poor ones, the multi-task learning can capture a representation of features being difficult learned by one task but can easily learned by another task <ref type="bibr" target="#b23">[23]</ref>. Thus the multi-task learning can be conducted not only for improving the performance of the systems which aims to predict multiple objectives but also can utilise for improving a specific task by leveraging the related domain-specific information contained in the auxiliary tasks. In this work, we explore the multi-learning for face recognition with facial expression. Thanks to the progress of the representing learning with the deep CNNs, face recognition has made remarkable progress in the recent decade <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b19">19]</ref>. These works have achieved or beyond the human-level performance on the benchmarks LFW <ref type="bibr" target="#b12">[13]</ref>, YTF <ref type="bibr" target="#b34">[34]</ref>. The challenges of face recognition such as the variation of the pose, the illumination and the occlusion have been well investigated in many researches, nevertheless face recognition for the face with the non-rigid deformation such as the ones introduced by the facial expression has not been sufficiently studied especially in the 2D face recognition domain. Some 3D based methods have been proposed to deal with this issue such as <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3]</ref>, in which <ref type="bibr" target="#b41">[41]</ref> presents the method by using the 3D facial model to normalise the facial expression and then maps the normalised face to the 2D image to employ face recognition. In order to leverage the promising progress in the numerous 2D face recognition and facial expression recognition researches particularly based on the deep neural networks, we propose to combine the face recognition task and the facial expression recognition task in the unified multi-task framework aiming to jointly learn the sharing features and task-specific features to boost the performance of each task. <ref type="figure">Figure 1</ref> shows the multi-task framework proposed in this work.</p><p>How to set the weights of tasks is a crucial issue in the multi-task learning. The weights determine the importance of the different tasks in the holistic networks. Many works simply set the equal values for all tasks or experimentally set the weights of the tasks. In <ref type="bibr" target="#b3">[4]</ref>, the authors assign equal weights to the ranking task and the binary classification task for the person re-identification. However the multi-task learning is an optimization problem for multiple objectives.  <ref type="figure">Figure 1</ref>: The proposed multi-task framework with dynamic weights of tasks to simultaneously perform face recognition and facial expression recognition. The dynamic weights of tasks can adapt automatically according to the importance of tasks.</p><p>The main task and the side tasks with different objective have different importance in the overall loss meanwhile the difficulty of the training of each task is also different. Thus it is arbitrary to assign equal weights for tasks for multi-task learning. We also verified this point in our work by manually setting the weights of tasks from 0 to 1 with the interval of 0.1. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, either for the facial expression recognition task or for the face recognition task, the best performance are obtained with the different weights of tasks rather than the equal weights of each task. Most of the multi-task learning methods search the optimal weights of the tasks by the experimental methods, for instance Hyperface <ref type="bibr" target="#b25">[25]</ref> manually set the weights of the tasks such as the face detection, landmarks localization, pose estimation and gender recognition according to their importance in the overall loss, and <ref type="bibr" target="#b32">[32]</ref> obtain the optimal weights by a greedy search for pedestrian detection tasks with the different attributes. Besides the cost of time and being exhausting, these methods setting the weights as the fix values to optimize the tasks ignore the variation of the the importance or the difficulty of the tasks during the training processing. Rather than the methods with fix weights which can be so called static weights methods, the methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">38]</ref> update the weights or part of the weights of the tasks during the training of the networks. <ref type="bibr" target="#b38">[38]</ref> set the weight of the main task as 1 while the auxiliary tasks are weighted by the dynamic weights λ t which are updated by an analytical solution. <ref type="bibr" target="#b15">[16]</ref> introduces a uncertainty coefficient θ to revise the softmax loss function of each task. Unlike these methods which need to introduce the additional hyperparameters to update the weights of tasks, we propose to use a softmax layer adding to the end of the hidden sharing layers of the multi-task networks to generate the dynamic weights of the tasks (see <ref type="figure">Figure 1</ref>). Each unit of this softmax layer is corresponding to a weight of a task and no more hyperparameter is introduced for updating the tasks weights. Rather than <ref type="bibr" target="#b36">[36]</ref> updating simultaneously the dynamic weights of tasks and the filters weights of the networks by the unify total loss of the networks, we propose a new loss function to update the dynamic weights which enable the networks to focus on the training of the hard task by automatically assigning a larger weight. On the contrary, <ref type="bibr" target="#b36">[36]</ref> always updates the smaller weight for the hard task and the larger weight for the easy task which results the hard task is far from being fully trained and the networks stuck in the worthless training of the over trained easy task. This is due to use the total loss of the networks to simultaneously update the weights of tasks, in which the dynamic weights of the tasks are also in the function of the weights of networks, i.e. L total (Θ) =</p><formula xml:id="formula_0">w 1 (Θ 0 )L 1 (Θ 1 )+w 2 (Θ 0 )L 2 (Θ 2 ) s.t. w 1 +w 2 = 1 and {Θ 0 , Θ 1 , Θ 2 } = Θ</formula><p>are the weights of the networks. The optimization of Θ by the total loss L total aims to decrease the total loss as much as possible, thus when the hard task has a large loss the fastest way to decrease the total loss is to shrinkage its weight w i so that the weighted loss of the hard task can be cut down rapidly. This is why the hard task always has a small task weight while the easy task has a large weight.</p><p>In a summary, our main contributions of this paper are.</p><p>• We propose a dynamic multi-task learning method which can automatically update the weight of task according to the importance of task during the training.</p><p>• Both the theoretical analysis and the experimental re-  sults demonstrate the proposed dynamic multi-task learning enable to focus on the training of the hard task to achieve better efficiency and performance.</p><p>• We have demonstrated that, for both face verification with facial expression and facial expression recognition tasks, the proposed multi-task learning can outperform the state-of-the-art performance on the datasets CK+ <ref type="bibr" target="#b21">[21]</ref>, OuluCASIA <ref type="bibr" target="#b39">[39]</ref>.</p><p>• The proposed method is simple and does not introduce the hyperparameters,which can be easily realized and reproduce in the other deep multi-task learning frameworks.</p><p>The remainder of this paper is organized as follows: Section II briefly reviews the related works; Section III describes the architecture of the dynamic multi-task network. Section IV presents the approach of multi-task learning with dynamic weights following by Section V where the experimental results are analyzed. Finally, in Section VI, we draw the conclusions and present the future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Multi-task learning not only helps to learn more than one task in a single network but also can improve upon your main task with an auxiliary task <ref type="bibr" target="#b26">[26]</ref>. In this work, we focus on the multi-task learning in the context of the deep CNNs. According to the means of updating the weights of tasks, the multi-task learning can be divided into two categories: the static method and dynamic method. In the static methods, the weights of tasks are set manually before the training of the networks and they are fixed during the whole training of the networks <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b32">32]</ref>; while the dynamic methods initialize the weights of the tasks at the beginning of the training and update the weights during the training processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b36">36]</ref>. There are two ways for setting the weights in the static methods. The first way is to simply set the equal weights of task such as Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">35]</ref>. In Fast R-CNN, the author uses a multi-task loss to jointly train the classification and bounding-box regression for object detection. The classification task is set as the main task and the bounding-box regression is set as the side task weighted by λ. In the experiments the λ is set to 1. The second way to set the weights is manually searching the optimal weights by the experimental methods. Hyperface <ref type="bibr" target="#b25">[25]</ref> proposed a multi-task learning algorithm for face detection, landmarks localization, pose estimation and gender recognition using deep CNNs. The tasks have been set the different weights according to the importance of the task. <ref type="bibr" target="#b3">[4]</ref> integrated the classification task and the ranking task in a multi-task networks for person reidentification problem. Each task has been set with a equal weight to jointly optimizing the two tasks simultaneously. Tian et al. <ref type="bibr" target="#b32">[32]</ref> fix the weight for the main task to 1, and obtain the weights of all side tasks via a greedy search within 0 and 1. In <ref type="bibr" target="#b4">[5]</ref> an additional loss in function of the gradient of the weighted losses of tasks is proposed to update the weights meanwhile an hyperparameter is introduced for balancing the training of different tasks. <ref type="bibr" target="#b15">[16]</ref> introduces a uncertainty coefficient θ to combine the multiple loss functions. The θ can be fixed manually or learned based on the total loss. Zhang et al. <ref type="bibr" target="#b38">[38]</ref> propose a multi-task networks for face landmarks detection and the recognition of the facial attributes. The face landmarks detection is set as the main task with the task weight 1 and the tasks for recognition of the different facial attributes are set as auxiliary tasks with dynamic weights λ t . An hyperparameter ρ as a scale factor is introduced to calculate the weight λ t . Yin et al. <ref type="bibr" target="#b36">[36]</ref> proposed a multi-task model for face pose-invariant recognition with an automatic learning of the weights for each task. The main task of is set to 1 and the auxiliary tasks are sharing the dynamic tasks generated by the softmax layer. However, the update of the weights of tasks by the total loss of the networks runs counter to the objective of the multi-task learning. Thanks to the progress of the representation learning based on the deep neural networks, the methods based on the deep CNNs such as Deep-Face <ref type="bibr" target="#b31">[31]</ref> , DeepIDs <ref type="bibr" target="#b30">[30]</ref>, Facenet <ref type="bibr" target="#b27">[27]</ref>, VGGFace <ref type="bibr" target="#b29">[29]</ref>, SphereFace <ref type="bibr" target="#b19">[19]</ref> have made a remarkable improvement comparing to the conventional methods based on the handcrafted features LBP, Gabor-LBP, HOG, SIFT <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">28]</ref>. The situation is same as facial expression recognition based on deep CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b22">22]</ref>. Even so, the studies on the face recognition with the facial expression images are limited. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b2">3]</ref> propose the 3D based methods to deal with this issue. Kakadiaris et al. <ref type="bibr" target="#b14">[15]</ref> present a fully automated framework for 3D face recognition using the Annotated Face Model to converted the raw image of face to a geometric model and a normal map. Then the face recognition is based on the processed image by using the Pyramid and Haar. Zhu et al. <ref type="bibr" target="#b41">[41]</ref> presents the method by using the 3D facial model to normalise the facial expression and then maps the normalised face to the 2D image to employ face recognition. Chang et al. <ref type="bibr" target="#b2">[3]</ref> describe a method using three different overlapping regions around the nose to employ the face recognition since this region is invariant in the presence of facial expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture</head><p>The proposed multi-task learning with dynamic weights is based on the deep CNNs (see <ref type="figure">Figure 1</ref>). The hard parameter sharing structure is adopted as our framework, in which the sharing hidden layers are shared between all tasks <ref type="bibr" target="#b26">[26]</ref>. The task-specific layers consisting of two branches are respectively dedicated to face verification and facial expression recognition. The two branches have almost identical structures facilitate the transfer learning of facial expression recognition from the pretrained face recognition task. Specifically, the BRANCH 1 can extract the embedded features of bottleneck layer for face verification and the BRANCH 2 uses the fully connected softmax layer to calculate the probabilities of the facial expressions. The deep CNNs in this work are based on the Inception-ResNet structure which have 13 million parameters of about 20 hidden layers in terms of the depth and 3 branches to the maximum in terms of the large. By the virtue of the Inception structure, the size of the parameters is much fewer than other popular deep CNNs such as VGGFace with 138 million parameters.</p><p>Dynamic-weight-unit The dynamic weights of tasks are generated by the softmax layer connecting to the end of the sharing hidden layers, which can be so called the Dynamicweight-unit. Each element in the Dynamic-weight-unit is corresponding to a weight of a task, thus the size of the Dynamic-weight-unit is equal to the number of weights of tasks, e.g. the size is 2 in this work. Since the weights are generated by the softmax layer, w1 + w2 = 1 which can well indicate the relative importance of the tasks. The parameters of this softmax layer are updated by the independent loss function L 3 during the training of the networks, which can automatically adjust the weights of tasks in light of the variation of the loss of tasks and drive the networks to always train the hard task firstly by assigning a larger weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-task learning with dynamic weights</head><p>The total loss of the proposed multi-task CNNs is the sum of the weighted losses of the multiple tasks.</p><p>(I) Multi-task loss L: The multi-task total loss L is defined as follows:</p><formula xml:id="formula_1">L(X; Θ; Ψ) = T i=1 w i (Ψ)L i (X i ; Θ i )<label>(1)</label></formula><p>where T is the number of the tasks, here T = 2. X i and Θ i are the feature and the parameters corresponding to each task, Θ = {Θ i } T i=1 are the overall parameters of the networks to be optimized by L. Ψ are the parameters of the softmax layer in the Dynamic-weight-unit used to generate the dynamic weights w i ∈ [0, 1] s.t.</p><formula xml:id="formula_2">w i = 1. Thus {X i , Θ i } ∈ R di , where d i is the dimension of the features X i , and {L i , w i } ∈ R 1 .</formula><p>Particularly, when w 1 = 1, w 2 = 0 the multi-task networks are degraded as the single-task networks for face verification (i.e. Branch 1 and sharing hidden layers) while w 1 = 0, w 2 = 1 is corresponding to the singletask networks for facial expression recognition (i.e. Branch 2 and sharing hidden layers).</p><p>(II) Face verification task loss L 1 : The loss for face verification task is measured by the center loss <ref type="bibr" target="#b33">[33]</ref> joint with the cross-entropy loss of softmax of Branch 1. The loss function of face verification task L 1 is given by:</p><formula xml:id="formula_3">L 1 (X 1 ; Θ 1 ) = L s1 (X 1 ; Θ 1 ) + αL c (X 1 ; Θ 1 )<label>(2)</label></formula><p>where L s1 is the cross-entropy loss of softmax of Branch 1, L c is the center loss weighted by the hyperparameter α. The L c can be treated as a regularization item of softmax loss L s1 which is given by:</p><formula xml:id="formula_4">L s1 (X 1 ; Θ 1 ) = K k=1 −y k logP (y k = 1|X 1 , θ k ) = − K k=1 y k log e f θ k (X1) K k e f θ k (X1)<label>(3)</label></formula><p>where K is the number of the classes, i.e. the number of identities in the training dataset, y k ∈ {0, 1} is the one-shot label of the feature X 1 , P (y k |X 1 , θ k ) is softmax function over the activation function f θ k (X 1 ) where {θ k } K k=1 = Θ 1 , θ k ∈ R d1 . The bottleneck layer of BRANCH 1 is extracted as the feature X 1 of the input image. The center loss L c is given by:</p><formula xml:id="formula_5">L c (X 1 ; Θ 1 ) = ||X 1 − C y k ||<label>(4)</label></formula><p>Where the C y k is the center of the class which X 1 belonging to, C y k ∈ R d1 . (III) Facial expression recognition task loss L 2 (X 2 ; Θ 2 ): The loss function of facial expression recognition task L 2 is the cross-entropy loss of the softmax layer of BRANCH 2. The equation of L 2 is as same as Equation 3 except the K in L 2 is the number of the categories of the facial expressions, X 2 is the bottleneck layer of BRANCH 2, Θ2 is corresponding parameters of this task.</p><p>(IV) Generation of the dynamic weights w i (Ψ): The dynamic weights w i are generated by the softmax layer of the dynamic-weight-unit which is given by:</p><formula xml:id="formula_6">w i (Z; Ψ) = e f ψ i (Z) T i e f ψ i (Z)<label>(5)</label></formula><p>where the Z ∈ R dz is the flat output of the last layer of the sharing hidden layers.T is the number of the tasks, here T =2. ψ i is parameters in the softmax layer of the dynamic-</p><formula xml:id="formula_7">weight-unit {ψ i } T i=1 = Ψ, ψ i ∈ R dz . f ψi (Z) is activation function which is given by: f ψi (Z) = ψ i Z T + b i<label>(6)</label></formula><p>Note that, we do not use the Relu function as the activation function since Relu discards the values minors zero. This shrinks the range of the variation of the dynamic weights w i . (V) Update of the dynamic weights w i : Rather than using the total loss to update the dynamic weights, we propose a new loss function to update the dynamic weights which can drive the networks always train the hard task. The proposed new loss function for updating the dynamic weights is given by:</p><formula xml:id="formula_8">L 3 (Z; Ψ) = T i=1 w i (ψ i ) L i (Θ i ) s.t. w i = 1<label>(7)</label></formula><p>Note that,</p><formula xml:id="formula_9">L i {Θ i } is independent with w i (ψ i ) since Θ i ∩ ψ i = ∅ , i ∈ [1,</formula><p>.., T ], thus L i is constant for the dynamic weight update loss function L 3 .</p><p>(VI) Qualitative analysis shows that when the loss of the task L i is small, i.e. the reciprocal of the L i is large, thus loss L 3 will try to reduce the loss by decreasing the value of w i . That is to say, when the task is easy with a small loss the weight of the task will be assigned by a small value. On the contrary, the hard task with a large loss will be assigned by a large weight, which enable the networks always focus on training the hard task firstly. The update of the dynamic weights w i is essentially the update of the parameters ψ i which generate the dynamic weights.</p><p>(VII) Quantitative analysis: Considering the Equation 5 and Equation 6, the gradient of the ψ i can be given by</p><formula xml:id="formula_10">∇ψ i = ∂L 3 ∂ψ i = 1 L i ∂w i (ψ i ) ∂ψ i = 1 L i a i T j =i a j ( T i a i ) 2 Z (8)</formula><p>where a i = e ψiZ T +bi , and the update of the parameters is ψ t+1 i = ψ t i − η∇ψ i t where η is the learning rate. Then the new value of the dynamic weight w t+1 i can be obtained by the Equation 5 and 6 with the ψ t+1 i . If we assume the b 0 i = 0, ψ 0 i = 0 (this is possible if we initialize the ψ i , b i by zero), the ψ t i can be given by</p><formula xml:id="formula_11">ψ t i = − 1 L i a i T j =i a j ( T i a i ) 2 Z<label>(9)</label></formula><p>if we consider the case for two tasks w 1 and w 2 when t = 1:</p><formula xml:id="formula_12">w t 1 w t 2 = e (ψ t 1 −ψ t 2 )Z T = e ( 1 L 2 − 1 L 1 ) a 1 a 2 (a 1 +a 2 ) 2 ZZ T<label>(10)</label></formula><p>We can see that a i &gt; 0 and ZZ T ≥ 0, so if L 2 &lt; L 1 the w1 w2 &gt; 1 namely w 1 &gt; w 2 . It means if the loss of task1 larger than the loss of task 2, the weight of the task1 is larger than the one of task2. It indicates that the proposed loss function L 3 can well update the weights of tasks to drive the networks always train the hard task firstly.</p><p>(VIII) Training protocol: The training of the entire deep CNNs includes two independent training: the training of the parameters of the networks Θ by the multi-task loss L(Θ) = 2 i=1 L i (θ i ) and the training of the parameters of weight-generate-module Ψ by the loss L 3 (Ψ). These can be conducted simultaneously in a parallel way.</p><formula xml:id="formula_13">Θ t−1 − η ∂L(Θ) ∂Θ → Θ t (11) Ψ t−1 − η ∂L 3 (Ψ) ∂Ψ → Ψ t<label>(12)</label></formula><p>where η ∈ (0, 1) is the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Since the proposed multi-task networks performs the face verification task and the facial expression recognition task simultaneously, the datasets including both identity labels and facial expression labels are necessary to the training and the evaluation of the model. However, the largescale datasets such as Celeb-A <ref type="bibr" target="#b20">[20]</ref> and FER2013 <ref type="bibr" target="#b11">[12]</ref> either do not include the facial expression or the identity labels. Finally 5184 (positive or negative) pairs of face images with both identity labels and facial expression labels  are extracted from OuluCASIA as well as 2099 pairs of images are extracted from CK+ to form two datasets respectively (see <ref type="figure" target="#fig_2">Fig. 3</ref> and <ref type="table" target="#tab_1">Table 1</ref> in which ID (identities), Neutral (Ne), Anger (An), Disgust (Di), Fear (Fe), Happy (Ha), Sad (Sa), Surprise (Su), Contempt (Co).).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental configuration</head><p>The faces have been detected by the MTCNN <ref type="bibr" target="#b37">[37]</ref> from the raw images. The RMSprop with the mini-batches of 90 samples are applied for optimizing the parameters. The learning rate is started from 0.1, and decay by 10 at the different iterations. The networks are initialized by Xavier <ref type="bibr" target="#b10">[11]</ref> and biases values are set to zero at beginning. The momentum coefficient is set to 0.99. The dropout with the probability of 0.5 and the weight decay of 5e-5 are applied. The weight of the center loss α is set to 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Pretrained model</head><p>Before the training of the proposed multi-task CNNs, a single-task network constituted of the sharing hidden layers and the BRANCH 1 is pretrained for face verification-task with large-scale dataset by loss function L 1 . Then the training of the dynamic multi-task CNNs can handling on the pretrained model. Moreover, in order to compare the multitask learning with the single-task learning, the BRANCH 2 is also trained independently by transferring the learning of the pretrained BRANCH 1 for facial expression recognition with loss function L 2 . Finally we obtain two models pretrained by the single-task learning for face verification (sharing layers + BRANCH 1) and facial expression recognition (sharing layers + BRANCH 2) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Dynamic multi-task learning / training</head><p>In order to distinguish our proposed method, we call the method in <ref type="bibr" target="#b36">[36]</ref> as naive dynamic method. Comparing to the naive dynamic method, the proposed dynamic method can adjust the weights of tasks according to their importance/training difficulty as shown in <ref type="figure">Figure 4</ref>. The training difficulty of the task is presented by its training loss. <ref type="figure">Figure 5</ref> shows the variation of the loss of tasks corresponding to the two different methods. From <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref>, we can see that the naive dynamic method always train the easy task namely facial expression recognition (denoted as Task 1) with smaller loss by assigning a large weight as shown in (a) on dataset CK+ or (c) on dataset OuluCASIA . However, the hard task namely face verification (denoted as Task 2) with large loss is always assigned by small weight less than 0.2. Contrarily, the weight of task generated by the proposed method can effectively adapt to the varied importance of the task in the multi-task learning. For instance, as shown in the (b) on dataset CK+, the hard task which is face verification (Task 2) with a large loss is assigned a large weight at the beginning of the training. The large weight of task drive the networks to fully train the hard task so that the loss of the hard task decreases rapidly and soon it is lower than the loss of the task of facial expression recognition (Task 1). Once the previous easy task become the hard task with a larger loss, the proposed method automatically assigns a larger weight to the current hard task as shown in (b) that the weight of the facial expression recognition (Task 1) augment promptly from the bottom to the top when the loss of the task becomes the larger one. Thus the networks are capable to switch to fully train the current hard task with the proposed dynamic method. <ref type="figure">Figure 6</ref> suggests that the proposed dynamic method can decrease the loss of the hard task, i.e. the face verification task, more quickly and achieve lower value of loss. For the easy task, namely the facial expression recognition task, these two methods decrease the loss similarly since the easy task can be sufficiently trained by both of the methods. Thus the proposed dynamic method is superior to the naive dynamic method in terms of the training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Gradient vanishing problem</head><p>In this section we analyse the problem of the gradient vanishing for updating the dynamic weights. From Equation 8, we can see that if the a i T j =i a j &lt;&lt; ( T i a i ) 2 , the ∇ψ i → 0 which means that the gradient vanishes. In this work, T = 2, if 0 &lt;&lt; a 2 1 + a 2 2 + a 1 a 2 , it will cause the problem of the gradient vanishing. Since the a i = e ψiZ T +bi &gt; 0, the condition of gradient vanishing is easy to satisfy provided the a i is relative large. In order to mitigate the problem of grand vanishing, we normalize the a i as the embedding feature for calculating the weights. As shown in (a) and (b) of <ref type="figure" target="#fig_5">Figure 7</ref>, the gradient vanishes when the a i is large than 8 * 10 6 . By applying the normalization of a i as shown in (d). the gradient return to the normal values as shown in (c). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Evaluation and ablation analysis</head><p>(I) Dynamic multi-task learning for face verification with facial expression To evaluate the effectiveness of the proposed dynamic multi-task learning method for face verification with facial expression, we firstly analyse the results from the single-task method with pretrained models trained on the general datasets and then the fine-tuning model based on the datasets used in this work. Furthermore, we compare the multi-task learning methods with manually setting weights (i.e. static multi-task learning), naive dynamic weights and our proposed dynamic weights to the singletask learning method. <ref type="table" target="#tab_2">Table 2</ref> firstly shows that the performance of the state-ofart methods such as DeepID, VGGFace, FaceNet, etc. with pretrained models for face verification with facial expression. Comparing to the performance on the general dataset such as LFW or YTF, we can see that the performances on the face images with facial expression in CK+ or Oulu-CASIA have degraded obviously, e.g. the face verification accuracy of DeepID has decreased from 99.47% on LFW to 91.70% on CK+, VGGFace has decreased from 98.95% on LFW to 92.20% on CK+ as well as FaceNet which is trained on the large scale dataset. This is quite probably resulted by the lack of the facial expression images in the general datasets for the training of the models. By fine-tuning our pretrained model with the facial expression datasets, the performance has improved. Thanks to the capacity of learning the features between the tasks, the multi-task learning with fixed weights further improve the performance comparing to the fine-tuning single task model. However, the performance of face verification with the naive dynamic multi-task learning is inferior to the static multi-task learning and even the fine-tuning single-task model, which is due to the face verification task has not been sufficiently trained with a small weight. Rather than the static multi-task learning method with the fix weights or the naive dynamic multitask learning, the proposed dynamic method can real-time update the weights of tasks according to the importance of tasks and achieve the best performance.</p><p>(II) Dynamic multi-task learning for facial expression recognition <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> compare the proposed dynamic multi-task learning for facial expression recognition with other methods on CK+ and OuluCASIA respectively. As well as the face verification task, the proposed dynamic multi-task learning achieves the best performance on both datasets. Since the facial expression recognition is the easy task, the naive dynamic multi-task learning has sufficiently trained this task and achieved the comparable results as the proposed method. The multi-task learning also show the significant improvement to the single-task methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a dynamic multi-task learning method which allows to dynamically update the weight of task according to the importance of the task during the training process. Comparing to the other multi-task learning  <ref type="figure">Figure 6</ref>: The training efficiency for decreasing the loss of task by the proposed dynamic multi-learning method and the naive dynamic multi-learning method on different tasks, i.e. face verification and facial expression recognition.  methods, our method does not introduce the hyperparameters and it enables the networks to focus on the training of the hard tasks which results a higher efficiency and better performance for training the multi-task learning networks. Either the theoretical analysis or the experimental results demonstrate the effectiveness of our method. This method can be also easily applied in the other deep multi-task learning frameworks such as Faster R-CNN for object detection. <ref type="table">Table 3</ref>: The evaluation of proposed multi-task networks for facial expression recognition task on dataset CK+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) LBPSVM <ref type="bibr" target="#b8">[9]</ref> 95.1 Inception <ref type="bibr" target="#b22">[22]</ref> 93.2 DTAGN <ref type="bibr" target="#b13">[14]</ref> 97.3 PPDN <ref type="bibr" target="#b40">[40]</ref> 97.3 AUDN <ref type="bibr" target="#b18">[18]</ref> 92.  <ref type="table">Table 4</ref>: The evaluation of proposed multi-task networks for facial expression recognition task on dataset OuluCASIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) HOG3D <ref type="bibr" target="#b16">[17]</ref> 70.63 AdaLBP <ref type="bibr" target="#b39">[39]</ref> 73.54 DTAGN <ref type="bibr" target="#b13">[14]</ref> 81.46 PPDN <ref type="bibr" target="#b40">[40]</ref> 84. Clusters, funded by the French government.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performances of the task of face verification for facial expression images (FV / blue) with the manually setting weight w1 and the task of facial expression recognition (EX / red) on the different datasets CK+ and OuluCASIA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The image from CK+ and OuluCASIA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>The weights of tasks generated by our proposed method and the naive dynamic method during the training. Task 1 is facial expression recognition and Task 2 is face verification. The loss of tasks corresponding to our proposed method and the naive dynamic method during the training. Task 1 is facial expression recognition and Task 2 is face verification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The gradient value before and after the normalization of a i . The normalization of the a i can mitigate the gradient vanishing problem caused by the large value of a i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1911.03281v1 [cs.CV] 8 Nov 2019</figDesc><table><row><cell></cell><cell cols="2">BRANCH 1</cell><cell>Face verificatoin</cell><cell></cell></row><row><cell>3x3x32</cell><cell cols="2">Inception block2</cell><cell>Loss1</cell><cell>*</cell><cell>W1 W1</cell></row><row><cell></cell><cell>Softmax</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>layer</cell><cell>fc1</cell><cell></cell><cell></cell></row><row><cell>Inception block1</cell><cell>W1 W2</cell><cell>Loss3</cell><cell></cell><cell></cell><cell>Total loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Facial expression</cell><cell></cell></row><row><cell>… Conv_1 Pooling Conv_6 …</cell><cell cols="2">BRANCH 2</cell><cell>recognition</cell><cell></cell></row><row><cell></cell><cell cols="2">Inception block3</cell><cell>Loss2</cell><cell>*</cell><cell>W1 W2</cell></row><row><cell></cell><cell></cell><cell>fc1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The datasets used in the multi-task learning for face verification and facial expression recognition in this work.</figDesc><table><row><cell></cell><cell>ID Ne An Di Fe Ha Sa Su Co</cell></row><row><cell>CK+</cell><cell>123 327 135 177 75 147 84 249 54</cell></row><row><cell cols="2">OuluCASIA 560 560 240 240 240 240 240 240 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The evaluation of face verification on facial expressions datasets with different methods (accuracy%). 6M 98.95 91.6 92.20 93.50 Centerloss [33] 0.7M 99.28 94.9 94.00 95.10 SphereFace [19] 0.7M 99.42 95.0 93.80 95.50 Single-task (pretrained) 1.1M 99.41 95.0 98.00 92.60 Single-task (fine-tuning) 1.1M 99.10 94.2 98.50 97.71 Static MTL 1.1M 99.23 94.1 98.50 98.00 Naive dynamic MTL 1.1M 99.23 94.1 98.15 95.14 Proposed dynamic MTL 1.1M 99.21 94.3 99.00 99.14</figDesc><table><row><cell>Method</cell><cell cols="4">Images LFW YTF CK+ Oulu.</cell></row><row><cell>DeepFace [31]</cell><cell>4M</cell><cell>97.35 91.4</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepID-2,3 [30]</cell><cell>-</cell><cell cols="3">99.47 93.2 91.70 96.50</cell></row><row><cell>FaceNet [27]</cell><cell cols="4">200M 99.63 95.1 98.00 97.50</cell></row><row><cell>VGGFace [29]</cell><cell>2.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hengxyz/Dynamic_ multi-task-learning.git</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the MOBIDEM project, part of the "Systematic Paris-Region" and "Images &amp; Network"</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the use of sift features for face authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bicego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lagorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshop, 2006. CVPRW&apos;06. Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="35" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple nose region matching for 3d face recognition under varying facial expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1695" to="1700" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-task deep network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3988" to="3994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gradnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02257</idno>
		<title level="m">Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New types of deep neural network learning for speech recognition and related applications: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8599" to="8603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face recognition using histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Déniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1598" to="1603" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="592" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Threedimensional face recognition in the presence of facial expressions: An annotated deformable model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murtuza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="649" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
	<note>British Machine Vision Conference</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition, 2013 IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops, 2010 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks. In Applications of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Winter Conference on</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive smoothed online multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4296" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fisher vector faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5079" to="5087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-task convolutional neural network for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="964" to="975" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="918" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facial expression recognition from near-infrared videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="607" to="619" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
