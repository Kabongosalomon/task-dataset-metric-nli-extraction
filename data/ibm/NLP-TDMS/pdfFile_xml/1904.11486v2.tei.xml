<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Making Convolutional Networks Shift-Invariant Again</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Making Convolutional Networks Shift-Invariant Again</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe increased accuracy in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe better generalization, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When downsampling a signal, such an image, the textbook solution is to anti-alias by low-pass filtering the signal <ref type="bibr" target="#b31">(Oppenheim et al., 1999;</ref><ref type="bibr" target="#b17">Gonzalez &amp; Woods, 1992)</ref>. Without it, high-frequency components of the signal alias into lowerfrequencies. This phenomenon is commonly illustrated in movies, where wheels appear to spin backwards, known as the Stroboscopic effect, due to the frame rate not meeting the classical sampling criterion <ref type="bibr" target="#b29">(Nyquist, 1928)</ref>. Interestingly, most modern convolutional networks do not worry about anti-aliasing.</p><p>Early networks did employ a form of blurred-downsampling -average pooling <ref type="bibr">(LeCun et al., 1990)</ref>. However, ample em-1 Adobe Research, San Francisco, CA. Correspondence to: Richard Zhang &lt;rizhang@adobe.com&gt;.</p><p>Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). pirical evidence suggests max-pooling provides stronger task performance <ref type="bibr">(Scherer et al., 2010)</ref>, leading to its widespread adoption. Unfortunately, max-pooling does not provide the same anti-aliasing capability, and a curious, recently uncovered phenomenon emerges -small shifts in the input can drastically change the output <ref type="bibr" target="#b11">(Engstrom et al., 2019;</ref><ref type="bibr" target="#b2">Azulay &amp; Weiss, 2018)</ref>. As seen in <ref type="figure" target="#fig_0">Figure 1</ref>, network outputs can oscillate depending on the input position.</p><p>Blurred-downsampling and max-pooling are commonly viewed as competing downsampling strategies <ref type="bibr">(Scherer et al., 2010)</ref>. However, we show that they are compatible. Our simple observation is that max-pooling is inherently composed of two operations: (1) evaluating the max operator densely and (2) naive subsampling. We propose to lowpass filter between them as a means of anti-aliasing. This viewpoint enables low-pass filtering to augment, rather than replace max-pooling. As a result, shifts in the input leave the output relatively unaffected (shift-invariance) and more closely shift the internal feature maps (shift-equivariance).</p><p>Furthermore, this enables proper placement of the low-pass filter, directly before subsampling. With this methodology, practical anti-aliasing can be achieved with any existing strided layer, such as strided-convolution, which is used in more modern networks such as ResNet <ref type="bibr" target="#b21">(He et al., 2016)</ref> and MobileNet <ref type="bibr" target="#b36">(Sandler et al., 2018)</ref>.</p><p>A potential concern is that overaggressive filtering can result in heavy loss of information, degrading performance. However, we actually observe increased accuracy in ImageNet classification <ref type="bibr" target="#b35">(Russakovsky et al., 2015)</ref> across architectures, as well as increased robustness and stability to corruptions and perturbations <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>. In summary:</p><p>• We integrate classic anti-aliasing to improve shiftequivariance of deep networks. Critically, the method is compatible with existing downsampling strategies. • We validate on common downsampling strategies -maxpooling, average-pooling, strided-convolution -in different architectures. We test across multiple tasks -image classification and image-to-image translation. • For ImageNet classification, we find, surprisingly, that accuracy increases, indicating effective regularization. • Furthermore, we observe better generalization. Performance is more robust and stable to corruptions such as rotation, scaling, blurring, and noise variants. The baseline (black) exhibits chaotic behavior, which is stabilized by our method (blue). We find this behavior across networks and datasets.</p><p>Here, we show selected examples using AlexNet on ImageNet (top) and VGG on CIFAR10 (bottom). Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Local connectivity and weight sharing have been a central tenet of neural networks, including the Neocognitron <ref type="bibr" target="#b15">(Fukushima &amp; Miyake, 1982)</ref>, <ref type="bibr">LeNet (LeCun et al., 1998)</ref> and modern networks such as Alexnet <ref type="bibr">(Krizhevsky et al., 2012)</ref>, VGG <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2015)</ref>, ResNet <ref type="bibr" target="#b21">(He et al., 2016)</ref>, and DenseNet <ref type="bibr" target="#b24">(Huang et al., 2017)</ref>. In biological systems, local connectivity was famously discovered in a cat's visual system <ref type="bibr" target="#b25">(Hubel &amp; Wiesel, 1962)</ref>. Recent work has strived to add additional invariances, such as rotation, reflection, and scaling <ref type="bibr" target="#b38">(Sifre &amp; Mallat, 2013;</ref><ref type="bibr" target="#b4">Bruna &amp; Mallat, 2013;</ref><ref type="bibr">Kanazawa et al., 2014;</ref><ref type="bibr" target="#b8">Cohen &amp; Welling, 2016;</ref><ref type="bibr">Worrall et al., 2017;</ref><ref type="bibr" target="#b12">Esteves et al., 2018)</ref>. We focus on shift-invariance, which is often taken for granted.  <ref type="bibr" target="#b16">(Girshick et al., 2014;</ref><ref type="bibr">Zhou et al., 2015)</ref>, actively maximizing hidden units <ref type="bibr" target="#b27">(Mordvintsev et al., 2015)</ref>, and mapping features back into pixel space <ref type="bibr">(Zeiler &amp; Fergus, 2014;</ref><ref type="bibr" target="#b22">Hénaff &amp; Simoncelli, 2016;</ref><ref type="bibr">Mahendran &amp; Vedaldi, 2015;</ref><ref type="bibr" target="#b9">Dosovitskiy &amp; Brox, 2016a;</ref><ref type="bibr" target="#b28">Nguyen et al., 2017)</ref>. Our analysis is focused on a specific, low-level property and is complementary to these approaches.</p><p>A more quantitative approach for analyzing networks is measuring representation or output changes (or robustness to changes) in response to manually generated perturbations to the input, such as image transformations <ref type="bibr" target="#b18">(Goodfellow et al., 2009;</ref><ref type="bibr">Lenc &amp; Vedaldi, 2015;</ref><ref type="bibr" target="#b2">Azulay &amp; Weiss, 2018)</ref>, geometric transforms <ref type="bibr" target="#b13">(Fawzi &amp; Frossard, 2015;</ref><ref type="bibr" target="#b34">Ruderman et al., 2018)</ref>, and CG renderings with various shape, poses, and colors <ref type="bibr" target="#b1">(Aubry &amp; Russell, 2015)</ref>. A related line of work is adversarial examples, where input perturbations are purposely directed to produce large changes in the output. These perturbations can be on pixels <ref type="bibr" target="#b19">(Goodfellow et al., 2014a;</ref>, a single pixel <ref type="bibr" target="#b41">(Su et al., 2019)</ref>, small deformations <ref type="bibr">(Xiao et al., 2018)</ref>, or even affine transformations <ref type="bibr" target="#b11">(Engstrom et al., 2019)</ref>. We aim to make the network robust to the simplest of these types of attacks and perturbations: shifts. In doing so, we also observe increased robustness across other types of corruptions and perturbations <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>.</p><p>Classic hand-engineered computer vision and image processing representations, such as SIFT (Lowe, 1999), wavelets, and image pyramids <ref type="bibr" target="#b0">(Adelson et al., 1984;</ref><ref type="bibr" target="#b5">Burt &amp; Adelson, 1987)</ref> also extract features in a sliding window manner, often with some subsampling factor. As discussed in <ref type="bibr" target="#b39">Simoncelli et al. (1992)</ref>, literal shift-equivariance cannot hold when subsampling. Shift-equivariance can be recovered if features are extracted densely, for example textons <ref type="bibr">(Leung &amp; Malik, 2001)</ref>, the Stationary Wavelet Transform <ref type="bibr" target="#b14">(Fowler, 2005)</ref>, and DenseSIFT <ref type="bibr">(Vedaldi &amp; Fulkerson, 2008)</ref>. Deep networks can also be evaluated densely, by removing striding and making appropriate changes to subsequent layers by usingá trous/dilated convolutions <ref type="bibr" target="#b6">(Chen et al., 2015;</ref><ref type="bibr">Yu &amp; Koltun, 2016;</ref><ref type="bibr">Yu et al., 2017)</ref>. This (stride 2) (stride 2) (stride 2) BlurPool (stride 2)  It is functionally equivalent to densely-evaluated pooling, followed by subsampling. The latter ignores the Nyquist sampling theorem and loses shift-equivariance. (Bottom) We low-pass filter between the operations. This keeps the first operation, while anti-aliasing the appropriate signal. Anti-aliasing and subsampling can be combined into one operation, which we refer to as BlurPool.</p><p>comes at great computation and memory cost. Our work investigates improving shift-equivariance with minimal additional computation, by blurring before subsampling.</p><p>Early networks employed average pooling <ref type="bibr">(LeCun et al., 1990)</ref>, which is equivalent to blurred-downsampling with a box filter. However, work <ref type="bibr">(Scherer et al., 2010)</ref> has found max-pooling to be more effective, which has consequently become the predominant method for downsampling. While previous work <ref type="bibr">(Scherer et al., 2010;</ref><ref type="bibr" target="#b22">Hénaff &amp; Simoncelli, 2016;</ref><ref type="bibr" target="#b2">Azulay &amp; Weiss, 2018)</ref>   <ref type="bibr" target="#b3">(Bietti &amp; Mairal, 2017)</ref>, CKNs perform at lower accuracy than contemporaries, resulting in limited usage. Interestingly, a byproduct of the derivation is a standard Gaussian filter; however, no guidance is provided on its proper integration with existing network components. Instead, we demonstrate practical integration with any strided layer, and empirically show performance increases on a challenging benchmark -ImageNet classification -on widely-used networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Deep convolutional networks as feature extractors Let an image with resolution H × W be represented by X ∈ R H×W ×3 . An L-layer CNN can be expressed as a feature extractor F l (X) ∈ R H l ×W l ×C l , with layer l ∈ {0, 1, ..., L}, spatial resolution H l × W l and C l channels. Each feature map can also be upsampled to original resolution, F l (X) ∈ R H×W ×C l .</p><p>Shift-equivariance and invariance A function F is shiftequivariant if shifting the input equally shifts the output, meaning shifting and feature extraction are commutable.</p><formula xml:id="formula_0">Shift ∆h,∆w ( F(X)) = F(Shift ∆h,∆w (X)) ∀ (∆h, ∆w)<label>(1)</label></formula><p>A representation is shift-invariant if shifting the input results in an identical representation.</p><p>F(X) = F(Shift ∆h,∆w (X)) ∀ (∆h, ∆w)</p><p>Periodic-N shift-equivariance/invariance In some cases, the definitions in Eqns. 1, 2 may hold only when shifts (∆h, ∆w) are integer multiples of N. We refer to such scenarios as periodic shift-equivariance/invariance. For example, periodic-2 shift-invariance means that even-pixel shifts produce an identical output, but odd-pixel shifts may not.</p><p>Circular convolution and shifting Edge artifacts are an important consideration. When shifting, information is lost on one side and has to be filled in on the other.</p><p>In our CIFAR10 classification experiments, we use circular shifting and convolution. When the convolutional kernel hits the edge, it "rolls" to the other side. Similarly, when shifting, pixels are rolled off one edge to the other.</p><formula xml:id="formula_2">[Shift ∆h,∆w (X)] h,w,c = X (h−∆h)%H,(w−∆w)%W,c ,</formula><p>where % is the modulus function</p><p>The modification minorly affects performance and could be potentially mitigated by additional padding, at the expense of memory and computation. But importantly, this affords us a clean testbed. Any loss in shift-equivariance is purely due to characteristics of the feature extractor.</p><p>An alternative is to take a shifted crop from a larger image. We use this approach for ImageNet experiments, as it more closely matches standard train and test procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Anti-aliasing to improve shift-equivariance</head><p>Conventional methods for reducing spatial resolution -maxpooling, average pooling, and strided convolution -all break shift-equivariance. We propose improvements, shown in <ref type="figure">Figure 2</ref>. We start by analyzing max-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MaxPool→MaxBlurPool</head><p>Consider the example [0, 0, 1, 1, 0, 0, 1, 1] signal in <ref type="figure" target="#fig_2">Figure 4</ref> (left).</p><p>Maxpooling (kernel k=2, stride s=2) will result in [0, 1, 0, 1]. Simply shifting the input results in a dramatically different answer of [1, 1, 1, 1]. Shift-equivariance is lost. These results are subsampling from an intermediate signal -the input densely max-pooled (stride-1), which we simply refer to as "max". As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> (top), we can write max-pooling as a composition of two functions:</p><formula xml:id="formula_4">MaxPool k,s = Subsample s • Max k .</formula><p>The Max operation preserves shift-equivariance, as it is densely evaluated in a sliding window fashion, but subsequent subsampling does not. We simply propose to add an anti-aliasing filter with kernel m × m, denoted as Blur m , as shown in <ref type="figure" target="#fig_2">Figure 4</ref> (right). During implementation, blurring and subsampling are combined, as commonplace in image processing. We call this function BlurPool m,s .</p><formula xml:id="formula_5">MaxPool k,s → Subsample s • Blur m • Max k = BlurPool m,s • Max k<label>(4)</label></formula><p>Sampling after low-pass filtering gives [.5, 1, .5, 1] and <ref type="bibr">[.75, .75, .75, .75</ref>]. These are closer to each other and better representations of the intermediate signal.</p><p>StridedConv→ConvBlurPool Strided-convolutions suffer from the same issue, and the same method applies.</p><formula xml:id="formula_6">Relu • Conv k,s → BlurPool m,s • Relu • Conv k,1 (5)</formula><p>Importantly, this analogous modification applies conceptually to any strided layer, meaning the network designer can keep their original operation of choice.</p><p>AveragePool→BlurPool Blurred downsampling with a box filter is the same as average pooling. Replacing it with a stronger filter provides better shift-equivariance. We examine such filters next.</p><formula xml:id="formula_7">AvgPool k,s → BlurPool m,s<label>(6)</label></formula><p>Anti-aliasing filter selection The method allows for a choice of blur kernel. We test m × m filters ranging from size 2 to 5, with increasing smoothing. The weights are normalized. The filters are the outer product of the following vectors with themselves.</p><p>• Rectangle-2 [1, 1]: moving average or box filter; equivalent to average pooling or "nearest" downsampling • Triangle-3 [1, 2, 1]: two box filters convolved together; equivalent to bilinear downsampling • Binomial-5 [1, 4, 6, 4, 1]: the box filter convolved with itself repeatedly; the standard filter used in Laplacian pyramids <ref type="bibr" target="#b5">(Burt &amp; Adelson, 1987)</ref> 4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Testbeds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR Classification</head><p>To begin, we test classification of low-resolution 32 × 32 images. The dataset contains 50k training and 10k validation images, classified into one of 10 categories. We dissect the VGG architecture <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2015)</ref>, showing that shift-equivariance is a signal-processing property, progressively lost in each downsampling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Classification</head><p>We then test on large-scale classification on 224 × 224 resolution images. The dataset contains 1.2M training and 50k validation images, classified into one of 1000 categories. We test across different architecture families -AlexNet (Krizhevsky &amp; Hinton, 2009), VGG <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2015)</ref>, ResNet <ref type="bibr" target="#b21">(He et al., 2016)</ref>, DenseNet <ref type="bibr" target="#b24">(Huang et al., 2017)</ref>, and MobileNet-v2 <ref type="bibr" target="#b36">(Sandler et al., 2018</ref>) -with different downsampling strategies, as described in <ref type="table">Table 1</ref>. Furthermore, we test the classifier robustness using the Imagenet-C and ImageNet-P datasets <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>.</p><p>Conditional Image Generation Finally, we show that the same aliasing issues in classification networks are also present in conditional image generation networks. We test on the Labels→Facades (Tyleček &amp;Šára, 2013; Isola et al., 2017) dataset, where a network is tasked to generated a 256×256 photorealistic image from a label map. There are 400 training and 100 validation images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Classification Generation</head><p>Alex <ref type="table">Table 1</ref>. Testbeds. We test across tasks (ImageNet classification and Labels→Facades) and network architectures. Each architecture employs different downsampling strategies. We list how often each is used here. We can antialias each variant. This convolution uses stride 4 (all others use 2). We only apply the antialiasing at stride 2. Evaluating the convolution at stride 1 would require large computation at full-resolution. ‡ For the same reason, we do not antialias the first strided-convolution in these networks.</p><formula xml:id="formula_8">-VGG Res-Dense-Mobile- U- Net Net Net Netv2 Net StridedConv 1 - 4 ‡ 1 ‡ 5 ‡ 8 MaxPool 3 5 1 1 - - AvgPool - - - 3 - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Shift-Invariance/Equivariance Metrics</head><p>Ideally, a shift in the input would result in equally shifted feature maps internally:</p><p>Internal feature distance. We examine internal feature maps with d(Shift ∆h,∆w ( F(X)), F(Shift ∆h,∆w (X))) (left &amp; right-hand sides of Eqn. 1). We use cosine distance, as common for deep features <ref type="bibr">(Kiros et al., 2015;</ref><ref type="bibr">Zhang et al., 2018)</ref>.</p><p>We can also measure the stability of the output:</p><p>Classification consistency.</p><p>For classification, we check how often the network outputs the same classification, given the same image with two different shifts: E X,h1,w1,h2,w2 1{arg max P (Shift h1,w1 (X)) = arg max P (Shift h2,w2 (X))}.</p><p>Generation stability. For image translation, we test if a shift in the input image generates a correspondingly shifted output. For simplicity, we test horizontal shifts. E X,∆w PSNR Shift 0,∆w (F(X))), F(Shift 0,∆w (X)) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Internal shift-equivariance</head><p>We first test on the CIFAR dataset using the VGG13-bn (Simonyan &amp; Zisserman, 2015) architecture.</p><p>We dissect the progressive loss of shift-equivariance by investigating the VGG architecture internally. The network contains 5 blocks of convolutions, each followed by maxpooling (with stride 2), followed by a linear classifier. For purposes of our understanding, MaxPool layers are broken into two components -before and after subsampling, e.g., max1 and pool1, respectively. In <ref type="figure">Figure 5</ref> (top), we show internal feature distance, as a function of all possible shiftoffsets (∆h, ∆w) and layers. All layers before the first downsampling, max1, are shift-equivariant. Once downsampling occurs in pool1, shift-equivariance is lost. However, periodic-N shift-equivariance still holds, as indicated by the stippling pattern in pool1, and each subsequent subsampling doubles the factor N.</p><p>(a) Baseline VGG13bn (using MaxPool) (b) Anti-aliased VGG13bn (using MaxBlurPool, Bin-5) <ref type="figure">Figure 5</ref>. Deviation from perfect shift-equivariance, throughout VGG. Feature distance between left &amp; right-hand sides of the shiftequivariance condition (Eqn 1). Each pixel in each heatmap is a shift (∆h, ∆w). Blue indicates perfect shift-equivariance; red indicates large deviation. Note that the dynamic ranges of distances are different per layer. For visualization, we calibrate by calculating the mean distance between two different images, and mapping red to half the value. Accumulated downsampling factor is in [brackets]; in layers pool5, classifier, and softmax, shift-equivariance and shift-invariance are equivalent, as features have no spatial extent. Layers up to max1 have perfect equivariance, as no downsampling yet occurs. (a) On the baseline network, shift-equivariance is reduced each time downsampling takes place. Periodic-N shift-equivariance holds, with N doubling with each downsampling. (b) With our antialiased network, shift-equivariance is better maintained, and the resulting output is more shift-invariant.</p><p>In <ref type="figure">Figure 5</ref> (bottom), we plot shift-equivariance maps with our anti-aliased network, using MaxBlurPool. Shiftequivariance is clearly better preserved. In particular, the severe drop-offs in downsampling layers do not occur. Improved shift-equivariance throughout the network cascades into more consistent classifications in the output, as shown by some selected examples in <ref type="figure" target="#fig_0">Figure 1</ref>. This study uses a Bin-5 filter, trained without data augmentation. The trend holds for other filters and when training with augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Large-scale ImageNet classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">SHIFT-INVARIANCE AND ACCURACY</head><p>We next test on large-scale image classification of Ima-geNet <ref type="bibr" target="#b35">(Russakovsky et al., 2015)</ref>. In <ref type="figure">Figure 6</ref>, we show classification accuracy and consistency, across variants of several architectures -VGG, ResNet, DenseNet, and MobileNet-v2. The off-the-shelf networks are labeled as Baseline, and we use standard training schedules from the publicly available PyTorch <ref type="bibr" target="#b32">(Paszke et al., 2017)</ref> repository for our anti-aliased networks. Each architecture has a different downsampling strategy, shown in <ref type="table">Table 1</ref>. We typically refer to the popular ResNet50 as a running example; note that we see similar trends across network architectures.</p><p>Improved shift-invariance We apply progressively stronger filters -Rect-2, Tri-3, Bin-5. Doing so increases ResNet50 stability by +0.8%, +1.7%, and +2.1%, respectively. Note that doubling layers -going to ResNet101only increases stability by +0.6%. Even a simple, small low-pass filter, directly applied to ResNet50, outpaces this. As intended, stability increases across architectures (points move upwards in <ref type="figure">Figure 6</ref>).</p><p>Improved classification Filtering improves the shiftinvariance. How does it affect absolute classification performance? We find that across the board, performance actually increases (points move to the right in <ref type="figure">Figure 6</ref>). The filters improve ResNet50 by +0.7% to +0.9%. For reference, doubling the layers to ResNet101 increases accuracy by +1.2%. A low-pass filter makes up much of this ground, without adding any learnable parameters. This is a surprising, unexpected result, as low-pass filtering removes information, and could be expected to reduce performance. On the contrary, we find that it serves as effective regularization, and these widely-used methods improve with simple anti-aliasing. As ImageNet-trained nets often serve as the backbone for downstream tuning, this improvement may be observed across other applications as well. Mobilenet-v2</p><p>Baseline Anti-aliased (Rect-2) Anti-aliased (Tri-3) Anti-aliased (Bin-5) <ref type="figure">Figure 6</ref>. ImageNet Classification consistency vs. accuracy. Up (more consistent to shifts) and to the right (more accurate) is better. Different shapes correspond to the baseline (circle) or variants of our anti-aliased networks (bar, triangle, pentagon for length 2, 3, 5 filters, respectively). We test across network architectures. As expected, low-pass filtering helps shift-invariance. Surprisingly, classification accuracy is also improved.</p><p>The best performing filter varies by architecture, but all filters improve over the baseline. We recommend using the Tri-3 or Bin-5 filter. If shift-invariance is especially desired, stronger filters can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">OUT-OF-DISTRIBUTION ROBUSTNESS</head><p>We have shown increased stability (to shifts), as well as accuracy. Next, we test the generalization capability the classifier in these two aspects, using datasets from <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>. We test stability to perturbations other than shifts.</p><p>We then test accuracy on systematically corrupted images. Results are shown in <ref type="table">Table 2</ref>, averaged across corruption types. We show the raw, unnormalized average, along with a weighted "normalized" average, as recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability to perturbations</head><p>The ImageNet-P dataset <ref type="bibr" target="#b23">(Hendrycks et al., 2019</ref>) contains short video clips of a single image with small perturbations added, such as variants of noise (Gaussian and shot), blur (motion and zoom), simulated weather (snow and brightness), and geometric changes (rotation, scaling, and tilt). Stability is measured by flip rate (mFR) -how often the top-1 classification changes, on average, in consecutive frames. Baseline ResNet50 flips 7.9% of the time; adding anti-aliasing Bin-5 reduces by 1.0%. While antialiasing provides increased stability to shifts by design, a "free", emergent property is increased stability to other perturbation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to corruptions</head><p>We observed increased accuracy on clean ImageNet. Here, we also observe more graceful degradation when images are corrupted. In addition  <ref type="table">Table 2</ref>. Accuracy and stability robustness. Accuracy in ImageNet-C, which contains systematically corrupted ImageNet images, measured by mean corruption error mCE (lower is better). Stability on ImageNet-P, which contains perturbed image sequences, measured by mean flip rate mFR (lower is better). We show raw, unnormalized scores, as well as scores normalized to AlexNet, as used in <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>. Anti-aliasing improves both accuracy and stability over the baseline. All networks are variants of ResNet50.</p><p>to the previously explored corruptions, ImageNet-C contains impulse noise, defocus and glass blur, simulated frost and fog, and various digital alterations of contrast, elastic transformation, pixelation and jpeg compression. The geometric perturbations are not used. ResNet50 has mean error rate of 60.6%. Anti-aliasing with Bin-5 reduces the error rate by 2.5%. As expected, the more "high-frequency" corruptions, such as adding noise and pixelation, show greater improvement. Interestingly, we see improvements even with "low-frequency" corruptions, such defocus blur and zoom blur operations as well.</p><p>Together, these results indicate that a byproduct of antialiasing is a more robust, generalizable network. Though motivated by shift-invariance, we actually observe increased stability to other perturbation types, as well as increased accuracy, both on clean and corrupted images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Conditional image generation (Label→Facades)</head><p>We test on image generation, outputting an image of a facade given its semantic label map (Tyleček &amp;Šára, 2013), in a GAN setup <ref type="bibr" target="#b19">(Goodfellow et al., 2014a;</ref><ref type="bibr">Isola et al., 2017)</ref>.</p><p>Our classification experiments indicate that anti-aliasing is a natural choice for the discriminator, and is used in the recent StyleGAN method <ref type="bibr">(Karras et al., 2019)</ref>. Here, we explore its use in the generator, for the purposes of obtaining a shift-equivariant image-to-image translation network.</p><p>Baseline We use the pix2pix method <ref type="bibr">(Isola et al., 2017)</ref>. The method uses U-Net <ref type="bibr" target="#b33">(Ronneberger et al., 2015)</ref>, which contains 8 downsampling and 8 upsampling layers, with skip connections to preserve local information. No antialiasing filtering is applied in down or upsampling layers in the baseline. In <ref type="figure" target="#fig_4">Figure 7</ref>, we show a qualitative example, focusing in on a specific window. In the baseline (top), as the input X shifts horizontally by ∆w, the vertical bars on the generated window also shift. The generations start with    <ref type="table">Table 3</ref>. Generation stability PSNR (higher is better) between generated facades, given two horizontally shifted inputs. More aggressive filtering in the down and upsampling layers leads to a more shift-equivariant generator. Total variation (TV) of generated images (closer to ground truth images 7.80 is better). Increased filtering decreases the frequency content of generated images.</p><p>two bars, to a single bar, and eventually oscillates back to two bars. A shift-equivariant network would provide the same resulting facade, no matter the shift.</p><p>Applying anti-aliasing We augment the stridedconvolution downsampling by blurring. The U-Net also uses upsampling layers, without any smoothing. Similar to the subsampling case, this leads to aliasing, in the form of grid artifacts <ref type="bibr" target="#b30">(Odena et al., 2016)</ref>. We mirror the downsampling by applying the same filter after upsampling. Note that applying the Rect-2 and Tri-3 filters while upsampling correspond to "nearest" and "bilinear" upsampling, respectively. By using the Tri-3 filter, the same window pattern is generated, regardless of input shift, as seen in <ref type="figure" target="#fig_4">Figure 7</ref> (bottom).</p><p>We measure similarity using peak signal-to-noise ratio between generated facades with shifted and non-shifted inputs: E X,∆w PSNR(Shift 0,∆w (F (X)), F (Shift 0,∆w (X)))). In <ref type="table">Table 3</ref>, we show that the smoother the filter, the more shift-equivariant the output.</p><p>A concern with adding low-pass filtering is the loss of ability to generate high-frequency content, which is critical for generating high-quality imagery. Quantitatively, in <ref type="table">Table 3</ref>, we compute the total variation (TV) norm of the generated images. Qualitatively, we observe that generation quality typically holds with the Tri-3 filter and subsequently degrades. In the supplemental material, we show examples of applying increasingly aggressive filters. We observe a boost in shift-equivariance while maintaining generation quality, and then a tradeoff between the two factors.</p><p>These experiments demonstrate that the technique can make a drastically different architecture (U-Net) for a different task (generating pixels) more shift-equivariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussion</head><p>Shift-equivariance is lost in modern deep networks, as commonly used downsampling layers ignore Nyquist sampling and alias. We integrate low-pass filtering to anti-alias, a common signal processing technique. The simple modification achieves higher consistency, across architectures and downsampling techniques. In addition, in classification, we observe surprising boosts in accuracy and robustness.</p><p>Anti-aliasing for shift-equivariance is well-understood. A future direction is to better understand how it affects and improves generalization, as we observed empirically. Other directions include the potential benefit to downstream applications, such as nearest-neighbor retrieval, improving temporal consistency in video models, robustness to adversarial examples, and high-level vision tasks such as detection. Adding the inductive bias of shift-invariance serves as "built-in" shift-based data augmentation. This is potentially applicable to online learning scenarios, where the data distribution is changing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Here, we show additional results and experiments for CI-FAR classification, ImageNet expanded results, and conditional image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CIFAR Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Classification results</head><p>We train both without and with shift-based data augmentation. We evaluate on classification accuracy and consistency. The results are shown in <ref type="table">Table 5</ref> and <ref type="figure">Figure 8</ref>.</p><p>In the main paper, we showed internal activations on the older VGG <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2015)</ref> network.</p><p>Here, we also present classification accuracy and consistency results on the output, along with the more modern DenseNet <ref type="bibr" target="#b24">(Huang et al., 2017)</ref> architecture.</p><p>Training without data augmentation Without the benefit of seeing shifts at training time, the baseline network produces inconsistent classifications -random shifts of the same image only agree 88.1% of the time. Our anti-aliased Architecture Classification (CIFAR)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG13-bn DenseNet-40-12</head><p>StridedConv --MaxPool 5 -AvgPool -2 <ref type="table">Table 4</ref>. Testbeds (CIFAR10 Architectures). We use slightly different architectures for VGG <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2015)</ref> and DenseNet <ref type="bibr" target="#b24">(Huang et al., 2017)</ref> than the ImageNet counterparts. network, with the MaxBlurPool operator, increases consistency. The larger the filter, the more consistent the output classifications. This result agrees with our expectation and theory -improving shift-equivariance throughout the network should result in more consistent classifications across shifts, even when such shifts are not seen at training.</p><p>In this regime, accuracy clearly increases with consistency, as seen with the blue markers in <ref type="figure">Figure 8</ref>. Filtering does not destroy the signal or make learning harder. On the contrary, shift-equivariance serves as "built-in" augmentation, indicating more efficient data usage.</p><p>Training with data augmentation In principle, networks can learn to be shift-invariant from data. Is data augmentation all that is needed to achieve shift-invariance? By applying the Rect-2 filter, a large increase in consistency, 96.6 → 97.6, can be had at a small decrease in accuracy 93.8 → 93.7. Even when seeing shifts at training, antialiasing increases consistency. From there, stronger filters can increase consistency, at the expense of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DenseNet results</head><p>We show a summary of VGG and DenseNet in <ref type="table">Table 4</ref>. DenseNet uses comparatively fewer downsampling layers -2 average-pooling layers instead of 5 max-pooling layers. With just two downsampling layers, the baseline still loses shift-invariance. Even when training with data augmentation, replacing average-pooling with blurred-pooling increases both consistency and even minorly improves accuracy. Note that the DenseNet architecture performs stronger than VGG to begin with. In this setting, the Bin-7 BlurPool operator works best for both consistency and accuracy. Again, applying the operator serves are "built-in" data augmentation, performing strongly even without shifts at train time.</p><p>How do the learned convolutional filters change? Our proposed change smooths the internal feature maps for purposes of downsampling. How does training with this layer affect the learned convolutional layers? We measure spatial smoothness using the normalized Total Variation (TV) metric proposed in <ref type="bibr" target="#b34">Ruderman et al. (2018)</ref>. A higher value indicates a filter with more high-frequency components. A lower value indicates a smoother filter. As shown in <ref type="figure">Figure</ref>   <ref type="table">Table 5</ref>. CIFAR Classification accuracy and consistency Results across blurring filters and training scenarios (without and with data augmentation). We evaluate classification accuracy without shifts (Accuracy -None) and on random shifts (Accuracy -Random), as well as classification consistency.</p><p>VGG-13 <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2015)</ref> DenseNet-40-12 <ref type="bibr" target="#b24">(Huang et al., 2017)</ref>   . Total Variation (TV) by layer. We compute average smoothness of learned conv filters per layer (lower is smoother). Baseline MaxPool is in black, and adding additional blurring is shown in colors. Note that the learned convolutional layers become smoother, indicating that a smoother feature extractor is induced. smoother filters throughout the network, relative to the baseline (black). Adding in more aggressive low-pass filtering further decreases the TV (increasing smoothness). This indicates that our method actually induces a smoother feature extractor overall.</p><p>Timing analysis The average speed of a forward pass of VGG13bn using batch size 100 CIFAR images on a GTX1080Ti GPU is 10.19ms. Evaluating Max at stride 1 instead of 2 adds 3.0%. From there, low-pass filtering with kernel sizes 3, 5, 7 adds additional 5.5%, 7.6%, 9.3% time, respectively, relative to baseline. The method can be implemented more efficiently by separating the low-pass filter into horizontal and vertical components, allowing added time to scale linearly with filter size, rather than quadratically. In total, the largest filter adds 12.3% per forward pass. This is significantly cheaper than evaluating multiple forward passes in an ensembling approach (1024× computation to evaluate every shift), or evaluating each layer more densely by exchanging striding for dilation (4×, 16×, 64×, 256× computation for conv2-conv5, respectively). Given computational resources, brute-force computation solves shiftinvariance.</p><p>Average accuracy across spatial positions In <ref type="figure" target="#fig_5">Figure 9</ref>, we train without augmentation, and show how accuracy systematically degrades as a function of spatial shift. We observe the following:</p><p>• On the left, the baseline heatmap shows that classification accuracy holds when testing with no shift, but quickly degrades when shifting. • The proposed filtering decreases the degradation. Bin-7 is largely consistent across all spatial positions. • On the right, we plot the accuracy when making diagonal shifts to the input. As increased filtering is added, classification accuracy becomes consistent in all positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification variation distribution</head><p>The consistency metric in the main paper looks at the hard classification, discounting classifier confidence. Similar to <ref type="bibr" target="#b2">Azulay &amp; Weiss (2018)</ref>, we also compute the variation in probability of correct classification (the traces shown in <ref type="figure" target="#fig_1">Figure 3</ref> in the main paper), given different shifts. We can capture the variation across all possible shifts: V ar h,w ({P correct class (Shift h,w (X))}).</p><p>In <ref type="figure" target="#fig_0">Figure 11</ref>, we show the distribution of classification variations, before and after adding in the low-pass filter. Even with a small 2 × 2 filter, immediately variation decreases.</p><p>As the filter size is increased, the output classification variation continues to decrease. This has a larger effect when training without data augmentation, but is still observable when training with data augmentation.</p><p>Training with data augmentation with the baseline network reduces variation. Anti-aliasing the networks reduces vari-10 4 10 3 10 2 10 1 10 0 Variation in probability of correct classification Robustness to shift-based adversary In the main paper, we show that anti-aliased the networks increases the classification consistency, while maintaining accuracy. A logical consequence is increased accuracy in presence of a shiftbased adversary. We empirically confirm this in <ref type="figure" target="#fig_0">Figure 12</ref> for VGG13 on CIFAR10. We compute classification accuracy as a function of maximum adversarial shift. A max shift of 2 means the adversary can choose any of the 25 positions within a 5 × 5 window. For the classifier to "win", it must correctly classify all of them correctly. Max shift of 0 means that there is no adversary. Conversely, a max shift of 16 means the image must be correctly classified at all 32 × 32 = 1024 positions.</p><p>Our primary observations are as follows:</p><p>• As seen in <ref type="figure" target="#fig_0">Figure 12</ref> (left), the baseline network (gray) is very sensitive to the adversary. • Adding larger Binomial filters (from red to purple) increases robustness to the adversary. In fact, Bin-7 filter (purple) without augmentation outperforms the baseline (black) with augmentation. • As seen in <ref type="figure" target="#fig_0">Figure 12</ref> (right), adding larger Binomial filters also increases adversarial robustness, even when training with augmentation.</p><p>These results corroborate the findings in the main paper, and demonstrate a use case: increased robustness to a shift-based adversarial attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Alternatives to MaxBlurPool</head><p>In the paper, we follow signal processing first principles, to arrive at our solution of MaxBlurPool, with a fixed blurring kernel. Here, we explore possible alternatives -swapping max and blur operations, combining max and blur in parallel through soft-gating, and learning the blur filter.</p><p>Swapping max and blur We blur after max, immediately before subsampling, which has solid theoretical backing in sampling theory. What happens when the operations are swapped? The signal before the max operator is undoubtedly related to the signal after. Thus, blurring before max provides "second-hand" anti-aliasing and still increases shift-invariance over the baseline. However, switching the order is worse than max and blurring in the correct, proposed order. For example, for Bin-7, accuracy (93.2 → 92.6) and consistency (98.8 → 98.6) both decrease. We consistently observe this across filters.</p><p>Softly gating between max-pool and average-pool Lee et al. (2016) investigate combining MaxPool and AvgPool in parallel, with a soft-gating mechanism, called "Mixed" Max-AvgPool. We instead combine them in series. We conduct additional experiments here. On CIFAR (VGG w/ aug, see Tab 5), MixedPool can offer improvements over Max-Pool baseline (96.6→97.2 consistency). However, by softly weighting AvgPool, some antialiasing capability is left on the table. MaxBlurPool provides higher invariance (97.6). All have similar accuracy -93.8, 93.7, and 93.7 for baseline MaxPool, MixedPool, and our MaxBlurPool, respectively. We use our Rect-2 variant here for clean comparison. Importantly, our paper proposes a methodology, not a pooling layer. The same technique to modify MaxPool (reduce stride, then BlurPool) applies to the MixedPool layer, increasing its shift-invariance (97.2→97.8).</p><p>Learning the blur filter We have shown that adding antialiasing filtering improves shift-equivariance. What if the blur kernel were learned? We initialize the filters with our fixed weights, Tri-3 and Bin-5, and allow them to be adjusted during training (while constraining the kernel symmetrical). The function space has more degrees of freedom and is strictly more general. However, we find that while accuracy holds, consistency decreases: relative to the fixed filters, we see 98.0 → 97.5 for length-3 and 98.4 → 97.3 for length-5. While shift-invariance can be learned, there is no explicit incentive to do so. Analogously, a fully connected network can learn convolution, but does not do so in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ImageNet Classification</head><p>We show expanded results and visualizations.</p><p>Classification and shift-invariance results In <ref type="table">Table 6</ref>, we show expanded results. These results are plotted in <ref type="figure">Figure 6</ref> in the main paper. All pretrained models are available at https://richzhang.github.io/ antialiased-cnns/.</p><p>Robustness results In the main paper, we show aggregated results for robustness tests on the Imagenet-C/P datasets <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>. In <ref type="table" target="#tab_12">Tables 8 and 7</ref> we show expanded results, separated by each corruption and perturbation type.</p><p>Antialiasing is motivated by shift-invariance. Indeed, using the Bin-5 antialiasing filter reduces flip rate by 22.3% to translations. <ref type="table" target="#tab_12">Table 8</ref> indicates increased stability to other perturbation types as well. We observe higher stability to geometric perturbations -rotation, tilting, and scaling. In addition, antialiasing also helps stability to noise. This is somewhat expected, as adding low-pass filtering helps can average away spurious noise. Surprisingly, adding blurring within the network also increases resilience to blurred images. In total, antialiasing increases stability almost across the board -9 of the 10 perturbations are reliably stabilized.</p><p>We also observe increased accuracy, in the face of corruptions, as shown in <ref type="table">Table 7</ref>. Again, adding low-pass filtering helps smooth away spurious noise on the input, helping better maintain performance. Other high-frequency perturbations, such as pixelation and jpeg compression, are also consistency improved with antialiasing. Overall, antialiasing increases robustness to perturbations -13 of the 15 corruptions are reliably improved.</p><p>In total, these results indicate that adding antialiasing provides a smoother feature extractor, which is more stable and robust to out-of-distribution perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative examples for Labels→Facades</head><p>In the main paper, we discussed the tension between needing to generate high-frequency content and low-pass filtering for shift-invariance. Here, we show an example of applying increasingly aggressive filters. In general, generation quality is maintained with the Rect-2 and Tri-3 filters, and then degrades with additional filtering.  <ref type="table">Table 6</ref>. Imagenet Classification. We show 1000-way classification accuracy and consistency (higher is better), across 4 architectures, with anti-aliasing filtering added. We test 3 possible filters, in addition to the off-the-shelf reference models. This shows results plotted in <ref type="figure">Figure 6</ref> in the main paper. Abs is the absolute performance, and ∆ is the difference to the baseline. As designed, classification consistency is improved across all methods. Interestingly, accuracy is also improved.  <ref type="table">Table 7</ref>. Generalization to Corruptions. (Top) Corruption error rate (lower is better) of Resnet50 on the Imagenet-C. With antialiasing, the error rate decreases, often times significantly, on most corruptions. (Bottom) The percentage reduction relative to the baseline ResNet50 (higher is better). The right two columns show mean across corruptions. "Unnorm" is the raw average. "Norm" is normalized to errors made from AlexNet, as proposed in <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>.   <ref type="figure" target="#fig_0">Figure 13</ref>. Example generations. We show generations with U-Nets trained with 5 different filters. In general, generation quality is well-maintained to Tri-3 filter, but decreases noticeably with Bin-4 and Bin-5 filters due to oversmoothing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Classification stability for selected images. Predicted probability of the correct class changes when shifting the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Anti-aliased max-pooling. (Top) Pooling does not preserve shift-equivariance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>BaselineFigure 4 .</head><label>4</label><figDesc>Illustrative 1-D example of sensitivity to shifts. We illustrate how downsampling affects shift-equivariance with a toy example. (Left) An input signal is in light gray line. Max-pooled (k = 2, s = 2) signal is in blue squares. Simply shifting the input and then max-pooling provides a completely different answer (red diamonds). (Right) The blue and red points are subsampled from a densely max-pooled (k = 2, s = 1) intermediate signal (thick black line). We low-pass filter this intermediate signal and then subsample from it, shown with green and magenta triangles, better preserving shift-equivariance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Selected example of generation instability. The left two images are generated facades from label maps. For the baseline method (top), input shifts cause different window patterns to emerge, due to naive downsampling and upsampling. Our method (bottom) stabilizes the output, generating the same window pattern, regardless the input shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Average accuracy as a function of shift. (Left) We show classification accuracy across the test set as a function of shift, given different filters. (Right) We plot accuracy vs diagonal shift in the input image, across different filters. Note that accuracy degrades quickly with the baseline, but as increased filtering is added, classifications become consistent across spatial positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Distribution of per-image classification variation. We show the distribution of classification variation in the test set, (left) without and (right) with data augmentation at training. Lower variation means more consistent classifications (and increased shiftinvariance). Training with data augmentation drastically reduces variation in classification. Adding filtering further decreases variation. Robustness to shift-based adversarial attack. Classification accuracy as a function of the number of pixels an adversary is allowed to shift the image. Applying our proposed filtering increases robustness, both without (left) and with right data augmentation. ation in both scenarios. More aggressive filtering further decreases variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Anti-aliasing common downsampling layers. (Top) Max-pooling, strided-convolution, and average-pooling can each be better antialiased (bottom) with our proposed architectural modification. An example on max-pooling is shown below.</figDesc><table><row><cell></cell><cell>Max (stride 1)</cell><cell>BlurPool (stride 2)</cell><cell></cell><cell>Conv (stride 1)</cell><cell></cell><cell>ReLU</cell><cell>BlurPool (stride 2)</cell></row><row><cell>Baseline</cell><cell>MaxPool (stride 2)</cell><cell></cell><cell></cell><cell>Conv (stride 2)</cell><cell cols="2">ReLU</cell><cell>AvgPool (stride 2)</cell></row><row><cell>Anti-aliased</cell><cell>Max (stride 1)</cell><cell>BlurPool (stride 2)</cell><cell></cell><cell>Conv (stride 1)</cell><cell>ReLU</cell><cell cols="2">BlurPool (stride 2)</cell><cell>BlurPool (stride 2)</cell></row><row><cell></cell><cell cols="2">Max Pooling</cell><cell></cell><cell cols="3">Strided-Convolution</cell><cell>Average Pooling</cell></row><row><cell cols="3">max( ) Figure 2. Shift-equivariance lost; Baseline (MaxPool) max( )</cell><cell></cell><cell cols="3">(1) Max (dense evaluation) max( ) max( )</cell><cell>(2) Subsampling</cell></row><row><cell></cell><cell>heavy aliasing</cell><cell></cell><cell></cell><cell cols="3">Preserves shift-equivariance</cell><cell>Shift-eq. lost; heavy aliasing</cell></row><row><cell></cell><cell cols="2">max( )</cell><cell>Blur kernel</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Anti-aliased</cell><cell></cell><cell></cell><cell></cell><cell>conv</cell><cell></cell><cell></cell></row><row><cell>(MaxBlurPool)</cell><cell cols="2">max( )</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">(1) Max (dense evaluation)</cell><cell cols="2">(2) Anti-aliasing filter</cell><cell></cell><cell>(3) Subsampling</cell></row><row><cell></cell><cell cols="2">Preserves shift-eq.</cell><cell></cell><cell cols="2">Preserves shift-eq.</cell><cell cols="2">Shift eq. lost, but with reduced aliasing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>ACKNOWLEDGMENTS I am especially grateful to Eli Shechtman for helpful discussion and guidance. Michaël Gharbi, Andrew Owens, and anonymous reviewers provided beneficial feedback on earlier drafts. I thank labmates and mentors, past and present Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-toimage translation with conditional adversarial networks. In CVPR, 2017. Kanazawa, A., Sharma, A., and Jacobs, D. Locally scaleinvariant convolutional neural networks. In NIPS Workshop, 2014. Tyleček, R. andŠára, R. Spatial pattern templates for recognition of objects with regular structure. In German Conference on Pattern Recognition, pp. 364-374. Springer, 2013. Vedaldi, A. and Fulkerson, B. VLFeat: An open and portable library of computer vision algorithms. http: //www.vlfeat.org/, 2008.</figDesc><table><row><cell>-Sylvain Paris, Oliver Wang, Alexei A. Efros, Angjoo Kanazawa, Taesung Park, and Phillip Isola -for their help-ful comments and encouragement. I thank Dan Hendrycks Isola, P., Karras, T., Laine, S., and Aila, T. A style-based generator</cell></row><row><cell>for discussion about robustness tests on ImageNet-C/P. architecture for generative adversarial networks. ICLR, Worrall, D. E., Garbin, S. J., Turmukhambetov, D., and</cell></row><row><cell>2019. Brostow, G. J. Harmonic networks: Deep translation and</cell></row><row><cell>CHANGELOG rotation equivariance. In CVPR, 2017.</cell></row><row><cell>Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, v1 ArXiv preprint. Paper accepted to ICML 2019. R., Torralba, A., and Fidler, S. Skip-thought vectors. In Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D.</cell></row><row><cell>v2 ICML camera ready. Added additional networks. Added NIPS, 2015. Spatially transformed adversarial examples. ICLR, 2018.</cell></row><row><cell>robustness measures. ImageNet consistency numbers and Krizhevsky, A. and Hinton, G. Learning multiple layers Yu, F. and Koltun, V. Multi-scale context aggregation by AlexNet results re-evaluated; small fluctuations but no of features from tiny images. Technical report, Citeseer, dilated convolutions. ICLR, 2016. changes in general trends. Compressed main paper to 8 pages. Cifar results moved to supplemental. Small changes 2009. Yu, F., Koltun, V., and Funkhouser, T. Dilated residual</cell></row><row><cell>to text. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet networks. In CVPR, 2017.</cell></row><row><cell>classification with deep convolutional neural networks. Zeiler, M. D. and Fergus, R. Visualizing and understanding In NIPS, 2012. convolutional networks. In ECCV, 2014.</cell></row><row><cell>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-</cell></row><row><cell>based learning applied to document recognition. Proceed-</cell></row><row><cell>ings of the IEEE, 86(11):2278-2324, 1998.</cell></row><row><cell>Lee, C.-Y., Gallagher, P. W., and Tu, Z. Generalizing pool-</cell></row><row><cell>ing functions in convolutional neural networks: Mixed,</cell></row><row><cell>gated, and tree. In AISTATS, 2016.</cell></row><row><cell>Lenc, K. and Vedaldi, A. Understanding image represen-</cell></row><row><cell>tations by measuring their equivariance and equivalence.</cell></row><row><cell>In CVPR, 2015.</cell></row><row><cell>Leung, T. and Malik, J. Representing and recognizing the</cell></row><row><cell>visual appearance of materials using three-dimensional</cell></row><row><cell>textons. IJCV, 2001.</cell></row><row><cell>Lowe, D. G. Object recognition from local scale-invariant</cell></row><row><cell>features. In ICCV, 1999.</cell></row><row><cell>Mahendran, A. and Vedaldi, A. Understanding deep image</cell></row><row><cell>representations by inverting them. In CVPR, 2015.</cell></row></table><note>LeCun, Y., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990.Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. Object detectors emerge in deep scene cnns. In ICLR, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figure 8. CIFAR10 Classification consistency vs. accuracy. VGG (left) and DenseNet (right) networks. Up (more consistent) and to the right (more accurate) is better. Number of sides corresponds to number of filter taps used (e.g., diamond for 4-tap filter); colors correspond to filters trained without (blue) and with (pink) shift-based data augmentation, using various filters. We show accuracy for no shift when training without shifts, and a random shift when training with shifts.</figDesc><table><row><cell></cell><cell cols="2">Delta (Baseline) Binomial-5</cell><cell>Rect-2 Binomial-6</cell><cell>0 15 25 30 20 10 5</cell><cell>0</cell><cell>Triangle-3 Binomial-7 5 10</cell><cell>15</cell><cell>Binomial-4 20 25</cell><cell>30</cell><cell>0.84 0.86 0.88 0.90 0.92 0.94</cell><cell>Accuracy</cell><cell>0.84 0.86 0.88 0.90 0.92 0.94</cell><cell>16 12 Delta (Baseline) 8 Rect-2</cell><cell cols="2">4 Diagonal Shift [pix] 0 4 Triangle-3 Binomial-4 Binomial-5 Binomial-6</cell><cell>8</cell><cell>12 Binomial-7</cell></row><row><cell></cell><cell>1.00</cell><cell cols="2">No augment. Rect-2 Binom-5</cell><cell cols="3">With augment. Tri-3 Binom-6</cell><cell cols="3">Delta-1 (baseline) Binom-4 Binom-7</cell><cell cols="2">1.00</cell><cell></cell><cell>No augment. Rect-2 (baseline) Binom-7</cell><cell cols="2">With augment. Tri-3</cell><cell>Delta-1 Binom-5</cell></row><row><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.98</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Consistency</cell><cell>0.92 0.94 0.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.92 0.94 0.96</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.90</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.88</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.91</cell><cell></cell><cell>0.92</cell><cell cols="3">0.93 Accuracy</cell><cell cols="2">0.94</cell><cell>0.95</cell><cell></cell><cell cols="2">0.91</cell><cell>0.92</cell><cell>0.93 Accuracy</cell><cell>0.94</cell><cell>0.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>+0.69 81.33 +3.15 72.15 +0.56 89.24 +0.72 74.01 +0.65 90.72 +1.48 Tri-3 56.90 +0.35 82.15 +3.97 72.20 +0.61 89.60 +1.08 73.91 +0.55 91.10 +1.86 Bin-5 56.58 +0.03 82.51 +4.33 72.33 +0.74 90.19 +1.67 74.05 +0.69 91.35 +2.11 77.82 +0.45 91.04 +1.23 75.04 +0.61 89.53 +0.72 72.63 +0.75 87.33 +0.83 Tri-3 78.13 +0.76 91.62 +1.81 75.14 +0.71 89.78 +0.97 72.59 +0.71 87.46 +0.96 Bin-5 77.92 +0.55 91.74 +1.93 75.03 +0.60 90.39 +1.58 72.50 +0.62 87.79 +1.29</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AlexNet</cell><cell></cell><cell></cell><cell cols="2">VGG16</cell><cell></cell><cell></cell><cell cols="2">VGG16bn</cell></row><row><cell>Filter</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell></row><row><cell></cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell></row><row><cell cols="2">Baseline 56.55</cell><cell>-</cell><cell>78.18</cell><cell>-</cell><cell>71.59</cell><cell>-</cell><cell>88.52</cell><cell>-</cell><cell>73.36</cell><cell>-</cell><cell>89.24</cell><cell>-</cell></row><row><cell>Rect-2</cell><cell cols="3">57.24 ResNet18</cell><cell></cell><cell></cell><cell cols="2">ResNet34</cell><cell></cell><cell></cell><cell cols="2">ResNet50</cell></row><row><cell>Filter</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell></row><row><cell></cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell></row><row><cell cols="2">Baseline 69.74</cell><cell>-</cell><cell>85.11</cell><cell>-</cell><cell>73.30</cell><cell>-</cell><cell>87.56</cell><cell>-</cell><cell>76.16</cell><cell>-</cell><cell>89.20</cell><cell>-</cell></row><row><cell>Rect-2</cell><cell cols="12">71.39 +1.65 86.90 +1.79 74.46 +1.16 89.14 +1.58 76.81 +0.65 89.96 +0.76</cell></row><row><cell>Tri-3</cell><cell cols="12">71.69 +1.95 87.51 +2.40 74.33 +1.03 89.32 +1.76 76.83 +0.67 90.91 +1.71</cell></row><row><cell>Bin-5</cell><cell cols="12">71.38 +1.64 88.25 +3.14 74.20 +0.90 89.49 +1.93 77.04 +0.88 91.31 +2.11</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet101</cell><cell></cell><cell></cell><cell cols="2">DenseNet121</cell><cell></cell><cell></cell><cell cols="2">MobileNetv2</cell></row><row><cell>Filter</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell><cell cols="2">Accuracy</cell><cell cols="2">Consistency</cell></row><row><cell></cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell><cell>Abs</cell><cell>∆</cell></row><row><cell cols="2">Baseline 77.37</cell><cell>-</cell><cell>89.81</cell><cell>-</cell><cell>74.43</cell><cell>-</cell><cell>88.81</cell><cell>-</cell><cell>71.88</cell><cell>-</cell><cell>86.50</cell><cell>-</cell></row><row><cell>Rect-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>ResNet50 on ImageNet-P<ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> Flip Rate (FR) (lower is better)</figDesc><table><row><cell></cell><cell cols="2">Noise</cell><cell>Blur</cell><cell></cell><cell cols="2">Weather</cell><cell></cell><cell cols="2">Geometric</cell><cell></cell><cell>Mean</cell><cell></cell></row><row><cell></cell><cell cols="8">Gauss Shot Motion Zoom Snow Bright Translate Rotate</cell><cell>Tilt</cell><cell cols="3">Scale Unnorm Norm</cell></row><row><cell cols="3">Baseline 14.04 17.38</cell><cell>6.00</cell><cell>4.29</cell><cell>7.54</cell><cell>3.03</cell><cell>4.86</cell><cell>6.79</cell><cell cols="2">4.01 11.32</cell><cell>7.92</cell><cell>57.99</cell></row><row><cell>Rect-2</cell><cell cols="2">14.08 17.16</cell><cell>5.98</cell><cell>4.21</cell><cell>7.34</cell><cell>3.20</cell><cell>4.42</cell><cell>6.43</cell><cell cols="2">3.80 10.61</cell><cell>7.72</cell><cell>56.70</cell></row><row><cell>Tri-3</cell><cell cols="2">12.59 15.57</cell><cell>5.39</cell><cell>3.79</cell><cell>6.98</cell><cell>3.01</cell><cell>3.95</cell><cell>5.80</cell><cell>3.53</cell><cell>9.90</cell><cell>7.05</cell><cell>51.91</cell></row><row><cell>Bin-5</cell><cell cols="2">12.39 15.22</cell><cell>5.44</cell><cell>3.72</cell><cell>6.76</cell><cell>3.15</cell><cell>3.78</cell><cell>5.67</cell><cell>3.44</cell><cell>9.45</cell><cell>6.90</cell><cell>51.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">Flip Rate (FR) [Percentage reduced from Baseline] (higher is better)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Noise</cell><cell>Blur</cell><cell></cell><cell cols="2">Weather</cell><cell></cell><cell cols="2">Geometric</cell><cell></cell><cell>Mean</cell><cell></cell></row><row><cell></cell><cell cols="8">Gauss Shot Motion Zoom Snow Bright Translate Rotate</cell><cell>Tilt</cell><cell cols="3">Scale Unnorm Norm</cell></row><row><cell>Baseline</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Rect-2</cell><cell>-0.25</cell><cell>1.27</cell><cell>0.30</cell><cell>1.73</cell><cell>2.65</cell><cell>-5.75</cell><cell>9.21</cell><cell>5.34</cell><cell>5.16</cell><cell>6.20</cell><cell>2.55</cell><cell>2.22</cell></row><row><cell>Tri-3</cell><cell cols="2">10.35 10.41</cell><cell>10.09</cell><cell>11.58</cell><cell>7.42</cell><cell>0.53</cell><cell>18.89</cell><cell cols="3">14.55 12.02 12.50</cell><cell>11.03</cell><cell>10.48</cell></row><row><cell>Bin-5</cell><cell cols="2">11.81 12.42</cell><cell>9.27</cell><cell cols="2">13.28 10.28</cell><cell>-4.10</cell><cell>22.27</cell><cell cols="3">16.59 14.11 16.50</cell><cell>12.91</cell><cell>11.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Top-5 Distance (T5D) (lower is better)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Noise</cell><cell>Blur</cell><cell></cell><cell cols="2">Weather</cell><cell></cell><cell cols="2">Geometric</cell><cell></cell><cell>Mean</cell><cell></cell></row><row><cell></cell><cell cols="8">Gauss Shot Motion Zoom Snow Bright Translate Rotate</cell><cell>Tilt</cell><cell cols="3">Scale Unnorm Norm</cell></row><row><cell>Baseline</cell><cell>3.92</cell><cell>4.55</cell><cell>1.63</cell><cell>1.20</cell><cell>1.95</cell><cell>1.00</cell><cell>1.68</cell><cell>2.15</cell><cell>1.40</cell><cell>3.01</cell><cell>2.25</cell><cell>78.36</cell></row><row><cell>Rect-2</cell><cell>3.94</cell><cell>4.54</cell><cell>1.63</cell><cell>1.19</cell><cell>1.91</cell><cell>1.06</cell><cell>1.56</cell><cell>2.07</cell><cell>1.34</cell><cell>2.89</cell><cell>2.21</cell><cell>77.40</cell></row><row><cell>Tri-3</cell><cell>3.67</cell><cell>4.28</cell><cell>1.50</cell><cell>1.10</cell><cell>1.85</cell><cell>1.00</cell><cell>1.43</cell><cell>1.92</cell><cell>1.25</cell><cell>2.72</cell><cell>2.07</cell><cell>72.36</cell></row><row><cell>Bin-5</cell><cell>3.65</cell><cell>4.22</cell><cell>1.53</cell><cell>1.09</cell><cell>1.78</cell><cell>1.04</cell><cell>1.39</cell><cell>1.89</cell><cell>1.25</cell><cell>2.66</cell><cell>2.05</cell><cell>71.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">Top-5 Distance (T5D) [Percentage reduced from Baseline] (higher is better)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Noise</cell><cell>Blur</cell><cell></cell><cell cols="2">Weather</cell><cell></cell><cell cols="2">Geometric</cell><cell></cell><cell>Mean</cell><cell></cell></row><row><cell></cell><cell cols="8">Gauss Shot Motion Zoom Snow Bright Translate Rotate</cell><cell>Tilt</cell><cell cols="3">Scale Unnorm Norm</cell></row><row><cell>Baseline</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Rect-2</cell><cell>-0.41</cell><cell>0.09</cell><cell>-0.12</cell><cell>0.39</cell><cell>1.71</cell><cell>-5.83</cell><cell>7.19</cell><cell>3.74</cell><cell>3.90</cell><cell>3.93</cell><cell>1.51</cell><cell>1.22</cell></row><row><cell>Tri-3</cell><cell>6.53</cell><cell>5.82</cell><cell>7.95</cell><cell>8.10</cell><cell>5.21</cell><cell>-0.65</cell><cell>15.11</cell><cell cols="3">10.82 10.26 9.80</cell><cell>7.86</cell><cell>7.65</cell></row><row><cell>Bin-5</cell><cell>7.03</cell><cell>7.26</cell><cell>6.24</cell><cell>9.15</cell><cell>8.45</cell><cell>-4.13</cell><cell>17.73</cell><cell cols="3">12.15 10.62 11.80</cell><cell>8.91</cell><cell>8.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Stability to Perturbations. Flip Rate (FR) and Top-5 Distance (T5D) of ResNet50 on ImageNet-P. Though our antialiasing is motivated by shift-invariance ("translate"), it adds additional stability across many other perturbation types.</figDesc><table><row><cell>Input Label Map</cell><cell>Delta (Baseline) [1]</cell><cell>Rectangle-2 [1 1]</cell><cell>Generated Facades Triangle-3 [1 2 1]</cell><cell>Binomial-4 [1 3 3 1]</cell><cell>Binomial-5 [1 4 6 4 1]</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding deep features with computer-generated imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why do deep convolutional networks generalize so poorly to small image transformations? In arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Invariance and stability of deep convolutional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="671" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A rotation and a translation suffice: Fooling cnns with simple transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Polar transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manitest</surname></persName>
		</author>
		<title level="m">Are classifiers really invariant? In BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The redundant discrete wavelet transform and additive noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="629" to="632" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neocognitron: A selforganizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982" />
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Pearson</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measuring invariances in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geodesics of learned representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepdream-a code example for visualizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosinski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Certain topics in telegraph transmission theory. Transactions of the American Institute of Electrical Engineers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nyquist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1928" />
			<biblScope unit="page" from="617" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconvolution</surname></persName>
		</author>
		<idno>doi: 10.23915/ distill.00003</idno>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Discrete-Time Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Buck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pooling is neither necessary nor sufficient for appropriate deformation stability in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno>ICANN. 2010</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shiftable multiscale transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="587" to="607" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
