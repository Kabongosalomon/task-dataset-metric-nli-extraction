<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Liu</surname></persName>
							<email>liutianyi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanhao</forename><surname>Zhou</surname></persName>
							<email>whzhou@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting relations is critical for knowledge base completion and construction in which distant supervised methods are widely used to extract relational facts automatically with the existing knowledge bases. However, the automatically constructed datasets comprise amounts of low-quality sentences containing noisy words, which is neglected by current distant supervised methods resulting in unacceptable precisions. To mitigate this problem, we propose a novel word-level distant supervised approach for relation extraction. We first build Sub-Tree Parse (STP) to remove noisy words that are irrelevant to relations. Then we construct a neural network inputting the subtree while applying the entity-wise attention to identify the important semantic features of relational words in each instance. To make our model more robust against noisy words, we initialize our network with a priori knowledge learned from the relevant task of entity classification by transfer learning. We conduct extensive experiments using the corpora of New York Times (NYT) and Freebase. Experiments show that our approach is effective and improves the area of Precision/Recall (PR) from 0.35 to 0.39 over the state-of-the-art work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction aims to extract relations between pairs of marked entities in raw texts. Traditional supervised methods are time-consuming for the requirement of large-scale manually labeled data. Thus, <ref type="bibr" target="#b14">Mintz et al. (2009)</ref> propose the distant supervised relation extraction, in which amounts of sentences are crawled from web pages of New York Times (NYT) and labeled with a known knowledge base automatically. The method assumes that if two entities have a relation in a known knowledge base, all instances that mention these two entities will express the same relation. Obviously, this assumption is too strong, since a sentence that mentions the two entities does not necessarily express the relation contained in a known knowledge base. As described in <ref type="bibr" target="#b20">Riedel et al. (2010)</ref>, the assumption leads to the wrong labeling problem. In order to tackle the wrong labeling problem, various multi-instance learning methods are adopted by mitigating noise between sentences <ref type="bibr" target="#b7">(Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b25">Zeng et al., 2015;</ref><ref type="bibr" target="#b11">Lin et al., 2016)</ref>. Despite the wrong labeling problem, distant supervised methods may suffer from the low quality of sentences which derive from the large-scale automatically constructed dataset by crawling web pages <ref type="bibr" target="#b23">(Yang et al., 2017)</ref>. To handle the problem of low-quality sentences, we have to face two major challenges: (1) Reduce word-level noise within sentences; (2) Improve the robustness of relation extraction against noise.</p><p>To explain the influence of word-level noise within sentences, we consider the following sentence as an example: [It is no accident that the main event will feature the junior welterweight champion miguel cotto, a puerto rican, against Paul Malignaggi, an Italian American from Brooklyn.], where Paul Malignaggi and Brooklyn are two corresponding entities. The subsentence [Paul Malignaggi, an Italian American from Brooklyn.] keeps enough words to express the relation /people/person/place of birth, and the other words could be regarded as noise that may hamper the extractor's performance. Meanwhile, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, half of the original sentences are longer than 40 words, which means that there are many irrelevant words inside sentences. To be more detail, there are about 12 noisy words in each sentence on average, and 99.4% of sentences in the NYT-10 dataset have noise. Although the Shortest Dependency Path (SDP) proposed by <ref type="bibr" target="#b22">Xu et al. (2015)</ref> tries to get rid of irrelevant words for relation extraction, it is not suitable to handle such informal sentences. Moreover, word-level attention has been leveraged to alleviate the impact of noisy words <ref type="bibr" target="#b27">(Zhou et al., 2016)</ref>, but it weakens the importance of entity features for relation extraction. As for the second challenge, a robust model could extract precise relation features even from low-quality sentences containing noisy words. However, previous neural methods are always lacking in robustness because parameters are initialized randomly and hard to tune with noisy training data, resulting in the poor performance of extractors. Inspired by <ref type="bibr" target="#b10">Kumagai (2016)</ref>, initializing neural networks with a priori knowledge learned from relevant tasks by transfer learning could improve the robustness of the target task. For the relation extraction, entity type classification can be used as the relevant task since entity types provide abundant background knowledge. For instance, the sentence [Alfead Kahn, the Cornell-University economist who led the fight to deregulate airplanes.] has a relation business/person/company, which is hard to decide without the information that Alfead Kahn is a person and Cornell-University is a company. Therefore, type features learned from entity type classification are proper a priori knowledge to initialize the relation extractor.</p><p>In this paper, we propose a novel word-level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words. To reduce innersentence noise, we utilize a novel Sub-Tree Parse (STP) method to remove irrelevant words by intercepting a subtree under the parent of entities' lowest common ancestor. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the average length of the parsed sentences is much shorter. Furthermore, the entity-wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task-relevant features. To tackle the second challenge, we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning. The experimental results show that our model can achieve satisfactory performance among the state-of-the-art works. Our contributions are summarized as follows:</p><p>• To handle the problem of low-quality sentences, we propose the STP to remove noisy words of sentences and the entity-wise attention mechanism to enhance semantic features of relational words.</p><p>• We first propose to initialize the neural relation extractor with a priori knowledge learned from entity type classification, which strengthens its robustness against low-quality corpus.</p><p>• Our model achieves significant results for distant supervised relation extraction, which improves the Precision/Recall (PR) curve area from 0.35 to 0.39 and increases top 100 predictions by 6.3% over the state-of-the-art work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The distant supervised method plays an increasingly essential role in relation extraction due to its less requirement of human labor <ref type="bibr" target="#b14">(Mintz et al., 2009</ref>). However, an evident drawback of the method is the wrong labeling problem. Thus, multi-instance and multi-label learning methods are proposed to address this issue <ref type="bibr" target="#b20">(Riedel et al., 2010;</ref><ref type="bibr" target="#b7">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Surdeanu et al., 2012)</ref>. Meanwhile, other researches <ref type="bibr" target="#b0">(Angeli et al., 2014;</ref><ref type="bibr" target="#b6">Han and Sun, 2016)</ref> incorporate humandesigned features and leverage Natural Language Processing (NLP) tools. As neural networks have been widely used, an increasing number of researches have been proposed. <ref type="bibr" target="#b25">Zeng et al. (2015)</ref> use a piecewise convolutional neural network with multi-instance learning. Furthermore, selective attention over instances with the neural network is proposed <ref type="bibr" target="#b11">(Lin et al., 2016)</ref>. Making use of entity description, <ref type="bibr" target="#b8">Ji et al. (2017)</ref> assign more precise attention weights. Focused on the imbalance of datasets, a soft label method has been proposed by .</p><p>Recently, reinforcement learning and adversarial learning are widely used to select the valid instances for relation extraction <ref type="bibr" target="#b5">(Feng et al., 2018;</ref><ref type="bibr">Qin et al., 2018b,a)</ref>.</p><p>However, above methods ignore inner-sentence noise. To better remove irrelevant words, the SDP between entities is proved to be effective (De <ref type="bibr" target="#b4">Marneffe and Manning, 2008;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014;</ref><ref type="bibr" target="#b22">Xu et al., 2015;</ref><ref type="bibr" target="#b15">Miwa and Bansal, 2016)</ref>. Nevertheless, in our observation, the SDP deals with informal texts difficultly (See Section 3.1 for details). Furthermore, word-level attention is adopted to focus on relational words for relation extraction <ref type="bibr" target="#b27">(Zhou et al., 2016)</ref>, but it hinders the effect of entity words.</p><p>Transfer learning proposed by Pratt (1993) provides a new approach to leverage knoweldge extracted by related tasks to enhance the performance of the target task. Furthermore, parameter transfer learning is proved to be effective to improve the stability of models by initializing model parameters reasonably (Pan and <ref type="bibr" target="#b16">Yang, 2010;</ref><ref type="bibr" target="#b10">Kumagai, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present our methodology for distant supervised relation extraction. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall architecture of our model. Our model is divided into three parts:</p><p>Sub-Tree Parser. Input instances are parsed to dependency parse trees by the Stanford parser 1 <ref type="bibr" target="#b2">(Chen and Manning, 2014)</ref> at first. Then words in the STP and relative positions are transformed to distributed representations.</p><p>Entity-Wise Neural Extractor. Given the representation of each subtree, Bidirectional Gated Recurrent Unit (BGRU) extracts specific features. Then, entity-wise attention combined with wordlevel attention is applied to reducing irrelevant features for relation extraction. Finally, the sentencelevel attention is used to alleviate the influence of wrong labeling sentences.</p><p>Parameter-Transfer Initializer. The transfer learning method pre-trains our model parameters from the task of entity type classification aiming at boosting the performance of relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sub-Tree Parser</head><p>Each instance is put into the dependency parse module to build the dependency parse tree in the 1 https://nlp.stanford.edu/software/lex-parser.shtml first place. Then we can tailor the sentences based on the STP method. Finally, we transform word tokens and position tokens of each instance to distributed representations by embedding matrixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-Tree Parse</head><p>In order to reduce inner-sentence noise and extract relational words, we propose the STP method which intercepts the subtree of each instance under the parent of entities' lowest common ancestor. For instance, in <ref type="figure" target="#fig_1">Figure 2</ref>(b), China and Shanghai are entities connected directly with the appositive relation. The instance <ref type="bibr">[In 1990</ref>, he lives in Shanghai, China.] will be transformed to [in Shanghai, China.] on the basis of the STP, where in is the parent of Shanghai and China lowest common ancestor and kept as important information for expressing the relation location/location/contain. Words connected by the imaginary line indicating the extracted subtree are reorganized into their original sequence order to form network inputs.</p><p>Among the parse tree, the SDP has been widely used by <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> and <ref type="bibr" target="#b22">Xu et al. (2015)</ref> to help models focus on relational words. However, in our observation, the SDP is not appropriate in the condition that key relation words are not in the SDP. Although additional information (dependency relations between words) is adopted to enhance the performance of SDP, we found they have the minor effect through our experiment. Thus, we do not make use of other types of linguistic information. As <ref type="figure" target="#fig_1">Figure 2</ref>(b) shows, in the SDP method, the original sentence will be transformed to [Shanghai China] because Shanghai and China are connected with each other directly in the dependency parse tree, which results in deleting the keyword in and may confuse the model when extracting relations. Compared with SDP, the STP method is more appropriate to extract useful information in informal sentences where relational words are always not in the SDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word and Position Embeddings</head><p>The inputs of the network are word and position tokens, which are transformed to the distributed representations before they are fed into the neural model. We map j th word in the i th instance to a vector of k dimensions denoted as x w ij ∈ R k through the Skip-Gram model <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref>. Like <ref type="bibr" target="#b26">Zeng et al. (2014)</ref>, we leverage Pos1 and Pos2 to specify entity pairs, which are defined as the relative distances of current word from head  entity and tail entity. For instance, in <ref type="figure" target="#fig_1">Figure 2</ref> relative distances of lived from Shanghai and China are -2 and -4 respectively. Then, the position token of each word is transformed to a vector in l dimensions. Position embeddings are denoted as x p1 ij ∈ R l and x p2 ij ∈ R l respectively. Finally, the input representation for x ij is concatenated by word embedding x w ij , position embeddings x p1 ij and x p2 ij , which is denoted as</p><formula xml:id="formula_0">x ij = [x w ij ; x p1 ij ; x p2 ij ] where x ij ∈ R k+2l .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity-Wise Neural Extractor</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we transform the STP into feature vectors by BGRU at first. Next, entity-wise attention combined with the hierarchical-level attention mechanism is applied to enhancing semantic features of each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BGRU over STP</head><p>Since the transfer learning and entity-wise attention require the specific features of entities in tree parsed instances as their input, we adopt Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> to be our based relation extractor, which can extract global information of each word by pointing out its corresponding position in the sequence. It can be briefly described as below:</p><formula xml:id="formula_1">h it = GRU (x it )<label>(1)</label></formula><p>where x it is the t th word representation in the i th parsed instance as described in the input layer, and h it ∈ R m is the hidden state of GRU in m dimensions. Furthermore, BGRU implementing GRU in a different direction can access future as well as past context. Under our circumstance, BGRU combined with the STP can extract semantic and syntactic features adequately. <ref type="figure" target="#fig_1">Figure 2(a)</ref> shows the processing of BGRU over STP. The following equation defines the operation mathematically.</p><formula xml:id="formula_2">h it = [ − → h it ⊕ ← − h it ]<label>(2)</label></formula><p>In above equation, the t th word output h it ∈ R m of BGRU is the element-wise addition of the t th hidden states of forward GRU and backward one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity-wise Attention</head><p>To reduce noise within sentences, we propose the entity-wise attention mechanism to help our model focus on relational words, especially entity words for relation extraction. Assume that H i is the i th instance matrix consisting of T word vectors [h i1 , h i2 , · · · , h iT ] produced by BGRU.</p><p>Not all words contribute equally to the representation of the sentence. Entity words are of great importance because they are significantly beneficial to relation extraction. In our model, entitywise attention assigns the weight α e it to focus on the target entity and removes noise further. It is defined as follows:</p><formula xml:id="formula_3">α e it = 1 t = head, tail 0 others<label>(3)</label></formula><p>In the above equation, α e it = 1 if t th word belongs to the head or tail entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical-level Attention</head><p>To reduce inner-sentence noise further and deemphasize noisy sentences, we incorporate wordlevel attention and sentence-level attention as hierarchical-level attention which is introduced in <ref type="bibr" target="#b24">Yang et al. (2016)</ref>.</p><p>Word-level Attention. It assigns an additional weight α w it to relational word h it due to its relevance to the relation as described by <ref type="bibr" target="#b27">Zhou et al. (2016)</ref>. It can be described as follows:</p><formula xml:id="formula_4">α w it = exp(h it A w r w ) T t=1 exp(h it A w r w )<label>(4)</label></formula><p>where A w is a weighted matrix, and vector r w can be seen as a high level representation in a fixed query what is the informative word over the other words.</p><p>The i th sentence representation S i ∈ R m is computed as a weighted sum of h it :</p><formula xml:id="formula_5">S i = T t=1 (α w it + α e it )h it<label>(5)</label></formula><p>Sentence-level Attention. After we get the instance representation S i , we adopt the selective attention mechanism over instances to de-emphasize the noisy sentence <ref type="bibr" target="#b11">(Lin et al., 2016)</ref>, which is described as follows:</p><formula xml:id="formula_6">S = i α s i S i (6) α s i = exp(S i A s r s ) i exp(S i A s r s )<label>(7)</label></formula><p>where A s is a weighted matrix, r s is the query vector associated with the relation, and S ∈ R m is the output of the sentence-level attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter-Transfer Initializer</head><p>The transfer learning method pre-trains our model parameters in the entity type classification task, which in turn contributes to the relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-learn the Entity Type</head><p>As entity type information plays a significant role in detecting relation types, the entity type classification task is considered to be the source task, which is learned before the relation extraction task. According to Eq. 6, outputs of the sentencelevel attention layer for the head entity and tail entity task are S head and S tail respectively. They are ultimately fed into the softmax layer: <ref type="formula">(8)</ref> where W i and b i are the weight and bias for the entity type classification task respectively,p i ∈ R zt is the predicted probability of each class and z t is the number of entity classes. The loss function of the source task is the negative log-likelihood of the true labels:</p><formula xml:id="formula_7">p i = sof tmax(W i S i +b i ); i ∈ {head, tail}</formula><formula xml:id="formula_8">J e (θ 0 , θ head , θ tail ) = β θ 0 2 + t (− 1 z t λ t zt i=1 y t i log(p t i ) + β θ t 2 ) t ∈ {head, tail}<label>(9)</label></formula><p>where λ t is the weight of each task, θ 0 is the shared model parameters, θ head and θ tail are individual parameters for the head and tail entity classification tasks respectively, y t ∈ R zt is the onehot vector representing ground truth, and β is the hyper-parameter for L2 regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train the Relation Extractor</head><p>Based on the pre-trained model in the entity type classification task, the relation extractor initializes shared parameters θ 0 within the best state of the pre-trained model and independent parameters θ r randomly. Same as the entity type classification task, the output S r of the attention layer for the relation extraction task is finally fed into the softmax layer and the loss is calculated by cross entropy, which is defined as follows:</p><formula xml:id="formula_9">p = sof tmax(W r S r + b r ) (10) J r (θ 0 , θ r ) = − 1 z r zr i=1 y i log(p i ) +β( θ 0 2 + θ r 2 )<label>(11)</label></formula><p>where W r , b r , y ∈ R zr ,p ∈ R zr , θ r and β are defined similarly in the entity type classification task. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, two tasks share all layers except attention and output layers. Assume that the set of total model parameters is θ. Thus, θ, θ 0 , θ r , θ head and θ tail have a relationship described in the following equations:</p><formula xml:id="formula_10">θ = θ 0 ∪ θ head ∪ θ tail ∪ θ r (12) θ i = {A w i , r w i , A s i , r s i , W i , b i } i ∈ {head, tail, r}<label>(13)</label></formula><p>where A w i , r w i , A s i , r s i , W i and b i are parameters in attention and output layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimize the Objective Function</head><p>At first, we minimize J to obtain θ 0 at the best model stateθ 0 for entity type classification. Then we minimize J r for the best performance of relation extraction under the initialization of θ 0 to bê θ 0 . Above process can be summarized as the following equation:</p><formula xml:id="formula_11">min J(θ) =λJ e (θ 0 , θ head , θ tail )+ (1 − λ)J r (θ 0 , θ r )<label>(14)</label></formula><p>where λ ∈ (0, 1) is the hyperparameter to determine the importance of each task at different training steps. We use Adam (Kingma and Ba, 2014) optimizer to minimize the objective J(θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are designed to demonstrate that our model alleviates the influence of word-level noise arising from low-quality sentences. In this section, we first introduce the dataset and evaluation metrics. Next, we describe parameter settings. Then we evaluate effects of the STP, entity-wise attention and the parameter-transfer initializer. Finally, we compare our model with the state-of-theart works by several evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>To evaluate the performance of our model, we adopt a widely used dataset NYT-10 developed by <ref type="bibr" target="#b20">Riedel et al. (2010)</ref>. NYT-10 dataset is constructed by aligning relational facts in Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>  Like previous works, we evaluate our model with the held-out metrics, which compare relations found by models with those in Freebase. The held-out evaluation provides a convenient way to assess models. We report both the PR curve and Precision at top N predictions (P@N) at various numbers of instances under each entity pair:</p><p>One: For each entity pair, we randomly select one instance to represent the relation.</p><p>Two: For each entity pair, we randomly select two instances to represent the relation.</p><p>All: For each entity pair, we select all instances to represent the relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>In the experiment, we utilize word2vec 2 to train word embeddings on NYT corpus. We use the cross-validation to tune our model and grid search to determine model parameters. The grid search approach is used to select optimal learning rate lr for Adam optimizer among {0.1, 0.001, 0.0005, 0.0001}, GRU size m ∈ {100, 160, 230, 400}, position embedding size l ∈ {5, 10, 15, 20}. <ref type="table" target="#tab_2">Table 1</ref> shows all parameters for our task. We follow experienced settings for other parameters because they make little influence to our model performance.</p><p>GRU size m 230 Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity-Task weights(λ head , λ tail ) 0.5,0.5 Entity-Relation Task weight λ 0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty β 0.0001 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Various Model Parts</head><p>In this section, we utilize the PR curve to evaluate the effects of three main parts in our model: the STP, entity-wise attention and the parametertransfer initializer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the STP</head><p>To demonstrate the effect of the STP, we adopt BGRU with Word-Level Attention (WLA) proposed by <ref type="bibr" target="#b27">Zhou et al. (2016)</ref> as our base model. We compare the performance of BGRU, BGRU+STP, and BGRU+SDP. From <ref type="figure" target="#fig_2">Figure 3</ref>, we can observe that the model with the STP performs best, and the SDP model obtains an even worse result than the pure one. The PR curve areas of BGRU+SDP and BGRU are about 0.332 and 0.337 respectively, while BGRU+STP increases it to 0.366. The result indicates: (1) Our STP can get rid of irrelevant words in each instance and obtain more precise sentence representation for relation extraction. It proves that our STP module is effective. (2) The SDP method is not appropriate to handle low-quality sentences where key relation words are not in the SDP.  To evaluate the effect of entity-wise attention combined with word-level attention, we utilize BGRU in three settings on our tree parsed data and original data. One setting is to use WLA mechanism only (BGRU). The second one is to replace WLA with the Entity-Wise Attention (EWA) mechanism (BGRU-WLA+EWA). The third one is to incorporate two mechanisms (BGRU+EWA). From <ref type="table" target="#tab_4">Table 2</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>, we can obtain: (1) Regardless of the dataset that we employ, BGRU-WLA(+STP)+EWA outperforms BGRU(+STP). To be more specific, the PR curve area has a relative improvement of over 2.3%, which demonstrates that entity-wise hidden states in the BGRU present more precise relational features than other word states. (2) BGRU(+STP)+EWA achieves further improvements and outperforms the baseline by over 4.6%, because it considers more information than entity or relational words alone. Thus, it indicates that entity words are essential for relation extraction, but they can not represent features of the whole sentence without other words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Entity-wise Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Parameter-Transfer Initializer</head><p>To evaluate the effect of the parameter-transfer initializer in our model, we leverage BGRU under four circumstances. The first one is to directly apply it on the original dataset. The second one tests BGRU combined with Transfer Learning (TL) on the original dataset. The third one uses BGRU on our STP dataset. The fourth one examines BGRU+TL on our STP dataset.</p><p>From <ref type="figure">Figure 5</ref>, we can conclude: (1) Regardless of the dataset that we use, models with TL achieve better performance, which improve the PR curve area by over 4.7%. It demonstrates that  transfer learning helps our model become more robust against noise. (2) BGRU+STP+TL achieves the best performance and increases the area to 0.383, while areas of BGRU, BGRU+STP and BGRU+TL are 0.337, 0.366 and 0.372 respectively. It means that the TL method works well with the STP and can resist noisy words further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Baselines</head><p>To evaluate our approach, we select the following six methods as our baseline:</p><p>Mintz <ref type="bibr" target="#b14">(Mintz et al., 2009)</ref> proposes the humandesigned feature model.</p><p>MultiR <ref type="bibr" target="#b7">(Hoffmann et al., 2011)</ref> puts forward a graphical model.</p><p>MIML <ref type="bibr" target="#b21">(Surdeanu et al., 2012)</ref> proposes a multi-instance multi-label model.</p><p>PCNN <ref type="bibr" target="#b25">(Zeng et al., 2015)</ref> puts forward a piecewise CNN for relation extraction.</p><p>PCNN+ATT <ref type="bibr" target="#b11">(Lin et al., 2016)</ref> proposes the selective attention mechanism with PCNN.</p><p>BGRU <ref type="bibr" target="#b27">(Zhou et al., 2016)</ref> proposes a BGRU with the word-level attention mechanism. As <ref type="figure" target="#fig_5">Figure 6</ref> shows, we can observe: (1) BGRU+STP+EWA achieves the best PR curve over baselines, which improves the area to 0.38 over 0.33 of PCNN, 0.34 of BGRU and 0.35 of PCNN+ATT. At the recall rate of 0.25, our model can still achieve a precision rate above 0.6. It demonstrates that BGRU+STP+EWA is effective because the STP and entity-wise attention combined with word-level attention can reduce inner-sentence noise at a fine-grained level.</p><p>(2) Integrated with transfer learning, BGRU+STP+EWA+TL performs much better and increases the PR curve area to 0.392. It means that the model is pre-trained for better parameter initialization so the TL model becomes more robust against noisy words. Parameter transfer learning can be applied in better feature extractors for further improvement.</p><p>Following previous works, we adopt P@N as a quantitative indicator to compare our model with baselines based on various instances under each relational tuple. In <ref type="table">Table 3</ref>, we report P@100, P@200, P@300 and the mean of them for each model in the held-out evaluation. We can find: (1) Compared with baselines, BGRU+STP+EWA+TL achieves the best performance in all test settings, which increases the performance of PCNN+ATT in three settings by 6.3%, 7.6%, and 7.7% respectively. It demonstrates that the integrated model is the most effective;</p><p>(2) Our STP and entity-wise attention combined with word-level attention reduce innersentence noise effectively, and outperform baselines by over 5%;</p><p>(3) Our neural extractor initialized with a priori knowledge learned from entity type classification is more robust against wordlevel noise where BGRU+STP+EWA+TL has an improvement of 2% over BGRU+STP+EWA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel word-level approach for distant supervised relation extraction. It aims at tackling the low-quality corpus by reducing inner-sentence noise and improving the robustness against noisy words. To alleviate the influence of word-level noise, we propose the STP. Meanwhile, entity-wise attention combined with word-level attention helps the model focus more on relational words. Furthermore, parameter transfer learning makes our model more robust against noise by reasonable initialization of parameters. The experimental results show that our model significantly and consistently outperforms the state-of-the-art method.</p><p>In the future, we will incorporate the SDP and STP to obtain more precise shortened sentences. Furthermore, we will conduct research in how to utilize entity information to assign more appropriate initial parameters of the relation extractor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of sentence length distribution between original data and parsed data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of our model is used for distant supervised relation extraction, expressing the process of handling instances. There are two modules described in detail: (a) One is the BGRU; (b) Another is the STP, where words in the red brackets represent entities (better viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>PR curves for BGRU, BGRU+SDP and BGRU+STP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>PR curves for BGRU, BGRU-WLA+EWA and BGRU+EWA on various datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5: PR curves for BGRU, BGRU+TL, BGRU+STP and BGRU+STP+TL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Performance comparison of the proposed method with baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>,950 relational facts. There are 53 relations including a special relation NA, which means that there is no relation between the entity pair in the instance. Meanwhile, all relations in Freebase are defined on head types and tail types. Therefore, we can construct datasets for type prediction tasks with the same dataset. The dataset has 29 head types and 26 tail types.</figDesc><table><row><cell>entity pairs, and 18,252 relational facts; for testing</cell></row><row><cell>data, there are 172,448 sentences, 96,678 entity</cell></row><row><cell>pairs and 1</cell></row><row><cell>with the NYT corpus, where sen-</cell></row><row><cell>tences from 2005-2006 are used as training set,</cell></row><row><cell>and sentences from 2007 are used for testing. For</cell></row><row><cell>training data, there are 522,611 sentences, 281,270</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Parameter Settings</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>PR curve areas for BGRU, BGRU-WLA+EWA and BGRU+EWA on various datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Mean 100 200 300 Mean 100 200 300 Mean Mintz 35.0 37.5 37.3 36.6 51.0 42.0 43.3 45.4 54.0 50.5 45.3 49.9 MultiR 64.0 61.5 53.7 59.7 62.0 61.5 58.7 61.1 75.0 65.0 62.0 67.3 MIML 62.0 59.0 54.7 58.6 69.0 59.5 59.0 62.5 70.0 64.5 60.3 64.9 PCNN 73.3 64.8 56.8 65.0 70.3 67.2 63.1 66.9 72.3 69.7 64.1 68.7 PCNN+ATT 78.</figDesc><table><row><cell>Test Settings</cell><cell>One</cell><cell>Two</cell><cell>All</cell></row><row><cell>P@N</cell><cell>100 200 300</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining distant and partial supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1556" to="1567" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5779" to="5786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2950" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning bound for parameter transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Kumagai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2721" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A soft-label method for noisetolerant distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1790" to="1795" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminability-based transfer between neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lorien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dsgan: Generative adversarial training for distant supervision relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename><surname>Weiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename><surname>Weiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2137" to="2147" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crowdsourced time-sync video tagging using semantic association graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="547" to="552" />
		</imprint>
	</monogr>
	<note>Multimedia and Expo (ICME</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
