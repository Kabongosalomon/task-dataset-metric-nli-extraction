<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Channel Gated Networks for Task-Aware Continual Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
							<email>jtomczak@qti.qualcomm.com</email>
							<affiliation key="aff1">
								<orgName type="department">Qualcomm AI Research</orgName>
								<address>
									<country>Qualcomm Technologies Netherlands B.V</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
							<email>tijmen@qti.qualcomm.com</email>
							<affiliation key="aff1">
								<orgName type="department">Qualcomm AI Research</orgName>
								<address>
									<country>Qualcomm Technologies Netherlands B.V</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Qualcomm AI Research</orgName>
								<address>
									<country>Qualcomm Technologies Netherlands B.V</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Channel Gated Networks for Task-Aware Continual Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks experience catastrophic forgetting when optimized on a sequence of learning problems: as they meet the objective of the current training examples, their performance on previous tasks drops drastically. In this work, we introduce a novel framework to tackle this problem with conditional computation. We equip each convolutional layer with task-specific gating modules, selecting which filters to apply on the given input. This way, we achieve two appealing properties. Firstly, the execution patterns of the gates allow to identify and protect important filters, ensuring no loss in the performance of the model for previously learned tasks. Secondly, by using a sparsity objective, we can promote the selection of a limited set of kernels, allowing to retain sufficient model capacity to digest new tasks. Existing solutions require, at test time, awareness of the task to which each example belongs to. This knowledge, however, may not be available in many practical scenarios. Therefore, we additionally introduce a task classifier that predicts the task label of each example, to deal with settings in which a task oracle is not available. We validate our proposal on four continual learning datasets. Results show that our model consistently outperforms existing methods both in the presence and the absence of a task oracle. Notably, on Split SVHN and Imagenet-50 datasets, our model yields up to 23.98% and 17.42% improvement in accuracy w.r.t. competing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning and deep learning models are typically trained offline, by sampling examples independently from the distribution they are expected to deal with at test time. However, when trained online in real-world settings, models may encounter multiple tasks as a sequential stream of * Research conducted during an internship at Qualcomm Technologies Netherlands B.V. † Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.</p><p>activities, without having any knowledge about their relationship or duration in time. Such challenges typically arise in robotics <ref type="bibr" target="#b1">[2]</ref>, reinforcement learning <ref type="bibr" target="#b30">[31]</ref>, vision systems <ref type="bibr" target="#b27">[28]</ref> and many more (cf. Chapter 4 in <ref type="bibr" target="#b6">[7]</ref>). In such scenarios, deep learning models suffer from catastrophic forgetting <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>, meaning they discard previously acquired knowledge to fit the current observations. The underlying reason is that, while learning the new task, models overwrite the parameters that were critical for previous tasks. Continual learning research (also called lifelong or incremental learning) tackles the above mentioned issues <ref type="bibr" target="#b6">[7]</ref>. The typical setting considered in the literature is that of a model learning disjoint classification problems one-by-one. Depending on the application requirements, the task for which the current input should be analyzed may or may not be known. The majority of the methods in the literature assume that the label of the task is provided during inference. Such a continual learning setting is generally referred to as task-incremental. In many real-world applications, such as classification and anomaly detection systems, a model can seamlessly instantiate a new task whenever novel classes emerge from the training stream. However, once deployed in the wild, it has to process inputs without knowing in which training task similar observations were encountered. Such a setting, in which task labels are available only during training, is known as class-incremental <ref type="bibr" target="#b36">[37]</ref>. Existing methods employ different strategies to mitigate catastrophic forgetting, such as memory buffers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref>, knowledge distillation <ref type="bibr" target="#b17">[18]</ref>, synaptic consolidation <ref type="bibr" target="#b14">[15]</ref> and parameters masking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>. However, recent evidence has shown that existing solutions fail, even for simple datasets, whenever task labels are not available at test time <ref type="bibr" target="#b36">[37]</ref>. This paper introduces a solution based on conditionalcomputing to tackle both task-incremental and classincremental learning problems. Specifically, our framework relies on separate task-specific classification heads (multihead architecture), and it employs channel-gating <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref> in every layer of the (shared) feature extractor. To this aim, we introduce task-dedicated gating modules that dynamically select which filters to apply conditioned on the input feature map. Along with a sparsity objective encouraging the use of fewer units, this strategy enables per-sample model selection and can be easily queried for information about which weights are essential for the current task. Those weights are frozen when learning new tasks, but gating modules can dynamically select to either use or discard them. Contrarily, units that are never used by previous tasks are reinitialized and made available for acquiring novel concepts. This procedure prevents any forgetting of past tasks and allows considerable computational savings in the forward propagation. Moreover, we obviate the need for a task label during inference by introducing a task classifier selecting which classification head should be queried for the class prediction. We train the task classifier alongside the classification heads under the same incremental learning constraints. To mitigate forgetting on the task classification side, we rely on example replay from either episodic or generative memories. In both cases, we show the benefits of performing rehearsal at a task-level, as opposed to previous replay methods that operate at a class-level <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b4">5]</ref>. To the best of our knowledge, this is the first work that carries out supervised task prediction in a class-incremental learning setting. We perform extensive experiments on four datasets of increasing difficulty, both in the presence and absence of a task oracle at test time. Our results show that, whenever task labels are available, our model effectively prevents the forgetting problem, and performs similarly to or better than state-of-the-art solutions. In the task agnostic setting, we consistently outperform competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Continual learning. Catastrophic forgetting has been a well-known problem of neural networks <ref type="bibr" target="#b23">[24]</ref>. Early approaches to alleviate the issue involved orthogonal representation learning and replay of prior samples <ref type="bibr" target="#b8">[9]</ref>. The recent advent in deep learning has led to the widespread use of deep neural networks in the continual learning field. First attempts, such as Progressive Neural Networks <ref type="bibr" target="#b31">[32]</ref> tackle the forgetting problem by introducing a new set of parameters for each new task at the expense of limited scalability. Another popular solution is to apply knowledge distillation by using the past parametrizations of the model as a reference when learning new tasks <ref type="bibr" target="#b17">[18]</ref>. Consolidation approaches emerged recently with the focus of identifying the weights that are critically important for prior tasks and preventing significant updates to them during the learning of new tasks. The relevance/importance estimation for each parameter can be carried out through the Fisher Information Matrix <ref type="bibr" target="#b14">[15]</ref>, the path integral of loss gradients <ref type="bibr" target="#b40">[41]</ref>, gradient magnitude <ref type="bibr" target="#b0">[1]</ref> and a posteriori uncertainty estimation in a Bayesian Neural Network <ref type="bibr" target="#b25">[26]</ref>. Other popular consolidation strategies rely on the estimation of binary masks that directly map each task to the set of parameters responsible for it. Such masks can be estimated either by random assignment <ref type="bibr" target="#b22">[23]</ref>, pruning <ref type="bibr" target="#b21">[22]</ref> or gradient descent <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>. However, existing mask-based approaches can only operate in the presence of an oracle providing the task label. Our work is akin to the above-mentioned models, with two fundamental differences: i) our binary masks (gates) are dynamically generated and depend on the network input, and ii) we promote mask-based approaches to class-incremental learning settings, by relying on a novel architecture comprising a task classifier. Several models allow access to a finite-capacity memory buffer (episodic memory), holding examples from prior tasks. A popular approach is iCaRL <ref type="bibr" target="#b28">[29]</ref>, which computes class prototypes as the mean feature representation of stored memories, and classifies test examples in a nearestneighbor fashion. Alternatively, other approaches intervene in the training algorithm, proposing to adjust the gradient computed on the current batch towards an update direction that guarantees non-destructive effects on the stored examples <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30]</ref>. Such an objective can imply the formalization of constrained optimization problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref> or the employment of meta-learning algorithms <ref type="bibr" target="#b29">[30]</ref>. Differently, generative memories do not rely on the replay of any real example whatsoever, in favor of generative models from which fake examples of past tasks can be efficiently sampled <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref>. In this work, we also rely on either episodic or generative memories to deal with the class-incremental learning setting. However, we carry out replay only to prevent forgetting of the task predictor, thus avoiding to update task-specific classification heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional computation.</head><p>Conditional computation research focuses on deep neural networks that adapt their architecture to the given input. Although the first work has been applied to language modeling <ref type="bibr" target="#b34">[35]</ref>, several works applied such concept to computer vision problems. In this respect, prior works employ binary gates deciding whether a computational block has to be executed or skipped. Such gates may either drop entire residual blocks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> or specific units within a layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref>. In our work, we rely on the latter strategy, learning a set of task-specific gating modules selecting which kernels to apply on the given input. To our knowledge, this is the first application of data-dependent channel-gating in continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem setting and objective</head><p>We are given a parametric model, i.e., a neural network, called a backbone or learner network, which is exposed to a sequence of N tasks to be learned, T = {T 1 , . . . , T N }. Each task T i takes the form of a classification problem, T i = {x j , y j } ni j=1 , where x j ∈ R m and y j ∈ {1, . . . , C i }.</p><p>A task-incremental setting requires to optimize:</p><formula xml:id="formula_0">max θ E t∼T E (x,y)∼Tt [log p θ (y|x, t)] ,<label>(1)</label></formula><p>where θ identifies the parametrization of the learner network, and x, y and t are random variables associated with the observation, the label and the task of each example, respectively. Such a maximization problem is subject to the continual learning constraints: as the model observes tasks sequentially, the outer expectation in Eq. 1 is troublesome to compute or approximate. Notably, this setting requires the assumption that the identity of the task each example belongs to is known at both training and test stages. Such information can be exploited in practice to isolate relevant output units of the classifier, preventing the competition between classes belonging to different tasks through the same softmax layer (multi-head). Class-incremental models solve the following optimization:</p><formula xml:id="formula_1">max θ E t∼T E (x,y)∼Tt [log p θ (y|x)] .<label>(2)</label></formula><p>Here, the absence of task conditioning prevents any form of task-aware reasoning in the model. This setting requires to merge the output units into a single classifier (single-head) in which classes from different tasks compete with each other, often resulting in more severe forgetting <ref type="bibr" target="#b36">[37]</ref>. Although the model could learn based on task information, this information is not available during inference.</p><p>To deal with observations from unknown tasks, while retaining advantages of multi-head settings, we will jointly optimize for class as well as task prediction, as follows:</p><formula xml:id="formula_2">max θ E t∼T E (x,y)∼Tt [log p θ (y, t|x)] = E t∼T E (x,y)∼Tt [log p θ (y|x, t) + log p θ (t|x)] .<label>(3)</label></formula><p>Eq. 3 describes a twofold objective. On the one hand, the term log p(y|x, t) is responsible for the class classification given the task, and resembles the multi-head objective in Eq. 1. On the other hand, the term log p(t|x) aims at predicting the task from the observation. This prediction relies on a task classifier, which is trained incrementally in a single-head fashion. Notably, the objective in Eq. 3 shifts the single-head complexities from a class prediction to a task prediction level, with the following benefits:</p><p>• given the task label, there is no drop in class prediction accuracy; • classes from different tasks never compete with each other, neither during training nor during test; • the challenging single-head prediction step is shifted from class to task level; as tasks and classes form a two-level hierarchy, the prediction of the former is arguably easier (as it acts at a coarser semantic level). Conv2D gates -th layer, t-th task <ref type="figure">Figure 1</ref>: The proposed gating scheme for a convolution layer. Depending on the input feature map, the gating module G l t decides which kernels should be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-head learning of class labels</head><p>In this section, we introduce the conditional computation model we used in our work. <ref type="figure">Fig. 1</ref> illustrates the gating mechanism used in our framework. We limit the discussion of the gating mechanism to the case of convolutional layers, as it also applies to other parametrized mappings such as fully connected layers or residual blocks. Consider h l ∈ R c l in ,h,w and h l+1 ∈ R c l out ,h ,w to be the input and output feature maps of the l-th convolutional layer respectively. Instead of h l+1 , we will forward to the following layer a sparse feature mapĥ l+1 , obtained by pruning uninformative channels. During the training of task t, the decision regarding which channels have to be activated is delegated to a gating module G l t , that is conditioned on the input feature map h l :ĥ</p><formula xml:id="formula_3">l+1 = G l t (h l ) h l+1 ,<label>(4)</label></formula><p>where G l t (h l ) = [g l 1 , . . . , g l c l out ], g l i ∈ {0, 1}, and refers to channel-wise multiplication. To be compliant with the incremental setting, we instantiate a new gating module each time the model observes examples from a new task. However, each module is designed as a light-weight network with negligible computation costs and number of parameters. Specifically, each gating module comprises a Multi-Layer Perceptron (MLP) with a single hidden layer featuring 16 units, followed by a batch normalization layer <ref type="bibr" target="#b11">[12]</ref> and a ReLU activation. A final linear map provides logprobabilities for each output channel of the convolution. Back-propagating gradients through the gates is challenging, as non-differentiable thresholds are employed to take binary on/off decisions. Therefore, we rely on the Gumbel-Softmax sampling <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, and get a biased estimate of the gradient utilizing the straight-through estimator <ref type="bibr" target="#b3">[4]</ref>. Specif-  </p><formula xml:id="formula_4">T 2 T t T 1 head 2 head t multi-head class prediction single-head task prediction Conv2D Conv2D … shared shared (a) (b)</formula><p>Figure 2: Illustration of the task prediction mechanism for a generic backbone architecture. First (block 'a'), the l-th convolutional layer is fed with multiple gated feature maps, each of which is relevant for a specific task. Every feature map is then convolved with kernels selected by the corresponding gating module G l x , and forwarded to the next module. At the end of the network the task classifier (block 'b') takes as input candidate feature maps and decides which task to solve.</p><p>ically, we employ the hard threshold in the forward pass (zero-centered) and the sigmoid function in the backward pass (with temperature τ = 2/3). Moreover, we penalize the number of active convolutional kernels with the sparsity objective:</p><formula xml:id="formula_5">L sparse = E (x,y)∼Tt λ s L L l=1 G l t (h l ) 1 c l out ,<label>(5)</label></formula><p>where L is the total number of gated layers, and λ s is a coefficient controlling the level of sparsity. The sparsity objective instructs each gating module to select a minimal set of kernels, allowing us to conserve filters for the optimization of future tasks. Moreover, it allows us to effectively adapt the capacity of the allocated network depending on the difficulty of the task and the observation at hand. Such a data-driven model selection contrasts with other continual learning strategies that employ fixed ratios for model growing <ref type="bibr" target="#b31">[32]</ref> or weight pruning <ref type="bibr" target="#b21">[22]</ref>. At the end of the optimization for task t, we compute a relevance score r l k for each unit in the l-th layer by estimating the firing probability of their gates on a validation set T val t :</p><formula xml:id="formula_6">r l,t k = E (x,y)∼T val t [p(I[g l k = 1])],<label>(6)</label></formula><p>where I[·] is an indicator function, and p(·) denotes a probability distribution. By thresholding such scores, we obtain two sets of kernels. On the one hand, we freeze relevant kernels for the task t, so that they will be available but not updatable during future tasks. On the other hand, we reinitialize non-relevant kernels, and leave them learnable by subsequent tasks. In all our experiments, we use a threshold equal to 0, which prevents any forgetting at the expense of a reduced model capacity left for future tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Single-head learning of task labels</head><p>The gating scheme presented in Sec. 3.2 allows the immediate identification of important kernels for each past task. However, it cannot be applied in the task-agnostic setting as is, since it requires the knowledge about which gating module G l x has to be applied for layer l, where x ∈ {1, . . . , t} represents the unknown task. Our solution is to employ all gating modules [G l ing modules which select a limited number of convolutional filters in each stream. After the last convolutional layer, indexed by L, we are given a list of t candidate feature maps</p><formula xml:id="formula_7">[ĥ L+1 1 , . . . ,ĥ L+1 t ]</formula><p>and as many classification heads. The task classifier is fed with a concatenation of all feature maps:</p><formula xml:id="formula_8">h = t i=1 [µ(ĥ L+1 i )],<label>(7)</label></formula><p>where µ denotes the global average pooling operator over the spatial dimensions and describes the concatenation along the feature axis. The architecture of the task classifier is based on a shallow MLP with one hidden layer featuring 64 ReLU units, followed by a softmax layer predicting the task label. We use the standard cross-entropy objective to train the task classifier. Optimization is carried out jointly with the learning of class labels at task t. Thus, the network not only learns features to discriminate the classes inside task t, but also to allow easier discrimination of input data from task t against all prior tasks. The single-head task classifier is exposed to catastrophic forgetting. Recent papers have shown that replay-based strategies represent the most effective continual learning strategy in single-head settings <ref type="bibr" target="#b36">[37]</ref>. Therefore, we choose to ameliorate the problem by rehearsal. In particular, we consider the following approaches.</p><p>Episodic memory. A small subset of examples from prior tasks is used to rehearse the task classifier. During the training of task t, the buffer holds C random examples from past tasks 1, . . . , t − 1 (where C denotes a fixed capacity). Examples from the buffer and the current batch (from task t) are re-sampled so that the distribution of task labels in the rehearsal batch is uniform. At the end of task t, the data in the buffer is subsampled so that each past task holds m = C/t examples. Finally, m random examples from task t are selected for storage.</p><p>Generative memory. A generative model is employed for sampling fake data from prior tasks. Specifically, we utilize Wasserstein GANs with Gradient Penalty (WGAN-GP <ref type="bibr" target="#b9">[10]</ref>). To overcome forgetting in the sampling procedure, we use multiple generators, each of which models the distribution of examples of a specific task.</p><p>In both cases, replay is only employed for rehearsing the task classifier and not the classification heads. To summarize, the complete objective of our model includes: the cross-entropy at a class level (p θ (y|x, t) in Eq. 3), the cross-entropy at a task level (p θ (t|x) in Eq. 3) and the sparsity term (L sparse in Eq. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and backbone architectures</head><p>We experiment with the following datasets:</p><p>• Split MNIST: the MNIST handwritten classification benchmark <ref type="bibr" target="#b16">[17]</ref> is split into 5 subsets of consecutive classes. This results into 5 binary classification tasks that are observed sequentially. • Split SVHN: the same protocol applied as in Split MNIST, but employing the SVHN dataset <ref type="bibr" target="#b24">[25]</ref>. • Split CIFAR-10: the same protocol applied as in Split MNIST, but employing the CIFAR-10 dataset <ref type="bibr" target="#b15">[16]</ref>. • Imagenet-50 <ref type="bibr" target="#b27">[28]</ref>: a subset of the iILSVRC-2012 dataset <ref type="bibr" target="#b7">[8]</ref> containing 50 randomly sampled classes and 1300 images per category, split into 5 consecutive 10-way classification problems. Images are resized to a resolution of 32x32 pixels.</p><p>As for the backbone models, for the MNIST and SVHN benchmarks, we employ a three-layer CNN with 100 filters per layer and ReLU activations (SimpleCNN in what follows). All convolutions except for the last one are followed by a 2x2 max-pooling layer. Gating is applied after the pooling layer. A final global average pooling followed by a linear classifier yields class predictions. For the CIFAR-10 and Imagenet-50 benchmarks we employed a ResNet-18 <ref type="bibr" target="#b10">[11]</ref> model as backbone. The gated version of a ResNet basic block is represented in <ref type="figure">Fig. 3</ref>. As illustrated, two independent sets of gates are applied after the first convolution and after the residual connection, respectively. All models were trained with SGD with momentum until convergence. After each task, model selection is performed for all models by monitoring the corresponding objective on a held-out set of examples from the current task (i.e., we don't rely on examples of past tasks for validation purposes). We apply the sparsity objective introduced in Sec. 3.2 only after a predetermined number of epochs, to provide the model the possibility to learn meaningful kernels before starting pruning the uninformative ones. We refer to the supplementary material for further implementation details.  <ref type="figure">Figure 3</ref>: The gating scheme applied to ResNet-18 blocks. Gating on the shortcut is only applied when downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split SVHN</head><p>Split CIFAR-10  <ref type="table">Table 1</ref>: Task-incremental results. For each method, we report the final accuracy on all task after incremental training.</p><formula xml:id="formula_9">T 1 T 2 T 3 T 4 T 5 avg T 1 T 2 T 3 T 4 T 5 avg T 1 T 2 T 3 T 4<label>T</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Task-incremental setting</head><p>In the task-incremental setting, an oracle can be queried for task labels during test time. Therefore, we don't rely on the task classifier, exploiting ground-truth task labels to select which gating modules and classification head should be active. This section validates the suitability of the proposed data-dependent gating scheme for continual learning. We compare our model against several competing methods:</p><p>-Joint: the backbone model trained jointly on all tasks while having access to the entire dataset. We considered its performance as the upper bound. -Ewc-On <ref type="bibr" target="#b32">[33]</ref>: the online version of Elastic Weight Consolidation, relying on the latest MAP estimate of the parameters and a running sum of Fisher matrices. -LwF <ref type="bibr" target="#b17">[18]</ref>: an approach in which the task loss is regularized by a distillation objective, employing the initial state of the model on the current task as a teacher. -HAT [34]: a mask-based model conditioning the active units in the network on the task label. Despite being the most similar approach to our method, it can only be applied in task-incremental settings.</p><p>Tab. 1 reports the comparison between methods, in terms of accuracy on all tasks after the whole training procedure. Despite performing very similarily for MNIST, the gap in the consolidation capability of different models emerges as the dataset grows more and more challenging. It is worth mentioning several recurring patterns. First, LwF struggles when the number of tasks grows larger than two. Although its distillation objective is an excellent regularizer against forgetting, it does not allow enough flexibility to the model to acquire new knowledge. Consequently, its accuracy on the most recent task gradually decreases during sequential learning, whereas the performance on the first task is kept very high. Moreover, results highlight the suitability of gating-based schemes (HAT and ours) with respect to other consolidation strategies such as EWC Online. Whereas the former ones prevent any update of relevant parameters, the latter approach only penalizes updating them, eventually incurring a significant degree of forgetting. Finally, the table shows that our model either performs on-par or outperforms HAT on all datasets, suggesting the beneficial effect of our data-dependent gating scheme and sparsity objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Class-incremental with episodic memory</head><p>Next, we move to a class-incremental setting in which no awareness of task labels is available at test time, significantly increasing the difficulty of the continual learning problem. In this section, we set up an experiment for which the storage of a limited amount of examples (buffer) is allowed. We compare against:</p><p>-Full replay: upper bound performance given by replay to the network of an unlimited number of examples. -iCaRL <ref type="bibr" target="#b28">[29]</ref> an approach based on a nearest-neighbor classifier exploiting examples in the buffer. We report the performances both with the original buffer-filling strategy (iCaRL-mean) and with the randomized algorithm used for our model (iCaRL-rand); -A-GEM <ref type="bibr" target="#b4">[5]</ref>: a buffer-based method correcting parameter updates on the current task so that they don't contradict the gradient computed on the stored examples.</p><p>Results are summarized in <ref type="figure" target="#fig_2">Fig. 4</ref>, illustrating the final average accuracy on all tasks at different buffer sizes for the class-incremental Split-MNIST and Split-SVHN benchmarks. The figure highlights several findings. Surprisingly, A-GEM yields a very low performance on MNIST, while providing higher results on SVHN. Further examination on the former dataset revealed that it consistently reaches competitive accuracy on the most recent task, while mostly forgetting the prior ones. The performance of iCaRL, on the other hand, does not seem to be significantly affected by changing its buffer filling strategy. Moreover, its accuracy seems not to scale with the number of stored examples. In contrast to these methods, our model primarily utilizes the few stored examples for the rehearsal of coarse-grained task prediction, while retaining the accuracy of fine-grained class prediction. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, our approach consistently outperforms competing approaches in the classincremental setting with episodic memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Class-incremental with generative memory</head><p>Next, we experiment with a class-incremental setting in which no examples are allowed to be stored whatsoever. A popular strategy in this framework is to employ generative models to approximate the distribution of prior tasks and rehearse the backbone network by sampling fake observations from them. Among these, DGM <ref type="bibr" target="#b27">[28]</ref> is the state-ofthe-art approach, which proposes a class-conditional GAN architecture paired with a hard attention mechanism similar to the one of HAT <ref type="bibr" target="#b33">[34]</ref>. Fake examples from the GAN generator are replayed to the discriminator, which includes an auxiliary classifier providing a class prediction. As for our model, as mentioned in Sec. 3.3, we rely on multiple task-specific generators. For a detailed discussion of the architecture of the employed WGANs, we refer the reader to the supplementary material. Tab. 2 compares the results of DGM and our model for the class-incremental setting with generative memory. Once again, our method of exploiting rehearsal for only the task classifier proves beneficial. DGM performs particularly well on Split MNIST, where hallucinated examples are almost indistinguishable from real examples. On the contrary, results suggest that class-conditional rehearsal becomes potentially unrewarding as the complexity of the modeled distribution increases, and the visual quality of generated samples degrades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Model analysis</head><p>Episodic vs. generative memory. To understand which rehearsal strategy has to be preferred when dealing with class-incremental learning problems, we raise the following question: What is more beneficial between a limited amount of real examples and a (potentially) unlimited amount of generated examples? To shed light on this matter, we report our models' performances on Split SVHN and Split CIFAR-10 as a function of memory budget. Specifically, we compute the memory consumption of episodic memories as the cumulative size of the stored examples. As for generative memories, we consider the number of bytes needed to store their parameters (in single-precision floating-point format), discarding the corresponding discriminators as well as inner activations generated in the sampling process. <ref type="figure" target="#fig_3">Fig. 5</ref> presents the result of the analysis. As can be seen, the variant of our model relying on memory buffers consistently outperforms its counterpart relying on generative modeling. In the case of CIFAR-10, the generative replay yields an accuracy  comparable with an episodic memory of ≈ 1.5 MBs, which is more than 20 times smaller than its generators. The gap between the two strategies shrinks on SVHN, due to the simpler image content resulting in better samples from the generators. Finally, our method, when based on memory buffers, outperforms the DGMw model <ref type="bibr" target="#b27">[28]</ref> on Split-SVHN, albeit requiring 3.6 times less memory.</p><p>Gate analysis. We provide a qualitative analysis of the activation of gates across different tasks in <ref type="figure" target="#fig_4">Fig. 6</ref>. Specifically, we use the validation sets of Split MNIST and Imagenet-50 to compute the probability of each gate to be triggered by images from different tasks 1 . The analysis of the figure suggests two pieces of evidence: First, as more tasks are observed, previously learned features are re-used. This pattern shows that the model does not fall into degenerate solutions, e.g., by completely isolating tasks into different sub-networks. On the contrary, our model profitably exploits pieces of knowledge acquired from previous tasks for the optimization of the future ones. Moreover, a significant number of gates never fire, suggesting that a considerable portion of the backbone capacity is available for learning even more tasks. Additionally, we showcase how images from different tasks activating the same filters show some resemblance in low-level or semantic features (see the caption for details). <ref type="bibr" target="#b0">1</ref> we report such probabilities for specific layers: layer 1 for Split MNIST (Simple CNN), block 5 for Imagenet-50 (ResNet-18).  On the cost of inference. We next measure the inference cost of our model as the number of tasks increases. Tab. 3 reports the average number of multiply-add operations (MAC count) of our model on the test set of Split MNIST and Split CIFAR-10 after learning each task. Moreover, we report the MACs of HAT <ref type="bibr" target="#b33">[34]</ref> as well as the cost of forward propagation in the backbone network (i.e. the cost of any other competing method mentioned it this section). In the task-incremental setting, our model obtains a meaningful saving in the number of operations, thanks to the data-dependent gating modules selecting only a small subset of filters to apply. In contrast, forward propagation in a class-incremental setting requires as many computational streams as the number of tasks observed so far. However, each of them is extremely cheap as few convolutional units are active. As presented in the  <ref type="table">Table 3</ref>: Average MAC counts (×10 6 ) of inference in Split MNIST and Split CIFAR-10. We compute MACs on the test sets, at different stages of the optimization (up to T t ), both in task-incremental (TI) and class-incremental (CI) setups.</p><p>operations never exceeds the cost of forward propagation in the backbone model. The reduction in inference cost is particularly significant for Split CIFAR-10, which is based on a ResNet-18 backbone.</p><p>Limitations and future works. Training our model can require a lot of GPU memory for bigger backbones. However, by exploiting the inherent sparsity of activation maps, several optimizations are possible. Secondly, we expect the task classifier to be susceptible to the degree of semantic separation among tasks. For instance, a setting where tasks are semantically well-defined, like T 1 = {cat,dog}, T 2 = {car,truck} (animals / vehicles), should favor the task classifier with respect to its transpose T 1 = {cat,car}, T 2 = {dog,truck}. However, we remark that in our experiments the assigment of classes to tasks is always random. Therefore, our model could perform even better in the presence of coherent tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We presented a novel framework based on conditional computation to tackle catastrophic forgetting in convolutional neural networks. Having task-specific light-weight gating modules allows us to prevent catastrophic forgetting of previously learned knowledge. Besides learning new features for new tasks, the gates allow for dynamic usage of previously learned knowledge to improve performance. Our method can be employed both in the presence and in the absence of task labels during test. In the latter case, a task classifier is trained to take the place of a task oracle. Through extensive experiments, we validated the performance of our model against existing methods both in task-incremental and class-incremental settings and demonstrated state-ofthe-art results in four continual learning datasets.</p><p>In this section we report training details and hyperparameters used for the optimization of our model. As already specified in Sec. 4.1 of the main paper, all models were trained with Stochastic Gradient Descent with momentum. Gradient clipping was utilized, ensuring the gradient magnitude to be lower than a predetermined threshold. Moreover, we employed a scheduler dividing the learning rate by a factor of 10 at certain epochs. Such details can be found, for each dataset, in Tab. 4, where we highlighted two sets of hyperparameters:</p><p>• optim: general optimization choices that were kept fixed both for our model and competing methods, in order to ensure fairness.</p><p>• our: hyperparameters that only concern our model, such as the weight of the sparsity loss and the number of epochs after which sparsity was introduced (patience).   <ref type="bibr" target="#b9">[10]</ref>). The reader can find the specification of the architecture in Tab. 9. For every dataset, we trained the WGANs for 2 × 10 5 total iterations, each of which was composed by 5 and 1 discriminator and generator updates respectively. As for the optimization, we rely on Adam <ref type="bibr" target="#b13">[14]</ref> with a learning rate of 10 −4 , fixing β 1 = 0.5 and β 2 = 0.9. The batch size was set to 64. The weight for gradient penalty <ref type="bibr" target="#b9">[10]</ref> was set to 10. Inputs were normalized before being fed to the discriminator. Specifically, for MNIST we normalize each image into the range [0, 1], whilst for other datasets we map inputs into the range [−1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">WGAN details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">On mixing real and fake images for rehearsal.</head><p>The common practice when adopting generative replay for continual learning is to exploit a generative model to synthesize examples for prior tasks {1, . . . , t − 1}, while utilizing real examples as representative of the current task t. In early experiments we followed this exact approach, but it led to sub-optimal results. Indeed, the task classifier consistently reached good discrimination capabilities during training, yielding very poor performances at test time. After an in-depth analysis, we conjectured that the task classifier, while being trained on a mixture of real and fake examples, fell into the following very poor classification logic <ref type="figure" target="#fig_6">(Fig. 7)</ref>. It first discriminated between the nature of the image (real/fake), learning to map real examples to task t. Only for inputs deemed as fake, a further categorization into tasks {1, . . . , t − 1} was carried out. Such a behavior, perfectly legit during training, led to terrible test performances. Indeed, during test only real examples are presented to the network, causing the task classifier to consistently label them as coming from task t. To overcome such an issue, we remove mixing of real and fake examples during rehearsal, by presenting to the task   MNIST Full Replay 0.9861 0.9861 0.9861 0.9861 A-GEM <ref type="bibr" target="#b4">[5]</ref> 0.1567 0.1892 0.1937 0.2115 iCaRL-rand <ref type="bibr" target="#b28">[29]</ref> 0.8493 0.8455 0.8716 0.8728 iCaRL-mean <ref type="bibr" target="#b28">[29]</ref> 0.8140 0.8443 0.8433 0.8426 ours 0.9401 0.9594 0.9608 0.9594 SVHN Full Replay 0.9081 0.9081 0.9081 0.9081 A-GEM <ref type="bibr" target="#b4">[5]</ref> 0.5680 0.5411 0.5933 0.5704 iCaRL-rand <ref type="bibr" target="#b28">[29]</ref> 0.4972 0.5492 0.4788 0.5484 iCaRL-mean <ref type="bibr" target="#b28">[29]</ref> 0.5626 0.5469 0.5252 0.5511 ours 0.6745 0.7399 0.7673 0.8102 <ref type="table">Table 5</ref>: Numerical results for <ref type="figure" target="#fig_2">Fig. 4</ref> in the main paper. Average accuracy for the episodic memory experiment, for different buffer sizes (C).</p><p>classifier fake examples also for the task t. In the incremental learning paradigm, this only requires to shift the training of the WGAN generators from the end of a given task to its beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Quantitative results for figures</head><p>To foster future comparisons with our work, we report in this section quantitative results that are represented in <ref type="figure" target="#fig_2">Fig. 4</ref> and 5 of the main paper. Such quantities can be found in Tab. 5 and 6 respectively.  <ref type="table">Table 6</ref>: Numerical values for the memory consumption experiment represented in <ref type="figure" target="#fig_3">Fig. 5</ref> of the main paper. class conditioning rehearsal level SVHN CIFAR-10 C-Gen class 0.7847 0.6384 ours task 0.8341 0.7006 <ref type="table">Table 7</ref>: Performance of our model based on generative memory against a baseline comprising a class-conditional generator for each task (C-Gen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison w.r.t. conditional generators</head><p>To validate the beneficial effect of the employment of generated examples for the rehearsal of task prediction only, we compare our model based on generative memory (Sec. 4.4 of the main paper) against a further baseline. To this end, we still train a WGAN-GP for each task, but instead of training unconditional models we train class-conditional ones, following the AC-GAN framework <ref type="bibr" target="#b26">[27]</ref>. After training N conditional generators, we train the backbone model by generating labeled examples in an i.i.d fashion. We refer to this baseline as C-Gen, and report the final results in Tab. 7.</p><p>The results presented for Split SVHN and Split CIFAR-10, illustrate that generative rehearsal at a task level, instead of at a class level, is beneficial in both datasets. We believe our method behaves better for two reasons. First, our model never updates classification heads guided by a loss function computed on generated examples (i.e., potentially poor in visual quality). Therefore, when the task label gets predicted correctly, the classification accuracy is comparable to the one achieved in a task-incremental setup. Moreover, given equivalent generator capacities, conditional generative modeling may be more complex than unconditional modeling, potentially resulting in higher degradation of generated examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Confidence of task-incremental results</head><p>To validate the gap between our model's performance with respect to HAT (Tab. 1 in the main paper), we report the confidence of such experiment by repeating it 5 times with different random seeds. Results in Tab. 8 show that the margin between our proposal and HAT is slight, yet consistent.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>conv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Final mean accuracy on all tasks when an episodic memory is employed, as a function of the buffer capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy as a function of replay memory budget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the gate execution patterns for continually trained models on MNIST (left) and Imagenet-50 (right) datasets. The histograms in the top left and top right show the firing probability of gates in the 1st layer and the 5th residual block respectively. For better illustration, gates are sorted by overall execution rate over all tasks. The bottom-left box shows images from different tasks either triggering or not triggering a specific gate on Split MNIST. The bottom-right box illustrates how -on Imagenet-50 -correlated classes from different tasks fire the same gates (e.g., fishes, different breeds of dogs, birds).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of (a) the degenerate behavior of the task classifier when rehearsed with a mix of real and generated examples and (b) the proposed solution. See Sec 2.1 for details. C = 500 C = 1000 C = 1500 C = 2000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Note that within this framework, it is trivial to monitor the number of learnable units left in each layer. As such, if the capacity of the backbone model saturates, we can quickly grow the network to digest new tasks. However, because the gating modules of new tasks can dynamically choose to use previously learned filters (if relevant for their input), learning of new tasks generally requires less learnable units. In practice, we never experienced the saturation of the backbone model for learning new tasks. Apart from that, because of our conditional channel-gated network design, increasing the model capacity for future tasks will have minimal effects on the computation cost at inference, as reported by the analysis in Sec. 4.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Class-incremental continual learning results, when replayed examples are provided by a generative model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table, also in the class-incremental setting, the number of</figDesc><table><row><cell></cell><cell cols="2">Split MNIST</cell><cell></cell><cell cols="3">Split CIFAR-10</cell></row><row><cell></cell><cell cols="2">(Simple CNN)</cell><cell></cell><cell></cell><cell>(ResNet-18)</cell></row><row><cell></cell><cell>HAT</cell><cell>our</cell><cell>our</cell><cell>HAT</cell><cell>our</cell><cell>our</cell></row><row><cell></cell><cell>TI</cell><cell>TI</cell><cell>CI</cell><cell>TI</cell><cell>TI</cell><cell>CI</cell></row><row><cell>Up to T 1</cell><cell cols="6">0.151 0.064 0.064 31.937 2.650 2.650</cell></row><row><cell>Up to T 2</cell><cell cols="6">0.168 0.101 0.209 32.234 4.628 9.199</cell></row><row><cell>Up to T 3</cell><cell cols="6">0.194 0.137 0.428 36.328 5.028 15.024</cell></row><row><cell>Up to T 4</cell><cell cols="6">0.221 0.136 0.559 38.040 5.181 20.680</cell></row><row><cell>Up to T 5</cell><cell cols="6">0.240 0.142 0.725 39.835 5.005 24.927</cell></row><row><cell>backbone</cell><cell></cell><cell>0.926</cell><cell></cell><cell></cell><cell>479.920</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This section illustrates architectures and training details for the generative models employed in Sec. 4.4 of the main</figDesc><table><row><cell></cell><cell></cell><cell>Split MNIST</cell><cell>Split SVHN</cell></row><row><cell></cell><cell>batch size</cell><cell>256</cell><cell>256</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>optim</cell><cell>momentum lr decay weight decay</cell><cell>0.9 -5e − 4</cell><cell>0.9 [400, 600] 5e − 4</cell></row><row><cell></cell><cell>epochs per task</cell><cell>400</cell><cell>800</cell></row><row><cell></cell><cell>grad. clip</cell><cell>1</cell><cell>1</cell></row><row><cell>our</cell><cell>λ s L sparse patience</cell><cell>0.5 20</cell><cell>0.5 20</cell></row><row><cell></cell><cell></cell><cell cols="2">Split CIFAR-10 Imagenet-50</cell></row><row><cell></cell><cell>batch size</cell><cell>64</cell><cell>64</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>optim</cell><cell>momentum lr decay weight decay</cell><cell>0.9 [100, 150] 5e − 4</cell><cell>0.9 [100, 150] 5e − 4</cell></row><row><cell></cell><cell>epochs per task</cell><cell>200</cell><cell>200</cell></row><row><cell></cell><cell>grad. clip</cell><cell>1</cell><cell>1</cell></row><row><cell>our</cell><cell>λ s L sparse patience</cell><cell>1 10</cell><cell>1 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters table.paper. As stated in the manuscript, we rely on the framework of Wasserstein GANs with Gradient Penalty (WGAN-GP,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>SVHN CIFAR-10 HAT 0.997 ±4.00e−4 0.964 ±1.72e−3 0.964 ±1.20e−3 our 0.998 ±4.89e−4 0.974 ±4.00e−4 0.966 ±1.67e−3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Task-IL results averaged across 5 runs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">, . . . , G l t ], and to propagate all gated layer outputs [ĥ l+1 1 , . . . ,ĥ l+1 t ] forward. In turn, the following layer l + 1 receives the list of gated outputs from layer l, applies its gating modules [G l+1 1 , . . . , G l+1 t ] and yields the list of outputs [ĥ l+2 1 , . . . ,ĥ l+2 t ]. This mechanism generates parallel streams of computation in the network, sharing the same layers but selecting different sets of units to activate for each of them(Fig. 2). Despite the fact that the number of parallel streams grows with the number of tasks, we found our solution to be computationally cheaper than the backbone network (seeSec. 4.5). This is because of the gat-</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Training details and hyperparameters</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaas</forename><surname>Kelchtermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Batch-shaped channel gated networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Babak Ehteshami Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient lifelong learning with agem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You look twice: Gaternet for dynamic filter selection in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>University of Toronto</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Masse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation. Elsevier</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Cuong V Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Variational continual learning. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jahnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to learn without forgetting by maximizing transfer and minimizing interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CHILD: A first step towards continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progress &amp; compress: A scalable framework for continual learning. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dídac</forename><surname>Surís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixtureof-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Gido M Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Memory replay gans: Learning to generate new categories without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenshen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">ReLU ConvTranspose2d</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>Linear(128,4096) ReLU Reshape(256,4,4) ConvTranspose2d(256. ks=(5,5)) ReLU ConvTranspose2d(64, 1, ks=(8,8), s=(2,2)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Convtranspose2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">BatchNorm2d ReLU ConvTranspose2d</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
	<note>ks=(3,3),s=</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>ks=(3,3),s=</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">BatchNorm2d ReLU ConvTranspose2d</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
	<note>ks=(3,3),s=</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>ks=(3,3),s=</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">BatchNorm2d ReLU ConvTranspose2d</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
	<note>ks=(3,3),s=</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv2d</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>ks=(3,3),s=</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Flatten Linear(8192,1) Table 9: Architecture of the WGAN employed for the generative experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leakyrelu</surname></persName>
		</author>
		<idno>ns=0.01</idno>
	</analytic>
	<monogr>
		<title level="m">the table, ks indicates kernel sizes, s identifies strides, and ns refers to the negative slope of Leaky ReLU activations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
