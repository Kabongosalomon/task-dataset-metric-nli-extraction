<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D3NET: DENSELY CONNECTED MULTIDILATED DENSENET FOR MUSIC SOURCE SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">D3NET: DENSELY CONNECTED MULTIDILATED DENSENET FOR MUSIC SOURCE SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-source separation</term>
					<term>DenseNet</term>
					<term>SiSEC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Music source separation involves a large input field to model a long-term dependence of an audio signal. Previous convolutional neural network (CNN) -based approaches address the large input field modeling using sequentially down-and up-sampling feature maps or dilated convolution. In this paper, we claim the importance of a rapid growth of a receptive field and a simultaneous modeling of multi-resolution data in a single convolution layer, and propose a novel CNN architecture called densely connected dilated DenseNet (D3Net). D3Net involves a novel multi-dilated convolution that has different dilation factors in a single layer to model different resolutions simultaneously. By combining the multi-dilated convolution with DenseNet architecture, D3Net avoids the aliasing problem that exists when we naively incorporate the dilated convolution in DenseNet. Experimental results on MUSDB18 dataset show that D3Net achieves state-of-the-art performance with an average signal to distortion ratio (SDR) of 6.01 dB. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Music source separation (MSS) has been intensively studied and neural-network-based approaches have shown impressive progress in recent years. Many types of neural network architecture have been proposed, including feedforward fully connected networks (FNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, recurrent neural networks (RNNs) <ref type="bibr" target="#b2">[3]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> and their combinations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. In particular, CNNs have attracted great attention because of their superior performance, parameter efficiency and generality for different types of data. Takahashi et. al. <ref type="bibr" target="#b9">[10]</ref> applied a CNN architecture with a dense skip connectivity pattern, called DenseNet <ref type="bibr" target="#b11">[12]</ref>, to MSS and obtained state-of-the-art results in SiSEC 2018 <ref type="bibr" target="#b12">[13]</ref>. Such dense connectivity allows maximum information flow and deeper CNN while keeping the model size small by efficiently reusing intermediate representations of preceding layers.</p><p>One of the benefits of a deeper CNN is its larger receptive field that allows a large context to be modeled, which is important since audio signals can have long time and wide frequency band dependences. Although the receptive field grows linearly with the number of layers stacked, it is not the optimal way to increase the receptive field only by stacking convolutional layers, as it requires too many layers to cover a sufficiently large input field to model global information, making the network training too difficult. A popular approach to incorporating a large context is to repeatedly downsample intermediate network outputs and apply operations in lower resolution representations. The low-resolution representations are again upsampled to recover the lost resolution while carrying over the global perspective from downsampled layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>. Another approach is a dilated convolution, which is shown to be effective for audio generation and MSS tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. The dilation factors are set to grow exponentially with the number of layers stacked and, therefore, the networks cover the large receptive field with a small number of layers. Although the down-upsampling structure and dilated convolution allow a large receptive field, each layer in the network sees only one resolution at a time. However, the simultaneous consideration of the local and global information can be useful, e.g., the local structure can be more precisely estimated by using global structure information and vise versa. DenseNet partially addresses this problem by means of the dense skip connectivity that allows the direct aggregation of features from early layers and features in later layers within a single convolution layer. However, it may still be too slow to transform local features to global features and it is inefficient to have many parameters, especially for high-resolution data.</p><p>In this work, we combine the advantages of DenseNet and dilated convolution, and propose a novel network architecture called dilated DenseNet (D2Net). To properly combine DenseNet with the dilated convolution, we propose a multidilated convolution layer that has a multiple dilation factor within a single layer. The dilation factor depends on which skip connection the channels come from, as shown in <ref type="figure" target="#fig_1">Fig.1</ref>. The multidilated convolution can prevent the aliasing that occurs when a standard dilated convolution is applied to feature maps with receptive fields smaller than the dilation factor. Although a naive combination of DenseNet with dilation has already been proposed <ref type="bibr" target="#b14">[15]</ref>, standard dilated convolutions are used and dilation factors are determined depends on the layer depth, which causes considerable aliasing. In contrast, we show the effectiveness of the proposed multidilated convolution in our ablation study. Furthermore, we propose a nested architecture of dilated  dense blocks to effectively repeat dilation factors multiple times with dense connections that ensure the sufficient depth required for modeling each resolution. We call the nested architecture densely connected dilated DenseNet (D3Net).</p><p>The contributions of this work are summarized below:</p><p>1. We claim that a naive incorporation of dilation in DenseNet architecture can cause a significant aliasing problem, and propose a multidilated convolution layer to properly incorporate the dilated convolution into DenseNet.</p><p>2. We further introduce the D3Net architecture of nested dilated dense blocks to effectively apply different dilation factors multiple times.</p><p>3. We experimentally show the effectiveness of the proposed architectures. The D3Net achieves state-of-the-art results on MUSDB18 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multidilated convolution for DenseNet</head><p>In DenseNet, the outputs of the lth convolutional layer x l are computed using filters k l and outputs of all preceding layers as</p><formula xml:id="formula_0">x l = ψ([x 0 , x 1 , · · · , x l−1 ]) k l ,<label>(1)</label></formula><p>where ψ() denotes the composite operation of batch normalization and nonlinearity, [x 0 , x 1 , · · · , x l−1 ] the concatenation of feature maps from 1, · · · , l − 1 layers, and the convolution. A naive way of incorporating dilated convolution is to replace the convolution with the dilated convolution d with the dilation factor d = 2 l−1 . However, this causes a severe aliasing problem; for instance, at the third layer, input is subsampled with 4 sample intervals without any anti-aliasing filtering because of the skip connections. Assuming that the kernel size is 3, only the path that passes through all convolution operations without any skip connection covers the input field without omission and all other paths from skip connections have blind spots in their receptive fields that inherently make it impossible for proper ant-aliasing filters to be learned in the preceding layers ( <ref type="figure" target="#fig_3">Fig. 2a</ref>). To overcome this problem, we propose the multidilated convolution m l defined as</p><formula xml:id="formula_1">Y l m l k l = l−1 i=0 y i di k i l ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">Y l = [y 0 , y 1 , · · · , y l−1 ] = ψ([x 0 , x 1 , · · · , x l−1 ])</formula><p>is the composite layer output, k i l the subset of filters that correspond to the ith skip connection, and d i = 2 i . As depicted in <ref type="figure" target="#fig_3">Fig. 2b</ref>, DenseNet with the proposed multidilated convolution has different dilation factors depending on which layer the channel comes from. This allows the receptive field to cover the input field without the loss of coverage between the samples the filters to be applied and, hence, to learn proper filters to prevent aliasing. One advantage of the dilated dense block (D2 block) is its ability to integrate information from very local to exponentially large receptive field within a single layer. This fast information flow provides more flexibility in modeling information in a wide range of resolutions.</p><p>Note that the multidilation convolution is not equivalent to applying the multibranch convolution where convolutions with different dilation factors are applied to the same input feature maps, similar to the Inception block <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, again causing the aliasing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">D3Net</head><p>Although the D2 block provides an exponentially large receptive field as the number of layers increases, it is also worthwhile to provide sufficient flexibility to transform feature maps in each resolution. In WaveNet <ref type="bibr" target="#b13">[14]</ref>, dilation factors are reset to one after several layers are stacked and repeated; that is, the dilation factor in the lth layer is given by d l = 2 l−1 mod M , where mod is the modulo operation and M is the number of layers at which the dilation factor is doubled. Inspired by this work, we propose a nested architecture of D2 blocks as shown in <ref type="figure">Fig. 3</ref>. D2 blocks are considered as single composite layers and are densely connected in the same way as within the D2 block itself. We also employ a channel reduction mechanism at the end of each D2 block to mitigate the growth of an excessive number of channels and thus improve computational efficiency. The channel reduc-   tion can be performed by either a 1 × 1 convolution or simply passing the output of the last N layers' to the next block. In this work, we take the latter approach since performance characteristics of both methods are similar, but the former approach requires slightly more computations. Note that without the channel reduction, the architecture is reduced to a standard dense connection with repeated multidilation factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset We evaluated the proposed method using the MUSDB18 dataset, prepared for SiSEC 2018 <ref type="bibr" target="#b12">[13]</ref>. In the dataset, approximately 10 hours of professionally recorded 150 songs in stereo format at 44.1kHz are available. For each song, a mixture and its four sources, bass, drums, other and vocals, are provided and thus, the task is to separate the four sources from the mixture. We adopted the official split of 100 and 50 songs for Dev and Test set, respectively. Short-time Fourier transform (STFT) magnitude frames of the mixture, windowed at 4096 samples with 75% overlap, with data augmentation <ref type="bibr" target="#b2">[3]</ref> were used as inputs.</p><p>Training The four networks for each source instrument were trained to estimate the source spectrogram by minimizing the mean square error with the Adam optimizer for 50 epochs. The patch length was set to 256 frames; thus, the dimensions of input were 2 × 256 × 2049. The batch size was set to 6. The learning rate was initially set to 0.001 and annealed to 0.0001 at 40 epochs.</p><p>Model architecture Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, in which the best results obtained in SiSEC 2018 were reported, we used the multiscale multiband architecture in which band-dedicated modules and a full band module, each with a bottleneck encoder-decoder architecture with skip connections, are placed. The network configuration is shown in <ref type="table" target="#tab_1">Table 1</ref>. The network outputs are used to calculate the multichannel Wiener filter (MWF) to obtain the final separations, as commonly performed in frequency domain audio source separation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The signal-to-distortion ratio (SDR) of our proposed method and existing state-of-the-arts methods are compared in <ref type="table" target="#tab_2">Table 2</ref>. The SDRs were computed using the museval package <ref type="bibr" target="#b12">[13]</ref> and median SDRs are reported. TAK1 <ref type="bibr" target="#b9">[10]</ref> and UHL2 <ref type="bibr" target="#b2">[3]</ref> are the two best performing methods in SiSEC 2018 (among submissions that do not use external data) and the network architectures are the combination of DenseNet and recurrent units for TAK1 and an ensemble of bi-directional LSTM models for UHL2. The proposed D3Net exhibited the best performance for vocals, drums and accompaniment (the summation of drums, bass and other) and performed comparably to the best method for other. The average SDR of four instruments is significantly better than all baseline values. The primally difference between MMDenseLSTM (TAK1) and the proposed method is that MMDenseLSTM incorporates LSTM units to further expand the receptive field, whereas the proposed method uses the multidilated convolution. Comparison of these methods indicate the effectiveness of the multidilated convolution. On the other hand, GRU dilation 1 <ref type="bibr" target="#b10">[11]</ref> consists of dilated convolution and dilated GRU units without a down-up-sampling path. This also highlights the effectiveness of the multiresolution modeling of the multidilation convolution with the dense connection. For bass, approaches that operate in the time domain perform better, as they are capable of recovering the target phase, which is easier in the low frequency range. Among the frequency domain approaches, D3Net performs the best.</p><p>We also conducted an ablation study to validate the effectiveness of the multidilated convolution. By replacing the multidilated convolutions with the standard convolutions without dilation, we obtained comparable results as the best performing model in SiSEC2018, TAK1 (MMDenseLSTM). When we replaced the multidilated convolution with the standard dilated convolution, we obtained a decent improvement over the D3Net without dilation even though the aliasing problem arises. However, the proposed multidilated convolution clearly outperforms the standard dilated convolution, showing the importance of handling the aliasing problem in order to incorporate dilation in DenseNet.</p><p>We further investigated the effects of the multidilated con-  volution by assessing the learned weights. We calculated the L1 norm of the convolution weights in the last layer of the first block for both the proposed D3Net with the multidilated convolution and the baseline D3Net with the standard dilated convolution. The norm was calculated separately for each skip connection and norm values were normalized by the norm of the path with no skip connection. <ref type="figure" target="#fig_4">Fig. 4</ref> shows that the weights of the skip connection from early layers have smaller norms than later layers. This trend is much more prominent for D3Net with the standard dilated convolution than for D3Net with the proposed multidilated convolution. This results also indicate that applying a dilated convolution to skip connections from early layers without handling the aliasing problem makes it difficult to extract information from them and, therefore, the network assigns Finally, we trained D3Net with 1500 extra songs to study how much D3Net can be generalized by using larger dataset. In <ref type="table" target="#tab_3">Table 3</ref>, we summarize the SDR values of D3Net and other methods that utilize extra data in addition to MUSDB. Although these methods are not directly comparable since the extra data are different for every methods, we observe that the performance of D3Net is greatly improved with the aid of extra data, and obtain state-of-the-art results on vocals, other, accompaniment and the average SDR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel neural network architecture called D3Net. D3Net employs the multidilated convolution with dense skip connections that enables the local and global feature information to be modeled simultaneously within a single layer. Experimental results showed that D3Net achieves state-of-the-art results for the MUSDB18 dataset. The ablation study demonstrated the importance of handling the aliasing problem when we combine DenseNet with the dilated convolution.</p><p>6 References</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of D2 block. (a) The connectivity pattern is the same as in DenseNet except that the D2 block involves the multi-dilated convolution. (b) Illustration of the multi-dilated convolution at the third layer. To produce a single feature map, it involves multiple dilation factors depending on the input channel. For clarity, we omit the normalization and nonlinearity from the illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of receptive fields at the third layer of (a) naive integration of dilated convolution and (b) proposed multi-dilated convolution (in the case of one dimension). Red dots denote the points to which filters are applied, and the colored background shows the receptive field covered by the red dot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of normalized L1 norm of weights in the last layer of first d3 block. a low norm to them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 3. D3 block densely connects D2 blocks with repeated dilation pattern.</figDesc><table><row><cell>d=1</cell><cell>d=1,2</cell><cell>d=1,2,4</cell><cell>d=1</cell><cell>d=1,2</cell><cell>d=1,2,4</cell><cell>d=1</cell><cell>d=1,2</cell><cell>d=1,2,4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Proposed architectures. All D3 blocks have 3×3 kernels with growth rate k, L layers, and M D2 blocks.</figDesc><table><row><cell>Layer</cell><cell>scale</cell><cell>low</cell><cell>Vocals, Other high</cell><cell>full</cell><cell>low</cell><cell>Drums high</cell><cell>full</cell><cell>low</cell><cell>Bass high</cell><cell>full</cell></row><row><cell>band split index</cell><cell></cell><cell>1-256</cell><cell>257-1600</cell><cell>-</cell><cell>1-128</cell><cell>128-1600</cell><cell>-</cell><cell>1-192</cell><cell>192-1600</cell><cell>-</cell></row><row><cell>conv (t×f,ch)</cell><cell>1</cell><cell>3×3, 32</cell><cell>3×3, 8</cell><cell>3×3, 32</cell><cell>3×3, 32</cell><cell>3×3, 8</cell><cell>3×3, 32</cell><cell>3×3, 32</cell><cell>3×3, 8</cell><cell>3×3, 32</cell></row><row><cell>D3 block 1 (k,L,M)</cell><cell></cell><cell>16, 5, 2</cell><cell>2, 1, 1</cell><cell>13, 4, 2</cell><cell>16, 5, 2</cell><cell>2, 1, 1</cell><cell>13, 4, 2</cell><cell>16, 5, 2</cell><cell>2, 1, 1</cell><cell>10, 4, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell></row><row><cell>D3 block 2 (k,L,M)</cell><cell>2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>14, 5, 2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>14, 5, 2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>10, 5, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell></row><row><cell>D3 block 3 (k,L,M)</cell><cell>4</cell><cell>20, 5, 2</cell><cell>2, 1, 1</cell><cell>15, 6, 2</cell><cell>20, 5, 2</cell><cell>2, 1, 1</cell><cell>15, 6, 2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>12, 6, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell></row><row><cell>D3 block 4 (k,L,M)</cell><cell>8</cell><cell>22, 5, 2</cell><cell>2, 1, 1</cell><cell>16, 7, 2</cell><cell>22, 4, 2</cell><cell>2, 1, 1</cell><cell>16, 7, 2</cell><cell>20, 5, 2</cell><cell>2, 1, 1</cell><cell>14, 7, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell><cell></cell><cell cols="2">avg. pool 2 × 2</cell></row><row><cell>D3 block 5 (k,L,M)</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>17, 8, 2</cell><cell>-</cell><cell>-</cell><cell>16, 8, 2</cell><cell>-</cell><cell>-</cell><cell>16, 8, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell></row><row><cell>concat. D3 block 6 (k,L,M)</cell><cell>1 8</cell><cell>--</cell><cell>--</cell><cell>D3 block 4 16, 6, 2</cell><cell>--</cell><cell>--</cell><cell>D3 block 4 16, 6, 2</cell><cell>--</cell><cell>--</cell><cell>D3 block 4 14, 6, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell></row><row><cell>concat. D3 block 7 (k,L,M)</cell><cell>1 4</cell><cell cols="9">D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 20, 4, 2 2, 1, 1 14, 5, 2 20, 4, 2 2, 1, 1 14, 6, 2 18, 4, 2 2, 1, 1 12, 6, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell></row><row><cell>concat. D3 block 8 (k,L,M)</cell><cell>1 2</cell><cell cols="9">D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 18, 4, 2 2, 1, 1 12, 4, 2 18, 4, 2 2, 1, 1 12, 4, 2 16, 4, 2 2, 1, 1 8, 4, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell><cell></cell><cell>t.conv 2 × 2</cell><cell></cell></row><row><cell>concat.</cell><cell>1</cell><cell cols="9">D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1</cell></row><row><cell>D3 block 9 (k,L,M)</cell><cell></cell><cell>16, 4, 2</cell><cell>2, 1, 1</cell><cell>11, 4, 2</cell><cell>16, 4, 2</cell><cell>2, 1, 1</cell><cell>11, 4, 2</cell><cell>16, 4, 2</cell><cell>2, 1, 1</cell><cell>8, 4, 2</cell></row><row><cell>concat. (axis)</cell><cell></cell><cell></cell><cell>freq</cell><cell>-</cell><cell></cell><cell>freq</cell><cell>-</cell><cell></cell><cell>freq</cell><cell>-</cell></row><row><cell>concat. (axis) d2 block (k,L)</cell><cell>1</cell><cell></cell><cell>channel 12, 3</cell><cell></cell><cell></cell><cell>channel 12, 3</cell><cell></cell><cell></cell><cell>channel 12, 3</cell><cell></cell></row><row><cell>gate conv (t×f,ch)</cell><cell></cell><cell></cell><cell>3 × 3, 2</cell><cell></cell><cell></cell><cell>3 × 3, 2</cell><cell></cell><cell></cell><cell>3 × 3, 2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>SDR values for MUSDB18 dataset. SDR values are median of median SDR of each song. '*' denotes method operating in time domain.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">SDR in dB</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Vocals Drums Bass Other Acco. Avg.</cell></row><row><cell>TAK1 (MMDenseLSTM) [10]</cell><cell>6.60</cell><cell>6.43</cell><cell>5.16</cell><cell>4.15</cell><cell cols="2">12.83 5.59</cell></row><row><cell>UHL2 (BLSTM ensemble) [3]</cell><cell>5.93</cell><cell>5.92</cell><cell>5.03</cell><cell>4.19</cell><cell cols="2">12.23 5.27</cell></row><row><cell>GRU dilation 1 [11]</cell><cell>6.85</cell><cell>5.86</cell><cell>4.86</cell><cell>4.65</cell><cell cols="2">13.40 5.56</cell></row><row><cell>UMX [19]</cell><cell>6.32</cell><cell>5.73</cell><cell>5.23</cell><cell>4.02</cell><cell>-</cell><cell>5.33</cell></row><row><cell>demucs* [7]</cell><cell>6.29</cell><cell>6.08</cell><cell>5.83</cell><cell>4.12</cell><cell>-</cell><cell>5.58</cell></row><row><cell>Meta-TasNet* [8]</cell><cell>6.40</cell><cell>5.91</cell><cell>5.58</cell><cell>4.19</cell><cell>-</cell><cell>5.52</cell></row><row><cell>Nachmani et. al.* [20]</cell><cell>6.92</cell><cell>6.15</cell><cell>5.88</cell><cell>4.32</cell><cell>-</cell><cell>5.82</cell></row><row><cell>D3Net w/o dilation</cell><cell>6.86</cell><cell>6.37</cell><cell>4.97</cell><cell>4.21</cell><cell cols="2">13.19 5.60</cell></row><row><cell>D3Net standard dilation</cell><cell>7.12</cell><cell>6.61</cell><cell>5.19</cell><cell>4.53</cell><cell cols="2">13.39 5.86</cell></row><row><cell>D3Net (proposed)</cell><cell>7.24</cell><cell>7.01</cell><cell>5.25</cell><cell>4.53</cell><cell cols="2">13.52 6.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of models that use external data for training. SDR values are for MUSDB18 test set. '*' denotes method operating in time domain.</figDesc><table><row><cell>SDR in dB</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The conference version of this paper, which includes extended works, is available. Please refer to Naoya Takahashi et al. "Densely connected multidilated convolutional networks for dense prediction tasks", CVPR2021</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multichannel music separation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUSIPCO</title>
		<meeting>EUSIPCO</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural network based instrument extraction from music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2135" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-scale Multi-band DenseNets for Audio Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WAS-PAA</title>
		<meeting>WAS-PAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wave-u-net: A multiscale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phasenet: Discretized phase modeling with deep neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3244" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metalearning extractors for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spleeter: A fast and state-of-the art music source separation tool with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Late-Breaking/Demo ISMIR 2019</title>
		<imprint>
			<publisher>Deezer Research</publisher>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MM-DenseLSTM: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWAENC</title>
		<meeting>IWAENC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dilated convolution with dilated GRU for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence Organization (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent dilated densenets for a time-series segmentation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Priewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Listening to the world improves speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inception v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spleeter: a fast and efficient music source separation tool with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">2154</biblScope>
			<date type="published" when="2020" />
			<publisher>Deezer Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A frugal approach to music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierson</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souviraà-Labastie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
