<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">MIT CSAIL, Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenges for learning-based approaches, as 3D object annotations are scarce in real images. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from domain adaptation when tested on real data. In this work, we propose MarrNet, an end-to-end trainable model that sequentially estimates 2.5D sketches and 3D object shape. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data. Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data. This is because we can easily render realistic 2.5D sketches without modeling object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations. Our model achieves state-of-the-art performance on 3D shape reconstruction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans quickly recognize 3D shapes from a single image. <ref type="figure" target="#fig_0">Figure 1a</ref> shows a number of images of chairs; despite their drastic difference in object texture, material, environment lighting, and background, humans easily recognize they have very similar 3D shapes. What is the most essential information that makes this happen?</p><p>Researchers in human perception argued that our 3D perception could rely on recovering 2.5D sketches <ref type="bibr" target="#b23">[Marr, 1982]</ref>, which include intrinsic images <ref type="bibr" target="#b2">[Barrow and</ref><ref type="bibr">Tenenbaum, 1978, Tappen et al., 2003</ref>] like depth and surface normal maps <ref type="figure" target="#fig_0">(Figure 1b</ref>). Intrinsic images disentangle object appearance variations in texture, albedo, lighting, etc., with its shape, which retains all information from the observed image for 3D reconstruction. Humans further combine 2.5D sketches and a shape prior learned from past experience to reconstruct a full 3D shape <ref type="figure" target="#fig_0">(Figure 1c</ref>). In the field of computer vision, there have also been abundant works exploiting the idea for reconstruction 3D shapes of faces <ref type="bibr" target="#b20">[Kemelmacher-Shlizerman and Basri, 2011]</ref>, objects <ref type="bibr" target="#b32">[Tappen et al., 2003]</ref>, and scenes <ref type="bibr" target="#b13">[Hoiem et al., 2005</ref><ref type="bibr" target="#b27">, Saxena et al., 2009</ref>.</p><p>Recently, researchers attempted to tackle the problem of single image 3D reconstruction with deep learning. These approaches usually regress a 3D shape from a single RGB image directly <ref type="bibr" target="#b33">[Tulsiani et al., 2017</ref><ref type="bibr" target="#b6">, Choy et al., 2016</ref><ref type="bibr" target="#b37">, Wu et al., 2016b</ref>. In contrast, we propose a two-step while end-to-end trainable pipeline, sequentially recovering 2.5D sketches (depth and normal maps) and a 3D shape. . The 2.5D sketches can be seen as an abstraction of the image, retaining all information about the 3D shape of the object inside. We combine the sketches with learned shape priors to reconstruct the full 3D shape (c).</p><p>We use an encoder-decoder structure for each component of the framework, and also enforce the reprojection consistency between the estimated sketch and the 3D shape. We name it MarrNet, for its close resemblance to David Marr's theory of perception <ref type="bibr" target="#b23">[Marr, 1982]</ref>.</p><p>Our approach offers several unique advantages. First, the use of 2.5D sketches releases the burden on domain transfer. As single image 3D reconstruction is a highly under-constrained problem, strong prior knowledge of object shapes is needed. This poses challenges to learning-based methods, as accurate 3D object annotations in real images are rare. Most previous methods turned to training purely on synthetic data <ref type="bibr" target="#b33">[Tulsiani et al., 2017</ref><ref type="bibr" target="#b6">, Choy et al., 2016</ref><ref type="bibr" target="#b11">, Girdhar et al., 2016</ref>. However, these approaches often suffer from the domain adaption issue due to imperfect rendering. Learning 2.5D sketches from images, in comparison, is much easier and more robust to transfer from synthetic to real images, as shown in Section 4.</p><p>Further, as our second step recovers 3D shape from 2.5D sketches -an abstraction of the raw input image, it can be trained purely relying on synthetic data. Though rendering diverse realistic images is challenging, it is straightforward to obtain almost perfect object surface normals and depths from a graphics engine. This further relieves the domain adaptation issue.</p><p>We also enforce differentiable constraints between 2.5D sketches and 3D shape, making our system end-to-end trainable, even on real images without any annotations. Given a set of unlabeled images, our algorithm, pre-trained on synthetic data, can infer the 2.5D sketches of objects in the image, and use it to refine its estimation of objects' 3D shape. This self-supervised feature enhances its performance on images from different domains.</p><p>We evaluate our framework on both synthetic images of objects from ShapeNet <ref type="bibr" target="#b4">[Chang et al., 2015]</ref>, and real images from the PASCAL 3D+ dataset <ref type="bibr" target="#b38">[Xiang et al., 2014]</ref>. We demonstrate that our framework performs well on 3D shape reconstruction, both qualitatively and quantitatively.</p><p>Our contributions are three-fold: inspired by visual cognition theory, we propose a two-step, disentangled formulation for single image 3D reconstruction via 2.5D sketches; we develop a novel, end-to-end trainable model with a differentiable projection layer that ensures consistency between 3D shape and mid-level representations; we demonstrate its effectiveness on 2.5D sketch transfer and 3D shape reconstruction on both synthetic and real data.</p><p>2 Related Work 2.5D Sketch Recovery Estimating 2.5D sketches has been a long-standing problem in computer vision. In the past, researchers have explored recovering 2.5D shape from shading, texture, or color images <ref type="bibr" target="#b14">[Horn and Brooks, 1989</ref><ref type="bibr" target="#b41">, Zhang et al., 1999</ref><ref type="bibr" target="#b32">, Tappen et al., 2003</ref><ref type="bibr" target="#b1">, Barron and Malik, 2015</ref><ref type="bibr" target="#b35">, Weiss, 2001</ref><ref type="bibr" target="#b3">, Bell et al., 2014</ref>. With the development of depth sensors <ref type="bibr" target="#b15">[Izadi et al., 2011]</ref> and larger scale RGB-D datasets <ref type="bibr" target="#b29">[Silberman et al., 2012</ref><ref type="bibr" target="#b31">, Song et al., 2017</ref><ref type="bibr" target="#b24">, McCormac et al., 2017</ref>, there have also been papers on estimating depth <ref type="bibr">[Chen et al., 2016, Eigen and</ref><ref type="bibr" target="#b9">Fergus, 2015]</ref>, surface normals <ref type="bibr" target="#b0">[Bansal and</ref><ref type="bibr">Russell, 2016, Wang et al., 2015]</ref>, and other intrinsic images <ref type="bibr" target="#b28">[Shi et al., 2017</ref>, shape estimation, and (c) a loss function for reprojection consistency. MarrNet first recovers object normal, depth, and silhouette images from an RGB image. It then regresses the 3D shape from the 2.5D sketches. In both steps, it uses an encoding-decoding network. It finally employs a reprojection consistency loss to ensure the estimated 3D shape aligns with the 2.5D sketches. The entire framework can be trained end-to-end. <ref type="bibr" target="#b17">Janner et al., 2017]</ref> with deep networks. Our method employs 2.5D estimation as a component, but targets reconstructing full 3D shape of an object.</p><p>Single Image 3D Reconstruction The problem of recovering object shape from a single image is challenging, as it requires both powerful recognition systems and prior shape knowledge. With the development of large-scale shape repository like ShapeNet <ref type="bibr" target="#b4">[Chang et al., 2015]</ref>, researchers developed models encoding shape prior for this task <ref type="bibr" target="#b11">[Girdhar et al., 2016</ref><ref type="bibr" target="#b6">, Choy et al., 2016</ref><ref type="bibr" target="#b33">, Tulsiani et al., 2017</ref><ref type="bibr" target="#b37">, Wu et al., 2016b</ref><ref type="bibr" target="#b19">, Kar et al., 2015</ref><ref type="bibr" target="#b18">, Kanazawa et al., 2016</ref><ref type="bibr" target="#b30">, Soltani et al., 2017</ref>, with extension to scenes <ref type="bibr" target="#b31">[Song et al., 2017]</ref>. These methods typically regress a voxelized 3D shape directly from an input image, and rely on synthetic data or 2D masks for training. In comparison, our formulation tackles domain difference better, as it can be end-to-end fine-tuned on images without any annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D-3D Consistency</head><p>It is intuitive and practically helpful to constrain the reconstructed 3D shape to be consistent with 2D observations. Researchers have explored this idea for decades <ref type="bibr" target="#b22">[Lowe, 1987]</ref>. This idea is also widely used in 3D shape completion from depths or silhouettes <ref type="bibr" target="#b10">[Firman et al., 2016</ref><ref type="bibr" target="#b26">, Rock et al., 2015</ref><ref type="bibr" target="#b8">, Dai et al., 2017</ref>. Recently, a few papers discussed enforcing differentiable 2D-3D constraints between shape and silhouettes, enabling joint training of deep networks for 3D reconstruction <ref type="bibr" target="#b36">[Wu et al., 2016a</ref><ref type="bibr" target="#b40">, Yan et al., 2016</ref><ref type="bibr" target="#b25">, Rezende et al., 2016</ref><ref type="bibr" target="#b33">, Tulsiani et al., 2017</ref>. In our paper, we exploit this idea to develop differentiable constraints on the consistency between various 2.5D sketches and 3D shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>To recover the 3D structure from a single view RGB image, our MarrNet contains three parts: first, a 2.5D sketch estimator, which predicts the depth, surface normal, and silhouette images of the object <ref type="figure" target="#fig_1">(Figure 2a</ref>); second, a 3D shape estimator, which infers 3D object shape using a voxel representation ( <ref type="figure" target="#fig_1">Figure 2b</ref>); third, a reprojection consistency function, enforcing the alignment between the estimated 3D structure and inferred 2.5D sketches <ref type="figure" target="#fig_1">(Figure 2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">2.5D Sketch Estimation</head><p>The first component of our network <ref type="figure" target="#fig_1">(Figure 2a</ref>) takes a 2D RGB image as input, and predicts its 2.5D sketch: surface normal, depth, and silhouette. The goal of the 2.5D sketch estimation step is to distill intrinsic object properties from input images, while discarding properties that are non-essential for the task of 3D reconstruction, such as object texture and lighting.</p><p>We use an encoder-decoder network architecture for 2.5D sketch estimation. Our encoder is a ResNet-18 <ref type="bibr" target="#b12">[He et al., 2015]</ref>, encoding a 256×256 RGB image into 512 feature maps of size 8×8. The decoder contains four sets of 5×5 fully convolutional and ReLU layers, followed by four sets of 1×1 convolutional and ReLU layers. It outputs the corresponding depth, surface normal, and silhouette images, also at the resolution of 256×256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Shape Estimation</head><p>The second part of our framework ( <ref type="figure" target="#fig_1">Figure 2b</ref>) infers 3D object shape from estimated 2.5D sketches.</p><p>Here, the network focuses on learning the shape prior that explains input well. As it takes only surface normal and depth images as input, it can be trained on synthetic data, without suffering from the domain adaption problem: it is straightforward to render nearly perfect 2.5D sketches, but much harder to render realistic images.</p><p>The network architecture is inspired by the TL network <ref type="bibr" target="#b11">[Girdhar et al., 2016]</ref>, and the 3D-VAE-GAN <ref type="bibr" target="#b37">[Wu et al., 2016b]</ref>, again with an encoding-decoding style. It takes a normal image and a depth image as input (both masked by the estimated silhouette), maps them to a 200-dim vector via five sets of convolutional, ReLU, and pooling layers, followed by two fully connected layers. The detailed encoder structure can be found in <ref type="bibr" target="#b11">Girdhar et al. [2016]</ref>. The vector then goes through a decoder, which consists of five fully convolutional and ReLU layers to output a 128×128×128 voxel-based reconstruction of the input. The detailed encoder structure can be found in <ref type="bibr" target="#b37">Wu et al. [2016b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reprojection Consistency</head><p>There have been works attempting to enforce the consistency between estimated 3D shape and 2D representations in a neural network <ref type="bibr" target="#b40">[Yan et al., 2016</ref><ref type="bibr" target="#b25">, Rezende et al., 2016</ref><ref type="bibr" target="#b36">, Wu et al., 2016a</ref><ref type="bibr" target="#b33">, Tulsiani et al., 2017</ref>. Here, we explore novel ways to include a reprojection consistency loss between the predicted 3D shape and the estimated 2.5D sketch, consisting of a depth reprojection loss and a surface normal reprojection loss.</p><p>We use v x,y,z to represent the value at position (x, y, z) in a 3D voxel grid, assuming that v x,y,z ∈ [0, 1], ∀x, y, z. We use d x,y to denote the estimated depth at position (x, y), and n x,y = (n a , n b , n c ) to denote the estimated surface normal. We assume orthographic projection in this work.</p><p>Depths The projected depth loss tries to guarantee that the voxel with depth v x,y,dx,y should be 1, and all voxels in front of it should be 0. This ensures the estimated 3D shape matches the estimated depth values.</p><p>As illustrated in <ref type="figure" target="#fig_2">Figure 3a</ref>, we define projected depth loss as follows:</p><formula xml:id="formula_0">L depth (x, y, z) =    v 2 x,y,z , z &lt; d x,y (1 − v x,y,z ) 2 , z = d x,y 0, z &gt; d x,y .<label>(1)</label></formula><p>The gradients are</p><formula xml:id="formula_1">∂L depth (x, y, z) ∂v x,y,z =    2v x,y,z , z &lt; d x,y 2(v x,y,z − 1), z = d x,y 0, z &gt; d x,y .<label>(2)</label></formula><p>When d x,y = ∞, our depth criterion reduces to a special case -the silhouette criterion. As shown in <ref type="figure" target="#fig_2">Figure 3b</ref>, for a line that has no intersection with the shape, all voxels in it should be 0.</p><p>Surface normals As vectors n x = (0, −n c , n b ) and n y = (−n c , 0, n a ) are orthogonal to the normal vector n x,y = (n a , n b , n c ), we can normalize them to obtain two vectors, n x = (0, −1, n b /n c ) and n y = (−1, 0, n a /n c ), both on the estimated surface plane at (x, y, z). The projected surface normal loss tries to guarantee that the voxels at (x, y, z) ± n x and (x, y, z) ± n +y should be 1 to match the estimated surface normals. These constraints only apply when the target voxels are inside the estimated silhouette.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3c</ref>, let z = d x,y , the projected surface normal loss is defined as</p><formula xml:id="formula_2">L normal (x, y, z) = 1 − v x,y−1,z+ n b nc 2 + 1 − v x,y+1,z− n b nc 2 + 1 − v x−1,y,z+ na nc 2 + 1 − v x+1,y,z− na nc 2 .<label>(3)</label></formula><p>Then the gradients along the x direction are</p><formula xml:id="formula_3">∂L normal (x, y, z) ∂v x−1,y,z+ na nc = 2 v x−1,y,z+ na nc − 1 and ∂L normal (x, y, z) ∂v x+1,y,z− na nc = 2 v x+1,y,z− na nc − 1 .</formula><p>(4) The gradients along the y direction are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training paradigm</head><p>We employ a two-step training paradigm. We first train the 2.5D sketch estimation and the 3D shape estimation components separately on synthetic images; we then fine-tune the network on real images.</p><p>For pre-training, we use synthetic images of ShapeNet objects. The 2.5D sketch estimator is trained using the ground truth surface normal, depth, and silhouette images with a L2 loss. The 3D interpreter is trained using ground truth voxels and a cross-entropy loss. Please see Section 4.1 for details on data preparation.</p><p>The reprojection consistency loss is used to fine-tune the 3D estimation component of our model on real images, using the predicted normal, depth, and silhouette. We observe that a straightforward implementation leads to shapes that explain 2.5D sketches well, but with unrealistic appearance. This is because the 3D estimation module overfits the images without preserving the learned 3D shape prior. See <ref type="figure">Figure 5</ref> for examples, and Section 4.2 for more details.</p><p>We therefore choose to fix the decoder of the 3D estimator and only fine-tune the encoder. During testing, our method can be self-supervised, i.e., we can fine-tune even on a single image without any annotations. In practice, we fine-tune our model separately on each image for 40 iterations. For each test image, fine-tuning takes up to 10 seconds on a modern GPU; without fine-tuning, testing time is around 100 milliseconds. We use SGD for optimization with a batch size of 4, a learning rate of 0.001, and a momentum of 0.9. We implemented our framework in Torch7 <ref type="bibr" target="#b7">[Collobert et al., 2011]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we present both qualitative and quantitative results on single image 3D reconstruction using variants of our framework. We evaluate our entire framework on both synthetic and real-life images on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D Reconstruction on ShapeNet</head><p>Data We start with experiments on synthesized images of ShapeNet chairs <ref type="bibr" target="#b4">[Chang et al., 2015]</ref>. We put objects in front of random backgrounds from the SUN database <ref type="bibr" target="#b39">[Xiao et al., 2010]</ref>, and render the corresponding RGB, depth, surface normal, and silhouette images. We use a physics-based renderer, Mitsuba <ref type="bibr" target="#b16">[Jakob, 2010]</ref>, to obtain more realistic images. For each of the 6,778 ShapeNet chairs, we render 20 images of random viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated normals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated depths</head><p>Ground truth Direct predictions Images MarrNet <ref type="figure">Figure 4</ref>: Results on rendered images of ShapeNet objects <ref type="bibr" target="#b4">[Chang et al., 2015]</ref>. From left to right: input, estimated normal map, estimated depth map, our prediction, a baseline algorithm that predicts 3D shape directly from RGB input without modeling 2.5D sketch, and ground truth. Both normal and depth maps are masked by predicted silhouettes. Our method is able to recover shapes with smoother surfaces and finer details.</p><p>Methods We follow the training paradigm described in Section 3.4, but without the final fine-tuning stage, as ground truth 3D shapes are available on this synthetic dataset. Specifically, the 2.5D sketch estimator is trained using ground truth depth, normal and silhouette images and a L2 reconstruction loss. The 3D shape estimation module takes in the masked ground truth depth and normal images as input, and predicts 3D voxels of size 128×128×128 with a binary cross entropy loss.</p><p>We compare MarrNet with a baseline that predicts 3D shape directly from an RGB image, without modeling 2.5D sketches. The baseline employs the same architecture as our 3D shape estimator (Section 3.2). We show qualitative results in <ref type="figure">Figure 4</ref>. Our estimated surface normal and depth images abstract out non-essential information like textures and lighting in the RGB image, while preserving intrinsic information about object shape. Compared with the direct prediction baseline, our model outputs objects with more details and smoother surfaces. For quantitative evaluation, previous works usually compute the Intersection-over-Union (IoU) <ref type="bibr" target="#b33">[Tulsiani et al., 2017</ref><ref type="bibr" target="#b6">, Choy et al., 2016</ref>]. Our full model achieves a higher IoU (0.57) than the direct prediction baseline (0.52).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Reconstruction on Pascal 3D+</head><p>Data PASCAL 3D+ dataset <ref type="bibr" target="#b38">[Xiang et al., 2014]</ref> provides (rough) 3D models for objects in real-life images. Here, we use the same test set of PASCAL 3D+ with earlier works <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>.</p><p>Methods We follow the paradigm described in Section 3.4: we first train each module separately on the ShapeNet dataset, and then fine-tune them on the PASCAL 3D+ dataset. Unlike previous works <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>, our model requires no silhouettes as input during fine-tuning; it instead estimates silhouette jointly.</p><p>As an ablation study, we compare three variants of our model: first, the model trained using ShapeNet data only, without fine-tuning; second, the fine-tuned model whose decoder is not fixed during  <ref type="figure">Figure 5</ref>: We present an ablation study, where we compare variants of our models. From left to right: input, estimated normal, estimated depth, 3D prediction before fine-tuning, two views of the 3D prediction after fine-tuning without fixing decoder, and two views of the 3D prediction after fine-tuning with the decoder fixed. When the decoder is not fixed, the model explains the 2.5D sketch well, but fails to preserve the learned shape prior. Fine-tuning with a fixed decoder resolves the issue.</p><p>fine-tuning; and third, the full model whose decoder is fixed during fine-tuning. We also compare with the state-of-the-art method (DRC) <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>, and the provided ground truth shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results of our ablation study are shown in <ref type="figure">Figure 5</ref>. The model trained on synthetic data provides a reasonable shape estimate. If we fine-tune the model on Pascal 3D+ without fixing the decoder, the output voxels explain the 2.5D sketch data well, but fail to preserve the learned shape prior, leading to impossible shapes from certain views. Our final model, fine-tuned with the decoder fixed, keeps the shape prior and provides more details of the shape.</p><p>We show more results in <ref type="figure">Figure 6</ref>, where we compare with the state-of-the-art (DRC) <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>, and the provided ground truth shapes. Quantitatively, our algorithm achieves a higher IoU over these methods (MarrNet 0.39 vs. DRC 0.34). However, we find the IoU metric sub-optimal for three reasons. First, measuring 3D shape similarity is a challenging yet unsolved problem, and IoU prefers models that predict mean shapes consistently, with no emphasize on details. Second, as object shape can only be reconstructed up to scale from a single image, it requires searching over all possible scales during the computation of IoU, making it less efficient. Third, as discussed in <ref type="bibr" target="#b33">Tulsiani et al. [2017]</ref>, PASCAL 3D+ has only rough 3D annotations (10 CAD chair models for all images). Computing IoU with these shapes would thus not be the most informative evaluation metric.</p><p>We instead conduct human studies, where we show users the input image and two reconstructions, and ask them to choose the one that looks closer to the shape in the image. We show each test image to 10 human subjects. As shown in <ref type="table">Table 1</ref>, our reconstruction is preferred 74% of the time to DRC, and 42% of the time to ground truth, showing a clear advantage.</p><p>We present some failure cases in <ref type="figure">Figure 7</ref>. Our algorithm does not perform well on recovering complex, thin structure, and sometimes fails when the estimated mask is very inaccurate. Also, while DRC may benefit from multi-view supervision, we have only evaluated MarrNet given a single view of the shape, though adapting our formulation to multi-view data should be straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images Ground truth</head><p>MarrNet DRC Images Ground truth MarrNet DRC <ref type="figure">Figure 6</ref>: 3D reconstructions of chairs on the Pascal 3D+ <ref type="bibr" target="#b38">[Xiang et al., 2014]</ref> dataset. From left to right: input, the ground truth shape from the dataset, 3D estimation by DRC <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>, and two views of MarrNet predictions. Our model recovers more accurate 3D shapes. <ref type="table">GT   DRC  50  26  17  MarrNet  74  50  42  Ground truth  83  58  50   Table 1</ref>: Human preferences on chairs in PAS-CAL 3D+ <ref type="bibr" target="#b38">[Xiang et al., 2014]</ref>. We compare MarrNet with the state-of-the-art (DRC) <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>, and the ground truth provided by the dataset. Each number shows the percentage of humans prefer the left method to the top one. MarrNet is preferred 74% of the time to DRC, and 42% of the time to ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DRC MarrNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated normals</head><p>Estimated depths MarrNet Images <ref type="figure">Figure 7</ref>: Failure cases on Pascal 3D+. Our algorithm does not perform well on recovering complex, thin structure, and sometimes fails when the estimated mask is very inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">3D Reconstruction on IKEA</head><p>Data The IKEA dataset <ref type="bibr" target="#b21">[Lim et al., 2013]</ref> contains images of IKEA furniture, along with accurate 3D shape and pose annotations. These images are challenging, as objects are often heavily occluded or cropped. We also evaluate our model on the IKEA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We show qualitative results in <ref type="figure">Figure 8</ref>, where we compare with estimations by 3D-VAE-GAN <ref type="bibr" target="#b37">[Wu et al., 2016b]</ref> and the ground truth. As shown in the figure, our model can deal with mild occlusions in real life scenarios. We also conduct human studies on the IKEA dataset. Results show that 61% of the subjects prefer our reconstructions to those of 3D-VAE-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Extensions</head><p>We also apply our framework on cars and airplanes. We use the same setup as that for chairs. As shown in <ref type="figure">Figure 9</ref>, shape details like the horizontal stabilizer and rear-view mirrors are recovered Images Ground truth MarrNet 3D-VAE-GAN Images Ground truth MarrNet 3D-VAE-GAN <ref type="figure">Figure 8</ref>: 3D reconstruction of chairs on the IKEA <ref type="bibr" target="#b21">[Lim et al., 2013]</ref> dataset. From left to right: input, ground truth, 3D estimation by 3D-VAE-GAN <ref type="bibr" target="#b37">[Wu et al., 2016b]</ref>, and two views of MarrNet predictions. Our modelrecovers more details compared to 3D-VAE-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images Ground truth</head><p>MarrNet DRC Images Ground truth MarrNet DRC <ref type="figure">Figure 9</ref>: 3D reconstructions of airplanes and cars from PASCAL 3D+. From left to right: input, the ground truth shape from the dataset, 3D estimation by DRC <ref type="bibr" target="#b33">[Tulsiani et al., 2017]</ref>, and two views of MarrNet predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth MarrNet Images</head><p>Ground truth MarrNet Images <ref type="figure" target="#fig_0">Figure 10</ref>: 3D reconstruction of objects from multiple categories on the PASCAL 3D+ <ref type="bibr" target="#b38">[Xiang et al., 2014]</ref> dataset. Under this setup MarrNet also recovers 3D shape well. by our model. We further train MarrNet jointly on all three object categories, and show results in <ref type="figure" target="#fig_0">Figure 10</ref>. Our model successfully recovers shapes of different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed MarrNet, a novel model that explicitly models 2.5D sketches for single image 3D shape reconstruction. The use of 2.5D sketches enhanced the model's performance, and made it easily adaptive to images across domains or even categories. We also developed differentiable loss functions for the consistency between 3D shape and 2.5D sketches, so that MarrNet can be end-to-end fine-tuned on real images without annotations. Experiments demonstrated that our model performs well, and is preferred by human annotators over competitors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Objects in real images (a) are subject to appearance variations regarding color, texture, lighting, material, background, etc. Despite this, their 2.5D sketches like surface normal and depth maps remain constant (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our model (MarrNet) has three major components: (a) 2.5D sketch estimation, (b) 3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Reprojection consistency between 2.5D sketches and 3D shape. Left and middle: the criteria for depths and silhouettes; right: the criterion for surface normals. See Section 3.3 for details.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Shubham Tulsiani for sharing the DRC results, and Chengkai Zhang for the help on shape visualization. This work is supported by NSF #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF #1231216), Toyota Research Institute, Samsung, and Shell.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1670" to="1687" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recovering intrinsic scene characteristics from images. Computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Harry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intrinsic images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shape from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brooks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Wenzel</surname></persName>
		</author>
		<ptr target="http://www.mitsuba-renderer.org.5" />
		<title level="m">Mitsuba renderer</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised intrinsic image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for single-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Category-specific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d face reconstruction from a single image using a single reference face shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="405" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing ikea objects: Fine pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Three-dimensional object recognition from single two-dimensional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="395" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>W. H. Freeman and Company</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning non-lambertian object intrinsics across shapenet categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Synthesizing 3d shapes via modeling multi-view depth maps and silhouettes with deep generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Amir Arsalan Soltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering intrinsic images from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marshall F Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deriving intrinsic images from image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shape-from-shading: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Sing</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Edwin</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
