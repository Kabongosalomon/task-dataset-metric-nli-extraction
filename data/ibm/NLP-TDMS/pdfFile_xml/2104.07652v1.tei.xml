<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry-Free View Synthesis: Transformers and no 3D Priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit2">IWR</orgName>
								<orgName type="institution" key="instit3">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit2">IWR</orgName>
								<orgName type="institution" key="instit3">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit2">IWR</orgName>
								<orgName type="institution" key="instit3">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry-Free View Synthesis: Transformers and no 3D Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*Both authors contributed equally to this work. Code is available at https://git.io/JOnwn.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. We present a probabilistic approach to Novel View Synthesis based on transformers, which does not require explicit 3D priors. Given a single source frame and a camera transformation (center), we synthesize plausible novel views that exhibit high fidelity (right). For comparison, SynSin [71] (left) yields uniform surfaces and unrealistic warps for large camera transformations.</p><p>Abstract Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformerbased model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Imagine looking through an open doorway. Most of the room on the other side is invisible. Nevertheless, we can estimate how the room likely looks. The few visible features enable an informed guess about the height of the ceiling, the position of walls and lighting etc. Given this limited information, we can then imagine several plausible realizations of the room on the other side. This 3D geometric reasoning and the ability to predict what the world will look like before we move is critical to orient ourselves in a world with three spatial dimensions. Therefore, we address the problem of novel view synthesis <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref> based on a single initial image and a desired change in viewpoint. In particular, we aim at specifically modeling large camera transformations, e.g. rotating the camera by 90 • and looking at previously unseen scenery. As this is an underdetermined problem, we present a probabilistic generative model that learns the distribution of possible target images and synthesizes them at high fidelity. Solving this task has the potential to transform the passive experience of viewing images into an interactive, 3D exploration of the depicted scene. This necessitates an approach that both understands the geometry of the scene and, when rendering novel views of an input image, considers their semantic relationships to the visible content. Interpolation vs. Extrapolation Recently, impressive synthesis results have been obtained with geometry-focused approaches in the multi-view setting <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b43">43]</ref>, where not just a single but a large number of images or a video of a scene are available such that the task is closer to a view interpolation than a synthesis of genuinely novel views. In contrast, if only a single image is available, the synthesis of novel views is always an extrapolation task. Solving this task is appealing because it allows a 3D exploration of a scene starting from only a single picture. While existing approaches for single-view synthesis make small camera transformations, such as a rotation by a few degrees, possible, we aim at expanding the possible camera changes to include large transformations. The latter necessitates a probabilistic framework: Especially when applying large transformation, the problem is underdetermined because there are many possible target images which are consistent with the source image and camera pose. This task cannot be solved with a reconstruction objective alone, as it will either lead to averaging, and hence blurry synthesis results, or, when combined with an adversarial objective, cause a significant mode-dropping when modeling the target distribution. Thus, in order to remedy these issues, we propose to model this task with a powerful, autoregressive transformer model, trained to directly maximize the likelihood of the target data. Explicit vs. Implicit Geometry The success of transformers is often attributed to the fact that they enforce less inductive biases compared to convolutional neural networks (CNNs), which are biased towards local context. Relying mainly on CNNs, this locality-bias required previous approaches for novel view synthesis to explicitly model the overall geometric transformation, thereby enforcing yet another inductive bias regarding the three dimensional structure. In contrast, by modeling interactions between farflung regions of source and target images, transformers have the potential to learn to represent the required geometric transformation implicitly without requiring such hand engineered operations. This raises the question whether it is at all necessary to explicitly include such biases in a transformer model. To address this question, we perform several experiments with varying degrees of inductive bias and find that our autoregressively trained transformer model is indeed capable of learning this transformation completely without built-in priors and can even learn to predict depth in an unsupervised fashion. To summarize our contributions, we (i) propose to learn a probabilistic model for single view synthesis that properly takes into account the uncertainties inherent in the task and show that this leads to significant benefits over previous state-of-the-art approaches; see <ref type="figure">Fig. 1</ref>. We (ii) analyze the need for explicit 3D inductive biases in transformer architectures and find that transformers make it obsolete to explicitly code 3D transformations into the model and instead can learn the required transformation implicitly themselves. We also (iii) find that the benefits of providing them geometric information in the form of explicit depth maps are relatively small, and investigate the ability to recover an explicit depth representation from the layers of a transformer which has learned to represent the geometric transformation implicitly and without any depth supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Novel View Synthesis We can identify three seminal works which illustrate different levels of reliance on geometry to synthesize novel views. <ref type="bibr" target="#b33">[34]</ref> describes an approach which requires no geometric model, but requires a large number of structured input views. <ref type="bibr" target="#b22">[23]</ref> describes a similar approach but shows that unstructured input views suffice if geometric information in the form of a coarse volumetric estimate is employed. <ref type="bibr" target="#b8">[9]</ref> can work with a sparse set of views but requires an accurate photogrammetric model. Subsequent work also analyzed the commonalities and trade-offs of these approaches <ref type="bibr" target="#b4">[5]</ref>. Ideally, an approach could synthesize novel views from a single image without having to rely on accurate geometric models of the scene and early works on deep learning for novel view synthesis explored the possibility to directly predict novel views <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b64">64]</ref> or their appearance flows <ref type="bibr" target="#b78">[78,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b62">62]</ref> with convolutional neural networks (CNNs). However, results of these methods were limited to simple or synthetic data and subsequent works combined geometric approaches with CNNs. Among these deep learning approaches that explicitly model geometry, we can distinguish between approaches relying on a proxy geometry to perform a warping into the target view, and approaches predicting a 3D representation that can subsequently be rendered in novel views. For the proxy geometry, <ref type="bibr" target="#b42">[42]</ref> relies on point clouds obtained from structure from motion (SfM) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b56">56]</ref> and multi-view stereo (MVS) <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b17">18]</ref>. To perform the warping, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b73">73]</ref> use plane-sweep volumes, <ref type="bibr" target="#b30">[31]</ref> estimates depth at novel views and <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b72">72]</ref> a depth probability volume. <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b54">54]</ref> postprocess MVS results to a global mesh and <ref type="bibr" target="#b25">[26]</ref> relies on per-view meshes <ref type="bibr" target="#b26">[27]</ref>. Other approaches learn 3D features per scene, which are associated with a point cloud <ref type="bibr" target="#b1">[2]</ref> or UV maps <ref type="bibr" target="#b66">[66]</ref>, and decoded to the target image using a CNN. However, all of these approaches rely on multi-view inputs to obtain an estimate for the proxy geometry. Approaches which predict 3D representations mainly utilize layered representations such as layered depth images (LDIs) <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, multi-plane images (MPIs) <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b15">16]</ref> and variants thereof <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b34">35]</ref>. While this allows an efficient rendering of novel views from the obtained representations, their layered nature limits the range of novel views that can be synthesized with them. Another emerging approach <ref type="bibr" target="#b43">[43]</ref> represents a five dimensional light field directly with a multi-layer-perceptron (MLP), but still requires a large number of input views to correctly learn this MLP. In the case of novel view synthesis from a single view, SfM approaches cannot be used to estimate proxy geometries and early works relied on human interaction to obtain a scene model <ref type="bibr" target="#b28">[29]</ref>. <ref type="bibr" target="#b61">[61]</ref> uses a large scale, scene-specific light field dataset to learn CNNs which predict light fields from a single image. <ref type="bibr" target="#b38">[38]</ref> assumes that scenes can be represented by a fixed set of planar surfaces. To handle more general scenes, most methods rely on monocular depth estimation <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b20">21]</ref> to predict warps <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b37">37]</ref> or LDIs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">59]</ref>, and <ref type="bibr" target="#b67">[67]</ref>   <ref type="figure">Figure 2</ref>. We formulate novel view synthesis as sampling from the distribution p(x dst |x src , T ) of target images x dst for a given source image x src and camera change T . We use a VQGAN to model this distribution autoregressively with a transformer and introduce a conditioning function f (x src , T ) to encode inductive biases into our model. We analyze explicit variants, which estimate scene depth d and warp source features into the novel view, as well as implicit variants without such a warping. The table on the right summarizes the variants for f . single view. To handle disocclusions, most of these methods rely on adversarial losses, inspired by generative adversarial networks (GANs) <ref type="bibr" target="#b21">[22]</ref>, to perform inpainting in these regions. However, the quality of these approaches quickly degrades for larger viewpoint changes because they do not model the uncertainty of the task. While adversarial losses can remedy an averaging effect over multiple possible realizations to some degree, our empirical results demonstrate the advantages of properly modeling the probabilistic nature of novel view synthesis from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention and Transformers</head><p>The transformer [69] is a sequence-to-sequence model that models interactions between learned representations of sequence elements by the so-called attention mechanism <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">46]</ref>. Importantly, this mechanism does not introduce locality biases such as those present in e.g. CNNs, as the importance and interactions of sequence elements are weighed regardless of their relative positioning. We build our autogressive transformer from the GPT-2 architecture <ref type="bibr" target="#b49">[49]</ref>, i.e. multiple blocks of multihead self-attention, layer norm <ref type="bibr" target="#b2">[3]</ref> and position-wise MLP.</p><p>Autoregressive Two Stage Approaches Our approach is based on work in neural discrete representation learning (VQVAE) <ref type="bibr" target="#b68">[68]</ref>, which aims to learn a discrete, compressed representation through either vector quantization or soft relaxation of the discrete assignment <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b29">30]</ref>. This training paradigm provides a suitable space <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7]</ref> to train autoregressive likelihood models on the latent representations and has been utilized to train generative models for hierarchical, class-conditional image synthesis <ref type="bibr" target="#b52">[52]</ref>, text-controlled image synthesis <ref type="bibr" target="#b50">[50]</ref> and music generation <ref type="bibr" target="#b10">[11]</ref>. Recently, <ref type="bibr" target="#b14">[15]</ref> demonstrated that adversarial training of the VQVAE improves compression while retaining high-fidelity reconstructions, subsequently enabling efficient training of an autoregressive transformer model on the learned latent space (yielding a so-called VQGAN). We directly build on this work and use VQGANs to represent both source and target views and, when needed, depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>To render a given image x src experienceable in a 3D manner, we allow the specification of arbitrary new viewpoints, including in particular large camera transformations T . As a result we expect multiple plausible realizations x dst for the novel view, which are all consistent with the input, since this problem is highly underdetermined. Consequently, we follow a probabilistic approach and sample novel views from the distribution</p><formula xml:id="formula_0">x dst ∼ p(x dst |x src , T ).<label>(1)</label></formula><p>To solve this task, a model must explicitly or implicitly learn the 3D relationship between both images and T . In contrast to most previous work that tries to solve this task with CNNs and therefore oftentimes includes an explicit 3D transformation, we want to use the expressive transformer architecture and investigate to what extent the explicit specification of such a 3D model is necessary at all. Sec. 3.1 describes how to train a transformer model in the latent space of a VQGAN. Next, Sec. 3.2 shows how inductive biases can be build into the transformer and describes all bias-variants that we analyze. Finally, Sec. 3.3 presents our approach to extract geometric information from a transformer where no 3D bias has been explicitly specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Probabilistic View Synthesis in Latent Space</head><p>Learning the distribution in Eq. (1) requires a model which can capture long-range interactions between source and target view to implicitly represent geometric transformations. Transformer architectures naturally meet these requirements, since they are not confined to short-range relations such as CNNs with their convolutional kernels and exhibit state-of-the-art performance <ref type="bibr" target="#b69">[69]</ref>. Since likelihoodbased models have been shown <ref type="bibr" target="#b55">[55]</ref> to spend too much capacity on short-range interactions of pixels when modeling images directly in pixel space, we follow <ref type="bibr" target="#b14">[15]</ref> and employ a two-stage training. The first stage performs adversarially guided discrete representation learning (VQGAN), obtaining an abstract latent space that has proved to be well-suited for efficiently training generative transformers <ref type="bibr" target="#b14">[15]</ref>.</p><p>Modeling Conditional Image Likelihoods VQGAN consists of an encoder E, a decoder G and a codebook Z =</p><formula xml:id="formula_1">{z i } |Z| i=1 of discrete representations z i ∈ R dz .</formula><p>The trained VQGAN allows to encode any x ∈ R H×W ×3 into the discrete latent space as E(x) ∈ R h×w×dz 1 . Unrolled in rasterscan order, this latent representation corresponds to a sequence s ∈ R h·w×dz and can be equivalently expressed as a sequence of integers which index the learned codebook Z. Following the usual designation <ref type="bibr" target="#b69">[69]</ref> we refer to the sequence elements as "tokens". An embedding function g = g(s) ∈ R h·w×de maps each of these tokens into the embedding space of the transformer T and adds learnable positional encodings. Similarly, to encode the input view x src and the camera transformation T , both are mapped into the embedding space by a function f :</p><formula xml:id="formula_2">f : (x src , T ) → f (x src , T ) ∈ R n×de ,<label>(2)</label></formula><p>where n denotes the length of the conditioning sequence.</p><p>By using different functions f various inductive biases can be incorporated into the architecture as described in Sec. 3.2. The transformer T then processes the concatenated sequence [f (x src , T ), g(s dst )] to learn the distribution of plausible novel views conditioned on x src and T ,</p><formula xml:id="formula_3">p T s dst |f (x src , T ) = i p T s dst i |s dst &lt;i , f (x src , T ) . (3)</formula><p>Hence, to train an autoregressive transformer by nexttoken prediction p T (s i |s &lt;i , f (x src , T )) we maximize the log-likelihood of the data, leading to the training objective</p><formula xml:id="formula_4">L T = E x src ,x dst ∼p(x src ,x dst ) − log p T s dst |f (x src , T ) . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoding Inductive Biases</head><p>Besides achieving high-quality novel view synthesis, we aim to investigate to what extent transformers depend on a 3D inductive bias. To this end, we compare approaches where a geometric transformation is built explicitly into the conditioning function f , and approaches where no such transformation is used. In the latter case, the transformer itself must learn the required relationship between source and target view. If successful, the transformation will be described implicitly by the transformer. Geometric Image Warping We first describe how an explicit geometric transformation results from the 3D relation of source and target images. For this, pixels of the source image are back-projected to three dimensional coordinates, which can then be re-projected into the target view. We assume a pinhole camera model, such that the projection of 3D points to homoegenous pixel coordinates is determined through the intrinsic camera matrix K. The transformation between source and target coordinates is given by a rigid motion, consisting of a rotation R and a translation t. Together, these parameters specify the desired control over the novel view to be generated, i.e. T = (K, R, t).</p><p>To project pixels back to 3D coordinates, we require information about their depth d, since this information has been discarded by their projection onto the camera plane. Since we assume access to only a single source view, we require a monocular depth estimate. Following by previous works <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b37">37]</ref>, we use MiDaS <ref type="bibr" target="#b51">[51]</ref> in all of our experiments which require monocular depth information.</p><p>The transformation can now be described as a mapping of pixels i ∈ {1, . . . , H}, j ∈ {1, . . . , W } in the source image x src ∈ R H×W ×3 to pixels i , j in the target image. In homogeneous coordinates, their relationship is given by</p><formula xml:id="formula_5">  j i 1   K   RK −1 d(i, j)   j i 1   + t  <label>(5)</label></formula><p>This relationship defines a forward flow field F src→dst = F src→dst (K, R, t, d) ∈ R H×W ×3 from source to target as a function of depth and camera parameters. The flow field can then be used to warp the source image x src into the target view with a warping operation S:</p><formula xml:id="formula_6">x wrp = S(F src→dst , x src ).<label>(6)</label></formula><p>Because the target pixels obtained from the flow are not necessarily integer valued, we follow <ref type="bibr" target="#b44">[44]</ref> and implement S by bilinearly splatting features across the four closest target pixels. When multiple source pixels map to the same target pixels, we use their relative depth to give points closer to the camera more weight-a soft variant of z-buffering.</p><p>In the simplest case, we can now describe the difference between explicit and implicit approaches in the way that they receive information about the source image and the desired target view. Here, explicit approaches receive source information warped using the camera parameters, whereas implicit approaches receive the original source image and the camera parameters themselves, i.e. explicit:</p><formula xml:id="formula_7">S(F src→dst (K, R, t, d), x src ) (7) implicit: (K, R, t, d, x src )<label>(8)</label></formula><p>Thus, in explicit approaches we enforce an inductive bias on the 3D relationship between source and target by making this relationship explicit, while implicit approaches have to learn it on their own. Next, we introduce a number of different variants for each, which are summarized in <ref type="figure">Fig. 2</ref>. Explicit Geometric Transformations In the following, we describe all considered variants in terms of the transformer's conditioning function f . Furthermore, e denotes a <ref type="table">Table 1</ref>. To assess the effect of encoding different degrees of 3D prior knowledge, we evaluate all variants on RealEstate and ACID using negative log-likelihood (NLL), FID <ref type="bibr" target="#b27">[28]</ref> and PSIM <ref type="bibr" target="#b75">[75]</ref>, PSNR and SSIM <ref type="bibr" target="#b70">[70]</ref>. We highlight best, second best and third best scores. learnable embedding mapping the discrete VQGAN codes E(x) into the embedding space of the transformer. Similarly, e pos ∈ R n×de denotes a learnable positional encoding. The flow field F src→dst (K, R, t, d) is always computed from x src and, to improve readability, we omit it from the arguments of the warping operation, i.e. S(·) = S(F src→dst (K, R, t, d), ·).</p><formula xml:id="formula_8">RealEstate10K ACID method FID ↓ NLL ↓ PSIM ↓ SSIM ↑ PSNR ↑ FID ↓ NLL ↓ PSIM ↓ SSIM ↑ PSNR ↑ impl.-</formula><p>(I) Our first explicit variant, expl.-img, warps the source image and encodes it in the same way as the target image:</p><p>f (x src , T ) = e(E(S(x src ))) + e pos <ref type="bibr" target="#b8">(9)</ref> (II) Inspired by previous works <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b1">2]</ref> we include a expl.-feat variant which first encodes the original source image, and subsequently applies the warping on top of these features. We again use the VQGAN encoder E to obtain f (x src , T ) = e(S(E(x src ))) + e pos <ref type="bibr" target="#b9">(10)</ref> (III) To account for the fact that the warped features in Eq. (10) remain fixed (due to E being frozen), we also consider a expl.-emb variant that warps the learnable embeddings and positional encodings of the transformer model. More precisely, we concatenate original embeddings with their warped variants and merge them with a learnable matrix. Doing this for both the embeddings of the codes and for the positional encodings using matrices W emb , W pos ∈ R de×2·de , the conditioning function f then reads:</p><formula xml:id="formula_9">f (x src , T ) =W emb [e(E(x src )), S(e(E(x src )))]+ W pos [e pos , S(e pos )]<label>(11)</label></formula><p>Implicit Geometric Transformations Next, we describe implicit variants that we use to analyze if transformerswith their ability to attend to all positions equally wellrequire an explicit geometric transformation built into the model. We use the same notation as for the explicit variants.  <ref type="formula" target="#formula_0">(12)</ref> Compared to the other variants, this sequence is roughly 3 2 times longer, resulting in twice the computational costs.</p><p>(V) Therefore, we also include a impl.-depth variant, which concatenates the discrete codes of depth and source image, and maps them with a matrix W ∈ R de×2·dz to the embedding space to avoid an increase in sequence length: <ref type="bibr" target="#b12">(13)</ref> (VI) Implicit approaches offer an intriguing possibility: Because they do not need an explicit estimate of the depth to perform the warping operation S, they hold the potential to solve the task without such a depth estimate. Thus, impl.nodepth uses only camera parameters and source imagethe bare minimum according to our task description.</p><formula xml:id="formula_10">f (x src , T ) = [W camT , W [E d (d), E(x src )]] + e pos</formula><p>f (x src , T ) = [W camT , e(E(x src ))] + e pos <ref type="bibr" target="#b13">(14)</ref> (VII) Finally, we analyze if explicit and implicit approaches offer complementary strengths. Thus, we add a hybrid variant whose conditioning function is the sum of the f 's of expl.-emb in Eq. (11) and impl.-depth in Eq. (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Readout</head><p>To investigate the ability to learn an implicit model of the geometric relationship between different views, we propose to extract an explicit estimate of depth from a trained model. To do so, we use linear probing <ref type="bibr" target="#b5">[6]</ref>, which is commonly used to investigate the feature quality of unsupervised approaches. More specifically, we assume a transformer model consisting of L layers and of type impl.nodepth, which is conditioned on source frame and transformation parameters only. Next, we specify a certain layer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>First, Sec. 4.1 integrates the different explicit and implicit inductive biases into the transformer to judge if such geometric biases are needed at all. Following up, Sec. 4.2 compares implicit variants to previous work and evaluates both the visual quality and fidelity of synthesized novel views. Finally, we evaluate the ability of the least biased variant, impl.-nodepth, to implicitly represent scene geometry, observing that they indeed capture such 3D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparing Implicit and Explicit Transformers</head><p>To investigates if transformers need (or benefit from) an explicit warping between source and target view we first compare how well the different variants from Sec. 3.2 (see also <ref type="figure">Fig. 2)</ref> can learn a probabilistic model for novel view synthesis. Afterwards, we directly evaluate both the quality and fidelity of samples obtained from these models.</p><p>To prepare, we first train VQGANs on frames of the RealEstate10K <ref type="bibr" target="#b77">[77]</ref> and ACID <ref type="bibr" target="#b37">[37]</ref> datasets, whose preparation is described in the supplementary. We then train the various transformer variants on the latent space of the respective first stage models. Note that this procedure ensures comparability of different settings within a given dataset, as the space in which the likelihood is measured remains fixed. Comparing Density Estimation Quality A basic measure for the performance of probabilistic models is the likelihood assigned to validation data. Hence, we begin our evaluation of the different variants by comparing their (minimal) negative log-likelihood (NLL) on RealEstate and ACID. Based on the results in Tab. 1, we can identify three groups with significant performance differences on ACID: The implicit variants impl.  <ref type="figure">Fig. 4</ref> for a visualization of variant impl.nodepth. The model is more confident in its predictions for regions which are visible in the source image. This indicates that it is indeed able to relate source and target via their geometry instead of simply predicting an arbitrary novel view. Measuring Image Quality and Fidelity Because NLL does not necessarily reflect the visual quality of the images <ref type="bibr" target="#b65">[65]</ref>, we evaluate the latter also directly. Comparing predictions for novel views with ground-truth helps to judge the faithfulness with which the model transforms the source view into the novel view as specified by the camera. However, since we are especially interested in the case of large camera movements where large parts of the target image have not been observed in the source view, we must also evaluate the quality of the content imagined by the model. Note that a sample from the model might be fairly different from the content in the ground-truth, since the latter is just one of many possible realizations of the real-world.</p><p>x src</p><p>x dst T 's entropy  To evaluate the image quality without a direct comparison to the ground-truth, we report FID scores <ref type="bibr" target="#b27">[28]</ref>. To evaluate the fidelity to the ground-truth, we report the low-level similarity metrics SSIM <ref type="bibr" target="#b70">[70]</ref> and PSNR, and the high-level similarity metric PSIM <ref type="bibr" target="#b75">[75]</ref>, which has been shown to better represent human assessments of visual similarity. Tab. 1 contains the results for RealEstate10K and ACID. In general, these results reflect the findings from the NLL values: Image quality and fidelity of implicit variants with access to depth are superior to explicit variants. The implicit variant without depth (impl.-nodepth) consistently achieves the same good FID scores as the implicit variants with depth (impl.-catdepth &amp; impl.-depth), but cannot achieve quite the same level of performance in terms of reconstruction fidelity. However, it is on par with the explicit variants, albeit requiring no depth supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to Previous Approaches</head><p>Next, we compare our best performing variants impl.-depth and impl.-nodepth to previous approaches for novel view synthesis: 3DPhoto <ref type="bibr" target="#b59">[59]</ref>, SynSin <ref type="bibr" target="#b71">[71]</ref> and InfNat <ref type="bibr" target="#b37">[37]</ref>. 3DPhoto <ref type="bibr" target="#b59">[59]</ref> has been trained on MSCOCO <ref type="bibr" target="#b35">[36]</ref> to work on arbitrary scenes, whereas SynSin <ref type="bibr" target="#b71">[71]</ref> and InfNat <ref type="bibr" target="#b37">[37]</ref> have been trained on RealEstate and ACID, respectively.</p><p>To assess the effect of formulating the problem probabilistically, we introduce another baseline to compare probabilistic and deterministic models with otherwise equal architectures. Specifically, we use the same VQGAN encoder and decoder architectures as used in the first stage described in Sec. 3.1. However, they are not trained as an autoencoder, but instead the encoder receives the warped source image x wrp , and the decoder predicts the target image x dst . This model, denote by expl.-det, represents an explicit and deterministic baseline. Finally, we include the warped source image itself as a baseline denoted by MiDaS <ref type="bibr" target="#b51">[51]</ref>.</p><p>Utilizing the probabilistic nature of our model, we analyze how close we can get to a particular target image with a fixed amount of samples. Tab. 2 and 3 report the reconstruction metrics with 32 samples per validation example of RealEstate and ACID. The probabilistic variants consistently achieve the best values for the similarity metrics PSIM, SSIM and PSNR on RealEstate, and are always among the best three values on ACID, where expl.det achieves the best PSIM values and the second best PSNR values after impl.-depth. We also show the reconstruction metrics on RealEstate as a function of the number of samples in <ref type="figure" target="#fig_1">Fig. 3</ref>. We observe that already with four samples, the performance of impl.-depth is better than all other approaches except for the SSIM values of 3DPhoto <ref type="bibr" target="#b59">[59]</ref>, which are overtaken by impl.-depth with 16 samples, and does not saturate even when using 32 samples, which demonstrates the advantages of a probabilistic formulation of novel view synthesis. These results should be considered along with the competitive FID scores in Tab. 2 and 3 (where the implicit variants always constitute the best and second best value) and the qualitative results in <ref type="figure">Fig. 5 and 6</ref>, underlining the high quality of our synthesized views. Furthermore, it is striking that IS assign the best scores to 3DPhoto <ref type="bibr" target="#b59">[59]</ref> and Mi-DaS <ref type="bibr" target="#b51">[51]</ref>, the two variants which contain large and plain regions of gray color in regions where the source image does not provide information about the content. In cases where the monocular depth estimation is accurate, 3DPhoto <ref type="bibr" target="#b59">[59]</ref> shows good results but it can only inpaint small areas. SynSin <ref type="bibr" target="#b71">[71]</ref> and InfNat <ref type="bibr" target="#b37">[37]</ref> can fill larger areas but, for large camera motions, their results quickly become blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target 3DPhoto <ref type="bibr" target="#b59">[59]</ref> InfNat <ref type="bibr" target="#b37">[37]</ref> expl.-det impl.-depth impl.-nodepth <ref type="figure">Figure 6</ref>. Qualitative Results on ACID: The outdoor setting of the ACID dataset yields similar results as the indoor setting in <ref type="figure">Fig. 5</ref>.</p><p>Here, we evaluate against the baselines 3DPhoto <ref type="bibr" target="#b59">[59]</ref>, InfNat <ref type="bibr" target="#b37">[37]</ref> and expl.-det. For InfNat <ref type="bibr" target="#b37">[37]</ref>, we use 5 steps to synthesize a novel view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Probing for Geometry</head><p>Based on the experiments in Sec. 4.1 and Sec. 4.2, which showed that the unbiased variant impl.-nodepth is mostly on-par with the others, we investigate the question whether this model is able to develop an implicit 3D "understanding" without explicit 3D supervision. To do so, we perform linear probing experiments as described in Sec. 3.3. <ref type="figure" target="#fig_4">Fig. 7</ref> plots the negative cross-entropy loss and the negative PSIM reconstruction error of the recovered depth maps against the layer depth of the transformer model. Both metrics are consistent and quickly increase when probing deeper representations of the transformer model. Furthermore, both curves exhibit a peak for l = 4 (i.e. after the third self-attention block) and then slowly decrease with increasing layer depth. The depth maps obtained from this linear map resemble the corresponding true depth maps qualitatively well as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. This figure demonstrates that a linear estimate of depth only becomes possible through  the representation learned by the transformer (l = 4) but not by the representation of the VQGAN encoder (l = 0). We hypothesize that, in order to map an input view onto a target view, the transformer indeed develops an implicit 3D representation of the scene to solve its training task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a probabilistic approach based on transformers for novel view synthesis from a single source image with strong changes in viewpoint. Comparing various explicit and implicit 3D inductive biases for the transformer showed that explicitly using a 3D transformation in the architecture does not help their performance. Moreover, even with no depth information as input the model learns to infer depth within its internal representations. Both of our implicit transformer approaches showed significant improvements over the state of the art in visual quality and fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry-Free View Synthesis</head><p>Transformers and no 3D Priors -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary, we provide additional results obtained with our models in Sec. A. Sec. B summarizes models, architectures and hyperparameters that were used in the main paper. After describing details on the training and test data in Sec. C, Sec. D concludes the supplementary material with details on the visualization of the transformer's uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head><p>Interactive Scene Exploration Interface <ref type="figure" target="#fig_6">Fig. 9</ref> shows a preview of the videos available at https://git.io/JOnwn, which demonstrate an interface for interactive 3D exploration of images. Starting from a single image, a user can use keyboard and mouse to move the camera freely in 3D. To provide orientation, we warp the starting image to the current view using a monocular depth estimate (corresponding to the MiDaS <ref type="bibr" target="#b51">[51]</ref> baseline in Sec. 4.2). This enables a positioning of the camera with real-time preview of the novel view. Once a desired camera position has been reached, the spacebar can be pressed to autoregressively sample a novel view with our transformer model. In the videos available at https://git.io/JOnwn, we use camera trajectories from the test sets of RealEstate10K and ACID, respectively. The samples are produced by our impl.-depth model, and for an additional visual comparison, we also include results obtained with the same methods that we compared to in Sec. 4.2. Additional Qualitative Results For convenience, we also include additional qualitative results directly in this supplementary. <ref type="figure" target="#fig_1">Fig. 11 and 13</ref> show additional qualitative comparisons on RealEstate10K and ACID, as in <ref type="figure">Fig. 5</ref> and 6 of the main paper. <ref type="figure">Fig. 12 and 14</ref> demonstrate the diversity and consistency of samples by showing them along with their pixel-wise standard deviation. <ref type="figure">Fig. 15</ref> contains results from the depth-probing experiment of Sec. 4.3, and <ref type="figure" target="#fig_10">Fig. 16</ref> from the entropy visualization of Sec. 4.1. <ref type="figure">Fig. 10</ref> reports the negative log-likelihood (NLL) over the course of training on RealEstate and ACID, respectively. The models overfit to the training split of ACID early which makes training on ACID much quicker and thus allows us to perform multiple training runs of each variant with different initializations. This enables an estimate of the significance of the results by computing the mean and standard deviation over three runs (solid line and shaded area in <ref type="figure">Fig. 10, respectively)</ref>.   <ref type="figure">Figure 10</ref>. Negative log-likelihood over the course of training on RealEstate10K (left) and ACID (right). Implicit variants achieve the best results, see Sec. 4.1 for a discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Variants Over the Course of Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architectures &amp; Hyperparameters</head><p>Transformer The architecture of all transformer models discussed in this work follows the GPT-2 architecture <ref type="bibr" target="#b49">[49]</ref>. More specifically, the transformer consists of L transformer blocks, where each block performs the following operation on an input sequence z ∈ R |z|×de (with |z| the length of z):</p><formula xml:id="formula_11">z 1 = LayerNorm(z) (15) z 2 = MultiHeadSelfAttention(z 1 ) + z (16) z 3 = LayerNorm(z 2 ) (17) z 4 = MLP(z 3 ) + z 2<label>(18)</label></formula><p>In contrast to the global attention operation, the MLP is applied position-wise. Given an input sequence s and an embedding produced by the conditioning function f (x src , T ) ∈ R n×de (see Sec. 3.2), the transformer maps s to a learnable embedding e(s) + e pos =:ê 0 ∈ R |s|×de , applies the L transformer blocks on the concatenated sequence [f (x src , T ),ê 0 ] and finally projects to |Z| logits π L via a linear transformation W head , which correspond to a categorical distribution over sequence elements, i.e. </p><p>e l = TransformerBlock(e l−1 ), l = 1 . . . L (20)</p><formula xml:id="formula_13">π L = W head · LayerNorm(e L ).<label>(21)</label></formula><p>Note that non-conditioning elements, i.e. the last |s| elements, are masked autoregressively <ref type="bibr" target="#b69">[69]</ref>. For all experiments, we use an embedding dimensionality d e = 1024, L = 32 transformer blocks, 16 attention heads, two-layer MLPs with hidden dimensionalities of 4 · d e and a codebook of size |Z| = 16384. This setting results in a transformer with 437M parameters. We train the model using the AdamW <ref type="bibr" target="#b40">[40]</ref> optimizer (with β 1 = 0.9, β 2 = 0.95) and apply weight decay of 0.01 on nonembedding parameters. We train for 500k steps, where we first linearly increase the learning rate from 2.5 · 10 −6 to 1.5 · 10 −4 during the first 5k steps, and then apply a cosine-decay learning rate schedule <ref type="bibr" target="#b39">[39]</ref> towards zero.</p><p>VQGAN The architecture and training procedure of the VQGANs is adopted from <ref type="bibr" target="#b14">[15]</ref>, where we use a downsampling factor of f = 2 4 . For the codebook Z, we use |Z| = 16384 entries and a dimensionality of d z = 256. This means that any input x ∈ R H×W ×C will be mapped to a latent representation of size E(x) ∈ R H/2 4 ×W/2 4 ×256 . For our experiments on RealEstate and ACID, where H = 208 and W = 368, this corresponds to a latent code of size 13 × 23 (which is then unrolled to a sequence of length |s| = 299). We use the authors' official implementation and pretrained models 2 and perform finetuning on frames of RealEstate and ACID for 50'000 steps on either dataset, resulting in two dataset-specific VQGANs.</p><p>Other models For monocular depth estimates, we use MiDaS v2.1 <ref type="bibr" target="#b2">3</ref> . We use the official implementations and pretrained models for the comparison with 3DPhoto [59] 4 , SynSin <ref type="bibr" target="#b71">[71]</ref>  <ref type="bibr" target="#b4">5</ref> and InfNat [37] 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training and Testing Data</head><p>Training our conditional generative model requires examples consisting of (x dst , x src , T ). Such training pairs can be obtained via SfM <ref type="bibr" target="#b77">[77]</ref> applied to image sequences, which provides poses (R i , t i ) for each frame x i with respect to an arbitrary world coordinate system. For two frames x src , x dst from the sequence, the relative transformation is then given by R = R dst R −1 src and t = t dst − Rt src . However, the scale of the camera translations obtained by SfM is also arbitrary, and without access to the full sequence, underspecified. To train the model and to meaningfully compute reconstruction errors for the evaluation, we must resolve this ambiguity. To do this, we also triangulate a sparse set of points for each sequence using COLMAP <ref type="bibr" target="#b56">[56]</ref>. We then compute a monocular depth estimate for each image using MiDaS <ref type="bibr" target="#b51">[51]</ref> and compute the optimal affine scaling to align this depth estimate with the scale of the camera pose. Finally, we normalize depth and camera translation by the minimum depth estimate. All qualitative and quantitative results are obtained on a subset of the test splits of RealEstate10K <ref type="bibr" target="#b77">[77]</ref> and ACID <ref type="bibr" target="#b37">[37]</ref>, consisting of 564 source-target pairs, which have been selected to contain medium-forward, large-forward, medium-backward and large-backward camera motions in equal parts. We will make this split publicly available along with our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Details on Entropy Evaluation</head><p>As discussed in Sec. 4.1, the relationship between a source view x src and a target view x dst can be quantified via the entropy of the probability distribution that the transformer assigns to a target view x dst , given a source frame x src , camera transformation T and conditioning function f . More specifically, we first encode target, camera and source via the encoder E and the conditioning function f (see Sec. 3.1), i.e. s dst = E(x dst ) and f (x src , T ). Next, for each element in the sequence s dst , the (trained) transformer assigns a probability conditioned on the source and camera:</p><formula xml:id="formula_14">p s dst i |s dst &lt;i , f (x src , T ) , 0 ≤ i &lt; |s dst |<label>(22)</label></formula><p>where for our experiments the length of the target sequence is always |s dst | = 13 · 23 = 299, see also Sec. B. The entropy H(s dst i , x src ) for each position i is then computed as H(s dst i , x src )) = − |Z| k p k (s dst i |s dst &lt;i , f (x src , T )) log p k (s dst i |s dst &lt;i , f (x src , T ))</p><p>Reshaping to the latent dimensionality h × w and bicubic upsampling to the input's size H × W then produces the visualizations of transformer entropy as in <ref type="figure">Fig. 4</ref> and <ref type="figure" target="#fig_10">Fig. 16</ref>. Note that this approach quantifies the transformers uncertainty/surprise from a single example only and does not need to be evaluated on multiple examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target 3DPhoto <ref type="bibr" target="#b59">[59]</ref> SynSin <ref type="bibr" target="#b71">[71]</ref> expl  <ref type="figure">Figure 15</ref>. Additional results on linearly probed depth maps for different transformer layers as in <ref type="figure" target="#fig_5">Fig. 8</ref>. See Sec. 4.3.</p><p>x src x dst T 's entropy x src x dst T 's entropy </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(IV) The first variant, impl.-catdepth, provides the transformer with all the same components which are used in the explicit variants: Camera parameters K, R, t, estimated depth d and source image x src . Camera parameters are flattened and concatenated toT , which is mapped via W cam ∈ R de×1 to the embedding space. Depth and source images are encoded by VQGAN encoders E d and E to obtain f (x src , T ) = [W camT , e(E d (d)), e(E(x src ))] + e pos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Average reconstruction error of the best sample as a function of the number of samples on RealEstate. With just four samples, impl.-depth reaches state-of-the-art performance in two out of three metrics, and with 16 samples in all three of them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-catdepth, impl.-depth, and impl.-nodepth and hybrid achieve the best performance, which indicates an advantage over the purely explicit variants. Adding an explicit warping as in the hybrid model does not help significantly. Moreover, expl.-feat is unfavorable, possibly due to the features E(x src ) remaining fixed while training the transformer. The learnable features which are warped in variant expl.-emb obtain a lower NLL and thereby confirm the former hypothesis. Still there are no improvements of warped features over warped pixels as in variant expl.-img. The results on RealEstate look similar but in this case the implicit variant without depth, impl.-nodepth, performs a bit worse than expl.-img. Presumably, accurate depth information obtained from a supervised, monocular depth estimation model are much more beneficial in the indoor setting of RealEstate compared to the outdoor setting of ACID. Visualizing Entropy of Predictions The NLL measures the ability of the transformer to predict target views. The entropy of the predicted distribution over the codebook entries for each position captures the prediction uncertainty of the model. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Visualization of the entropy of the predicted target code distribution for impl.-nodepth. Increased confidence (darker colors) in regions which are visible in the source image indicate its ability to relate source and target geometrically, without 3D bias. Qualitative Results on RealEstate10K: We compare three deterministic convolutional baselines (3DPhoto<ref type="bibr" target="#b59">[59]</ref>, SynSin<ref type="bibr" target="#b71">[71]</ref>, expl.-det) to our implicit variants impl.-depth and impl.-nodepth. Ours is able to synthesize plausible novel views, whereas others produce artifacts or blurred, uniform areas. The depicted target is only one of many possible realizations; we visualize samples in the supplement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Minimal validation loss and reconstruction quality of depth predictions obtained from linear probing as a function of different transformer layers. The probed variant is impl.-nodepth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Linearly probed depth maps for different transformer layer. The results mirror the curve inFig. 7: After a strong initial increase, the quality for layer 4 is best. The depth reconstructions in the right column provide an upper bound on achievable quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Preview of the videos available at https://git.io/JOnwn, which demonstrate an interface for interactive 3D exploration of images. Starting from a single image, it allows users to freely move around in 3D. See also Sec. A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>e 0 = [f (x src , T ), e(s) + e pos ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .Figure 12 .Figure 13 .Figure 14 .</head><label>11121314</label><figDesc>Additional qualitative comparisons on RealEstate10K. Source σ samples of variant impl.-depth Additional samples on RealEstate10K. The second column depicts the pixel-wise standard deviation σ obtained from n = 32 samples. Additional qualitative comparisons on ACID. Source σ samples of variant impl.-depth Additional samples on ACID. The second column depicts the pixel-wise standard deviation σ obtained from n = 32 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 .</head><label>16</label><figDesc>Additional visualizations of the entropy of the predicted target code distribution for impl.-nodepth. Increased confidence (darker colors) in regions which are visible in the source image indicate its ability to relate source and target geometrically, without 3D bias. See also Sec. 4.1 and Sec. D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>nodepth 48.59 4.956 3.17 ±0.43 0.42 ±0.13 12.16 ±2.54 42.88 5.365 ±0.007 2.90 ±0.53 0.40 ±0.15 15.17 ±3.40 hybrid 48.84 4.913 3.09 ±0.46 0.44 ±0.13 12.51 ±2.69 44.47 5.341 ±0.008 2.83 ±0.54 0.41 ±0.15 15.54 ±3.52 impl.-depth 49.15 4.836 3.05 ±0.46 0.44 ±0.13 12.66 ±2.68 42.93 5.353 ±0.011 2.86 ±0.52 0.41 ±0.15 15.53 ±3.34 expl.-img 49.63 4.924 3.18 ±0.46 0.43 ±0.13 12.11 ±2.66 47.72 5.414 ±0.006 3.00 ±0.51 0.40 ±0.14 14.83 ±3.20 impl.-catdepth 50.04 4.860 3.10 ±0.45 0.43 ±0.13 12.43 ±2.66 47.44 5.350 ±0.004 2.86 ±0.55 0.42 ±0.15 15.54 ±3.57 expl.-emb 50.35 5.004 3.15 ±0.45 0.43 ±0.13 12.30 ±2.66 47.08 5.416 ±0.007 2.88 ±0.54 0.42 ±0.15 15.45 ±3.61 expl.-feat 54.82 5.159 3.31 ±0.43 0.41 ±0.13 11.75 ±2.58 52.65 5.657 ±0.003 3.14 ±0.52 0.38 ±0.15 14.06 ±3.28</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison on RealEstate. Reconstruction metrics are reported with 32 samples, see Fig. 3 for other values. Our implicit variants outperform previous approach in all metrics except for IS, with drastic improvements for FID. impl.-nodepth 48.59 4.24 ±0.30 2.95 ±0.43 0.49 ±0.12 14.06 ±2.41 impl.-depth 49.15 4.17 ±0.52 2.86 ±0.45 0.50 ±0.12 14.47 ±2.51 expl.-det 66.66 4.47 ±0.49 2.97 ±0.55 0.42 ±0.15 13.60 ±2.56 3DPhoto [59] 85.43 5.10 ±0.39 3.20 ±0.54 0.49 ±0.12 12.80 ±2.33 SynSin [71] 113.88 3.70 ±0.30 3.30 ±0.51 0.47 ±0.13 12.87 ±2.46 MiDaS [51] 132.13 5.63 ±0.77 3.38 ±0.56 0.46 ±0.15 13.09 ±2.16</figDesc><table><row><cell>method</cell><cell>FID ↓</cell><cell>IS ↑</cell><cell>PSIM ↓</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison on ACID using 32 samples for reconstruction metrics. We indicate number of steps used for Inf-Nat<ref type="bibr" target="#b37">[37]</ref> in parentheses. Our impl.-depth approach outperforms previous works in all metrics except for IS. ±0.<ref type="bibr" target="#b13">14</ref> 2.77 ±0.54 0.46 ±0.14 16.49 ±3.33 impl.-depth 42.93 2.62 ±0.23 2.73 ±0.53 0.46 ±0.14 16.80 ±3.24 expl.-det 53.77 2.60 ±0.18 2.72 ±0.56 0.41 ±0.16 16.60 ±3.43 InfNat [37](5) 76.07 2.44 ±0.21 3.28 ±0.47 0.39 ±0.15 15.24 ±2.87 3DPhoto [59] 76.17 3.50 ±0.47 3.01 ±0.64 0.45 ±0.14 14.87 ±3.08 InfNat [37](1) 79.00 2.71 ±0.23 3.11 ±0.58 0.42 ±0.15 15.35 ±3.50 InfNat [37](10) 88.81 2.52 ±0.20 3.44 ±0.41 0.35 ±0.14 14.32 ±2.55 MiDaS [51] 106.10 3.62 ±0.36 3.11 ±0.68 0.45 ±0.15 14.82 ±2.85 A similar observation holds for expl.-det, except that it replaces blurriness with repetitive patterns. Both of the two probabilistic variants impl.-depth and impl.-nodepth consistently produce plausbile results which are largely consistent with the source image, although small details sometimes differ from the source image. Overall, the results show that only the probabilistic variants are able to synthesize high quality images for large camera changes.</figDesc><table><row><cell>method</cell><cell>FID ↓</cell><cell>IS ↑</cell><cell>PSIM ↓</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell></row><row><cell>impl.-nodepth</cell><cell>42.88 2.63</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This includes the vector quantization step as described in<ref type="bibr" target="#b68">[68]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">≤ l ≤ L (where l = 0 denotes the input) and extract its latent representation e l , corresponding to the positions of the provided source frame x src . We then train a position-wise linear classifier W to predict the discrete, latent representation of the depth-encoder E d (see Sec. 3.2) via a crossentropy objective from e l . Note that both the weights of the transformer and the VQGANs remain fixed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">see https://github.com/CompVis/taming-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">see https://github.com/intel-isl/MiDaS 4 see https://github.com/vt-vl-lab/3d-photo-inpainting 5 see https://github.com/facebookresearch/synsin/ 6 see https://github.com/google-research/google-research/tree/master/infinite_nature</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural point-based graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kara-Ali</forename><surname>Aliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sevastopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Kolos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (22)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12367</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unstructured lumigraph rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extreme view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><forename type="middle">J</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7780" to="7789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling and rendering architecture from photographs: A hybrid geometryand image-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Peeking behind objects: Layered depth prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Jukebox: A generative model for music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2005.00341, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Musings on typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/2012.09841</idno>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepview: View synthesis with learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Broxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Duvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">S</forename><surname>Overbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno>abs/1506.06825</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Vijay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3827" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Casual 3d photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhib</forename><surname>Alsisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<idno>234:1- 234:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Instant 3d photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<idno>101:1-101:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep blending for freeviewpoint image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">True</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno>257:1-257:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scalable inside-out image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno>231:1-231:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tour into the picture: using a spidery mesh interface to make animation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youichi</forename><surname>Horry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Anjyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoshi</forename><surname>Arai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning-based view synthesis for light field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Nima Khademi Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
		<idno>193:1-193:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">One shot 3d photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ocean</forename><surname>Suhib Alsisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">F</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crowdsampling the plenoptic function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="178" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Infinite nature: Perpetual view generation of natural scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno>abs/2012.09855</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometry-aware deep network for single-image novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4616" to="4624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Meshry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="6878" to="6887" />
		</imprint>
	</monogr>
	<note>Neural rerendering in the wild</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5436" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d ken burns effect from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<idno>184:1- 184:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="702" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Soft 3d reconstruction for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Penner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno>235:1-235:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2102.12092, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
	<note>Aäron van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Free view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (19)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">12364</biblScope>
			<biblScope unit="page" from="623" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Stable view synthesis. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multiview stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Layered depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li-Wei He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">3d photography using context-aware layered depth inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Li</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of view extrapolation with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to synthesize a 4d RGBD light field from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Sreelal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2262" to="2270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-view to novel view: Synthesizing novel views with self-learned confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Shao-Hua Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hong</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="162" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Stereo matching with transparency and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="61" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno>66:1-66:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Single-view view synthesis with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Synsin: End-to-end view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep view synthesis from sparse photometric images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno>76:1-76:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentangling with recurrent transformations for 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1099" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6612" to="6619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Stereo magnification: learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno>65:1-65:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (4)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
