<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Framework For Contrastive Self-Supervised Learning And Designing A New Approach Technical Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Falcon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>NY Lightning Labs</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Framework For Contrastive Self-Supervised Learning And Designing A New Approach Technical Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive self-supervised learning (CSL) is an approach to learn useful representations by solving a pretext task which selects and compares anchor, negative and positive (APN) features from an unlabeled dataset. We present a conceptual framework which characterizes CSL approaches in five aspects (1) data augmentation pipeline, (2) encoder selection, (3) representation extraction, (4) similarity measure, and (5) loss function. We analyze three leading CSL approaches-AMDIM, CPC and SimCLR-, and show that despite different motivations, they are special cases under this framework. We show the utility of our framework by designing Yet Another DIM (YADIM) which achieves competitive results on CIFAR-10, STL-10 and ImageNet, and is more robust to the choice of encoder and the representation extraction strategy. To support ongoing CSL research, we release the PyTorch implementation of this conceptual framework along with standardized implementations of AMDIM, CPC (V2), SimCLR, BYOL, Moco (V2) and YADIM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A goal of self-supervised learning is to learn to extract representations of an input using a large amount of unlabelled data. This representation is used to solve downstream tasks which often have only a few labelled instances. Self-supervised learning achieves this goal by solving a pretext task which creates different types of supervision signals from unlabelled data based on careful inspection of underlying regularities in the data. In computer vision, there has been a stream of novel pretext tasks proposed over the past few years, including colorization <ref type="bibr" target="#b39">[40]</ref>, patch relatedness <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>, transformation prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>, in-painting <ref type="bibr" target="#b34">[35]</ref> and self-supervised jigsaw puzzle <ref type="bibr" target="#b31">[32]</ref>. Recently, in computer vision, contrastive methods have achieved state-of-the-art results on ImageNet <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16]</ref>. In natural language processing, BERT <ref type="bibr" target="#b9">[10]</ref> has become the de facto standard in low-resource text classification <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b0">1</ref> BERT is trained to predict (artificially) missing words given surrounding context as a pretext task.</p><p>To understand the differences between the CSL approaches in computer vision, we formulate a framework which characterizes CSL in five parts; (1) an encoder, (2) data augmentation pipeline, (3) a representation extraction, (4) a similarity measure and (5) a loss function. We use our framework to analyze three leading self-supervised learning approaches from which recent CSL approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16]</ref> build on; augmented multi-scale deep information maximization (AMDIM; <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>), contrastive predictive coding (CPC; <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>) and a simple framework for contrastive learning of visual representations (SimCLR; <ref type="bibr" target="#b5">[6]</ref>). AMDIM was designed to maximize the mutual information (MI) between representations obtained from a single image while minimizing the MI between representations obtained from two separate images. CPC, on the other hand, extracts representations which can tell whether they come from the same or a different image. SimCLR, using the same ideas of AMDIM, maximizes the similarity between representations obtained from a single image, while minimizing the similarity between representations obtained from other images. Our analysis finds that despite different motivations behind these approaches, existing CSL algorithms are only slightly different from one another. We demonstrate the usefulness of the proposed framework by formulating a new CSL variant that merges the data processing pipelines of AMDIM and CPC. Our resulting approach, YADIM, produces comparable results on downstream tasks including CIFAR-10, STL-10, and ImageNet, while improving the robustness to the choice of the encoder and sampling task. These results suggest that this framework is capable of generating CSL variant that perform just as well as the three leading approaches; AMDIM, CPC and SimCLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contrastive self-supervised learning</head><p>Contrastive self-supervised learning (CSL) has been recently found successful for semi-supervised learning (SSL) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16]</ref>. In CSL, the goal is to generate representations of instances such that similar instances are near each other and far from dissimilar ones. In supervised learning, associative labels determine the similarities among instances. Without labels however, we must develop methods to exploit similarities implicitly embedded within instances. CSL does this by generating anchor, positive, and negative samples from an unlabeled dataset.</p><p>Let D = {x 1 , x 2 , ..., x N } be an unlabeled dataset of size N . CSL builds on the assumption that each instance defines and belongs to its own class <ref type="bibr" target="#b11">[12]</ref>. This assumption implies that we have N classes. To create samples that belong to the same class we generate two features (v a , v + ) from the same example x ∈ D. We refer to v a as an anchor feature and v + as a positive feature. To create an example from a different class, we generate a feature v − from a different example x . We call v − a negative feature. Depending on the task, such a feature can be a vector v ∈ R n or a multi-dimensional tensor v ∈ R n×...×m . We propose the following five-part framework which lets us easily characterize existing CSL approaches.</p><p>(1) Data Augmentation Pipeline The goal of the data augmentation pipeline is to generate anchor, positive and negative (APN) features to be used in contrastive learning. Let a n define a stochastic input augmentation process such as random flip and random channel drop. Then, A = (a 1 , . . . , a N ) defines a pipeline that applies these augmentations sequentially. We can apply A to x to generate a new sample v i which preserves the same underlying semantics as x. This strategy gives us a way to generate multiple samples of the same class defined by the example x which we can use as a supervisory signal.</p><p>To generate the anchor and positive features we can take many approaches. One way to generate v a and v + is to sample two subsets of vectors from the same feature, v a , v + ⊆ v x . A second way is to apply A to the same input twice, v a ∼ A(x), v + ∼ A(x), which produces two distinct sets of features due to the stochastic nature of A. The negative feature, v − ∼ A(x ) is sampled via the same process but taken from a different sample x .</p><p>(2) Encoder Let f θ define an encoder parameterized with θ. This encoder can be any function approximator such as a fully-connected or convolutional neural network (CNN) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>. The encoder maps an input v into a set of vectors r which we call the representation of x. When x is an image with s input channels, width w, and height h, then f performs the following mapping f θ : R s×w×h → R k×c . In other words the encoder returns k c-dimensional feature vectors as the representation of the input. When the encoder is a convolutional network, r is a set of vectors from a feature map m where m ∈ R s×w×h .</p><p>(3) Representation extraction For contrastive learning we need to extract representations that can be compared against one another. Let r + = f θ (v + ) be the positive representation, r a = f θ (v a ) the anchor representation and r − = f θ (v − ) the negative representation. Representations are extracted from an encoder or a sequence of encoders applied to v · . There are many ways to perform the representation extraction; one way is to generate a single d-dimensional vector as the final output of the encoder for each representation r · ∈ R d . Another way is to output a matrix for each representation r · ∈ R n×k and compare a subset of r a against another subset of r − to generate multiple negative scores.</p><p>(4) Similarity measure Let Φ(r a , r b ) measure the similarity between two representations, r a and r b . This function outputs a scalar score s which measures the similarity between r a and r b . Examples of similarity measures are the dot product, cosine similarity, or bi-linear transformations such as s = r a · W r b , in which case Φ has its own parameter W .</p><p>(5) Loss Function Refer to s + = Φ(r a , r + ) as the positive score and to s − = Φ(r a , r − ) as the negative score. We define a loss function as the combination of the positive and negative scores to reflect the progress of learning. Minimizing this loss function corresponds to maximizing the positive score and minimizing the negative scores.</p><p>Widely used loss functions include the negative contrastive estimation (NCE) loss <ref type="bibr" target="#b29">[30]</ref>, the triplet loss <ref type="bibr" target="#b36">[37]</ref> and InfoNCE <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Special Case 1: Augmented Multiscale Deep InfoMax (AMDIM)</head><p>The first example of CSL that we describe under the proposed framework is AMDIM by Bachman et al. <ref type="bibr" target="#b2">[3]</ref>, which was recently proposed for self-supervised learning of image representations. The motivation behind AMDIM is to maximize mutual information between features extracted from intermediate layers of a CNN, generated from two views of the same image. AMDIM operates on a dataset of d-channel images with width w and height h. Let D = {x 1 , x 2 , ..., x n } be a collection of images x ∈ R w×h×d .</p><p>The implementation details of AMDIM can be found in Appendix A.</p><p>(1) Data Augmentation Pipeline Let x be an example from D, and a n a stochastic image augmentation stage. The augmentation pipeline for x in AMDIM consists of five stages; random flip, image jitter, color jitter, random gray scale and normalization of mean and standard deviation. We refer the readers to <ref type="bibr" target="#b2">[3]</ref> for the details of each. AMDIM generates the positive v + and anchor v a features by applying A twice to the same input x, v a ∼ A(x) and v + ∼ A(x). The negative v − feature is generated by applying A to a different input v − ∼ A(x ).</p><p>(2) Encoder The encoder, f θ , in AMDIM is based on a residual network (ResNet) <ref type="bibr" target="#b16">[17]</ref> with three design considerations. The first ensures that the encoder design minimizes information shared between two feature vectors from the same image. To achieve this, Bachman et al. minimize the overlap between receptive fields and do not use batch normalization <ref type="bibr" target="#b19">[20]</ref>. Second, the encoder does not use padding in order to avoid learning artifacts introduced by padding. Third, the number of channels in this modified ResNet is 5 times more than the number of channels found in ResNet-34 which is the most similar commonly-used architecture. Specifically, ResNet-34 has 64, 128, 256 and 512 feature maps, whereas the AMDIM ResNet has 320, 640, 1280 and 2560 feature maps, respectively. AMDIM was tested to work with this particular variant of a ResNet.</p><p>(3) Representation extraction AMDIM draws (r a , r + , r − ) triplets from feature maps extracted at different scales. Here we offer a brief summary of this task.</p><formula xml:id="formula_0">Let M = {m 1 , m 2 , ..., m l } = f θ (x)</formula><p>define the set of all feature maps generated by an encoder from each layer l when applied to an input</p><formula xml:id="formula_1">x. Let M a = f θ (v a ), M + = f θ (v + ) and M − = f θ (v − ).</formula><p>Let m j and m k be two feature maps at layers j and k. The anchor representation r a ∼ m a j is taken from m a j ⊂ M a . Multiple positive representations R + are used, where R = m + k are taken from the m + k ⊂ M + . To create the negative samples we use all the m − k generated from every other example. These samples define the set of negative representations,</p><formula xml:id="formula_2">R − = mi∈M x m − i .</formula><p>In other words, AMDIM requires an encoder that returns representations of the inputs at multiple scales.</p><p>(4) Similarity Measure AMDIM uses a dot product between two vector representations with c channels,</p><formula xml:id="formula_3">Φ(a, b) = a · b to measure their similarity, where a ∈ R c , b ∈ R c and Φ(a, b) ∈ R.</formula><p>This means that the dimensions of intermediate representations generated by the encoder must be the same.</p><p>(5) Loss Function AMDIM uses an NCE loss N θ (r a , R + , R − ) given a representations triplet <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_4">N θ (r a , R + , R − ) = − log r + i ∈R + exp(Φ(r a , r + i )) r − i ∈R − exp(Φ(r a , r − i )) .<label>(1)</label></formula><p>AMDIM minimizes the NCE loss above from the following combinations of the final three feature maps obtained from the encoder:</p><formula xml:id="formula_5">L AMDIM = − 1 3 N θ (r a j , R + k−1 , R − k−1 ) + N θ (r a j , R + k−2 , R − k−2 ) + N θ (r a j−1 , R + k−1 , R − k−1 ) ,<label>(2)</label></formula><p>requiring an encoder that outputs at least three feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Special Case 2: Contrastive Predictive Coding (CPC)</head><p>The second example of CSL we describe under the proposed framework is contrastive predictive coding (CPC) by Henaff et al <ref type="bibr" target="#b17">[18]</ref>. CPC learns to extract representation by making the feature vector of a patch from an image predictive of another patch spatially above the original patch. CPC differs from AMDIM in the encoder design, data augmentation pipeline, sampling task and loss function.</p><p>The implementation details can be found in Appendix B.</p><p>(1) Data Augmentation Pipeline The CPC data augmentation pipeline is the same as that of AMDIM with the following new transformation stage a appended at the end. This stage a breaks an input image x with width w, height h and channels d, into a list of p-many (q × q) overlapping patches with d channels each. This transformation maps x : R w×h×d → R p×q×q×d . For example, for a 256 × 256 image, patch size q = 64 with a 32-pixel overlap, x :</p><formula xml:id="formula_6">R 256×256×3 → R 7×7×64×64×3 .</formula><p>Define v ∼ A(x) as the set of features for image x, and v the set of features for image x .</p><p>(2) Encoder The encoder f θ in CPC is a modified version of the ResNet-101 <ref type="bibr" target="#b16">[17]</ref>. The ResNet-101 has four stacks of residual blocks. Each residual block has three convolutional layers of which the second one, the bottleneck layer, produces a fewer feature maps. CPC modifies the third stack in three ways; 1) they double the number of residual blocks from 23 to 46, 2) they double the bottleneck layer dimension from 256 to 512, and 3) they increase the number of feature maps from 1024 to 4096. CPC replaces all the batch normalization <ref type="bibr" target="#b19">[20]</ref> layers with layer normalization <ref type="bibr" target="#b1">[2]</ref> to minimize information sharing between two feature vectors from the same image. For example, the vector at row 0 is used to predict the vectors at all the rows 1, 2, ..., h. This prediction task first applies masked convolution to H via a context encoder g ψ to generate C = g ψ (H), where C ∈ R c×h×w . Each c i,j summarizes the context around every H i,j . For each c i,j , the target is then chosen from each row whose index is k &gt; i. PredictionĤ i+k,j = W k c i,j is finally generated using a prediction matrix W k .</p><p>This prediction r a =Ĥ i+k,j is the anchor representation, the target to predict r + = H i+k,j the positive representation, and all the other H\{h i,j } serve as the negative representations R − . This task can be viewed as correctly pickingĤ i+k,j from the set which has the real H i+k,j and distractors H i+k,j , H i+(k+1),j , . . . , H i+k+n,j .</p><p>(4) Similarity Measure CPC uses a dot product ofĤ i+k,j and H i+k,j .</p><p>(5) Loss Function Just like AMDIM, CPC uses the NCE loss:</p><formula xml:id="formula_7">L θ (r a , r + , R − ) = − log exp(Φ(r a , r + i )) exp(Φ(r a , r + i )) + r − i ∈R − exp(Φ(r a , r − i )) .<label>(3)</label></formula><p>This loss drives the anchor and positive samples together while driving the anchor and negative samples apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Special Case 3: A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)</head><p>The third example of CSL we describe under the proposed framework is a simple framework for contrastive learning of visual representations (SimCLR) by Chen et al <ref type="bibr" target="#b5">[6]</ref>. SimCLR extracts representation by maximizing the similarity between representations extracted from two views of the same image, just like AMDIM does. SimCLR is similar to AMDIM with a series of a few minor tweaks. First, it uses a non-customized, generic ResNet. Second, it uses a modified data augmentation pipeline. Third, it adds a parametrized similarity measure using a projection head. Finally, it adds a scaling coefficient (τ ) to the NCE loss.</p><p>The implementation details of SimCLR can be found in Appendix C.</p><p>(1) Data Augmentation Pipeline The augmentation pipeline of SimCLR follows the same ideas introduced by AMDIM. This pipeline applies a stochastic augmentation pipeline twice, to the same input v a ∼ A(x), v + ∼ A(x). The data augmentations consist of random resize and crop, random horizontal flip, color jitter, random gray scale and Gaussian Blur. We refer the reader to <ref type="bibr" target="#b5">[6]</ref> for the details of each.</p><p>(2) Encoder The encoder f θ in SimCLR is a ResNet of varying width and depth. The ResNets in SimCLR use batch normalization.</p><p>(3) Representation extraction Let r = f θ (v) be the output of an encoder, where r ∈ R c×h×w . We obtain vector by reshaping r :</p><formula xml:id="formula_8">R c×h×w → R c·h·w . The representation r a = f θ (v a ) is the anchor representation, r + = h + = f θ (v + )</formula><p>the positive representation and R − the set of negative representations generated from all the other samples x .</p><p>(4) Similarity Measure SimCLR uses a projection head z = f φ to map the representation vector from the encoder to another vector space, i.e., f φ : R c → R c . The cosine similarity between the (z i , z j ) pair is used as a similarity score. The composition of the projection and cosine similarity can be viewed as a parametrized similarity measure.</p><p>(5) Loss Function SimCLR uses the NCE loss with a temperature τ ∈ R to adjust the scaling of the similarity scores</p><formula xml:id="formula_9">L θ (z a , z + , Z − ) = − log exp(Φ(z a , z + )/τ ) z − i ∈Z − exp(Φ(z a , z − i )/τ ) .<label>(4)</label></formula><p>This loss drives the anchor and positive samples together while driving the anchor and negative samples apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">From CPC and AMDIM to YADIM</head><p>In this section we show the usefulness of our conceptual framework by creating Yet Another variant of DIM, called YADIM, which combines the ideas from both CPC and AMDIM. We first introduce the general design principles behind YADIM. For the components that differ between CPC and AMDIM, we perform ablations to determine which one to use. We end the section by summarizing the final YADIM design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">YADIM: Yet Another DIM</head><p>When viewed under our framework, CPC and AMDIM are more similar than expected from their different, underlying motivations. However, they do differ on the particular encoder design, data augmentation pipeline, representation extraction, and the similarity measure, while using the same loss. We formulate YADIM by empirically assessing the impact of each of these subtle design differences on the performance of both AMDIM and CPC. When any particular design choice does not have much impact on the performance, we choose the (simplified) union of both. We focus on CPC and AMDIM, as we found SimCLR to be a minor variant of AMDIM. Below, we present the general YADIM formulation.</p><p>(1) Data Augmentation Pipeline Recall the data augmentation pipeline of AMDIM which consists of five stages; random flip, image jitter, color jitter, random gray scale and z-normalization. CPC appends to this pipeline by AMDIM a transform which breaks an input image x with width w, height h and channels d, into a list of p-many (q × q) overlapping patches with d channels each. This transform maps x :</p><formula xml:id="formula_10">R w×h×d → R p×q×q×d</formula><p>The independent nature of each pipeline lends itself to a natural joint formulation that is the union of the CPC and AMDIM pipelines. The YADIM pipeline applies all six transforms to an input in sequence to generate two version of the same input, v a ∼ A(x) and v + ∼ A(x), and uses it to generate the negative sample from a different input v − ∼ A(x ).</p><p>(2) Encoder CPC and AMDIM both use customized CNN encoders. For YADIM we use the same encoder used by AMDIM.</p><p>(3) Representation extraction AMDIM compares the triplets of representations generated by the encoder at different spatial scales, while CPC uses a context encoder to predict a representation spatially lower in a feature map. These two tasks make specialized and unique domain assumptions which makes them difficult to merge. Instead, we test variations of the AMDIM sampling task. For YADIM, we eventually choose the task with the fewest assumptions while achieving the near-best performance.</p><p>(4) Similarity measure AMDIM uses a dot product φ(a, b) = a · b, while CPC uses a parametrized dot product between the representationĤ i+k,j = W k c i,j and the target representation H i+k,j . For YADIM, we resort to using dot product, because the encoder can subsume linear transformation in the parametrized measured used by CPC, if necessary.</p><p>(5) Loss Function YADIM uses the NCE loss which was used by both CPC and AMDIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Systematic investigation of CPC and AMDIM design differences</head><p>For the two main elements of CPC and AMDIM that differ, the encoder and representation extraction, we perform ablative experiments on each and investigate how each alternative decision affects CPC, AMDIM and YADIM.</p><p>Encoder Architecture CPC uses a modified ResNet-101 with wider layers, while AMDIM uses a modified ResNet-34. In this ablation, we replace the encoder in each approach with 9 standard ResNet architectures taken from the torchvision package. <ref type="bibr" target="#b1">2</ref> For the baseline YADIM, we use the wide  <ref type="table">Table 2</ref>: Robustness to the representation extraction approach on CIFAR-10: Here we show the results of using a simple representation extraction for each approach. This task is to use only the last feature map from the encoder for contrastive learning. The success of AMDIM depends heavily on the particular task, whereas YADIM does not. ResNet-34 which is the encoder from AMDIM. <ref type="table" target="#tab_0">Table 1</ref> shows the sensitivity of each approach to the choice of encoder.</p><p>The first column demonstrates the drop in performance with AMDIM performance when using standard ResNets as the encoder. The second column shows that CPC does not suffer from the same drop in performance when using a standard ResNet. In the final column, we observe that YADIM is less sensitive to the choice of encoder. These results suggest that the choice of encoder in YADIM and CPC is less important than it was with AMDIM. Although It has recently been noted by <ref type="bibr" target="#b5">[6]</ref> that the network width has a significant impact on the performance, we do not observe such a dramatic difference in performance at least with CPC and YADIM. Furthermore, in AMDIM, the widest network (Wide-ResNet 101) performs worse than the smallest, non-wide network (ResNet-18).</p><p>Representation extraction AMDIM compares feature maps at different stages of an encoder, while CPC simply uses the last feature map. To evaluate the impact of each comparison approach, we evaluate AMDIM with a similar strategy and try two others that we design.</p><p>Let (j : k) denote comparing feature maps, m a j and m + k , and we use j = −1 to refer to the final feature map generated by the encoder and j = −2 second to the last. With this notation, AMDIM performs the comparison of (−1 : −2) + (−1 : −3) + (−2 : −2). To evaluate the sensitivity to the choice of feature map locations, we define five comparison strategies: (1) the last feature maps only (−1 : −1), (2) the AMDIM strategy (−1 : −2) + (−1 : −3) + (−2 : −2), (3) the last feature map to a random feature map (−1 : k ∼ U (−1, −k)), and (4) the feature maps at all levels separately (−1 : −1) + (−2 : −2) + (−3 : −3). In <ref type="table">Table 2</ref>, we see that AMDIM is highly sensitive to the particular representation extraction comparison, which is not the case with the proposed YADIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Final formulation of YADIM</head><p>Given the results of the ablations above, we finalize the design of the proposed YADIM in this section.</p><p>(1) Data Augmentation Pipeline We define a data augmentation pipeline for YADIM as the union of the CPC and AMDIM pipelines. This new pipeline applies all six transforms sequentially to an input twice to generate two version of the same input v a ∼ A(x), v + ∼ A(x). The same pipeline generates the negative sample from a different input v − ∼ A(x ).</p><p>(2) Encoder We use the wide ResNet-34 from AMDIM, although the choice of any other encoder would not have a significant impact the final performance.</p><p>(3) Representation Extraction YADIM compares the triplets from the last feature maps generated by the encoder (r a −1 , r + −1 , R − −1 ), unlike AMDIM.</p><p>(4) Similarity measure YADIM uses a dot product φ(a, b) = a · b without any extra parameter, unlike CPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>Datasets We use four standard image classification datasets to thoroughly evaluate the proposed approach together with AMDIM and CPC:</p><p>• CIFAR-10 <ref type="bibr" target="#b23">[24]</ref> consists of 32x32 images categorized into 10 classes. It has 50,000 training images and 10,000 test images. We use a 45,000/5,000 training/validation split. We report results on the 10,000 test images. • STL-10 [8] consists of 96x96 images categorized into 10 classes. We downsample each image to be 64x64 following Bachman et al. <ref type="bibr" target="#b2">[3]</ref>. The training set has 100,000 unlabeled and 5,000 labeled training examples. The test set has 8,000 labeled examples. We train our model and baselines using the unlabeled training examples, while using 500 labeled ones for validation. • ImageNet (ILSCRV-12) <ref type="bibr" target="#b8">[9]</ref> has images of varying sizes categorized into 1,000 classes.</p><p>We downsample each image to be 128x128 following the protocol from <ref type="bibr" target="#b2">[3]</ref>. The dataset has approximately 1.3 million images in the training set and 50,000 images in the official validation set. We make our own 50,000 image validation set from the training set and use the official validation set as the test set.</p><p>Optimization and Hyperparameters We use Adam <ref type="bibr" target="#b21">[22]</ref> and search for the optimal learning rate via grid search based on the loss function computed on the unlabeled training examples. We fix the other hyperparameters of Adam to the same values used in <ref type="bibr" target="#b2">[3]</ref>. For SimCLR, we use LARS <ref type="bibr" target="#b38">[39]</ref>, as originally used by the authors, with a learning rate of 10 −6 .</p><p>Finetuning In <ref type="bibr" target="#b22">[23]</ref>, the authors conducted extensive study on how to evaluate self-supervised learning algorithms for image representation. We thus follow their evaluation protocol in our experiments: 1) choose a dataset, 2) drop all labels, 3) pretrain the encoder on these unlabeled examples, 4) freeze the encoder, and 5) train a separate neural network on top of this frozen encoder using a subset of labeled examples.</p><p>We use a multi-layer perceptron (MLP) with a single hidden layer consisting of 1,024 ReLU <ref type="bibr" target="#b30">[31]</ref> units. Our results differ slightly from those reported because different CSL approaches use different variants of MLP or ResNet. We however find it more informative for comparison to use the same MLP across all CSL approaches.</p><p>Compute Infrastructure We use multiple NVIDIA V100 GPUs with 32G memory each, for each experiment. We use two V100 GPUs for up to 24 hours on CIFAR-10. We use eight V100 GPUs for up to 72 hours on STL-10. We use 96 V100 GPUs for up to 72 hours on ImageNet when AMDIM is used and 32 V100 GPUs for 21+ days when CPC was used. YADIM trained for 14 days using 256 V100 GPUs to cope with the increased memory requirement incurred by the use of both non-overlapping patches and double applications of data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>The CSL approaches often use different evaluation protocols and do not re-implement individual approaches for rigorous comparison <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref>. To avoid any inconsistency arising from different implementations and protocols, we re-implement AMDIM, CPC, SimCLR, Moco and CMC using PyTorch Lightning.</p><p>PyTorch Lightning PyTorch Lightning (PL) <ref type="bibr" target="#b12">[13]</ref> is a framework which decouples scientific components and engineering details in the code written for PyTorch <ref type="bibr" target="#b33">[34]</ref>. PL enables our implementations of the CSL approaches to be hardware agnostic, more easily readable, and accessible to researchers with lower computational resources since it enables running the same code on arbitrary hardware. In addition, it allows us to use the exactly same dataset splits, same fine-tuning protocol, early-stopping criterion and transformation pipelines to ensure the consistency across various experimental settings.</p><p>Negative samples Through our experiments, we observe that CSL performance positively correlates with the number of negative samples. There are multiple ways to achieve this. One way is to build a memory bank that stores pre-computed representations for k consecutive samples <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6]</ref>. Another way is to share data across training processes, where all the batches across GPUs can be used for the denominator of softmax <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>For our experiments we use distributed softmax via PyTorch Lightning, which splits a batch of samples across GPUs on the machine and aggregates them on a single machine for an effective large denominator for softmax.</p><p>CSL reproducibility As will be evident from the results later, we have largely reproduced AMDIM and CPC which we attribute to the open-sourced code of AMDIM provided by its authors and helpful discussion with the authors of CPC, respectively. Due to the limitation on available computational resources, however, we were not able to perform thorough hyperparameter search with CPC and AMDIM, which prevented us from fully reproducing their reported results on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and analysis</head><p>In this section we verify our implementations. To our knowledge, this is the first comparison between AMDIM, CPC and SimCLR using the same standardized implementation and evaluation protocol.</p><p>Class Separability In this set of experiments, we test whether representations capture class separation without observing the actual class labels. To measure this, we apply the fine-tuning protocol we describe in 4. For training a classifier, we use the all the labels of CIFAR-10, STL-10 and ImageNet. <ref type="table">Table 3</ref> compares the class separability of the representations from AMDIM, CPC, SimCLR and YADIM. Our implementation of AMDIM achieves the performance close to that reported by Bachman et al. on CIFAR-10 and STL-10. On ImageNet, our implementation lags behind the latest reported accuracy but is still on par with the original accuracy from the earlier version of <ref type="bibr" target="#b2">[3]</ref>.</p><p>Our CPC implementation sets new state-of-the-art scores on STL-10 self-supervised pretrained models. On ImageNet our CPC implementation achieves close to the reported result, but due to computational constraints we cannot train the model fully to achieve the accuracy reported by Henaff et al. <ref type="bibr" target="#b17">[18]</ref>. Our SimCLR implementation is trained on a single V100 for 14 hours and achieves 87.60 accuracy which is reasonably close to the reported 93.70. Finally, the proposed YADIM achieves comparable accuracy on CIFAR-10, SLT-10 and ImageNet to both AMDIM and CPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Pretext tasks for self-supervised learning have been studied extensively over the past few years. In <ref type="bibr" target="#b10">[11]</ref> the authors sample two neighboring patches within an image, then train a siamese network to predict the relative location of the patches. Isola et al. <ref type="bibr" target="#b20">[21]</ref> use a similar approach but instead predict whether patches were taken from nearby locations in the image or not. Pathak et al. <ref type="bibr" target="#b34">[35]</ref> attempt to learn representations by inpainting <ref type="bibr" target="#b4">[5]</ref>. Noroozi et al. <ref type="bibr" target="#b31">[32]</ref> train a CNN to solve jigsaw puzzles. In a different approach, <ref type="bibr" target="#b39">[40]</ref> use the gray components of an image to predict the color components of the same image. More recently, Gidaris et al. <ref type="bibr" target="#b13">[14]</ref> train CNNs to detect a 2d rotation applied to the input image.</p><p>Unlike the ones above which are specific to images, there have been a class of self-supervised learning algorithms that are less specific to images and are more generally applicable. They include <ref type="table">Table 3</ref>: Class separability Test accuracies are computed from finetuning a 1,024-unit MLP on top of a frozen, pretrained encoder. The encoder was trained on each dataset without labels. We underline the best accuracy per dataset inclusive of previously reported ones, and bold-face the best accuracy among our own implementations. <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> reported in the first and second versions of <ref type="bibr" target="#b2">[3]</ref>, respectively. Due to computational constraints we could not complete experiments using our own implementation of SimCLR on ImageNet. † our own implementation. reported in <ref type="bibr" target="#b17">[18]</ref>. AMDIM <ref type="bibr" target="#b18">[19]</ref>, CPC <ref type="bibr" target="#b17">[18]</ref>, SimCLR <ref type="bibr" target="#b5">[6]</ref>, CMC <ref type="bibr" target="#b37">[38]</ref> and MOCO <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>. We refer to them as contrastive self-supervised learning (CSL). In this paper we attempt at providing a unified framework behind these CSL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dosovitskiy et al. <ref type="bibr" target="#b11">[12]</ref> introduce the idea of generating multiple views of the same data by applying a stochastic augmentation pipeline to the same image multiple times. AMDIM pairs this idea with comparisons of feature maps generated from intermediate layers of an encoder. However, in this work we showed that the particular choice of which feature maps to compare is highly subjective and that performance deteriorates as the strategy changes. CPC on the other hand introduces the idea of generating positive pairs by taking patches within the same image and then predicts patches well-separated spatially. However, we observe that removing the context encoder of this task results in trivial solutions for both CPC and SimCLR.</p><p>CMC uses the same ideas from AMDIM but differs in two key aspects. First, the NCE loss is regularized by a discriminator <ref type="bibr" target="#b14">[15]</ref>. Second, a memory bank is used to increase the number of negative samples, which leads to the increase in the memory requirement of the system. SimCLR differs from AMDIM in three key aspects. First, it adds random resize and crop to the data augmentation pipeline. Second, it parametrizes the similarity metric with a non-linear transformation of the representation followed by dot product. MOCO (v2) modifies the SimCLR projection head g θ (x) and the data augmentation pipeline.</p><p>As Hjelm et al. <ref type="bibr" target="#b18">[19]</ref> demonstrated, CSL approaches outperform other approaches such as autoencoders <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref> and generative adversarial networks (GAN) <ref type="bibr" target="#b14">[15]</ref>. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> similarly showed that representations learned by GANs and autoencoders do not transfer well to other tasks including image classification, although these models excel at image denoising and image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we proposed a conceptual framework to more easily characterize various CSL approaches. We showed that AMDIM, CPC and SimCLR are special cases of our framework. We evaluated each key design choice of AMDIM and CPC, which are two representative CSL algorithms, and used our framework to construct a new approach to which we refer as YADIM (Yet another DIM). YADIM performs just as well as CPC and AMDIM on CIFAR-10, STL-10 and ImageNet but has two key advantages. First, it is robust to the choice of the encoder architecture. Second, it uses a simpler representation extraction strategy.</p><p>By comparing the proposed YADIM against AMDIM and CPC, we have learned three lessons. First, the choice of encoder is not important as long as it is wide with many feature maps at each layer. Second, it is enough to use a simple contrastive loss, consisting of noise contrastive loss with dot product without any extra parameter. Third, it is largely a strong data augmentation pipeline that leads to strong downstream task results, even with a simple representation extraction strategy or a simple encoder architecture. Furthermore, we find that SimCLR, CMC and MOCO do not differ much from each other and from both CPC and AMDIM, and the design choices they make are easily interpreted under the proposed conceptual framework.</p><p>Finally, we release all the code for implementing these contrastive self-supervised learning approaches under PyTorch Lightning. We hope this release enables objective and clear comparison between all approaches and encourages researchers to push the frontier of contrastive self-supervised learning further. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(A) The contrastive self-supervised pretext task consists of finding ways to generate anchor, positive and negative features used as training signals for unlabeled datasets. (B) The positive and anchor pairs are pushed close together, and away from the negative features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>Representation extraction Let H = f θ (v) be the output of the encoder, where v ∼ A(x) and H ∈ R c×h×w . CPC frames representation extraction as predicting the feature vectors k units spatially below at H i+k,j based on the feature vectors in the neighbourhood of H i,j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>C. 2 C. 4</head><label>24</label><figDesc>x = p a t c h _ a u g m e n t a t i o n ( x ) # L a t e n t f e a t u r e s Z = e n c o d e r ( x ) # infoNCE l o s s n c e _ l o s s = s e l f . c o n t r a s t i v e _ t a s k ( Z ) l o s s = n c e _ l o s s r e t u r n l o s s C SimCLR bolts implementation code C.1 Encoder from p l _ b o l t s . m o d e l s . s e l f _ s u p e r v i s e d . r e s n e t s import r e s n e t 5 0 _ b n Transforms from p l _ b o l t s . m o d e l s . s e l f _ s u p e r v i s e d . c p c import SimCLREvalTransformsSTL10 d a t a s e t = STL10 ( t r a n s f o r m s = SimCLREvalTransformsSTL10 ( ) ) C.3 Representation extraction d e f t r a i n i n g _ s t e p ( s e l f , b a t c h , b a t c h _ i d x ) : ( img1 , img2 ) , y = b a t c h # ENCODE # e n c o d e −&gt; r e p r e s e n t a t i o n s # ( b , 3 , 3 2 , 3 2 ) −&gt; ( b , 2 0 4 8 , 2 , 2 ) h1 = s e l f . e n c o d e r ( img1 ) h2 = s e l f . e n c o d e r ( img2 ) Loss d e f n t _ x e n t _ l o s s ( z1 , z2 , t e m p e r a t u r e ) : " " " L o s s u s e d i n SimCLR " " " o u t = t o r c h . c a t ( [ z1 , z2 ] , dim = 0 ) n _ s a m p l e s = l e n ( o u t ) # F u l l s i m i l a r i t y m a t r i x cov = t o r c h .mm( o u t , o u t . t ( ) . c o n t i g u o u s ( ) ) sim = t o r c h . exp ( cov / t e m p e r a t u r e ) # N e g a t i v e s i m i l a r i t y mask =~t o r c h . e y e ( n _ s a m p l e s , d e v i c e = sim . d e v i c e ) . b o o l ( ) neg = sim . m a s k e d _ s e l e c t ( mask ) . view ( n _ s a m p l e s , −1).sum ( dim=−1) # P o s i t i v e s i m i l a r i t y : p o s = t o r c h . exp ( t o r c h . sum ( o u t _ 1 * o u t _ 2 , dim=−1) / t e m p e r a t u r e ) p o s = t o r c h . c a t ( [ pos , p o s ] , dim = 0 ) l o s s = −t o r c h . l o g ( p o s / neg ) . mean ( ) r e t u r n l o s s C.5 Full SimCLR pseudocode d e f t r a i n i n g _ s t e p ( s e l f , b a t c h , b a t c h _ i d x ) : ( img1 , img2 ) , y = b a t c h # ENCODE # e n c o d e −&gt; r e p r e s e n t a t i o n s # ( b , 3 , 3 2 , 3 2 ) −&gt; ( b , 2 0 4 8 , 2 , 2 ) h1 = s e l f . e n c o d e r ( img1 ) h2 = s e l f . e n c o d e r ( img2 ) # t h e b o l t s r e s n e t s r e t u r n a l i s t o f f e a t u r e maps i f i s i n s t a n c e ( h1 , l i s t ) : h1 = h1 [ −1] h2 = h2 [ −1] # PROJECT # img −&gt; E −&gt; h −&gt; | | −&gt; z # ( b , 2 0 4 8 , 2 , 2 ) −&gt; ( b , 1 2 8 ) z1 = s e l f . p r o j e c t i o n ( h1 ) z2 = s e l f . p r o j e c t i o n ( h2 ) l o s s = s e l f . n t _ x e n t _ l o s s ( z1 , z2 , s e l f . h p a r a m s . l o s s _ t e m p e r a t u r e ) r e t u r n l o s s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Encoder robustness on CIFAR-10. (higher is better, bold results are the highest per ResNet). All major ResNet architectures found in PyTorch<ref type="bibr" target="#b33">[34]</ref> are trained with either AMDIM, CPC or proposed YADIM.</figDesc><table><row><cell></cell><cell>Network</cell><cell cols="3">AMDIM CPC YADIM</cell></row><row><cell></cell><cell>Reported</cell><cell>93.10</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Our implementation</cell><cell>92.00</cell><cell>84.52</cell><cell>90.33</cell></row><row><cell></cell><cell>ResNet 18</cell><cell>63.25</cell><cell>83.14</cell><cell>85.51</cell></row><row><cell>We pretrain each network on CIFAR-10 without labels and train an MLP with 1,024 hidden units on CIFAR-10 with all labels on top of the pretrained and frozen encoder. YADIM's performance remains stable across different choice of encoders, as does CPC while AMDIM performance suffers.</cell><cell>ResNet 34 ResNet 50 ResNet 101 ResNet 152 ResNet 50 (32 x4d) ResNet 101 (32 x8d) Wide-ResNet 50 Wide-ResNet 101</cell><cell>57.50 61.43 58.91 53.40 64.08 59.37 59.37 60.07</cell><cell>83.91 83.76 79.18 84.21 85.08 83.65 85.09 84.15</cell><cell>84.57 85.83 83.09 85.03 86.69 86.62 85.65 85.60</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">see https://gluebenchmark.com/leaderboard.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/docs/stable/torchvision/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>WF thanks Facebook AI Research, DeepMind and NSF for their support. In addition, thank you to Stephen Roller, Margaret Li, Shubho Sengupta, Ananya Harsh Jha, Cinjon Resnick, Tullie Murrell, Carl Doersch, Devon Hjelm, Eero Simoncelli and Yann LeCun for helpful discussions. KC thanks support by CIFAR, NVIDIA, eBay, Google and Naver.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wa Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/williamFalcon/pytorch-lightningCitedby" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a backpropagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06476</idno>
		<title level="m">Parlai: A dialog research software platform</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noisecontrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning internal representation by error propogation, parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rumelhalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
