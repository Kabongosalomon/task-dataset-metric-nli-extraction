<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">USB: Universal-Scale Object Detection Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Shinya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hinagiku Kenzan! ©Minene Sakurano</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">USB: Universal-Scale Object Detection Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Universal-scale object detection. For realizing human-level perception, object detection systems must detect both tiny and large objects, even if they are out of natural image domains. To this end, we introduce the Universal-Scale object detection Benchmark (USB) that consists of the COCO dataset (left), Waymo Open Dataset (middle), and Manga109-s dataset (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Benchmarks, such as COCO, play a crucial role in object detection. However, existing benchmarks are insufficient in scale variation, and their protocols are inadequate for fair comparison. In this paper, we introduce the Universal-Scale object detection Benchmark (USB). USB has variations in object scales and image domains by incorporating COCO with the recently proposed Waymo Open Dataset and Manga109-s dataset. To enable fair comparison, we propose USB protocols by defining multiple thresholds for training epochs and evaluation image resolutions. By analyzing methods on the proposed benchmark, we designed fast and accurate object detectors called UniverseNets, which surpassed all baselines on USB and achieved stateof-the-art results on existing benchmarks. Specifically, Uni-verseNets achieved 54.1% AP on COCO test-dev with 20 epochs training, the top result among single-stage detectors on the Waymo Open Dataset Challenge 2020 2D detection, and the first place in the NightOwls Detection Challenge 2020 all objects track. The code is available at http s://github.com/shinya7y/UniverseNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans can detect various objects. See <ref type="figure">Figure 1</ref>. One can detect close equipment in everyday scenes, far vehicles in traffic scenes, and texts and persons in manga (Japanese * This work was done independently of the author's employer.  <ref type="figure">Figure 2</ref>. Speed-accuracy trade-offs in the current standard COCO benchmark. Most works train models with standard settings (e.g., within 24 epochs), while some works train with abnormal settings (e.g., 300 epochs). To enable fair comparison, we propose USB protocols that urge the latter works to report results with standard settings. Additionally, we design UniverseNets that achieve stateof-the-art results with standard settings. We show some detection examples of UniverseNet-20.08 in <ref type="figure">Figure 1</ref>. comics). If computers can automatically detect various objects, they will yield significant benefits to humans. For example, they will help impaired people and the elderly, save lives by autonomous driving, and provide safe entertainment during pandemics by automatic translation. Researchers have pushed the limits of object detection systems by establishing datasets and benchmarks <ref type="bibr" target="#b36">[38]</ref>. One of the most important milestones is PASCAL VOC <ref type="bibr" target="#b15">[17]</ref>. It has enabled considerable research on object detection, leading to the success of deep learning-based methods and suc-cessor datasets such as ImageNet <ref type="bibr" target="#b47">[49]</ref> and COCO <ref type="bibr" target="#b35">[37]</ref>. Currently, COCO serves as the standard dataset and benchmark for object detection because it has several advantages over PASCAL VOC <ref type="bibr" target="#b15">[17]</ref>. COCO contains more images, categories, and objects (especially small objects) in their natural context <ref type="bibr" target="#b35">[37]</ref>. Using COCO, researchers can develop and evaluate methods for multi-scale object detection. However, the current object detection benchmarks, especially COCO, have the following three problems.</p><p>Problem 1: Variations in object scales and image domains remain limited. To realize human-level perception, computers must handle various object scales and image domains as humans can. Among various domains <ref type="bibr" target="#b60">[62]</ref>, the traffic and artificial domains have extensive scale variations (see Sec. <ref type="bibr" target="#b1">3)</ref>. COCO is far from covering them. Nevertheless, the current computer vision community is overconfident in COCO results. For example, most studies on state-of-the-art methods in 2020 only report COCO results <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b11">13]</ref> or those for bounding box object detection <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b43">45]</ref>. Readers cannot assess whether these methods are specialized for COCO or generalizable to other datasets and domains.</p><p>Problem 2: Protocols for training and evaluation are not well established. There are standard experimental settings for the COCO benchmark <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b31">33]</ref>. Many works train detectors within 24 epochs using a learning rate of 0.01 or 0.02 and evaluate them on images within 1333×800. These settings are not obligations but nonbinding agreements for fair comparison. Some works do not follow the standard settings for accurate and fast detectors 1 . Their abnormal and scattered settings hinder the assessment of the most suitable method (see <ref type="figure">Figure 2</ref>). Furthermore, by "buying stronger results" <ref type="bibr" target="#b48">[50]</ref>, they build a barrier for those without considerable funds to develop and train detectors.</p><p>Problem 3: The analysis of methods for multi-scale object detection is insufficient. Numerous studies have proposed methods for multi-scale object detection <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b12">14]</ref>. In recent years, improvements for network components have made significant progress in COCO (e.g., Res2Net <ref type="bibr" target="#b17">[19]</ref> for the backbone, SEPC <ref type="bibr" target="#b61">[63]</ref> for the neck, and ATSS <ref type="bibr" target="#b67">[69]</ref> for the head). These works have an insufficient analysis of combinability, effectiveness, and characteristics, especially on datasets other than COCO.</p><p>This study makes the following three contributions to resolve the problems.</p><p>Contribution 1: We introduce the Universal-Scale object detection Benchmark (USB) that consists of three datasets. In addition to COCO, we selected the Waymo Open Dataset <ref type="bibr" target="#b53">[55]</ref> and Manga109-s <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b1">3]</ref> to cover vari-1 YOLOv4 was trained for 273 epochs <ref type="bibr" target="#b3">[5]</ref>, DETR for 500 epochs <ref type="bibr" target="#b7">[9]</ref>, EfficientDet-D6 for 300 epochs <ref type="bibr" target="#b57">[59]</ref>, and EfficientDet-D7x for 600 epochs <ref type="bibr" target="#b58">[60]</ref>. SpineNet uses a learning rate of 0.28 <ref type="bibr" target="#b14">[16]</ref>, and YOLOv4 uses a searched learning rate of 0.00261 <ref type="bibr" target="#b3">[5]</ref>. EfficientDet finely changes the image resolution from 512×512 to 1536×1536 <ref type="bibr" target="#b57">[59]</ref>. ous object scales and image domains. They are the largest public datasets in their domains and enable reliable comparisons. We conducted experiments using eight methods and found weaknesses of existing COCO-biased methods.</p><p>Contribution 2: We established the USB protocols for fair training and evaluation for more inclusive object detection research. USB protocols enable fair, easy, and scalable comparisons by defining multiple thresholds for training epochs and evaluation image resolutions.</p><p>Contribution 3: We designed fast and accurate object detectors called UniverseNets by analyzing methods developed for multi-scale object detection. UniverseNets outperformed all baselines on USB and achieved state-of-the-art results on existing benchmarks. In particular, our finding on USB enables a 9.3 points higher score than YOLOv4 <ref type="bibr" target="#b3">[5]</ref> on the Waymo Open Dataset Challenge 2020 2D detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection Methods</head><p>Deep learning-based detectors dominate the recent progress in object detection <ref type="bibr" target="#b36">[38]</ref>. They can be divided as <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b45">47]</ref> single-stage detectors without region proposal <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b34">36]</ref> and multi-stage (including two-stage) detectors with region proposal <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b6">8]</ref>. Our UniverseNets are single-stage detectors for efficiency <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>Detecting multi-scale objects is a fundamental challenge in object detection <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b5">7]</ref>. Various components have been improved, including backbones and modules <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b12">14]</ref>, necks <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b57">59]</ref>, heads and training sample selection <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b67">69]</ref>, and multi-scale training and testing <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b67">69]</ref> (see Supplementary Material for details).</p><p>Some recent or concurrent works <ref type="bibr" target="#b65">[67,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b27">29]</ref> have combined multiple methods. Unlike these works, we analyzed various components developed for scale variations, without computation for neural architecture search <ref type="bibr" target="#b65">[67]</ref>, long training (273 epochs) <ref type="bibr" target="#b3">[5]</ref>, and multi-stage detectors <ref type="bibr" target="#b27">[29]</ref>.</p><p>Most prior studies have evaluated detectors on limited image domains. We demonstrate the superior performance of our UniverseNets across various object scales and image domains through the proposed benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection Benchmarks</head><p>There are numerous object detection benchmarks. For specific (category) object detection, recent benchmarks such as WIDER FACE <ref type="bibr" target="#b64">[66]</ref> and TinyPerson <ref type="bibr" target="#b66">[68]</ref> contain tiny objects. Although they are useful for evaluation for a specific category, many applications should detect multiple categories. For autonomous driving, KITTI <ref type="bibr" target="#b19">[21]</ref> and Waymo Open Dataset <ref type="bibr" target="#b53">[55]</ref>  number of categories has been further expanded by recent benchmarks, such as Open Images <ref type="bibr" target="#b30">[32]</ref>, Objects365 <ref type="bibr" target="#b49">[51]</ref>, and LVIS <ref type="bibr" target="#b22">[24]</ref>. All the above datasets comprise photographs, whereas Clipart1k, Watercolor2k, Comic2k <ref type="bibr" target="#b28">[30]</ref>, and Manga109-s <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b1">3]</ref> comprise artificial images. Detectors evaluated on a specific dataset may perform worse on other datasets or domains. To address this issue, some benchmarks consist of multiple datasets. In the Robust Vision Challenge 2020 [1], detectors were evaluated on three datasets in the natural and traffic image domains. For universal-domain object detection, the Universal Object Detection Benchmark (UODB) <ref type="bibr" target="#b60">[62]</ref> comprises 11 datasets in the natural, traffic, aerial, medical, and artificial image domains. Although it is suitable for evaluating detectors in various domains, variations in object scales are limited. Unlike UODB, our USB focuses on universal-scale object detection. Datasets in USB contain more instances, including tiny objects, than the datasets used in UODB.</p><p>As discussed in Sec. 1, the current benchmarks allow extremely unfair settings (e.g., 25× training epochs). We resolved this problem by establishing USB protocols for fair training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Benchmark Protocols of USB</head><p>Here, we present the principle, datasets, protocols, and metrics of USB. See Supplementary Material for additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Principle</head><p>We focus on the Universal-Scale Object Detection (USOD) task that aims to detect various objects in terms of object scales and image domains. Unlike separate discussions for multi-scale object detection (Sec. 2) and universal (-domain) object detection <ref type="bibr" target="#b60">[62]</ref>, USOD does not ignore the relation between scales and domains (Sec. 3.2).</p><p>For various applications and users, benchmark protocols should cover from short to long training and from small to large test scales. On the other hand, they should not be scattered for meaningful benchmarks. To satisfy the conflicting requirements, we define multiple thresholds for training epochs and evaluation image resolutions. Furthermore, we urge rich participants to report results with standard training settings. This request enables fair comparison and allows many people to develop and compare object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Domain Color Main sources of scale variation COCO <ref type="bibr" target="#b35">[37]</ref> Natural RGB Categories, distance WOD <ref type="bibr" target="#b53">[55]</ref> Traffic RGB Distance Manga109-s <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b1">3]</ref> Artificial Grayscale * Viewpoints, page layouts </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Datasets</head><p>To establish USB, we selected the COCO <ref type="bibr" target="#b35">[37]</ref>, Waymo Open Dataset (WOD) <ref type="bibr" target="#b53">[55]</ref>, and Manga109-s (M109s) <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b1">3]</ref>. WOD and M109s are the largest public datasets with many small objects in the traffic and artificial domains, respectively. Object scales in these domains vary significantly with distance and viewpoints, unlike those in the medical and aerial domains. The USB covers extensive scale variations quantitatively ( <ref type="figure" target="#fig_3">Figure 3</ref>) and qualitatively (Table 1). As shown in <ref type="table">Table 2</ref>, these three datasets in USB contain more instances than their counterpart datasets in UODB <ref type="bibr" target="#b60">[62]</ref> (COCO <ref type="bibr" target="#b35">[37]</ref> val2014 subset, KITTI <ref type="bibr" target="#b19">[21]</ref>, and Comic2k <ref type="bibr" target="#b28">[30]</ref>). USOD needs to evaluate detectors on datasets with many instances because more instances enable more reliable comparisons of scale-wise metrics.</p><p>For the first dataset, we adopted the COCO dataset <ref type="bibr" target="#b35">[37]</ref>. COCO contains natural images of everyday scenes collected from the Internet. Annotations for 80 categories are used in the benchmark. As shown in <ref type="figure">Figure 1</ref> (left), object scales mainly depend on the categories and distance. Although COCO contains objects smaller than those of PAS-CAL VOC <ref type="bibr" target="#b15">[17]</ref>, objects in everyday scenes (especially indoor scenes) are relatively large. Since COCO is the current standard dataset for multi-scale object detection, we adopted the same training split train2017 (also known as trainval35k) as the COCO benchmark to eliminate the need for retraining across benchmarks. We adopted the val2017 split (also known as minival) as the test set.</p><p>For the second dataset, we adopted the WOD, which is a large-scale, diverse dataset for autonomous driving <ref type="bibr" target="#b53">[55]</ref> with many annotations for tiny objects <ref type="figure" target="#fig_3">(Figure 3</ref>). The images were recorded using five high-resolution cameras mounted on vehicles. As shown in <ref type="figure">Figure 1</ref> (middle), object scales vary mainly with distance. The full data splits of the WOD are too large for benchmarking methods. Thus, we extracted 10% size subsets from the predefined training split  <ref type="table">Table 3</ref>. USB training protocols. For models trained with masks, 0.5 is added. AHPO: Aggressive hyperparameter optimization.</p><p>(798 sequences) and validation split (202 sequences) <ref type="bibr" target="#b53">[55]</ref>. Specifically, we extracted splits based on the ones place of the frame index (frames 0, 10, ..., 190) in each sequence. We call the subsets f0train and f0val splits. Each sequence in the splits contains ∼20 frames (20 s, 1 Hz), and each frame contains five images for five cameras. We used three categories (vehicle, pedestrian, and cyclist) following the official ALL NS setting <ref type="bibr" target="#b0">[2]</ref> used in WOD competitions. For the third dataset, we adopt the M109s <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b1">3]</ref>. M109s contains artificial images of manga (Japanese comics) and annotations for four categories (body, face, frame, and text). Many characteristics differ from those of natural images. Most images are grayscale. The objects are highly overlapped <ref type="bibr" target="#b41">[43]</ref>. As shown in <ref type="figure">Figure 1</ref> (right), object scales vary unrestrictedly with viewpoints and page layouts. Small objects differ greatly from downsampled versions of large objects because small objects are drawn with simple lines and points. For example, small faces look like a sign (∵). This characteristic may ruin techniques developed mainly for natural images. Another challenge is ambiguity in annotations. Sometimes, a small-scale object is annotated, and sometimes, a similar scale object on another page is not annotated. Since annotating small objects is difficult and labor-intensive, this is an important and practical challenge. We carefully selected 68, 4, and 15 volumes for training, validation, and testing splits, and we call them the 68train, 4val, and 15test, respectively.</p><p>We selected the test splits from images with publicly available annotations to reduce the labor required for submissions. Participants should not fine-tune hyperparameters based on the test splits to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Protocols</head><p>For fair training, we propose the USB training protocols shown in <ref type="table">Table 3</ref>. By analogy with the backward compatibility of the Universal Serial Bus, USB training protocols emphasize compatibility between protocols. Importantly, participants should report results with not only higher protocols but also lower protocols. For example, when a participant trains a model for 150 epochs with standard hyperparameters, it corresponds to USB 3.0. The participant should also report the results of models trained for 24 and 73 epochs in a paper. The readers of the paper can judge whether the method is useful for standard training epochs.</p><p>The number of maximum epochs for USB 1.0 is 24,  which is the most popular setting in COCO (see <ref type="table" target="#tab_2">Table 10</ref>). We adopted 73 epochs for USB 2.0, where models trained from scratch can catch up with those trained for 24 epochs from ImageNet pre-trained models <ref type="bibr" target="#b23">[25]</ref>. We adopted 300 epochs for USB 3.x such that YOLOv4 <ref type="bibr" target="#b3">[5]</ref> and most Effi-cientDet models <ref type="bibr" target="#b58">[60]</ref> correspond to this protocol. Models trained for more than 300 epochs are regarded as Freestyle.</p><p>They are not suitable for benchmarking methods, although they may push the empirical limits of detectors <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b7">9]</ref>. For models trained with mask annotations, 0.5 is added to their number of protocols. Results without mask annotations should be reported if possible for their algorithms.</p><p>For ease of comparison, we limit the pre-training datasets to the three and ImageNet (ILSVRC 1,000-class classification). Other datasets are welcome only when the results with and without additional datasets are reported. Participants should describe how to use the datasets. A possible way is to fine-tune the models on WOD and M109s from COCO pre-trained models. Another way is to train a single model jointly <ref type="bibr" target="#b60">[62]</ref> on the three datasets.</p><p>In addition to long training schedules, hyperparameter optimization is resource-intensive. If authors of a paper fine-tune hyperparameters for their architecture, other people without sufficient computational resources cannot compare methods fairly. We recommend roughly tuning the minimum hyperparameters, such as batch sizes and learning rates (e.g., from choices {0.1, 0.2, 0.4, 0.8, ...}, {0.1, 0.2, 0.5, 1.0, ...}, and {0.1, 0.3, 1.0, ...}). When participants optimize hyperparameters aggressively by manual fine-tuning or automatic algorithms, they should report both results with and without aggressive optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Protocols</head><p>For fair evaluation, we propose the USB evaluation protocols shown in <ref type="table" target="#tab_5">Table 4</ref>. By analogy with the size variations of the Universal Serial Bus connectors for various devices, USB evaluation protocols have variations in test image scales for various devices and applications.</p><p>The maximum resolution for Standard USB follows the popular test scale of 1333×800 in the COCO benchmark (see <ref type="table" target="#tab_2">Table 10</ref> and <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b20">22]</ref>). For Mini USB, we limit the resolution based on 512×512. This resolution is popular in the PASCAL VOC benchmark <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b37">39]</ref>, which contains small images and large objects. It is also popular in real-time detectors <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b3">5]</ref>. We adopted a further small-scale 224×224 for Micro USB. This resolution is popular in ImageNet classification <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b24">26]</ref>. Although small object detection is extremely difficult, it is suitable for low-power devices. Additionally, this protocol enables people to manage object detection tasks using one or few GPUs. To cover larger test scales than Standard USB, we define Large USB and Huge USB based on WOD resolutions. The maximum resolution of the Huge USB is determined by the top methods on WOD (see Sec. 5.3). Although larger inputs (regarded as Freestyle) may be preferable for accuracy, excessively large inputs significantly reduce the practicality of detectors.</p><p>In addition to test image scales, the presence and degree of Test-Time Augmentation (TTA) make large differences in accuracy and inference time. When using TTA, participants should report its details (including the number of scales of multi-scale testing) and results without TTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation Metrics</head><p>We mainly use the COCO metrics <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b32">34]</ref> to evaluate the performance of detectors on each dataset. We provide data format converters for WOD 2 and M109s 3 .</p><p>We first describe the calculation of COCO metrics according to the official evaluation code <ref type="bibr" target="#b32">[34]</ref>. True or false positives are judged by measuring the Intersection over Union (IoU) between predicted bounding boxes and ground truth bounding boxes <ref type="bibr" target="#b15">[17]</ref>. For each category, the Average Precision (AP) is calculated as precision averaged over 101 recall thresholds {0, 0.01, ..., 1}. The COCO-style AP (CAP) for a dataset d is calculated as</p><formula xml:id="formula_0">CAP d = 1 |T | t∈T 1 |C d | c∈C d AP t,c ,<label>(1)</label></formula><p>where T = {0.5, 0.55, ..., 0.95} denotes the predefined 10 IoU thresholds, C d denotes categories in the dataset d, | · | denotes the cardinality of a set (e.g., |C d | = 80 for COCO), and AP t,c denotes AP for an IoU threshold t and a category c. For detailed analysis, five additional AP metrics (averaged over categories) are evaluated. AP 50 and AP 75 denote AP at single IoU thresholds of 0.5 and 0.75, respectively. AP S , AP M , and AP L are variants of CAP, where target objects are limited to small (area ≤ 32 2 ), medium (32 2 ≤ area ≤ 96 2 ), and large (96 2 ≤ area) objects, respectively. The area is measured using mask annotations for COCO and bounding box annotations for WOD and M109s. As the primary metric for USB, we use the mean COCOstyle AP (mCAP) averaged over all datasets D as</p><formula xml:id="formula_1">mCAP = 1 |D| d∈D CAP d .<label>(2)</label></formula><p>Since USB adopts the three datasets described in Sec. <ref type="bibr" target="#b1">3</ref> and AP L by averaging them over the datasets. We plan to define finer scale-wise metrics for USOD in future work. For ease of quantitative evaluation, we limit the number of detections per image to 100 across all categories, following the COCO benchmark <ref type="bibr" target="#b32">[34]</ref>. For qualitative evaluation, participants may raise the limit to 300 (1% of images in the M109s 15test set contain more than 100 annotations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">UniverseNets</head><p>For fast and accurate detectors for USOD, we designed UniverseNets. Single-stage detectors were adopted for efficiency. See Supplementary Material for details of the methods and architectures used in UniverseNets.</p><p>As a baseline model, we used RetinaNet <ref type="bibr" target="#b34">[36]</ref> implemented in MMDetection <ref type="bibr" target="#b8">[10]</ref>. Specifically, the backbone is ResNet-50-B <ref type="bibr" target="#b25">[27]</ref> (a variant of ResNet-50 <ref type="bibr" target="#b24">[26]</ref>, also known as the PyTorch style). The neck is FPN <ref type="bibr" target="#b33">[35]</ref>. We used focal loss <ref type="bibr" target="#b34">[36]</ref>, single-scale training, and single-scale testing.</p><p>Built on the RetinaNet baseline, we designed Uni-verseNet by collecting human wisdom about multi-scale object detection as of May 2020. We used ATSS <ref type="bibr" target="#b67">[69]</ref> and SEPC without iBN <ref type="bibr" target="#b61">[63]</ref> (hereafter referred to as ATSEPC). The backbone is Res2Net-50-v1b <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref>. Deformable Convolutional Networks (DCN) <ref type="bibr" target="#b12">[14]</ref> were adopted in the backbone and neck. We used multi-scale training. Unless otherwise stated, we used single-scale testing for efficiency.</p><p>By adding GFL <ref type="bibr" target="#b31">[33]</ref>, SyncBN <ref type="bibr" target="#b42">[44]</ref>, and iBN <ref type="bibr" target="#b61">[63]</ref>, we designed three variants of UniverseNet around August 2020. UniverseNet-20.08d heavily uses DCN <ref type="bibr" target="#b12">[14]</ref>. UniverseNet-20.08 speeds up inference (and training) by the light use of DCN <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b61">63]</ref>. UniverseNet-20.08s further speeds up inference using the ResNet-50-C <ref type="bibr" target="#b25">[27]</ref> backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Here, we present benchmark results on USB and comparison results with state-of-the-art methods on the three datasets. Thereafter, we analyze the characteristics of detectors by additional experiments. See Supplementary Material for details of the experimental settings and results, including the KITTI-style AP <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b53">55]</ref> on WOD and the effects of COCO pre-training on M109s.   Our code is built on MMDetection <ref type="bibr" target="#b8">[10]</ref> v2. We used the COCO pre-trained models of the repository for existing methods (Faster R-CNN <ref type="bibr" target="#b45">[47]</ref> with FPN <ref type="bibr" target="#b33">[35]</ref>, Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>, RetinaNet <ref type="bibr" target="#b34">[36]</ref>, ATSS <ref type="bibr" target="#b67">[69]</ref>, and GFL <ref type="bibr" target="#b31">[33]</ref>). We trained all models with Stochastic Gradient Descent (SGD).  The default hyperparameters are listed in <ref type="table" target="#tab_8">Table 5</ref>. Most values follow standard settings <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b33">35]</ref>. We used some dataset-dependent values. For M109s, we roughly tuned the learning rates (LR) based on a preliminary experiment with the RetinaNet <ref type="bibr" target="#b34">[36]</ref> baseline model. Test scales were determined within the standard USB protocol, considering the typical aspect ratio of the images in each dataset. The ranges for multi-scale training for COCO and M109s follow prior work <ref type="bibr" target="#b61">[63]</ref>. We used larger scales for WOD because the objects in WOD are especially small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>COCO models were fine-tuned from ImageNet pretrained backbones. We trained the models for WOD and M109s from the corresponding COCO pre-trained models. We follow the learning rate schedules of MMDetection <ref type="bibr" target="#b8">[10]</ref>. We mainly used the 1× schedule (12 epochs). For comparison with state-of-the-art methods on COCO, we used the 2× schedule (24 epochs) for most models and the 20e schedule (20 epochs) for UniverseNet-20.08d due to overfitting with the 2× schedule. For comparison with state-of-the-art methods on WOD, we trained UniverseNet on the WOD full training set for 7 epochs. We used a learning rate of 10 −3 for 6 epochs and 10 −4 for the last epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmark Results on USB</head><p>We trained and evaluated methods on the USB. All methods follow the Standard USB 1.0 protocol using the default hyperparameters in Sec. 5.1. The results are shown in Table 6. UniverseNet-20.08 achieves the highest results on all datasets, resulting in 52.1% mCAP. In most cases, methods that work on COCO also work on the other datasets. Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref> and ATSS <ref type="bibr" target="#b67">[69]</ref> achieve over 2% more mCAP than Faster R-CNN <ref type="bibr" target="#b45">[47]</ref> and RetinaNet <ref type="bibr" target="#b34">[36]</ref>, respectively. In some cases, methods that work on COCO show small or negative effects on WOD and M109s. Thus, USB can impose a penalty on COCO-biased methods.</p><p>To compare the effectiveness of each method on each dataset, we show the correlation between mCAP and CAP on each dataset in <ref type="figure" target="#fig_1">Figure 4</ref>. SEPC <ref type="bibr" target="#b61">[63]</ref> improves COCO CAP and deteriorates WOD CAP. Multi-stage detectors <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b6">8]</ref> show relatively high CAP on WOD and relatively low CAP on COCO. Adding GFL <ref type="bibr" target="#b31">[33]</ref> is especially effective on M109s (see improvements from ATSS to GFL and from UniverseNet to UniverseNet-20.08).</p><p>We also show detailed results on each dataset. <ref type="table">Table 7</ref> shows the COCO results. Since the effectiveness of existing methods has been verified on COCO, their improvements are steady. <ref type="table">Table 8</ref> shows the WOD results. Adding SEPC <ref type="bibr" target="#b61">[63]</ref> to ATSS <ref type="bibr" target="#b67">[69]</ref> decreases all metrics except for AP L . We found that this reduction does not occur at large test scales in higher USB evaluation protocols (see Sec. 5.4). UniverseNet-20.08 shows worse results than Uni-verseNet in some metrics, probably due to the light use of DCN for fast inference (see <ref type="table" target="#tab_2">Table 12e</ref>). <ref type="table">Table 9</ref> shows the M109s results. Interestingly, improvements by ATSS <ref type="bibr" target="#b67">[69]</ref> are smaller than those on COCO and WOD due to the drop of face AP. We conjecture that this phenomenon comes from the domain differences discussed in Sec. 3.2 and prior work <ref type="bibr" target="#b41">[43]</ref>, although we should explore it in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art</head><p>COCO. We show state-of-the-art methods on COCO test-dev (as of November 14, 2020) in <ref type="table" target="#tab_2">Table 10</ref>. Our UniverseNet-20.08d achieves the highest AP (51.3%) in the Standard USB 1.0 protocol. Despite 12.5× fewer epochs, the speed-accuracy trade-offs of our models are comparable to those of EfficientDet <ref type="bibr" target="#b57">[59]</ref> (see also <ref type="figure">Figure 2</ref>). With 13-scale TTA, UniverseNet-20.08d achieves the highest AP (54.1%) in the Huge USB 1.0 protocol. Even with 5-scale TTA in the Large USB 1.0, it achieves 53.8% AP, which is higher than other methods in the USB 1.0 protocols.</p><p>WOD. For comparison with state-of-the-art methods on WOD, we submitted the detection results of UniverseNet to the Waymo Open Dataset Challenge 2020 2D detection, a competition held at a CVPR 2020 workshop. The primary metric is AP/L2, a KITTI-style AP evaluated with LEVEL 2 objects <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b0">2]</ref>. We used multi-scale testing with soft-NMS <ref type="bibr" target="#b4">[6]</ref>. The shorter side pixels of test scales are (960, 1600, 2240), including 8 pixels padding. These scales enable utilizing SEPC <ref type="bibr" target="#b61">[63]</ref> (see Sec. 5.4) and detecting small objects. <ref type="table" target="#tab_2">Table 11</ref> shows the top teams' results. UniverseNet achieves 67.42% AP/L2 without multistage detectors, ensembles, expert models, or heavy backbones, unlike other top methods. RW-TSDet <ref type="bibr" target="#b27">[29]</ref> overwhelms other multi-stage detectors, whereas UniverseNet overwhelms other single-stage detectors. These two methods used light backbones and large test scales <ref type="bibr" target="#b2">[4]</ref>. Interestingly, the maximum test scales are the same (3360×2240). We conjecture that this is not a coincidence but a convergence caused by searching the accuracy saturation point.  <ref type="table" target="#tab_2">Table 10</ref>. State-of-the-art methods on COCO test-dev. We classify methods by proposed protocols. X in the Backbone column denotes ResNeXt <ref type="bibr" target="#b62">[64]</ref>. See method papers for other backbones. TTA: Test-time augmentation including horizontal flip and multi-scale testing (numbers denote scales). FPS values without and with parentheses were measured on V100 with mixed precision and other environments, respectively. We measured the FPS of GFL <ref type="bibr" target="#b31">[33]</ref> models in our environment and estimated those of ATSS <ref type="bibr" target="#b67">[69]</ref> and RelationNet++ <ref type="bibr" target="#b11">[13]</ref> based on the measured values and <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b11">13]</ref>. The settings of other methods are based on conference papers, their arXiv versions, and authors' codes. The values shown in gray were estimated from descriptions in papers and codes. Some FPS values are from <ref type="bibr" target="#b31">[33]</ref>.  <ref type="table" target="#tab_2">Table 11</ref>. Waymo Open Dataset Challenge 2020 2D detection <ref type="bibr" target="#b0">[2]</ref>.</p><p>Manga109-s. To the best of our knowledge, no prior work has reported detection results on the Manga109-s dataset (87 volumes). Although many settings differ, the state-of-the-art method on the full Manga109 dataset (109 volumes, non-public to commercial organizations) achieves 77.1-92.0% (mean: 84.2%) AP 50 on ten test volumes <ref type="bibr" target="#b41">[43]</ref>. The mean AP 50 of UniverseNet-20.08 on the 15test set (92.5%) is higher than those results. (g) UniverseNet-20.08 with different backbones.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analyses and Discussions</head><p>Chaotic state of the art. As shown in <ref type="table" target="#tab_2">Table 10</ref>, stateof-the-art detectors on the COCO benchmark were trained with various settings. Comparisons across different training epochs are especially difficult because long training does not decrease FPS, unlike large test scales. Nevertheless, the EfficientDet <ref type="bibr" target="#b57">[59]</ref>, YOLOv4 <ref type="bibr" target="#b3">[5]</ref>, and SpineNet <ref type="bibr" target="#b14">[16]</ref> papers compare methods in their tables without specifying the difference in training epochs. The compatibility of the USB training protocols (Sec. 3.3) resolves this disorder. We hope that many papers report results with the protocols for inclusive, healthy, and sustainable development of detectors.</p><p>Ablation studies. We show the results of ablation studies for UniverseNets on COCO in <ref type="table" target="#tab_2">Table 12</ref>. SEPC <ref type="bibr" target="#b61">[63]</ref>, Res2Net-v1b <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref>, DCN <ref type="bibr" target="#b12">[14]</ref>, multi-scale training, GFL <ref type="bibr" target="#b31">[33]</ref>, SyncBN <ref type="bibr" target="#b42">[44]</ref>, and iBN <ref type="bibr" target="#b61">[63]</ref> improve AP. UniverseNet-20.08d is much more accurate (48.6% AP) than other models trained for 12 epochs using ResNet-50-level backbones (e.g., ATSS: 39.4% <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b8">10]</ref>, GFL: 40.2% <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b8">10]</ref>). As shown in <ref type="table" target="#tab_2">Table 12e</ref>, UniverseNet-20.08 is 1.4× faster than UniverseNet-20.08d at the cost of a ∼1% AP drop. UniverseNet-20.08s, the variant with ResNet-50-C backbone in <ref type="table" target="#tab_2">Table 12g</ref>, shows a good speedaccuracy trade-off by achieving 45.8% AP and over 30 FPS.</p><p>Generalization. To evaluate the generalization ability, we show the results on another dataset out of the USB. We trained UniverseNet on the NightOwls <ref type="bibr" target="#b39">[41]</ref>, a dataset for person detection at night, from the WOD pre-trained model in Sec. 5.3. The top teams' results of the NightOwls Detection Challenge 2020 are shown in <ref type="table" target="#tab_2">Table 13</ref>. UniverseNet is more accurate than other methods, even without TTA, and should be faster than the runner-up method that uses larger test scales and a heavy model (Cascade R-CNN, ResNeXt-101, CBNet, Double-Head, DCN, and soft-NMS) <ref type="bibr" target="#b40">[42]</ref>.</p><p>Test scales. We show the results on WOD at different test scales in <ref type="figure" target="#fig_2">Figure 5</ref>. Single-stage detectors require larger test scales than multi-stage detectors to achieve peak performance, probably because they cannot extract features from precisely localized region proposals. Although ATSEPC shows lower AP than ATSS at the default test scale (1248×832 in Standard USB), it outperforms ATSS at larger test scales (e.g., 1920×1280 in Large USB). We conjecture that we should enlarge object scales in images to utilize SEPC <ref type="bibr" target="#b61">[63]</ref> because its DCN <ref type="bibr" target="#b12">[14]</ref> enlarges effective receptive fields. SEPC and DCN prefer large objects empirically (see <ref type="table" target="#tab_2">Tables 12a, 12f</ref>, <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b12">14]</ref>), and DCN <ref type="bibr" target="#b12">[14]</ref> cannot increase the sampling points for objects smaller than the kernel size in principle. By utilizing the characteristics of SEPC and multi-scale training, UniverseNets achieve the highest AP in a wide range of test scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduced USB, a benchmark for universal-scale object detection. To resolve unfair comparisons in existing benchmarks, we established USB training/evaluation protocols. Our UniverseNets achieved state-of-the-art results on USB and existing benchmarks. We found some weaknesses in the existing methods to be addressed in future research.</p><p>There are three limitations in this work. (1) USB depends on datasets with many instances. Reliable scale-wise metrics for small datasets should be considered. (2) We adopted single-stage detectors for UniverseNets and trained detectors in the USB 1.0 protocol. Although these settings are practical, it is worth exploring multi-stage detectors in higher protocols. The proposed USB protocols can be applied to other tasks with modifications. We believe that our work is an important step toward recognizing universal-scale objects by connecting various experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Components for Multi-Scale Object Detection</head><p>Backbones and modules. Inception module <ref type="bibr" target="#b54">[56]</ref> arranges 1×1, 3×3, and 5×5 convolutions to cover multiscale regions. Residual block <ref type="bibr" target="#b24">[26]</ref> adds multi-scale features from shortcut connections and 3×3 convolutions. ResNet-C and ResNet-D <ref type="bibr" target="#b25">[27]</ref> replace the first layer of ResNet with the deep stem (three 3×3 convolutions) <ref type="bibr" target="#b55">[57]</ref>. Res2Net module <ref type="bibr" target="#b17">[19]</ref> stacks 3×3 convolutions hierarchically to represent multi-scale features. Res2Net-v1b <ref type="bibr" target="#b18">[20]</ref> adopts deep stem with Res2Net module. Deformable convolution module in Deformable Convolutional Networks (DCN) <ref type="bibr" target="#b12">[14]</ref> adjusts receptive field adaptively by deforming the sampling locations of standard convolutions. These modules are mainly used in backbones.</p><p>Necks. To combine and enhance backbones' representation, necks follow backbones. Feature Pyramid Networks (FPN) <ref type="bibr" target="#b33">[35]</ref> adopt top-down path and lateral connections like architectures for semantic segmentation. Scale-Equalizing Pyramid Convolution (SEPC) <ref type="bibr" target="#b61">[63]</ref> introduces pyramid convolution across feature maps with different resolutions and utilizes DCN to align the features.</p><p>Heads and training sample selection. Faster R-CNN <ref type="bibr" target="#b45">[47]</ref> spreads multi-scale anchors over a feature map. SSD <ref type="bibr" target="#b37">[39]</ref> spreads multi-scale anchors over multiple feature maps with different resolutions. ATSS <ref type="bibr" target="#b67">[69]</ref> eliminates the need for multi-scale anchors by dividing positive and negative samples according to object statistics across pyramid levels.</p><p>Multi-scale training and testing. Traditionally, the image pyramid is an essential technique to handle multi-scale objects <ref type="bibr" target="#b46">[48]</ref>. Although recent detectors can output multiscale objects from a single-scale input, many works use multi-scale inputs to improve performance <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b61">63]</ref>. In a popular implementation <ref type="bibr" target="#b8">[10]</ref>, multi-scale training randomly chooses a scale at each iteration for (training-time) data augmentation. Multi-scale testing infers multi-scale inputs and merges their outputs for Test-Time Augmentation (TTA). SNIP <ref type="bibr" target="#b51">[53]</ref> limits the range of object scales at each image scale during training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Protocols</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Dataset Splits for Manga109-s</head><p>The Manga109-s dataset (87 volumes) is a subset of the full Manga109 dataset (109 volumes) <ref type="bibr" target="#b1">[3]</ref>. Unlike the full Manga109 dataset, the Manga109-s dataset can be used by commercial organizations. The dataset splits for the full Manga109 dataset used in prior work <ref type="bibr" target="#b41">[43]</ref> cannot be used for the Manga109-s dataset.  We defined the Manga109-s dataset splits shown in Table <ref type="bibr" target="#b12">14</ref>. Unlike alphabetical order splits used in the prior work <ref type="bibr" target="#b41">[43]</ref>, we selected the volumes carefully. The 15test set was selected to be well-balanced for reliable evaluation. Five volumes in the 15test set were selected from the 10 test volumes used in <ref type="bibr" target="#b41">[43]</ref> to enable partially direct comparison. All the authors of the 15test and 4val set are different from those of the 68train set to evaluate generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Number of Images</head><p>There are 118,287 images in COCO train2017, 5,000 in COCO val2017, 79,735 in WOD f0train, 20,190 in WOD f0val, 6,760 in M109s 68train, 419 in M109s 4val, and 1,354 in M109s 15test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Exceptions</head><p>The rounding error of epochs between epoch-and iteration-based training can be ignored when calculating the maximum epochs. Small differences of eight pixels or less can be ignored when calculating the maximum resolution. For example, DSSD513 <ref type="bibr" target="#b16">[18]</ref> will be compared in Mini USB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of UniverseNets</head><p>We show the detailed architectures of UniverseNets in <ref type="table" target="#tab_2">Table 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Details of Experiments</head><p>Here, we show the details of experimental settings and results. See also the code to reproduce our settings including minor hyperparameters.   <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref>. PConv (Pyramid Convolution) and iBN (integrated Batch Normalization) are the components of SEPC <ref type="bibr" target="#b61">[63]</ref>. The DCN columns indicate where to apply DCN. "P": The PConv modules in the combined head of SEPC <ref type="bibr" target="#b61">[63]</ref>. "LC": The extra head of SEPC for localization and classification <ref type="bibr" target="#b61">[63]</ref>. "c3-c5": conv3 x, conv4 x, and conv5 x layers in ResNet-style backbones <ref type="bibr" target="#b24">[26]</ref>. "c5": conv5 x layers in ResNet-style backbones <ref type="bibr" target="#b24">[26]</ref>. ATSEPC: ATSS with SEPC (without iBN). MStrain: Multi-scale training. FPS: Frames per second on one V100 with mixed precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Common Settings</head><p>We follow the learning rate schedules of MMDetection <ref type="bibr" target="#b8">[10]</ref>, which are similar to those of Detectron <ref type="bibr" target="#b20">[22]</ref>. Specifically, the learning rates are reduced by 10× in two predefined epochs. Epochs for the first learning rate decay, the second decay, and ending training are <ref type="bibr" target="#b6">(8,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12)</ref> for the 1× schedule, <ref type="bibr" target="#b14">(16,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b22">24)</ref> for the 2× schedule, and <ref type="bibr" target="#b14">(16,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20)</ref> for the 20e schedule. To avoid overfitting by small learning rates <ref type="bibr" target="#b50">[52]</ref>, the 20e schedule is reasonable.</p><p>We mainly used ImageNet pre-trained backbones that are standard in MMDetection <ref type="bibr" target="#b8">[10]</ref>. Some pre-trained Res2Net backbones not supported in MMDetection were downloaded from the Res2Net repository <ref type="bibr" target="#b18">[20]</ref>. We trained most models with mixed precision and 4 GPUs (× 4 images per GPU). All results on USB and all results of Uni-verseNets are single model results without ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Settings on COCO</head><p>For comparison with state-of-the-art methods with TTA on COCO, we used soft voting with 13-scale testing and horizontal flipping following the original implementation of ATSS <ref type="bibr" target="#b67">[69]</ref>. Specifically, shorter side pixels are (400, 500, 600, 640, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1800), while longer side pixels are their 1.667×. For the 13 test scales, target objects are limited to corresponding 13 predefined ranges ((96, ∞), (96, ∞), (64, ∞), (64, ∞), (64, ∞), (0, ∞), (0, ∞), (0, ∞), (0, 256), (0, 256), (0, 192), (0, 192), (0, 96)), where each tuple denotes the minimum and maximum object scales. Each object scale is measured by √ wh, where w and h denote the object's width and height, respectively. We also evaluated 5-scale TTA because the above-mentioned ATSS-style TTA is slow. We picked (400, 600, 800, 1000, 1200) for shorter side pixels, and ((96, ∞), (64, ∞), (0, ∞), (0, ∞), (0, 256)) for object scale ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Settings on NightOwls</head><p>NightOwls <ref type="bibr" target="#b39">[41]</ref> is a dataset for person detection at night. It contains three categories (pedestrian, bicycle driver, and motorbike driver). In contrast to WOD, it is important to detect medium or large objects because the evaluation of NightOwls follows the reasonable setting <ref type="bibr" target="#b13">[15]</ref> where small objects (less than 50 pixels tall) are ignored. We prevented the overfitting of the driver categories (bicycle driver and motorbike driver) in two ways. The first is to map the classifier layer of the WOD pre-trained model. We transferred the weights for cyclists learned on the richer WOD to those for the NightOwls driver categories. The second is early stopping. We trained the model for 2 epochs (4,554 iterations) without background images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Ablation Studies for UniverseNets</head><p>We describe the results of ablation studies for Uni-verseNets on COCO in more detail. As shown in <ref type="table" target="#tab_2">Table 12a</ref>, ATSEPC (ATSS <ref type="bibr" target="#b67">[69]</ref> with SEPC without iBN <ref type="bibr" target="#b61">[63]</ref>) outperforms ATSS by a large margin. The effectiveness of SEPC for ATSS is consistent with those for other detectors reported in the SEPC paper <ref type="bibr" target="#b61">[63]</ref>. As shown in <ref type="table" target="#tab_2">Table 12b</ref>, Uni-verseNet further improves AP metrics by ∼5% by adopting Res2Net-v1b <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref>, DCN <ref type="bibr" target="#b12">[14]</ref>, and multi-scale training. As shown in <ref type="table" target="#tab_2">Table 12c</ref>, adopting GFL <ref type="bibr" target="#b31">[33]</ref> improves AP by 0.8%. There is room for improvement of AP S in the Quality Focal Loss of GFL <ref type="bibr" target="#b31">[33]</ref>. As shown in <ref type="table" target="#tab_2">Table 12d</ref>, UniverseNet-20.08d achieves 48.6% AP by making more use of BatchNorm (SyncBN <ref type="bibr" target="#b42">[44]</ref> and iBN <ref type="bibr" target="#b61">[63]</ref>). It is much more accurate than other models trained for 12 epochs using ResNet-50-level backbones (e.g., ATSS: 39.4% <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b8">10]</ref>, GFL: 40.2% <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b8">10]</ref>). On the other hand, the inference is not so fast (less than 20 FPS) due to the heavy use of DCN <ref type="bibr" target="#b12">[14]</ref>. UniverseNet-20.08 speeds up inference by the light use of DCN <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b61">63]</ref>. As shown in <ref type="table" target="#tab_2">Table 12e</ref>, UniverseNet-20.08 is 1.4× faster than UniverseNet-20.08d at the cost of a ∼1% AP drop. To further verify the effectiveness of each technique, we conducted ablation from UniverseNet-20.08 shown in <ref type="table" target="#tab_2">Table 12f</ref>. All techniques contribute to the high AP of UniverseNet-20.08. Ablating the Res2Net-v1b backbone (replacing Res2Net-50-v1b <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref> with ResNet-50-B <ref type="bibr" target="#b25">[27]</ref>) has the largest effects. Res2Net-v1b improves AP by 2.8% and increases the inference time by 1.3×. To further investigate the effectiveness of backbones, we trained variants of UniverseNet-20.08 as shown in <ref type="table" target="#tab_2">Table 12g</ref>. Although the Res2Net module <ref type="bibr" target="#b17">[19]</ref> makes inference slower, the deep stem used in ResNet-50-C <ref type="bibr" target="#b25">[27]</ref> and Res2Net-50-v1b <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref> improves AP metrics with similar speeds. UniverseNet-20.08s (the variant using ResNet-50-C backbone) shows a good speed-accuracy trade-off by achieving 45.8% AP and over 30 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Differences by Metrics</head><p>To analyze differences by metrics, we evaluated the KITTI-style AP (KAP) on WOD. KAP is a metric used in benchmarks for autonomous driving <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b53">55]</ref>. Using different IoU thresholds (0.7 for vehicles, and 0.5 for pedestrians and cyclists), KAP is calculated as KAP = (AP 0.7,veh. + AP 0.5,ped. + AP 0.5,cyc. )/3. The results of KAP are shown in <ref type="figure" target="#fig_4">Figure 6b</ref>. For ease of comparison, we show again the results of CAP in <ref type="figure" target="#fig_4">Figure 6a</ref>. GFL <ref type="bibr" target="#b31">[33]</ref> and Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>, which focus on localization quality, are less effective for KAP.  <ref type="table" target="#tab_2">Table 16</ref>. UniverseNet-20.08 fine-tuned on Manga109-s 15test from different pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6. Effects of COCO Pre-Training</head><p>To verify the effects of COCO pre-training, we trained UniverseNet-20.08 on M109s from different pre-trained models. <ref type="table" target="#tab_2">Table 16</ref> shows the results. COCO pre-training improves all the metrics, especially body AP.</p><p>We also trained models with the eight methods on M109s from ImageNet pre-trained backbones. We halved the learning rates in <ref type="table" target="#tab_8">Table 5</ref> and doubled warmup iterations <ref type="bibr" target="#b21">[23]</ref> (from 500 to 1,000) because the training of single-stage detectors without COCO pre-training or SyncBN <ref type="bibr" target="#b42">[44]</ref> is unstable. The CAP without COCO pre-training is 1.9% lower than that with COCO pre-training <ref type="table" target="#tab_9">(Table 6</ref>) on average.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Hyperparameters</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Correlation between mCAP and CAP on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Test scales vs. CAP on WOD f0val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 3 )</head><label>3</label><figDesc>The architectures and results of Uni-verseNets are still biased toward COCO due to ablation studies and pre-training on COCO. Less biased and more universal detectors should be developed in future research.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Test scales vs. different AP metrics on WOD f0val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 3. Object scale distributions. USB covers extensive scale variations quantitatively. The relative scale is the square root of the ratio of bounding box area to image area<ref type="bibr" target="#b66">[68,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b27">29]</ref>.</figDesc><table><row><cell>Cumulative distribution</cell><cell>0.0 0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8 COCO WOD Manga109-s 1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Relative scale</cell><cell></cell></row></table><note>mainly evaluate three cat- egories (car, pedestrian, and cyclist) in their leaderboards. For generic object detection, PASCAL VOC [17] and COCO [37] include 20 and 80 categories, respectively. The</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Characteristics of datasets. USB covers many qualitative variations related to scales and domains. * : Few RGB images.</figDesc><table><row><cell cols="2">Benchmark Dataset</cell><cell cols="3">Domain Classes Boxes Images B/I</cell></row><row><cell></cell><cell>COCO [37]</cell><cell>Natural</cell><cell>80</cell><cell>897 k 123 k 7.3</cell></row><row><cell>USB (Ours)</cell><cell>WOD [55] v1.2 f0</cell><cell>Traffic</cell><cell>3</cell><cell>1.0 M 100 k 10.0</cell></row><row><cell></cell><cell>Manga109-s [40, 3]</cell><cell>Artificial</cell><cell>4</cell><cell>401 k 8.5 k 47.0</cell></row><row><cell></cell><cell cols="2">COCO [37] val2014 Natural</cell><cell>80</cell><cell>292 k 41 k 7.2</cell></row><row><cell>UODB [62]</cell><cell>KITTI [21]</cell><cell>Traffic</cell><cell>3</cell><cell>35 k 7.5 k 4.7</cell></row><row><cell></cell><cell>Comic2k [30]</cell><cell>Artificial</cell><cell>6</cell><cell>6.4 k 2.0 k 3.2</cell></row><row><cell cols="5">Table 2. Statistics of datasets in USB and counterpart datasets in</cell></row><row><cell cols="5">UODB [62]. Values are based on publicly available annotations.</cell></row><row><cell cols="3">B/I: Average number of boxes per image.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ProtocolMax reso.Typical scale Reference Standard USB 1,066,667 1333× 800 Popular in COCO [37, 10, 22] Mini USB 262,144 512× 512 Popular in VOC [17, 39] Micro USB 50,176 224× 224 Popular in ImageNet<ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b24">26]</ref> </figDesc><table><row><cell>Large USB</cell><cell cols="3">2,457,600 1920×1280 WOD front cameras [55]</cell></row><row><cell>Huge USB</cell><cell cols="3">7,526,400 3360×2240 WOD top methods ([29], ours)</cell></row><row><cell>Freestyle</cell><cell>∞</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>USB evaluation protocols.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Default hyperparameters.</figDesc><table /><note>* : Shorter side pixels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Method mCAP AP 50 AP 75 AP S AP M AP L COCO WOD M109s Faster R-CNN [47] 45.9 68.2 49.1 15.2 38.9 62.5 37.4 34.5 65.8 Cascade R-CNN [8] 48.1 68.5 51.5 15.6 41.3 65.9 40.3 36.4 67.6 RetinaNet [36] 44.8 66.0 47.4 12.9 37.3 62.6 36.5 32.5 65.3 ATSS [69] 47.1 68.0 50.2 15.5 39.5 64.7 39.4 35.4 66.5 ATSEPC [69, 63] 48.1 68.5 51.2 15.5 40.5 66.8 42.1 35.0 67.1 GFL [33] 47.7 68.3 50.6 15.8 39.9 65.8 40.2 35.7 67.3 UniverseNet 51.4 72.1 55.1 18.4 45.0 70.7 46.7 38.6 68.9 UniverseNet-20.08 52.1 72.9 55.5 19.2 45.8 70.8 47.5 39.0 69.9 Benchmark results on USB.</figDesc><table><row><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell>UniverseNet-20.08</cell></row><row><cell>COCO CAP</cell><cell>44 36 38 40 42 44 46</cell><cell>46</cell><cell>48</cell><cell>50</cell><cell>52</cell><cell>WOD CAP</cell><cell>44 32 33 34 35 36 37 38</cell><cell>46</cell><cell>48</cell><cell>50</cell><cell>52</cell><cell>M109s CAP</cell><cell>44 65 66 67 68 69</cell><cell>46 RetinaNet 48 Faster R-CNN 50 Cascade R-CNN 52 ATSS ATSEPC GFL UniverseNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mCAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mCAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mCAP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .Table 8 .Table 9 .</head><label>789</label><figDesc>Method AP AP50 AP75 APS APM APL Faster R-CNN [47] 37.4 58.1 40.4 21.2 41.0 48.1 Cascade R-CNN [8] 40.3 58.6 44.0 22.5 43.8 52.9 RetinaNet [36] 36.5 55.4 39.1 20.4 40.3 48.1 ATSS [69] 39.4 57.6 42.8 23.6 42.9 50.3 ATSEPC [69, 63] 42.1 59.9 45.5 24.6 46.1 55.0 GFL [33] 40.2 58.4 43.3 23.3 44.0 52.2 UniverseNet 46.7 65.0 50.7 29.2 50.6 61.4 UniverseNet-20.08 47.5 66.0 51.9 28.9 52.1 61.9 Results on COCO minival. AP AP50 AP75 APS APM APL veh. ped. cyc. Faster 34.5 55.3 36.3 6.0 35.8 67.4 42.7 34.6 26.1 Cascade 36.4 56.3 38.6 6.5 38.1 70.6 44.5 36.3 28.5 RetinaNet 32.5 52.2 33.7 2.6 32.8 67.9 40.0 32.5 25.0 ATSS 35.4 56.2 37.0 6.1 36.6 69.8 43.6 35.6 27.0 ATSEPC 35.0 55.3 36.5 5.8 35.5 70.5 43.5 35.3 26.3 GFL 35.7 56.0 37.1 6.2 36.7 70.7 44.0 36.0 27.1 Univ 38.6 59.8 40.9 7.4 41.0 74.0 46.0 37.6 32.3 Univ20.08 39.0 60.2 40.4 8.3 41.7 73.3 47.1 38.7 31.0 Results on WOD f0val. AP AP50 AP75 APS APM APL body face frame text Faster 65.8 91.1 70.6 18.4 39.9 72.1 58.3 47.5 90.1 67.1 Cascade 67.6 90.6 72.0 17.9 41.9 74.3 60.8 48.2 92.5 69.0 RetinaNet 65.3 90.5 69.5 15.7 38.9 71.9 58.3 46.3 88.8 67.7 ATSS 66.5 90.1 70.8 16.8 38.9 74.0 60.9 44.6 91.3 69.0 ATSEPC 67.1 90.2 71.5 16.2 39.8 74.9 62.3 44.6 92.1 69.4 GFL 67.3 90.6 71.5 17.9 38.9 74.4 61.7 45.7 92.2 69.4 Univ 68.9 91.4 73.7 18.7 43.4 76.6 65.8 46.6 93.0 70.3 Univ20.08 69.9 92.5 74.3 20.5 43.6 77.1 66.6 48.0 93.7 71.2 Results on Manga109-s 15test. Max test scale TTA FPS AP AP 50 AP 75 AP S AP M AP</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell></row><row><cell>Protocol</cell><cell>Method</cell><cell>Backbone</cell><cell cols="5">DCN Epoch L Reference</cell></row><row><cell cols="2">Standard USB 1.0 Faster R-CNN [47, 35]</cell><cell>ResNet-101</cell><cell>22</cell><cell>1333× 800</cell><cell cols="3">(14.2) 36.2 59.1 39.0 18.2 39.0 48.2 CVPR17</cell></row><row><cell cols="2">Standard USB 1.0 Cascade R-CNN [8]</cell><cell>ResNet-101</cell><cell>19</cell><cell>1312× 800</cell><cell cols="3">(11.9) 42.8 62.1 46.3 23.7 45.5 55.2 CVPR18</cell></row><row><cell cols="2">Standard USB 1.0 RetinaNet [36]</cell><cell>ResNet-101</cell><cell>18</cell><cell>1333× 800</cell><cell cols="3">(13.6) 39.1 59.1 42.3 21.8 42.7 50.2 ICCV17</cell></row><row><cell cols="2">Standard USB 1.0 FCOS [61]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">( 8.9) 44.7 64.1 48.4 27.6 47.5 55.6 ICCV19</cell></row><row><cell cols="2">Standard USB 1.0 ATSS [69]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">10.6 47.7 66.5 51.9 29.7 50.8 59.4 CVPR20</cell></row><row><cell cols="3">Standard USB 1.0 FreeAnchor+SEPC [63] X-101 (64×4d)</cell><cell>24</cell><cell>1333× 800</cell><cell>-</cell><cell cols="2">50.1 69.8 54.3 31.3 53.3 63.7 CVPR20</cell></row><row><cell cols="2">Standard USB 1.0 PAA [31]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>1333× 800</cell><cell>-</cell><cell cols="2">49.0 67.8 53.3 30.2 52.8 62.2 ECCV20</cell></row><row><cell cols="2">Standard USB 1.0 PAA [31]</cell><cell>X-152 (32×8d)</cell><cell>24</cell><cell>1333× 800</cell><cell>-</cell><cell cols="2">50.8 69.7 55.1 31.4 54.7 65.2 ECCV20</cell></row><row><cell cols="2">Standard USB 1.0 RepPoints v2 [12]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">( 3.8) 49.4 68.9 53.4 30.3 52.1 62.3 NeurIPS20</cell></row><row><cell cols="2">Standard USB 1.0 RelationNet++ [13]</cell><cell>X-101 (64×4d)</cell><cell>20</cell><cell>1333× 800</cell><cell cols="3">10.3 50.3 69.0 55.0 32.8 55.0 65.8 NeurIPS20</cell></row><row><cell cols="2">Standard USB 1.0 GFL [33]</cell><cell>ResNet-50</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">37.2 43.1 62.0 46.8 26.0 46.7 52.3 NeurIPS20</cell></row><row><cell cols="2">Standard USB 1.0 GFL [33]</cell><cell>ResNet-101</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">29.5 45.0 63.7 48.9 27.2 48.8 54.5 NeurIPS20</cell></row><row><cell cols="2">Standard USB 1.0 GFL [33]</cell><cell>ResNet-101</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">22.8 47.3 66.3 51.4 28.0 51.1 59.2 NeurIPS20</cell></row><row><cell cols="2">Standard USB 1.0 GFL [33]</cell><cell>X-101 (32×4d)</cell><cell>24</cell><cell>1333× 800</cell><cell cols="3">15.4 48.2 67.4 52.6 29.2 51.7 60.2 NeurIPS20</cell></row><row><cell cols="2">Standard USB 1.0 UniverseNet-20.08s</cell><cell>ResNet-50-C</cell><cell>24</cell><cell>1333× 800</cell><cell cols="2">31.6 47.4 66.0 51.4 28.3 50.8 59.5</cell><cell>(Ours)</cell></row><row><cell cols="2">Standard USB 1.0 UniverseNet-20.08</cell><cell>Res2Net-50-v1b</cell><cell>24</cell><cell>1333× 800</cell><cell cols="2">24.9 48.8 67.5 53.0 30.1 52.3 61.1</cell><cell>(Ours)</cell></row><row><cell cols="2">Standard USB 1.0 UniverseNet-20.08d</cell><cell>Res2Net-101-v1b</cell><cell>20</cell><cell>1333× 800</cell><cell cols="2">11.7 51.3 70.0 55.8 31.7 55.3 64.9</cell><cell>(Ours)</cell></row><row><cell>Large USB 1.0</cell><cell>UniverseNet-20.08d</cell><cell>Res2Net-101-v1b</cell><cell>20</cell><cell>1493× 896</cell><cell cols="2">11.6 51.5 70.2 56.0 32.8 55.5 63.7</cell><cell>(Ours)</cell></row><row><cell>Large USB 1.0</cell><cell>UniverseNet-20.08d</cell><cell>Res2Net-101-v1b</cell><cell>20</cell><cell>2000×1200</cell><cell>5 -</cell><cell>53.8 71.5 59.4 35.3 57.3 67.3</cell><cell>(Ours)</cell></row><row><cell>Huge USB 1.0</cell><cell>ATSS [69]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>3000×1800</cell><cell>13 -</cell><cell cols="2">50.7 68.9 56.3 33.2 52.9 62.4 CVPR20</cell></row><row><cell>Huge USB 1.0</cell><cell>PAA [31]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>3000×1800</cell><cell>13 -</cell><cell cols="2">51.4 69.7 57.0 34.0 53.8 64.0 ECCV20</cell></row><row><cell>Huge USB 1.0</cell><cell>PAA [31]</cell><cell>X-152 (32×8d)</cell><cell>24</cell><cell>3000×1800</cell><cell>13 -</cell><cell cols="2">53.5 71.6 59.1 36.0 56.3 66.9 ECCV20</cell></row><row><cell>Huge USB 1.0</cell><cell>RepPoints v2 [12]</cell><cell>X-101 (64×4d)</cell><cell>24</cell><cell>3000×1800</cell><cell>13 -</cell><cell cols="2">52.1 70.1 57.5 34.5 54.6 63.6 NeurIPS20</cell></row><row><cell>Huge USB 1.0</cell><cell>RelationNet++ [13]</cell><cell>X-101 (64×4d)</cell><cell>20</cell><cell>3000×1800</cell><cell>13 -</cell><cell cols="2">52.7 70.4 58.3 35.8 55.3 64.7 NeurIPS20</cell></row><row><cell>Huge USB 1.0</cell><cell>UniverseNet-20.08d</cell><cell>Res2Net-101-v1b</cell><cell>20</cell><cell>3000×1800</cell><cell>13 -</cell><cell>54.1 71.6 59.9 35.8 57.2 67.4</cell><cell>(Ours)</cell></row><row><cell>Huge USB 2.0</cell><cell>TSD [54]</cell><cell>SENet-154</cell><cell>34</cell><cell>2000×1400</cell><cell>4 -</cell><cell cols="2">51.2 71.9 56.0 33.8 54.8 64.2 CVPR20</cell></row><row><cell>Huge USB 2.5</cell><cell>DetectoRS [45]</cell><cell>X-101 (32×4d)</cell><cell>40</cell><cell>2400×1600</cell><cell>5 -</cell><cell cols="2">54.7 73.5 60.1 37.4 57.3 66.4 arXiv20</cell></row><row><cell>Mini USB 3.0</cell><cell>EfficientDet-D0 [59]</cell><cell>EfficientNet-B0</cell><cell>300</cell><cell>512× 512</cell><cell cols="3">98.0 33.8 52.2 35.8 12.0 38.3 51.2 CVPR20</cell></row><row><cell>Mini USB 3.1</cell><cell>YOLOv4 [5]</cell><cell>CSPDarknet-53</cell><cell>273</cell><cell>512× 512</cell><cell>83</cell><cell cols="2">43.0 64.9 46.5 24.3 46.1 55.2 arXiv20</cell></row><row><cell cols="2">Standard USB 3.0 EfficientDet-D2 [59]</cell><cell>EfficientNet-B2</cell><cell>300</cell><cell>768× 768</cell><cell cols="3">56.5 43.0 62.3 46.2 22.5 47.0 58.4 CVPR20</cell></row><row><cell cols="2">Standard USB 3.0 EfficientDet-D4 [59]</cell><cell>EfficientNet-B4</cell><cell cols="2">300 1024×1024</cell><cell cols="3">23.4 49.4 69.0 53.4 30.3 53.2 63.2 CVPR20</cell></row><row><cell cols="2">Standard USB 3.1 YOLOv4 [5]</cell><cell>CSPDarknet-53</cell><cell>273</cell><cell>608× 608</cell><cell>62</cell><cell cols="2">43.5 65.7 47.3 26.7 46.7 53.3 arXiv20</cell></row><row><cell>Large USB 3.0</cell><cell>EfficientDet-D5 [59]</cell><cell>EfficientNet-B5</cell><cell cols="2">300 1280×1280</cell><cell cols="3">13.8 50.7 70.2 54.7 33.2 53.9 63.2 CVPR20</cell></row><row><cell>Large USB 3.0</cell><cell>EfficientDet-D6 [59]</cell><cell>EfficientNet-B6</cell><cell cols="2">300 1280×1280</cell><cell cols="3">10.8 51.7 71.2 56.0 34.1 55.2 64.1 CVPR20</cell></row><row><cell>Large USB 3.0</cell><cell>EfficientDet-D7 [59]</cell><cell>EfficientNet-B6</cell><cell cols="2">300 1536×1536</cell><cell cols="3">8.2 52.2 71.4 56.3 34.8 55.5 64.6 CVPR20</cell></row><row><cell>Freestyle</cell><cell cols="2">RetinaNet+SpineNet [16] SpineNet-190</cell><cell cols="2">400 1280×1280</cell><cell>-</cell><cell cols="2">52.1 71.8 56.5 35.4 55.0 63.6 CVPR20</cell></row><row><cell>Freestyle</cell><cell>EfficientDet-D7x [60]</cell><cell>EfficientNet-B7</cell><cell cols="2">600 1536×1536</cell><cell cols="3">6.5 55.1 74.3 59.9 37.2 57.9 68.0 arXiv20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Method AP AP50 AP75 APS APM APL ATSS [69] 39.4 57.6 42.8 23.6 42.9 50.3 ATSEPC [69, 63] 42.1 59.9 45.5 24.6 46.1 55.0 (a) AP improvements by SEPC without iBN [63]. DCN FPS AP AP50 AP75 APS APM APL UniverseNet-20.08d heavy 17.3 48.6 67.1 52.7 30.1 53.0 63.8 UniverseNet-20.08 light 24.9 47.5 66.0 51.9 28.9 52.1 61.9(e) Speeding up by the light use of DCN<ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b61">63]</ref>.</figDesc><table><row><cell>Method</cell><cell>AP AP50 AP75 APS APM APL</cell><cell>Method</cell><cell>FPS AP AP50 AP75 APS APM APL</cell></row><row><cell>ATSEPC [69, 63] UniverseNet</cell><cell>42.1 59.9 45.5 24.6 46.1 55.0 46.7 65.0 50.7 29.2 50.6 61.4</cell><cell>UniverseNet-20.08 w/o SEPC [63]</cell><cell>24.9 47.5 66.0 51.9 28.9 52.1 61.9 26.7 45.8 64.6 50.0 27.6 50.4 59.7</cell></row><row><cell cols="2">(b) AP improvements by Res2Net-v1b [19, 20], DCN [14], and multi-scale training.</cell><cell cols="2">w/o Res2Net-v1b [19, 20] 32.8 44.7 62.8 48.4 27.1 48.8 59.5 w/o DCN [14] 27.8 45.9 64.5 49.8 28.9 49.9 59.0 w/o multi-scale training 24.8 45.9 64.5 49.6 27.4 50.5 60.1</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o SyncBN, iBN [44, 63] 25.7 45.8 64.0 50.2 27.9 50.0 59.8</cell></row><row><cell cols="2">Method UniverseNet UniverseNet+GFL 47.5 65.8 51.8 29.2 51.6 62.5 AP AP50 AP75 APS APM APL 46.7 65.0 50.7 29.2 50.6 61.4</cell><cell cols="2">(f) Ablation from UniverseNet-20.08. Replacing Res2Net-v1b backbone with ResNet-B [27] has the largest effects.</cell></row><row><cell cols="2">(c) AP improvements by GFL [33].</cell><cell>Backbone</cell><cell>FPS AP AP50 AP75 APS APM APL</cell></row><row><cell>Method</cell><cell>AP AP50 AP75 APS APM APL</cell><cell>ResNet-50-B [27] ResNet-50-C [27]</cell><cell>32.8 44.7 62.8 48.4 27.1 48.8 59.5 32.4 45.8 64.2 50.0 28.8 50.1 60.0</cell></row><row><cell cols="2">UniverseNet+GFL 47.5 65.8 51.8 29.2 51.6 62.5</cell><cell>Res2Net-50 [19]</cell><cell>25.0 46.3 64.7 50.3 28.2 50.6 60.8</cell></row><row><cell cols="2">UniverseNet-20.08d 48.6 67.1 52.7 30.1 53.0 63.8</cell><cell cols="2">Res2Net-50-v1b [19, 20] 24.9 47.5 66.0 51.9 28.9 52.1 61.9</cell></row><row><cell cols="2">(d) AP improvements by SyncBN [44], iBN [63].</cell><cell></cell><cell></cell></row></table><note>Method</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>Ablation studies on COCO minival.</figDesc><table><row><cell cols="2">Rank Method</cell><cell>Test scale</cell><cell cols="2">Test flip MR</cell></row><row><cell>1</cell><cell>UniverseNet</cell><cell>1280×800, 1536×960</cell><cell></cell><cell>5.67</cell></row><row><cell>-</cell><cell>UniverseNet</cell><cell>1280×800</cell><cell></cell><cell>7.49</cell></row><row><cell>2</cell><cell cols="2">DeepBlueAI [42] 1920×1280, 2048×1280</cell><cell></cell><cell>8.06</cell></row><row><cell>3</cell><cell>dereyly [42]</cell><cell>-</cell><cell>-</cell><cell>10.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 .</head><label>13</label><figDesc>NightOwls Detection Challenge 2020 all objects track. MR: Average Miss Rate (%) on test set under reasonable setting.</figDesc><table><row><cell></cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WOD CAP</cell><cell>38 40 36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Faster R-CNN Cascade R-CNN RetinaNet ATSS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ATSEPC</cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UniverseNet</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UniverseNet-20.08</cell></row><row><cell></cell><cell>832</cell><cell>960</cell><cell>1088</cell><cell>1216</cell><cell>1344</cell><cell>1472</cell><cell>1600</cell><cell>1728</cell><cell>1856</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Test scales (shorter side)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 .</head><label>14</label><figDesc>Manga109-s dataset splits (87 volumes in total).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>ATSS GFL PConv DCN iBN Res2 DCN SyncBN MStrainAP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>Method</cell><cell>Head</cell><cell>Neck</cell><cell>Backbone</cell><cell>Input</cell><cell>FPS</cell><cell>COCO (1× schedule)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 .</head><label>15</label><figDesc>Architectures of UniverseNets with a summary of ablation studies on COCO minival. See Sec. D.4 for step-by-step improvements. All results are based on MMDetection [10] v2. The "Head" methods (ATSS and GFL) affect losses and training sample selection.</figDesc><table /><note>Res2: Res2Net-v1b</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Pre-training AP AP 50 AP 75 AP S AP M AP L body face frame text ImageNet 68.9 92.2 73.3 19.9 42.6 75.8 64.3 47.6 93.0 70.7 COCO 1× 69.9 92.5 74.3 20.5 43.6 77.1 66.6 48.0 93.7 71.2 COCO 2× 69.8 92.3 74.0 20.5 43.4 77.0 66.5 47.8 93.8 71.2</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Dr. Hirokatsu Kataoka for helpful comments. We thank all contributors for the datasets and software libraries. The original image of <ref type="figure">Figure 1 (left)</ref> is satellite office by Taiyo FUJII (CC BY 2.0).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Waymo Open Dataset 2D detection leaderboard</title>
		<ptr target="https://waymo.com/open/challenges/2d-detection/" />
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building a manga dataset &quot;Manga109&quot; with annotations for multimedia applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Tsubota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hikaru</forename><surname>Ikuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shallow networks for highaccuracy road object-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01561</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft-NMS -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards Universal Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>UC San Diego</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">2nd place solution for Waymo Open Dataset challenge -2D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangzhuang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15507</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RepPoints v2: Verification meets regression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RelationNet++: Bridging visual representations for object detection via transformer decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SpineNet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">DSSD : Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Res2Net: A new multi-scale backbone architecture. TPAMI, 2020. 2, 5, 8, 12</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<ptr target="https://github.com/Res2Net/Res2Net-PretrainedModels" />
	</analytic>
	<monogr>
		<title level="j">Res2Net Pretrained Models</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020-05-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01365</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>1st place solutions of Waymo Open Dataset challenge 2020 -2D object detection track</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with IoU prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized Focal Loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<ptr target="https://github.com/cocodataset/cocoapi" />
		<imprint>
			<date type="published" when="2020-11-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep learning for generic object detection: A survey. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using Manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NightOwls: A pedestrians at night dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scharfenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Piegert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Mistr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Prokofyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Thiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">NightOwls detection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Shinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presentations at CVPR Workshop on Scalability in Autonomous Driving</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Object detection for comics using Manga109 annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08670</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MegDet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Detec-toRS: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding the effects of pre-training for object detectors via eigenspectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Shinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Neural Architects</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo Open Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In CVPR, 2020. 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientdet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09070v7</idno>
		<title level="m">Scalable and efficient object detection</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dashan Gao, and Nuno Vasconcelos. Towards universal object detection by domain attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scale-equalizing pyramid convolution for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Single Cascade-RCNN with backbone architecture adaption for Waymo 2D detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spnas-Noah</surname></persName>
		</author>
		<ptr target="https://sites.google.com/view/cvpr20-scalability/wod-reports" />
		<imprint>
			<date type="published" when="2020-06-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">SM-NAS: Structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scale match for tiny person detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Universenet</surname></persName>
		</author>
		<idno>c5 17.3 46.7 65.0 50.7 29.2 50.6 61.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Universenet+gfl</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>c5 17.5 47.5 65.8 51.8 29.2 51.6 62.5</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Architectures of UniverseNets with a summary of ablation studies on COCO minival</title>
		<imprint/>
	</monogr>
	<note>Table 15</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
