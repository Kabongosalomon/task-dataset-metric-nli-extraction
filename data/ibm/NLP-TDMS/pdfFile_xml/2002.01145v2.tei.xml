<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntactically Look-Ahead Attention Network for Sentence Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
							<email>kamigaito@lr.pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Syntactically Look-Ahead Attention Network for Sentence Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence compression is the task of compressing a long sentence into a short one by deleting redundant words. In sequence-to-sequence (Seq2Seq) based models, the decoder unidirectionally decides to retain or delete words. Thus, it cannot usually explicitly capture the relationships between decoded words and unseen words that will be decoded in the future time steps. Therefore, to avoid generating ungrammatical sentences, the decoder sometimes drops important words in compressing sentences. To solve this problem, we propose a novel Seq2Seq model, syntactically look-ahead attention network (SLAHAN), that can generate informative summaries by explicitly tracking both dependency parent and child words during decoding and capturing important words that will be decoded in the future. The results of the automatic evaluation on the Google sentence compression dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively. SLAHAN also improved the summarization performance on longer sentences. Furthermore, in the human evaluation, SLAHAN improved informativeness without losing readability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Sentence compression is the task of producing a shorter sentence by deleting words in the input sentence while preserving its grammaticality and important information. To compress a sentence so that it is still grammatical, tree trimming methods <ref type="bibr" target="#b8">(Jing 2000;</ref><ref type="bibr" target="#b13">Knight and Marcu 2000;</ref><ref type="bibr" target="#b1">Berg-Kirkpatrick, Gillick, and Klein 2011;</ref><ref type="bibr" target="#b4">Filippova and Altun 2013)</ref> have been utilized. However, these methods often suffer from parsing errors. As an alternative, <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> proposed a method based on sequenceto-sequence (Seq2Seq) models that do not rely on parse trees but produce fluent compression. However, the vanilla Seq2Seq model has a problem that it is not so good for compressing longer sentences.</p><p>To solve the problem, <ref type="bibr" target="#b10">Kamigaito et al. (2018)</ref> expanded Seq2Seq models to capture the relationships between longdistance words through recursively tracking dependency parents from a word with their recursive attention module.</p><p>Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Their model learns dependency trees and compresses sentences jointly to avoid the effect of parsing errors. This improvement enables their model to compress a sentence while preserving the important words and its fluency.</p><p>However, since their method focuses only on parent words, important child words of the currently decoded word would be lost in compressed sentences. That is, in Seq2Seq models, because the decoder unidirectionally compresses sentences, it cannot usually explicitly capture the relationships between decoded words and unseen words which will be decoded in the future time steps. As the result, to avoid producing ungrammatical sentences, the decoder sometimes drops important words in compressing sentences. To solve the problem, we need to track both parent and child words to capture unseen important words that will be decoded in the future time steps. <ref type="figure">Fig.1</ref> shows an example of sentence compression 1 that needs to track both parent and child words. Since the input sentence mentions the export of the plane between two countries, we have to retain the name of the plane, import country and export country in the compressed sentence.</p><p>When the decoder reads "Japan", it should recursively track both the parent and child words of "Japan". Then, it can decide to retain "hold" that is the parent of "Japan" and the syntactic head of the sentence. By retaining "hold" in the compressed sentence, it can also retain "Japan", "and" and "India" because these are the child and grandchild of "hold' (the top case in <ref type="figure">Fig.1</ref>).</p><p>When the decoder reads "hold", it should find the important phrase "Japan's export" by recursively tracking child words from "hold". The tracking also supports the decoder for retaining "talks" and "on" to produce grammatical compression (the middle case).</p><p>When the decoder reads "export", it should track child words to find the important phrase "US2 rescue plane" and retain "of" for producing grammatical compression (the bottom case).</p><p>Note that a decoder that tracks only parent words cannot find the important phrases or produce grammatical compres- <ref type="figure">Figure 1</ref>: An example sentence and its dependency tree during the decoding process. The gray words represent deleted words, and the words in black frames are currently decoded words. Already decoded words are underlined. The tracking of parent nodes is represented as blue edges, and that of child nodes is represented as red edges. The bold words represent the important words in this sentence. <ref type="figure" target="#fig_0">Figure 2</ref>: The proportion of words retained later that are linked from right to the retained words in the summary as a parent or a child word in the left-to-right decoding. sion in this example. Furthermore, <ref type="figure" target="#fig_0">Fig.2</ref> shows that tracking only parent words is not sufficient for Seq2Seq models to cover explicitly important words which will be decoded in the future time steps, especially in long sentences 2 .</p><p>To incorporate this idea into Seq2Seq models, we propose syntactically look-ahead attention network (SLA-HAN), which can generate informative summaries by considering important words that will be decoded in the future time steps by explicitly tracking both parent and child words during decoding. The recursive tracking of dependency trees in SLAHAN is represented as attention distributions and is jointly learned with generating summaries to alleviate the effect of parse errors. Furthermore, to avoid the bias of parent and child words, the importance of the information from recursively tracked parent and child words is automatically decided with our gate module.</p><p>The evaluation results on the Google sentence compression dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively. SLA-HAN also improved the summarization performance on longer sentences. In addition, the human evaluation results showed that SLAHAN improved informativeness without losing readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Base Seq2Seq Model</head><p>Sentence compression is a kind of text generation task. However, it can also be considered as a sequential tagging task, where given a sequence of input tokens x = (x 0 , ..., x n ), a sentence summarizer predicts an output label y t from specific labels ("keep", "delete" or "end of a sentence") for each corresponding input token x t (1  t  n). Note that x 0 is the start symbol of a sentence.</p><p>To generate a grammatically correct summary, we choose Seq2Seq models as our base model. For constructing a robust baseline model, we introduce recently proposed contextualized word embeddings such as ELMo <ref type="bibr" target="#b20">(Peters et al. 2018</ref>) and BERT <ref type="bibr" target="#b3">(Devlin et al. 2018)</ref> into the sentence compression task. As described later in our evaluation results, this baseline exceeds the state-of-the-art F 1 scores reported by <ref type="bibr" target="#b25">Zhao, Luo, and Aizawa (2018)</ref>.</p><p>Our base model consists of embedding, encoder, decoder, and output layers. In the embedding layer, the model extracts features from an input token x i as a vector e i as follows:</p><formula xml:id="formula_0">e i = k |F| j=1 F i,j ,<label>(1)</label></formula><p>where k represents the vector concatenation, F i,j is a vector of the j-th feature for token x i , and |F| is the number of features (at most 3). We choose features from GloVe (Pennington, Socher, and Manning 2014), ELMo or BERT vectors. Because ELMo and BERT have many layers, we treat their weighted sum as F i,j as follows:</p><formula xml:id="formula_1">F i,j = P |L| k=1 j,k · L i,j,k , j,k =exp( j,k · L i,j,k )/ P |L| l=1 exp( j,l · L i,j,l ),<label>(2)</label></formula><p>where L i,j,k represents the k-th layer of the j-th feature for the token x i , and j,k is the weight vector for the k-th layer of the j-th feature. In BERT, to align the input token and the output label, we treat the average of sub-word vectors as a single word vector. The encoder layer first converts e i into a hidden state (3) <ref type="figure">Figure 1</ref>: An example sentence and its dependency tree during the decoding process. The gray words represent deleted words, and the words in black frames are currently decoded words. Already decoded words are underlined. The tracking of parent nodes is represented as blue edges, and that of child nodes is represented as red edges. The bold words represent the important words in this sentence. sion in this example. Furthermore, <ref type="figure" target="#fig_0">Fig.2</ref> shows that tracking only parent words is not sufficient for Seq2Seq models to cover explicitly important words which will be decoded in the future time steps, especially in long sentences 2 .</p><formula xml:id="formula_2">! h i = LST M ! ✓ ( ! h i 1 , e i )</formula><p>To incorporate this idea into Seq2Seq models, we propose syntactically look-ahead attention network (SLA-HAN), which can generate informative summaries by considering important words that will be decoded in the future time steps by explicitly tracking both parent and child words during decoding. The recursive tracking of dependency trees in SLAHAN is represented as attention distributions and is jointly learned with generating summaries to alleviate the effect of parse errors. Furthermore, to avoid the bias of parent and child words, the importance of the information from recursively tracked parent and child words is automatically decided with our gate module.</p><p>The evaluation results on the Google sentence compression dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively. SLA-HAN also improved the summarization performance on longer sentences. In addition, the human evaluation results showed that SLAHAN improved informativeness without losing readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Base Seq2Seq Model</head><p>Sentence compression is a kind of text generation task. However, it can also be considered as a sequential tagging task, where given a sequence of input tokens x = (x 0 , ..., x n ), a sentence summarizer predicts an output label y t from specific labels ("keep", "delete" or "end of a sentence") for each corresponding input token x t (1 ≤ t ≤ n). Note that x 0 is the start symbol of a sentence.</p><p>To generate a grammatically correct summary, we choose Seq2Seq models as our base model. For constructing a robust baseline model, we introduce recently proposed contextualized word embeddings such as ELMo <ref type="bibr" target="#b20">(Peters et al. 2018</ref>) and BERT <ref type="bibr" target="#b3">(Devlin et al. 2018)</ref> into the sentence compression task. As described later in our evaluation results, this baseline exceeds the state-of-the-art F 1 scores reported by <ref type="bibr" target="#b25">Zhao, Luo, and Aizawa (2018)</ref>.</p><p>Our base model consists of embedding, encoder, decoder, and output layers. In the embedding layer, the model extracts features from an input token x i as a vector e i as follows:</p><formula xml:id="formula_3">e i = |F| j=1 F i,j ,<label>(1)</label></formula><p>where represents the vector concatenation, F i,j is a vector of the j-th feature for token x i , and |F| is the number of features (at most 3). We choose features from GloVe (Pennington, Socher, and Manning 2014), ELMo or BERT vectors. Because ELMo and BERT have many layers, we treat their weighted sum as F i,j as follows:</p><formula xml:id="formula_4">F i,j = |L| k=1 ψ j,k · L i,j,k , ψ j,k =exp(φ j,k · L i,j,k )/ |L| l=1 exp(φ j,l · L i,j,l ),<label>(2)</label></formula><p>where L i,j,k represents the k-th layer of the j-th feature for the token x i , and φ j,k is the weight vector for the k-th layer of the j-th feature. In BERT, to align the input token and the output label, we treat the average of sub-word vectors as a single word vector.</p><p>The encoder layer first converts e i into a hidden state </p><formula xml:id="formula_5">− → h i = LST M− → θ ( − → h i−1 , e i )</formula><formula xml:id="formula_6">i = [ − → h i , ← − h i ].</formula><p>Through this process, the encoder layer converts the embedding e into a sequence of hidden states:</p><formula xml:id="formula_7">h = (h 0 , ..., h n ).<label>(3)</label></formula><p>The final state of the backward LSTM ← − h 0 is inherited by the decoder as its initial state.</p><p>At time step t, the decoder layer encodes the concatenation of a 3-bit one-hot vector determined by the predicted label y t−1 , the final hidden state d t−1 (which we will explain later), and the token embedding e t into the decoder hidden state − → s t , by using a forward-LSTM. The output layer predicts an output label probability as follows:</p><formula xml:id="formula_8">P (y t | y &lt;t , x) =sof tmax(W o d t ) · δ yt , d t =tanh(W d [h t , − → s t ] + b d ),<label>(4)</label></formula><p>where W d is the weight matrix, b d is the bias term, W o is the weight matrix of the softmax layer, and δ yt is the binary vector where the y t -th element is set to 1 and the other elements are set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactically Look-Ahead Attention Network</head><p>In this section, we first explain the graph representation for a dependency tree that is used in SLAHAN and then introduce its entire network structure and the modules inside it. Both the graph representation and network parameters are jointly updated, as described in the later section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Representation of Dependency Relationships</head><p>We explain the details of our representation for tracking parent and child words from a word in a dependency tree. As described in <ref type="bibr" target="#b7">Hashimoto and Tsuruoka (2017)</ref>, a dependency relationship can be represented as a weighted graph.</p><p>Given sentence x = (x 0 , ..., x n ), the parent of each word x j is selected from x. We treat x 0 as a root node. We represent the probability of x j being the parent of x t in x as P head (x j |x t , x). By using P head (x j |x t , x), <ref type="bibr" target="#b10">Kamigaito et al. (2018)</ref> show that α d,t,j , a probability of x j being the d-th order parent of x t , is calculated as follows:</p><formula xml:id="formula_9">α d,t,j = n k=1 α d−1,t,k · α 1,k,j (d &gt; 1) P head (x j |x t , x) (d = 1) .<label>(5)</label></formula><p>Because the 1st line of Eq.(5) is a definition of matrix multiplication, by using a matrix A d , which satisfies A d j,t = α d,t,j , Eq.(5) is reformulated as follows:</p><formula xml:id="formula_10">A d = A d−1 A 1 .<label>(6)</label></formula><p>We call A d the d-th parent graph hereafter.</p><p>We expand Eq.(6) to capture d-th child words of a word x j . At first, we define P child (x t |x j , x), the probability of x t being a child of x j in x, P x (x j = p), the probability of x j being a parent word in x, P x (x t = c), the probability of x t being a child word in x, and P x (x j , x t ), the probability of x j and x t having a link in x. Assuming the probability of words having a link is independent of each other, the following equations are satisfied:</p><formula xml:id="formula_11">P x (x j , x t ) = P child (x t |x j , x) · P x (x j = p), P x (x j , x t ) = P head (x j |x t , x) · P x (x t = c).<label>(7)</label></formula><p>This can be reformulated as follows:</p><formula xml:id="formula_12">P child (x t |x j , x)=P head (x j |x t , x)·P x (x t = c)/P x (x j = p).<label>(8)</label></formula><p>Here, P x (x t = c) is always 1 because of the dependency tree definition, and in this formulation, we treat x j as a parent; thus, P x (x j = p) is a constant value. Therefore, we can obtain the following relationship:</p><formula xml:id="formula_13">P child (x t |x j , x) ∝ P head (x j |x t , x).<label>(9)</label></formula><p>Based on Eq.(9), we can define β d,t,j , the strength of x j being the d-th order child of x t , as follows:</p><formula xml:id="formula_14">β d,t,j = n k=1 β d−1,t,k · β 1,k,j (d &gt; 1) P head (x t |x j , x) (d = 1) .<label>(10)</label></formula><p>Similar to Eq.(5), by using a matrix B d , which satisfies B d j,t = β d,t,j , Eq.(10) is reformulated as follows:</p><formula xml:id="formula_15">B d = B d−1 B 1 .<label>(11)</label></formula><p>We call B d the d-th child graph hereafter. Note that from the definition of the 2nd lines in Eq.(5) and Eq.(10), A 1 and B 1 always satisfy B 1 tj = A 1 jt . This can be reformulated as B 1 = (A 1 ) T . Furthermore, from the definition of the transpose of a matrix, we can obtain the following formulation:</p><formula xml:id="formula_16">B d = B 1 B 1 · · · B 1 = (A 1 ) T (A 1 ) T · · · (A 1 ) T = (A d ) T .<label>(12)</label></formula><p>Thus, once we calculate Eq.(6), we do not need to compute Eq.(11) explicitly. Therefore, letting d be a dimension size of hidden vectors, the computational cost of SLAHAN is O(n 2 d 2 ), similar to <ref type="bibr" target="#b10">Kamigaito et al. (2018)</ref>. This is based on the assumption that d is larger than n in many cases. Note that the computational cost of the base model is O(nd 2 ). <ref type="figure" target="#fig_1">Fig.3</ref> shows the entire structure of SLAHAN. It is constructed on our base model, as described in the previous section. After encoding the input sentence, the hidden states are passed to our network modules. The functions of each module are as follows: Head Attention module makes a dependency graph of a sentence by calculating the probability of x j being the parent of x t based on h j and h t in Eq.(3) for each x t . Parent Recursive Attention module calculates d-th parent graph A d and extracts a weighted sum of important hidden states µ parent t from h in Eq.(3) based on α d,t,j (= A d j,t ) for each decoder time step t. Child Recursive Attention module uses d-th child graph B d to extract µ child t , a weighted sum of important hidden states from h in Eq.(3) based on β d,t,j (= B d j,t ) for each decoder time step t. Selective Gate module supports the decoder to capture important words that will be decoded in the future by calculating Ω t , the weighted sum of µ parent t and µ child t , based on the current context. Ω t is inherited to the decoder for deciding the output label y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structure</head><p>The details of each module are described in the following subsections.</p><p>Head Attention Similar to <ref type="bibr" target="#b24">Zhang, Cheng, and Lapata (2017)</ref>, we calculate P head (x j |x t , x) as follows: </p><formula xml:id="formula_17">P head (x j |x t , x) = sof tmax(g(h j , h t )) · δ xj , g(h j , h t ) = v T a · tanh(U a · h j + W a · h t ),<label>(13)</label></formula><formula xml:id="formula_18">! h 0 ! h 1 ! h 2 ! h 3 ! h 4 ! h 5 ! h 6 ! h 7 ! h 8 h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8</formula><p>Head Attention where v a , U a and W a are weight matrices of g. In a dependency tree, the root has no parent, and a token does not depend on itself. In order to satisfy these rules, we impose the following constraints on P head (x j |x t , x):</p><formula xml:id="formula_19">h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 Max Pooling h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 Max Pooling h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 Max Pooling h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 Max Pooling h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 ⌃ h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 ⌃ h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 ⌃ h 0 h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 ⌃ d=1 d=2 d=3 d=4 d=1 d=2 d=3 d=4 Parent Recursive Atttention Child Recursive Attention ⌃ ⌃ Selective Gate ⇥ ⇥ ⌃ tied ! s 1 ! s 3 ! s 4</formula><formula xml:id="formula_20">P head (x j |x t , x) = ( 1 (t = 0^j = 0) 0 (t = 0^j &gt; 0) 0 (t 6 = 0^t = j).<label>(14)</label></formula><p>The 1st and 2nd lines of Eq. <ref type="formula" target="#formula_0">(14)</ref> represent the case where the parent of root is also root. These imply that root does not have a parent. The 3rd line of Eq. <ref type="formula" target="#formula_0">(14)</ref> prevents a token from depending on itself. In the training phase, P head (x j |x t , x) is jointly learned with output label probability P (y | x), as described in the objective function section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Recursive Attention</head><p>The parent recursive attention module recursively calculates ↵ d,t,j by using P head (x j |x t , x) based on Eq.(5). The calculated ↵ d,t,j is used to weight the bi-LSTM hidden layer h as follows:</p><formula xml:id="formula_21">d,t = P n k=j ↵ d,t,j · h j .<label>(15)</label></formula><p>To select suitable dependency order d for the input sentence, d,t is further weighted and summed to µ parent t by using weighting parameter ⌘ d,t , according to the current context as follows:</p><formula xml:id="formula_22">c t = [ h 0 , ! h n , h t , ! s t ], ⌘ d,t = sof tmax( d,t W parent d c t ) · d , µ parent t = P d2d ⌘ d,t · d,t ,<label>(16)</label></formula><p>where W parent d is the weight matrix, d is the group of dependency orders, and c t is the vector representing the current context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Child Recursive Attention</head><p>The child recursive attention module weights the bi-LSTM hidden layer h based on d-th child graph B d . Unlike the parent recursive attention module, B d is not a probability, and a word sometimes has more than two children. For that reason, we use max-pooling rather than the attention distribution in Eq.(15). In the child recursive attention module, the bi-LSTM hidden layer h is weighted by d,t,j and then pooled as follows:</p><formula xml:id="formula_23">⇢ d,t = MaxP ool(k n k=j ( d,t,j · h j ) T ).</formula><p>(17) To select suitable dependency order d for the input sentence, ⇢ d,t is further weighted and summed to µ child t by using weighting parameter ⌘ d,t , according to the current context as follows:</p><formula xml:id="formula_24">⌘ d,t = sof tmax(⇢ d,t W child d c t ) · d , µ child t = P d2d ⌘ d,t · ⇢ d,t ,<label>(18)</label></formula><p>where W child d is the weight matrix.</p><p>Selective Gate This module calculates ⌦ t , a weighted sum of parent information µ parent t and child information µ child t . The weight is decided by a gate z t by considering whether µ parent t or µ child t is more important in the current context. Specifically, ⌦ t is calculated as follows:</p><formula xml:id="formula_25">⌦ t = z t µ parent t + (1 z t ) µ child t , z t = (W z [µ parent t , µ child t , c t ]),<label>(19)</label></formula><p>where is the element-wise product, is the sigmoid function, and W z is the weight matrix. Then, d t in Eq.(4) is replaced by a concatenated vector d 0 t = [h t , ⌦ t , ! s t ]; furthermore, instead of d t , d 0 t is also fed to the decoder input at t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Function</head><p>To alleviate the effect of parse errors, we jointly update dependency parent probability P head (x j |x t ) and label probability P (y|x) <ref type="bibr" target="#b10">(Kamigaito et al. 2018)</ref>. We denote the existence of an edge between parent word w j and child word w t on a dependency tree as a t,j = 1. In contrast, we denote the absence of an edge as a t,j = 0. By using these notations, our objective function is defined as follows:</p><formula xml:id="formula_26">logP (y|x) · P n j=1 P n t=1 a t,j · log↵ 1,t,j ,<label>(20)</label></formula><p>where is a hyper-parameter balancing the importance of output labels and parse trees in training steps. To investigate the importance of the syntactic information, we used = 1.0 for the with syntax (w/ syn) setting and = 0 for the without syntax (w/o syn) setting. where v a , U a and W a are weight matrices of g. In a dependency tree, the root has no parent, and a token does not depend on itself. In order to satisfy these rules, we impose the following constraints on P head (x j |x t , x):</p><formula xml:id="formula_27">P head (x j |x t , x) = 1 (t = 0 ∧ j = 0) 0 (t = 0 ∧ j &gt; 0) 0 (t = 0 ∧ t = j).<label>(14)</label></formula><p>The 1st and 2nd lines of Eq. <ref type="formula" target="#formula_0">(14)</ref> represent the case where the parent of root is also root. These imply that root does not have a parent. The 3rd line of Eq.(14) prevents a token from depending on itself. In the training phase, P head (x j |x t , x) is jointly learned with output label probability P (y | x), as described in the objective function section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Recursive Attention</head><p>The parent recursive attention module recursively calculates α d,t,j by using P head (x j |x t , x) based on Eq.(5). The calculated α d,t,j is used to weight the bi-LSTM hidden layer h as follows:</p><formula xml:id="formula_28">γ d,t = n k=j α d,t,j · h j .<label>(15)</label></formula><p>To select suitable dependency order d for the input sentence, γ d,t is further weighted and summed to µ parent t by using weighting parameter η d,t , according to the current context as follows:</p><formula xml:id="formula_29">c t = [ ← − h 0 , − → h n , h t , − → s t ], η d,t = sof tmax(γ d,t W parent d c t ) · δ d , µ parent t = d∈d η d,t · γ d,t ,<label>(16)</label></formula><p>where W parent d is the weight matrix, d is the group of dependency orders, and c t is the vector representing the current context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Child Recursive Attention</head><p>The child recursive attention module weights the bi-LSTM hidden layer h based on d-th child graph B d . Unlike the parent recursive attention module, B d is not a probability, and a word sometimes has more than two children. For that reason, we use max-pooling rather than the attention distribution in Eq.(15). In the child recursive attention module, the bi-LSTM hidden layer h is weighted by β d,t,j and then pooled as follows:</p><formula xml:id="formula_30">ρ d,t = M axP ool( n k=j (β d,t,j · h j ) T ).</formula><p>(17) To select suitable dependency order d for the input sentence, ρ d,t is further weighted and summed to µ child t by using weighting parameter η d,t , according to the current context as follows:</p><formula xml:id="formula_31">η d,t = sof tmax(ρ d,t W child d c t ) · δ d , µ child t = d∈d η d,t · ρ d,t ,<label>(18)</label></formula><p>where W child d is the weight matrix.</p><p>Selective Gate This module calculates Ω t , a weighted sum of parent information µ parent t and child information µ child t . The weight is decided by a gate z t by considering whether µ parent t or µ child t is more important in the current context. Specifically, Ω t is calculated as follows:</p><formula xml:id="formula_32">Ω t = z t • µ parent t + (1 − z t ) • µ child t , z t = σ(W z [µ parent t , µ child t , c t ]),<label>(19)</label></formula><p>where • is the element-wise product, σ is the sigmoid function, and W z is the weight matrix. Then, d t in Eq.(4) is replaced by a concatenated vector d t = [h t , Ω t , − → s t ]; furthermore, instead of d t , d t is also fed to the decoder input at t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Function</head><p>To alleviate the effect of parse errors, we jointly update dependency parent probability P head (x j |x t ) and label probability P (y|x) <ref type="bibr" target="#b9">(Kamigaito et al. 2017)</ref>. We denote the existence of an edge between parent word w j and child word w t on a dependency tree as a t,j = 1. In contrast, we denote the absence of an edge as a t,j = 0. By using these notations, our objective function is defined as follows:</p><formula xml:id="formula_33">−logP (y|x) − λ · n j=1 n t=1 a t,j · logα 1,t,j ,<label>(20)</label></formula><p>where λ is a hyper-parameter balancing the importance of output labels and parse trees in training steps. To investigate the importance of the syntactic information, we used λ = 1.0 for the with syntax (w/ syn) setting and λ = 0 for the without syntax (w/o syn) setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>For comparing our proposed models with the baselines, we conducted both automatic and human evaluations. The following subsections describe the evaluation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>Datasets We used the Google sentence compression dataset (Google dataset) (Filippova and Altun 2013) for our evaluations. To evaluate the performances on an out-ofdomain dataset, we also used the Broadcast News Compression Corpus (BNC Corpus) 3 . The setting for these datasets is as follows: Google dataset: Similar to the previous researches <ref type="bibr" target="#b5">(Filippova et al. 2015;</ref><ref type="bibr" target="#b22">Tran et al. 2016;</ref><ref type="bibr" target="#b23">Wang et al. 2017;</ref><ref type="bibr" target="#b10">Kamigaito et al. 2018;</ref><ref type="bibr" target="#b25">Zhao, Luo, and Aizawa 2018)</ref>, we used the first 1,000 sentences of comp-data.eval.json as the test set. We used the last 1,000 sentences of comp-data.eval.json as our development set. Following recent researches <ref type="bibr" target="#b10">(Kamigaito et al. 2018;</ref><ref type="bibr" target="#b25">Zhao, Luo, and Aizawa 2018)</ref>, we used all 200,000 sentences in sent-comp.train*.json as our training set. We also used the dependency trees contained in this dataset.</p><p>To investigate the summarization performances on long sentences, we additionally performed evaluations on 417 sentences that are longer than the average sentence length (= 27.04) in the test set. BNC Corpus: This dataset contains spoken sentences and their summaries created by three annotators. To evaluate the compression performances on long sentences in the out-ofdomain setting, we treated sentences longer than the average sentence length, 19.83, as the test set (595 sentences), and training was conducted with the Google dataset. Because this dataset does not contain any dependency parsing results, we parsed all sentences in this dataset by using the Stanford dependency parser 4 . In all evaluations, we report the average scores for three annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Models</head><p>The baseline models are as follows. We used ELMo, BERT and GloVe vectors for all models in our experiments. Tagger: This is a bi-LSTM tagger which is used in various sentence summarization researches <ref type="bibr" target="#b12">(Klerke, Goldberg, and Søgaard 2016;</ref><ref type="bibr" target="#b23">Wang et al. 2017)</ref>. LSTM: This is an LSTM-based sentence summarizer, which was proposed by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. LSTM-Dep: This is an LSTM-based sentence summarizer with dependency features, called LSTM-Par-Pres in <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. Base: Our base model explained in the 2nd section. Attn: This is an improved attention-based Seq2Seq model with ConCat attention, described in <ref type="bibr" target="#b17">Luong, Pham, and Manning (2015)</ref>. To capture the context of long sentences, we also feed input embedding into the decoder, similar to the study of <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. Parent: This is a variant of SLAHAN that does not have the child recursive attention module. This model captures only parent words, similar to HiSAN in the study of <ref type="bibr" target="#b10">Kamigaito et al. (2018)</ref>. For the fair comparisons, we left the gate layer in <ref type="figure">Eq.(19)</ref>. Our proposed models are as follows: SLAHAN: This is our proposed model which is described in the 3rd section. Child: This is a variant of SLAHAN that does not have the parent recursive attention module. Similar to Parent, we left the gate layer in Eq.(19).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Parameters</head><p>We used GloVe (glove.840B.300d), 3-layers of ELMo and 12-layers of BERT (cased L-12 H-768 A-12) as our features. We first investigated the best combination of GloVe, ELMo, and BERT vectors as shown in <ref type="table" target="#tab_3">Table 1</ref>. Following this result, we used the combination of all of GloVe, ELMo and BERT for all models.</p><p>The dimensions of the LSTM layer and the attention layer were set to 200. The depth of the LSTM layer was set to 2. These sizes were based on the setting of the LSTM NER tagger with ELMo in the study of <ref type="bibr" target="#b20">Peters et al. (2018)</ref>. All parameters were initialized with Glorot and Bengio (2010)'s method. For all methods, we applied Dropout <ref type="bibr" target="#b21">(Srivastava et al. 2014)</ref> to the input of the LSTM layers. All dropout rates were set to 0.3. We used Adam <ref type="bibr" target="#b11">(Kingma and Ba 2014)</ref> with an initial learning rate of 0.001 as our optimizer. All gradients were averaged by the number of sentences in each minibatch. The clipping threshold value for the gradients was set to 5.0. The maximum training epoch was set to 20. We used {1, 2, 3, 4} as d in Eq.(16) and Eq.(18). The maximum minibatch size was set to 16, and the order of mini-batches was shuffled at the end of each training epoch. We adopted early stopping to the models based on maximizing per-sentence accuracy (i.e., how many summaries are fully reproduced) of the development data set.</p><p>To obtain a compressed sentence, we used greedy decoding, following the previous research <ref type="bibr" target="#b10">(Kamigaito et al. 2018</ref>). We used Dynet <ref type="bibr" target="#b18">(Neubig et al. 2017)</ref> to implement our neural networks 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Evaluation</head><p>Evaluation Metrics In the evaluation, we used kepttoken-based-F 1 measures (F 1 ) for comparing to the previously reported scores. In this metric, precision is defined as the ratio of kept tokens that overlap with the gold summary, and recall is defined as the ratio of tokens in the gold summary that overlap with the system output summary. For more concrete evaluations, we additionally used ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) (Lin and Och <ref type="bibr" target="#b25">(Zhao, Luo, and Aizawa 2018)</ref> 85.0 ----2.7 -----Evaluator-SLM <ref type="bibr" target="#b25">(Zhao, Luo, and Aizawa 2018)</ref>    <ref type="table">Table 3</ref>: Results on the BNC Corpus. † indicates the same as in <ref type="table" target="#tab_5">Table 2.</ref> 2004) 6 with limitation by reference byte lengths 7 as evaluation metrics. We used ∆C = system compression ratio− gold compression ratio <ref type="bibr" target="#b10">(Kamigaito et al. 2018</ref>) to evaluate how close the compression ratio of system outputs was to that of gold compressed sentences. Note that the gold compression ratios of all the sentences and the long sentences in the Google test set are respectively 43.7 and 32.4. Those of all the sentences and the long sentences in the BNC corpus are respectively 76.3 and 70.8. We used the macro-average for all reported scores. All scores are reported as the average scores of three randomly initialized trials.</p><formula xml:id="formula_34">ALL LONG F1 R-1 R-2 R-L ∆C F1 R-1 R-2 R-L ∆C Evaluator-LM</formula><p>Results <ref type="table" target="#tab_5">Table 2</ref> shows the evaluation results on the Google dataset. SLAHAN achieved the best scores on both all the sentences and the long sentences. Through these gains, we can understand that SLAHAN successfully captures important words by tracking both parent and child words. Child achieved better scores than Parent. This result coincides with our investigation that tracking child words is important especially for long sentences, as shown in <ref type="figure" target="#fig_0">Fig.2</ref>. We can also observe the score of SLAHAN w/o syn is comparable to that of SLAHAN w/ syn. This result indicates that dependency graphs can work on the in-domain dataset without relying on given dependency parse trees.  <ref type="table">Table 4</ref>: Results of the human evaluation. The numbers in parentheses are the percentages of over four ratings. † indicates the same as in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>We also show the evaluation results on the BNC corpus, the out-of-domain dataset, in <ref type="table">Table 3</ref>. We can clearly observe that SLAHAN w/ syn outperforms other models for all metrics. Comparing between Base, Parent, Child and SLAHAN, we can understand that SLAHAN w/ syn captured important words during the decoding step even in the BNC corpus. The remarkable performance of SLAHAN w/ syn supports the effectiveness of explicit syntactic information. That is, in the out-of-domain dataset, the dependency graph learned with implicit syntactic information obtained lower scores than that learned with explicit syntactic information. The result agrees with the findings of the previous research <ref type="bibr" target="#b23">(Wang et al. 2017)</ref>. From these results, we can conclude that SLAHAN is effective even for both long and outof-domain sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human evaluation</head><p>In the human evaluation, we compared the models 8 that achieved the top five R-L scores in the automatic evaluation. We filtered out sentences whose compressions are the same for all the models and selected the first 100 sentences from the test set of the Google dataset. Those sentences were evaluated for both readability (Read) and informativeness (Info) by twelve raters, who were asked to rate them in a five-point Likert scale, ranging from one to five for each metric. To Input: British mobile phone giant Vodafone said Tuesday it was seeking regulatory approval to take full control of its Indian unit for $ 1.65 billion , after New Delhi relaxed foreign ownership rules in the sector . Gold: Vodafone said it was seeking regulatory approval to take full control of its Indian unit . Base: Vodafone said it was seeking regulatory approval to take control of its unit . Parent w/ syn: Vodafone said it was seeking approval to take full control of its Indian unit . Child w/ syn: Vodafone said it was seeking regulatory approval to take control of its Indian unit . SLAHAN w/ syn: Vodafone said it was seeking regulatory approval to take full control of its Indian unit .</p><p>Input: Broadway 's original Dreamgirl Jennifer Holliday is coming to the Atlanta Botanical Garden for a concert benefiting Actor 's Express . Gold: Broadway 's Jennifer Holliday is coming to the Atlanta Botanical Garden . Base: Jennifer Holliday is coming to the Atlanta Botanical Garden . Parent w/ syn: Broadway 's Jennifer Holliday is coming to the Atlanta Botanical Garden . Child w/ syn: Jennifer Holliday is coming to the Atlanta Botanical Garden . SLAHAN w/ syn: Broadway 's Jennifer Holliday is coming to the Atlanta Botanical Garden .</p><p>Input: Tokyo , April 7 Japan and India will hold working-level talks here Wednesday on Japan 's export of US2 rescue plane to India , Japan 's defence ministry said Monday . Gold: Japan and India will hold talks on Japan 's export of US2 rescue plane to India . Base: Japan and India will hold talks Wednesday on export of plane to India . Parent w/ syn: Japan and India will hold talks on Japan 's export plane . Child w/ syn: Japan and India will hold talks on Japan 's export of US2 rescue plane to India . SLAHAN w/ syn: Japan and India will hold talks on Japan 's export of plane to India . reduce the effect by outlier rating, we excluded raters with the highest and lowest average ratings. Thus, we report the average rates of the ten raters. <ref type="table">Table 4</ref> shows the results. SLAHAN w/ syn and Child w/ syn improved informativeness without losing readability, compared to the baselines. These improvements agreed with the automatic evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>In <ref type="table" target="#tab_3">Table 1</ref>, BERT underperforms ELMo and GloVe. Recently, <ref type="bibr" target="#b16">Lin et al. (2019)</ref> reported that ELMo is better than BERT in sentence-level discourse parsing and <ref type="bibr" target="#b0">Akbik, Bergmann, and Vollgraf (2019)</ref> reported that LSTM with GloVe is better than BERT in named entity recognition. As <ref type="bibr" target="#b2">Clarke and Lapata (2007)</ref> discussed, discourse and named entity information are both important in the sentence compression task. Therefore, our observation is consistent with the previous researches. These observations indicate that the best choice of word embedding types depends on a task. <ref type="table" target="#tab_7">Table 5</ref> shows the actual outputs from each model. In the first example, we can see that only SLAHAN can compress the sentence correctly. However, both Parent and Child lack the words "regulatory" and "full", respectively, because Parent and Child can track only either parent or child words. This result indicates that the selective gate module of SLA-HAN can work well in a long sentence.</p><p>In the second example, SLAHAN and Parent compress the sentence correctly, whereas Child wrongly drops the words "Broadway 's". This is because Child cannot explicitly track "Jennifer Holliday" from "Broadway 's" in the dependency tree. This result also indicates that the selective gate of SLAHAN correctly switches the tracking direction from the parent or child in this case.</p><p>In the third example, only Child can compress the sentence correctly. This is because in this sentence the model can mostly retain important words by tracking only child words for each decoding step, as shown in <ref type="figure">Fig.1</ref>. In contrary, SLAHAN's compressed sentence lacks the words "US2 rescue". Because SLAHAN decides to use either the parent or child dependency graph by using the selective gate module, we can understand that this wrong deletion is caused by the incorrect weights at the selective gate. This result suggests that for compressing sentences more correctly, we need to make further improvement to the selective gate module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In the sentence compression task, many researches have adopted tree trimming methods <ref type="bibr" target="#b8">(Jing 2000;</ref><ref type="bibr" target="#b13">Knight and Marcu 2000;</ref><ref type="bibr" target="#b1">Berg-Kirkpatrick, Gillick, and Klein 2011;</ref><ref type="bibr" target="#b4">Filippova and Altun 2013)</ref>. As an alternative, LSTMbased models <ref type="bibr" target="#b5">(Filippova et al. 2015;</ref><ref type="bibr" target="#b12">Klerke, Goldberg, and Søgaard 2016)</ref> were introduced to avoid the effect of parsing errors in the tree trimming approach. For using syntactic information in LSTM-based models, <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> additionally proposed a method to use parent words on a parsed dependency tree to compress a sentence. <ref type="bibr" target="#b23">Wang et al. (2017)</ref> used a LSTM-based tagger as a score function of an ILP-based tree trimming method to avoid the overfitting to the in-domain dataset. These approaches have a merit to capture the syntactic information explicitly, but they were affected by parsing errors. <ref type="bibr" target="#b10">Kamigaito et al. (2018)</ref> proposed a Seq2Seq model that can consider the higher-order dependency parents by tracking the dependency tree with their attention distributions. Unlike the previous models, their model can avoid parse errors by jointly learning the summary generation probability and the dependency parent probability. Similarly, <ref type="bibr" target="#b25">Zhao, Luo, and Aizawa (2018)</ref> proposed a syntax-based language model that can compress sentences without using explicit parse trees.</p><p>Our SLAHAN uses strong language-model features, ELMo and BERT, and can track both parent and child words in a dependency tree without being affected by parse errors. In addition, SLAHAN can retain important words by explicitly considering words that will be decoded in the future with our selective gate module during the decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a novel Seq2Seq model, syntactically look-ahead attention network (SLAHAN), that can generate informative summaries by explicitly tracking parent and child words for capturing the important words in a sentence. The evaluation results showed that SLA-HAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores on the Google dataset in both all the sentence and the long sentence settings. In the BNC corpus, SLAHAN also achieved the best kept-tokenbased-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores, and showed its effectiveness on both long sentences and outof-domain sentences. In human evaluation, SLAHAN improved informativeness without losing readability. From these results, we can conclude that in Seq2Seq models, capturing important words that will be decoded in the future based on dependency relationships can help to compress long sentences during the decoding steps. A. Results of all sentences on the BNC Corpus We also report the results of all sentences on the BNC Corpus in <ref type="table">Table 6</ref>.  <ref type="table">Table 6</ref>: The bold values indicate the best scores. † indicates that the difference of the score from the best baseline is statistically significant. We used paired-bootstrap-resampling with 1,000,000 random samples for the significance test (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compression ratios in characters</head><p>We used compression ratios in tokens to evaluate each method in this paper. However, compression ratios in characters are also used for evaluating sentence compression performance. Thus, we also report compression ratios in characters of our methods to support a fair comparison between sentence compression methods. <ref type="table" target="#tab_10">Table 7</ref> and <ref type="table">Table 8</ref> show compression ratios in characters (CR) of methods for each setting in this paper. Note that in both tables, ∆C is calculated with compression ratios in characters.   <ref type="table">Table 8</ref>: Compression ratios in characters on the BNC Corpus. The notations are the same as in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALL</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proportion of words retained later that are linked from right to the retained words in the summary as a parent or a child word in the left-to-right decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The entire network structure of our Syntactically Look-Ahead Attention Network (SLAHAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The entire network structure of our Syntactically Look-Ahead Attention Network (SLAHAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>F 1 scores for Base with various features in the development data. The bold score represents the highest score.</figDesc><table><row><cell>Glove ELMo BERT F1</cell><cell>86.2 86.0 85.9 85.4 85.5 85.9 84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results on the Google dataset. ALL and LONG represent, respectively, the results for all sentences and only for long sentences (longer than average length 27.04) in the test dataset. The bold values indicate the best scores. † indicates that the difference of the score from the best baseline (mostly Base) is statistically significant. 8</figDesc><table><row><cell></cell><cell></cell><cell>F1</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>∆C</cell></row><row><cell>Tagger LSTM LSTM-Dep Attn Base Parent Parent</cell><cell>w/ syn w/o syn</cell><cell>54.6 54.8 55.1 54.1 55.4 54.2 54.0</cell><cell>36.8 36.6 36.9 36.1 37.4 36.3 35.8</cell><cell>27.7 28.0 28.2 27.4 28.5 27.7 27.2</cell><cell>36.4 36.2 36.5 35.6 36.9 35.9 35.4</cell><cell>-39.1 -39.2 -38.8 -39.6 -38.6 -39.1 -40.1</cell></row><row><cell>Child Child</cell><cell>w/ syn w/o syn</cell><cell>55.6 54.8</cell><cell>37.8 36.7</cell><cell>28.5 28.1</cell><cell>37.3 36.3</cell><cell>-38.2 -39.2</cell></row><row><cell cols="2">SLAHAN w/ syn SLAHAN w/o syn</cell><cell cols="5">57.7  † 40.1  † 30.6  † 39.6  † −35.9  † 54.6 36.4 27.8 36.0 -39.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Example compressed sentences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Compression ratios in characters on the Google dataset. The notations are the same as inTable 6.</figDesc><table><row><cell></cell><cell></cell><cell>ALL</cell><cell cols="2">LONG</cell></row><row><cell></cell><cell>CR</cell><cell>∆C</cell><cell>CR</cell><cell>∆C</cell></row><row><cell>Gold</cell><cell>76.5</cell><cell>0.0</cell><cell>71.6</cell><cell>0.0</cell></row><row><cell>Tagger LSTM LSTM-Dep Attn Base Parent Parent</cell><cell>51.2 48.2 49.4 48.5 49.9 49.8 w/o syn 49.0 w/ syn</cell><cell>-25.3 -28.3 -27.1 -28.0 -26.6 -26.7 -27.5</cell><cell>31.2 31.0 31.4 30.5 31.8 31.0 30.1</cell><cell>-40.4 -40.6 -40.2 -41.1 -39.8 -40.6 -41.5</cell></row><row><cell>Child Child</cell><cell>w/ syn w/o syn 48.4 49.6</cell><cell>-26.9 -28.1</cell><cell>32.0 31.1</cell><cell>-39.6 -40.5</cell></row><row><cell cols="5">SLAHAN w/ syn SLAHAN w/o syn 49.3 51.7 −24.8  † 34.4 −37.2  † -27.2 30.7 -40.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This statistic is calculated on the gold compression and its dependency parse result, which are contained in the training set of the Google sentence compression dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This statistic is calculated on the gold compression and its dependency parse result, which are contained in the training set of the Google sentence compression dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.jamesclarke.net/research/resources 4 https://nlp.stanford.edu/software/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our implementation is publicly available on GitHub at https://github.com/kamigaito/slahan.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We used the ROUGE-1.5.5 script with option "-n 2 -m -d -a". 7 If a system output exceeds the reference summary byte length, we truncated the exceeding tokens.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We are thankful to Dr. Tsutomu Hirao for his useful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="724" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modelling compression with discourse constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural machine translation with source-side latent graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sentence reduction for automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Applied Natural Language Processing</title>
		<meeting>the Sixth Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="310" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised attention for sequenceto-sequence constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Higher-order syntactic attention network for long sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1716" to="1726" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving sentence compression by learning to predict gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1528" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistics-based summarizationstep one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skipbigram statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="605" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified linear-time framework for sentence-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jwalapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4190" to="4200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective attention-based neural architectures for sentence compression with bidirectional long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-T</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Symposium on Information and Communication Technology</title>
		<meeting>the Seventh Symposium on Information and Communication Technology</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Can syntax help? improving an lstm-based sentence compression model for new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1385" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A language model based evaluator for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="170" to="175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
