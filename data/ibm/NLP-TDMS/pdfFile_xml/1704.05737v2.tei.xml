<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Video Object Segmentation with Visual Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari Inria</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
						</author>
						<title level="a" type="main">Learning Video Object Segmentation with Visual Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a "visual memory" in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given a video frame as input, our approach assigns each pixel an object or background label based on the learned spatio-temporal features as well as the "visual memory" specific to the video, acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results. For example, our approach outperforms the top method on the DAVIS dataset by nearly 6%. We also provide an extensive ablative analysis to investigate the influence of each component in the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation is the task of extracting spatio-temporal regions that correspond to object(s) moving in at least one frame in the video sequence. The top-performing methods for this problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> continue to rely on hand-crafted features and do not leverage a learned video representation, despite the impressive results achieved by convolutional neural networks (CNN) for other vision tasks, e.g., image segmentation <ref type="bibr" target="#b35">[36]</ref>, object detection <ref type="bibr" target="#b36">[37]</ref>. Very recently, there have been attempts to build CNNs for video object segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44]</ref>. They are <ref type="bibr">Figure 1</ref>. Sample results on the DAVIS dataset. Segmentations produced by MP-Net <ref type="bibr" target="#b43">[44]</ref> (left) and our approach (right), overlaid on the video frame.</p><p>indeed the first to use deep learning methods for video segmentation, but suffer from various drawbacks. For example, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> rely on a manually-segmented subset of frames (typically the first frame of the video sequence) to guide the segmentation pipeline. Our previous work <ref type="bibr" target="#b43">[44]</ref> relies solely on optical flow between pairs of frames to segment independently moving objects in a video, making it susceptible to errors in flow estimation. It also can not extract objects if they stop moving. Furthermore, none of these methods has a mechanism to memorize relevant features of objects in a scene. In this paper, we propose a novel framework to address these issues; see sample results in <ref type="figure">Figure 1</ref>.</p><p>We present a two-stream network with an explicit memory module for video object segmentation (see <ref type="figure" target="#fig_0">Figure 2</ref>). The memory module is a convolutional gated recurrent unit (GRU) that encodes the spatio-temporal evolution of object(s) in the input video sequence. This spatio-temporal representation used in the memory module is extracted from two streams-the appearance stream which describes static features of objects in the video, and the temporal stream which captures motion cues.</p><p>The appearance stream is the DeepLab network <ref type="bibr" target="#b6">[7]</ref> pretrained on the PASCAL VOC segmentation dataset and operates on individual video frames. The temporal one is a motion prediction network <ref type="bibr" target="#b43">[44]</ref> pretrained on the synthetic FlyingThings3D dataset and takes optical flow computed from pairs of frames as input, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The two streams provide complementary cues for object segmentation. With these spatio-temporal CNN features in hand, we train the convolutional GRU component of the framework to learn a visual memory representation of object(s) in the scene. Given a frame t from the video sequence as input, the network extracts its spatio-temporal features and then: (i) computes the segmentation using the memory representation aggregated from all frames previously seen in the video, and (ii) updates the memory unit with features from t. The segmentation is improved further by processing the video bidirectionally in the memory unit, with our bidirectional convolutional GRU.</p><p>The contributions of the paper are two-fold. First, we present an approach for moving object segmentation in unconstrained videos that does not require any manuallyannotated frames in the input video (see §3). Our network architecture incorporates a memory unit to capture the evolution of object(s) in the scene (see §4). To our knowledge, this is the first recurrent network based approach to accomplish the video segmentation task. It helps address challenging scenarios where the motion patterns of the object change over time; for example, when an object in motion stops to move, abruptly, and then moves again, with potentially a different motion pattern. Second, we present stateof-the-art results on two video object segmentation benchmarks, namely DAVIS <ref type="bibr" target="#b34">[35]</ref> and Freiburg-Berkeley motion segmentation (FBMS) dataset <ref type="bibr" target="#b30">[31]</ref> (see §5.5). Additionally, we provide an extensive experimental analysis, with ablation studies to investigate the influence of all the components of our framework (see §5.3) and visualize the internal states of our memory unit (see §5.6). We will make the source code and the models available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Video object segmentation. Several approaches have been proposed over the years to accomplish the task of segmenting objects in video. One of the more successful ones presented in <ref type="bibr" target="#b3">[4]</ref> clusters pixels spatio-temporally based on motion features computed along individual point trajectories. Improvements to this framework include dense trajectory-level segmentation <ref type="bibr" target="#b29">[30]</ref>, an alternative clustering method <ref type="bibr" target="#b21">[22]</ref>, and detection of discontinuities in the trajectory spectral embedding <ref type="bibr" target="#b12">[13]</ref>. These trajectory based approaches lack robustness in cases where feature matching fails.</p><p>An alternative to using trajectories is formulating the segmentation problem as a foreground-background classification task <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref>. These methods first estimate a region <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref> or regions <ref type="bibr" target="#b24">[25]</ref>, which correspond(s) to the foreground object, and then use them to compute foreground and background appearance models. The final object segmentation is obtained by integrating these appearance models with other cues, e.g., saliency maps <ref type="bibr" target="#b44">[45]</ref>, shape estimates <ref type="bibr" target="#b24">[25]</ref>, pairwise constraints <ref type="bibr" target="#b31">[32]</ref>. Variants to this framework have introduced occlusion relations to compute a layered video segmentation <ref type="bibr" target="#b41">[42]</ref>, and long-range interactions to group re-occurring regions in video <ref type="bibr" target="#b10">[11]</ref>. Two methods from this class of segmentation approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> show a good performance on the DAVIS benchmark. While our proposed method is similar in spirit to this class of approaches, in terms of formulating segmentation as a classification problem, we differ from previous work significantly. We propose an integrated approach to learn appearance and motion features and update them with a memory module, in contrast to estimating an initial region heuristically and then propagating it over time. Our robust model outperforms all these methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, as shown in Section 5.5.</p><p>Video object segmentation is also related to the task of segmenting objects in motion, irrespective of camera motion. Two recent methods to address this task use optical flow computed between pairs of frames <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44]</ref>. Classical methods in perspective geometry and RANSAC-based feature matching are used in <ref type="bibr" target="#b2">[3]</ref> to estimate moving objects from optical flow. It achieved state-of-the-art performance on a subset of the Berkeley motion segmentation (BMS) dataset <ref type="bibr" target="#b3">[4]</ref>, but lacks robustness due to a heuristic initialization, as shown in the evaluation on DAVIS in <ref type="table">Table 3</ref>. Our previous approach MP-Net <ref type="bibr" target="#b43">[44]</ref> learns to recognize motion patterns in a flow field. This frame-based approach is the state of the art on DAVIS and is on par with [3] on the BMS subset. Despite its excellent performance, MP-Net is limited by its frame-based nature and also overlooks appearance features of objects. These issues are partially addressed in a heuristic post-processing step with objectness cues in <ref type="bibr" target="#b43">[44]</ref>. Nevertheless, the approach fails to extract objects if they stop moving, i.e., if no motion cues are present. We use MP-Net as the temporal stream of our approach (see <ref type="figure" target="#fig_0">Figure 2</ref>). We show a principled way to integrate this stream with appearance information and a new visual memory module based on convolutional gated recurrent units (ConvGRU). As shown in <ref type="table">Table 2</ref>, our approach outperforms MP-Net.</p><p>Very recently, two CNN-based methods for video object segmentation were proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. Starting with CNNs pre-trained for image segmentation, they find objects in video by fine-tuning on the first frame in the sequence. Note that this setup, referred to as semi-supervised segmentation, is very different from the more challenging unsupervised case we address in this paper, where no manually-annotated frames are available for the test video. Furthermore, these two CNN architectures are primarily developed for images, and do not model temporal information in video. We, on the other hand, propose a recurrent network specifically for the video segmentation task. Recurrent neural networks (RNNs). RNN <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref> is a popular model for tasks defined on sequential data. Its main component is an internal state that allows to accumulate information over time. The internal state in classical RNNs is updated with a weighted combination of the input and the previous state, where the weights are learned from training data for the task at hand. Long short-term memory (LSTM) <ref type="bibr" target="#b19">[20]</ref> and gated recurrent unit (GRU) <ref type="bibr" target="#b7">[8]</ref> architectures are improved variants of RNN, which partially mitigate the issue of vanishing gradients <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. They introduce gates with learnable parameters, to update the internal state selectively, and can propagate gradients further through time.</p><p>Recurrent models, originally used for text and speech recognition, e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>, are becoming increasingly popular for visual data. Initial work on vision tasks, such as image captioning <ref type="bibr" target="#b8">[9]</ref>, future frame prediction <ref type="bibr" target="#b40">[41]</ref> and action recognition <ref type="bibr" target="#b28">[29]</ref>, has represented the internal state of the recurrent models as a 1D vector-without encoding any spatial information. LSTM and GRU architectures have been extended to address this issue with the introduction of Con-vLSTM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> and ConvGRU <ref type="bibr" target="#b1">[2]</ref> respectively. In these convolutional recurrent models the state and the gates are 3D tensors and the weight vectors are replaced by 2D convolutions. These models have only recently been applied to vision tasks, such as video frame prediction <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>, action recognition and video captioning <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we employ a visual memory module based on a convolutional GRU (ConvGRU) and show that it is an effective way to encode the spatio-temporal evolution of objects in video for segmentation. Further, to fully benefit from all the frames in a video sequence, we apply the recurrent model bidirectionally <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>, i.e., apply two identical model instances on the sequence in forward and backward directions, and combine the predictions for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our model takes video frames together with their estimated optical flow as input, and outputs binary segmentations of moving objects, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We target the most general form of this task, wherein objects are to be segmented in the entire video if they move in at least one frame. The proposed model is comprised of three key components: appearance and motion networks, and a visual memory module. Appearance network. The purpose of the appearance stream is to produce a high-level encoding of a frame that will later aid the visual memory module in forming a representation of the moving object. It takes an RGB frame as input and produces a 128 × w/8 × h/8 feature representation (shown in green in <ref type="figure" target="#fig_0">Figure 2</ref>). This encodes the semantic content of the scene. We use a state-of-the-art CNN for this stream, namely the largeFOV version of the DeepLab network <ref type="bibr" target="#b6">[7]</ref>. This network relies on dilated convolutions <ref type="bibr" target="#b6">[7]</ref>, which preserve a relatively high spatial resolution of features, and also incorporate context information in each pixel's representation. It is pretrained on a semantic segmentation dataset, PASCAL VOC 2012 <ref type="bibr" target="#b9">[10]</ref>, resulting in features that can distinguish objects from background as well as from each other-a crucial aspect for the video object segmentation task. We extract features from the fc6 layer of the network, which has a feature dimension of 1024 for each pixel. This feature map is further passed through two 1 × 1 convolutional layers, interleaved with tanh nonlinearities, to reduce the dimension to 128. These layers are trained together with ConvGRU (see §5.2 for details). Motion network. For the temporal stream we employ MP-Net <ref type="bibr" target="#b43">[44]</ref>, a CNN pretrained for the motion segmentation task. It is trained to estimate independently moving objects (i.e., irrespective of camera motion) based on optical flow computed from a pair of frames as input (shown in yellow in <ref type="figure" target="#fig_0">Figure 2</ref>). This stream produces a w/4 × h/4 motion prediction output, where each value represents the likelihood of the corresponding pixel being in motion. Its output is further downsampled by a factor 2 (in w and h) to match the dimensions of the appearance stream output.</p><p>The intuition behind using two streams is to benefit from their complementarity for building a strong representation of objects that evolves over time. For example, both appearance and motion networks are equally effective when an object is moving in the scene, but as soon as it becomes stationary, the motion network can not estimate the object, unlike the appearance network. We leverage this comple- mentary nature, as done by two-stream networks for other vision tasks <ref type="bibr" target="#b39">[40]</ref>. Note that our approach is not specific to the particular networks described above, but is in fact a general framework for video object segmentation. As shown is the Section 5.3, its components can easily be replaced with other networks, providing scope for future improvement. Memory module. The third component, i.e., a visual memory module based on convolutional gated units (ConvGRU), takes the concatenation of appearance and motion stream outputs as its input. It refines the initial estimates from these two networks, and also memorizes the appearance and location of objects in motion to segment them in frames where: (i) they are static, or (ii) motion prediction fails; see the example in <ref type="figure">Figure 1</ref>. The output of this ConvGRU memory module is a 64×w/8×h/8 feature map obtained by combining the two-stream input with the internal state of the memory module, as described in detail in Section 4. We further improve the model by processing the video bidirectionally; see Section 4.1. The output from the ConvGRU module is processed by a 1 × 1 convolutional layer and softmax nonlinearity to produce the final pixelwise segmentation result. These layers are trained together with ConvGRU, as detailed in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visual memory module</head><p>The key component of the ConvGRU module is the state matrix h, which encodes the visual memory. For frame t in the video sequence, ConvGRU uses the two-stream representation x t and the previous state h t−1 to compute the new state h t . The dynamics of this computation are guided by an update gate z t , a forget gate r t . The states and the gates are 3D tensors, and can characterize spatio-temporal patterns in the video, effectively memorizing which objects move, and where they move to. These components are computed with convolutional operators and nonlinearities as follows.</p><formula xml:id="formula_0">z t = σ(x t * w xz + h t−1 * w hz + b z ), (1) r t = σ(x t * w xr + h t−1 * w hr + b r ), (2) h t = tanh(x t * w xh + r t h t−1 * w hh + bh), (3) h t = (1 − z t ) h t−1 + z t h t ,<label>(4)</label></formula><p>where denotes element-wise multiplication, * represents a convolutional operation, σ is the sigmoid function, w's are learned transformations, and b's are bias terms.</p><p>The new state h t in <ref type="formula" target="#formula_0">(4)</ref> is a weighted combination of the previous state h t−1 and the candidate memoryh t . The update gate z t determines how much of this memory is incorporated into the new state. If z t is close to zero, the memory represented byh t is ignored. The reset gate r t controls the influence of the previous state h t−1 on the candidate memoryh t in (3), i.e., how much of the previous state is let through into the candidate memory. If r t is close to zero, the unit forgets its previously computed state h t−1 .</p><p>The gates and the candidate memory are computed with convolutional operations over x t and h t−1 shown in equations (1-3). We illustrate the computation of the candidate memory stateh t in <ref type="figure" target="#fig_1">Figure 3</ref>. The state at t − 1, h t−1 , is first multiplied (element-wise) with the reset gate r t . This modulated state representation and the input x t are then convolved with learned transformations, w hh and w xh respectively, summed together with a bias term bh, and passed through a tanh nonlinearity. In other words, the visual memory representation of a pixel is determined not only by the input and the previous state at that pixel, but also its local neighborhood. Increasing the size of the convolutional kernels allows the model to handle spatio-temporal patterns with larger motion.</p><p>The update and reset gates, z t and r t , are computed in an analogous fashion using a sigmoid function instead of tanh. Our ConvGRU applies a total of six convolutional operations at each time step. All the operations detailed here are fully differentiable, and thus the parameters of the convolutions (w's and b's) can be trained in an end-to-end fashion with back propagation through time <ref type="bibr" target="#b45">[46]</ref>. In summary, the model learns to combine appearance features of the current frame with the memorized video representation to refine motion predictions, or even fully restore them from the previous observations in case a moving object becomes stationary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bidirectional processing</head><p>Consider an example where an object is stationary at the beginning of a video sequence, and starts to move in the latter frames. Our approach described so far, which processes video frames sequentially (in the forward direction), can not segment the object in the initial frames. This is due to the lack of prior memory representation of the object in the first frame. We improve our framework with a bidirectional processing step, inspired by the application of recurrent models bidirectionally in the speech domain <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The bidirectional variant of our ConvGRU is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. It is composed of two ConvGRU instances with identical learned weights, which are run in parallel. The first one processes frames in the forward direction, starting with the first frame (shown at the bottom in the <ref type="figure">figure)</ref>. The second instance process frames in the backward direction, starting with the last video frame (shown at the top in the <ref type="figure">figure)</ref>. The activations from these two directions are concatenated at each time step, as shown in the figure, to produce a 128 × w/8 × h/8 output. It is then passed through a 3×3 convolutional layer to finally produce a 64×w/8×h/8 for each frame. Pixel-wise segmentation is then obtained with a final 1 × 1 convolutional layer and a softmax nonlinearity, as in the unidirectional case.</p><p>Bidirectional ConvGRU is used both in training and in testing, allowing the model to learn to aggregate information over the entire video. In addition to handling cases where objects move in the latter frames, it improves the ability of the model to correct motion prediction errors. As discussed in the experimental evaluation, bidirectional ConvGRU improves segmentation performance by nearly 3% on the DAVIS dataset (see <ref type="table">Table 1</ref>). The influence of bidirectional processing is more prominent on the FBMS dataset, where objects can be static in the beginning of a video, with 5% improvement over the unidirectional variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We train our visual memory module with the back propagation through time algorithm <ref type="bibr" target="#b45">[46]</ref>, which unrolls the recurrent network for n time steps and keeps all the intermediate activations to compute the gradients. Thus, our ConvGRU model, which has 6 internal convolutional layers, trained on a video sequence of length n, is equivalent to a 6n layer CNN for the unidirectional variant, or 12n for the bidirectional model at training time. This memory requirement makes it infeasible to train the whole model, including appearance and motion streams, end-to-end. We resort to using pretrained versions of the appearance and motion networks and train the ConvGRU.</p><p>We use the training split of the DAVIS dataset <ref type="bibr" target="#b34">[35]</ref> for learning the ConvGRU weights. Objects move in all the frames in this dataset, which biases the memory module towards the presence of an uninterrupted motion stream. This results in the ConvGRU learned from this data failing, when an object stops to move in a test sequence. We augment the training data to simulate such stop-and-go scenarios and thus learn a more robust model for realistic videos.</p><p>We create additional training sequences, where ground truth moving object segmentation (instead of responses from the motion network) is provided for all the frames, except for the last five frames, which are duplicated, simulating a case where objects stop moving. No motion input is used for these last five frames. These artificial examples are used in place of the regular ones for a fixed fraction of iterations. Replacing motion stream predictions with ground truth segmentations for these sequences allows to decouple the task of motion mistake correction from the task of object tracking, which simplifies the learning. Given that ground truth segmentation determines the loss for training, i.e., it is used for all the frames, ConvGRU explicitly memorizes the moving object in the initial part of the sequence, and then segments it in frames where motion is missing. We do a similar training set augmentation by duplicating the first five frames in a batch, to simulates the cases where an object is static in the beginning of a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and evaluation</head><p>We use four datasets in the experimental analysis: DAVIS for training and test, FBMS and SegTrack-v2 only for test, and FT3D for training a variant of our approach.</p><p>DAVIS. It contains 50 full HD videos with accurate pixellevel annotation in all the frames <ref type="bibr" target="#b34">[35]</ref>. The annotations correspond to the task of video object segmentation. Following the 30/20 training/validation split provided with the dataset, we train on the 30 sequences, and test on the 20 validation videos. We also follow the standard protocol for evaluation from <ref type="bibr" target="#b34">[35]</ref>, and report intersection over union, F-measure for contour accuracy and temporal stability.</p><p>FBMS. The Freiburg-Berkeley motion segmentation dataset <ref type="bibr" target="#b30">[31]</ref> is composed of 59 videos with ground truth annotations in a subset of the frames. In contrast to DAVIS, it has multiple moving objects in several videos with instance-level annotations. Also, objects may move only in a fraction of the frames, but they are annotated in frames where they do not exhibit independent motion. The dataset is split into training and test sets. Following the standard protocol on this dataset <ref type="bibr" target="#b21">[22]</ref>, we do not train on any of these sequences, and evaluate separately for both with precision, recall and F-measure scores. We also convert instance-level annotation to binary ones by merging all the foreground labels into a single category, as in <ref type="bibr" target="#b41">[42]</ref>.</p><p>SegTrack-v2. It contains 14 videos with instance-level moving object annotations in all the frames. We convert these annotations into a binary form for evaluation and use intersection over union as a performance measure.</p><p>FT3D. The FlyingThings3D dataset <ref type="bibr" target="#b26">[27]</ref> consists of 2250 synthetic videos for training, composed of 10 frames, where objects are in motion along random trajectories in rendered scenes. Ground truth optical flow, depth, camera parameters, and instance segmentations are provided by <ref type="bibr" target="#b26">[27]</ref>, and the ground truth motion segmentation is available from <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>We train our model by minimizing binary crossentropy loss using back-propagation through time and RM-SProp <ref type="bibr" target="#b42">[43]</ref>   <ref type="bibr" target="#b43">[44]</ref> variants on the DAVIS validation set. "Obj" refers to the objectness cues used in <ref type="bibr" target="#b43">[44]</ref>. MP-Net-V(ideo) and MP-Net-F(rame) are the variants of MP-Net which use FST <ref type="bibr" target="#b31">[32]</ref> and CRF respectively, in addition to objectness.</p><p>is gradually decreased after every epoch. The weight decay is set to 0.005. Initialization of all the convolutional layers, except for those inside the ConvGRU, is done with the standard xavier method <ref type="bibr" target="#b13">[14]</ref>. We clip the gradients to the [−50, 50] range before each parameter update, to avoid numerical issues <ref type="bibr" target="#b14">[15]</ref>. We form batches of size 14 by randomly selecting a video, and a subset of 14 consecutive frames in it. Random cropping and flipping of the sequences is also performed for data augmentation. Our full model uses 7 × 7 convolutions in all the ConvGRU operations. The weights of the two 1 × 1 convolutional (dimensionality reduction) layers in the appearance network and the final 1 × 1 convolutional layer following the memory module are learned jointly with the memory module. The model is trained for 30000 iterations and the proportion of batches with additional sequences (see Section 4.2) is set to 20%.</p><p>Our final model uses a fully-connected CRF <ref type="bibr" target="#b23">[24]</ref> to refine boundaries in a post-processing step. The parameters of this CRF are taken from <ref type="bibr" target="#b43">[44]</ref>. In the experiments where objectness is used, it is also computed according to <ref type="bibr" target="#b43">[44]</ref>. We use LDOF <ref type="bibr" target="#b4">[5]</ref> for optical flow estimation and convert the raw flow to flow angle field, as in <ref type="bibr" target="#b43">[44]</ref>. We used the code and the trained models for MP-Net available at <ref type="bibr" target="#b0">[1]</ref>. Our method is implemented in the Torch framework and will be made available online. Many sequences in FBMS are several hundred frames long and do not fit into GPU memory during evaluation. We apply our method in a sliding window fashion in such cases, with a window of 130 frames and a step size of 50. <ref type="table">Table 1</ref> demonstrates the influence of different components of our approach on the DAVIS validation set. First, we study the role of the appearance stream. As a baseline, we remove it completely ("no" in the "App stream" in the table), i.e., the output of the motion stream is the only input to our visual memory module. In this setting, the memory module lacks sufficient information to produce accurate segmentations, which results in an <ref type="bibr" target="#b25">26</ref>  <ref type="table">Table 3</ref>. Comparison to state-of-the-art methods on DAVIS with intersection over union (J ), F-measure (F), and temporal stability (T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>Ground truth CUT <ref type="bibr" target="#b21">[22]</ref> FST <ref type="bibr" target="#b31">[32]</ref> MP-Net-Frame [44] Ours <ref type="figure">Figure 5</ref>. Qualitative comparison with top-performing methods on DAVIS. Left to right: ground truth, results of CUT <ref type="bibr" target="#b21">[22]</ref>, FST <ref type="bibr" target="#b31">[32]</ref>, MP-Net-Frame <ref type="bibr" target="#b43">[44]</ref>, and our method.</p><p>performance compared to the method where the appearance stream with fc6 features is used ("Ours" in the table). We then provide raw RGB frames, concatenated with the motion prediction, as input to the ConvGRU. This simplest form of image representation leads to a 14.8% improvement, compared to the motion only model, showing the importance of the appearance features. The variant where RGB input is passed through two convolutional layers, interleaved with tanh nonlinearities, that are trained jointly with the memory module ("2-layer CNN"), further improves this. This shows the potential of learning appearance representation as a part of the video segmentation pipeline. Finally, we compare features extracted from the fc7 and conv5 layers of the DeepLab model to those from fc6 used by default in our method. Features from fc7 and fc6 show comparable performance, but fc7 ones are more expensive to compute. Conv5 features perform significantly worse, perhaps due to a smaller field of view.</p><p>The importance of appearance network pretrained on the semantic segmentation task is highlighted by the "ImageNet only" variant in <ref type="table">Table 1</ref>, where the PASCAL VOC pre-trained segmentation network is replaced with a network trained on ImageNet classification. Although ImageNet pretraining provides a rich feature representation, it is less suitable for the video object segmentation task, which is confirmed by an 6% drop in performance. Discarding the motion information ("no" in "Motion stream"), although being 10.5% below our complete method, still outperforms most of the motion-based approaches on DAVIS (see <ref type="table">Table 3</ref>). This variant learns foreground/background segmentation, which is sufficient for videos with a single dominant object, but fails in more challenging cases.</p><p>Next, we evaluate the design choices in the visual memory module. Using a simple recurrent model (ConvRNN) results in a slight decrease in performance. Such simpler architectures can be used in case of a memory vs segmentation quality trade off. The other variant using ConvLSTM is comparable to ConvRNN, possibly due to the lack of sufficient training data. We also evaluated the influence of the memory module (ConvGRU) by replacing it with a stack of 6 convolutional layers to obtain a memoryless variant of our model ("no" in "Memory module" in <ref type="table">Table 1</ref>  <ref type="table">Table 4</ref>. Comparison to state-of-the-art methods on FBMS with precision (P), recall (R), and F-measure (F).</p><p>same number of parameters. This variant results in a 6% drop in performance compared to our full model. The performance of the memoryless variant is comparable to <ref type="bibr" target="#b43">[44]</ref> (63.3), the approach without any memory. Performing a unidirectional processing instead of a bidirectional one decreases the performance by nearly 3% ("no" in "Bidir processing"). Lastly, we train two variants ("FT3D GT Flow" and "FT3D LDOF Flow") on the synthetic FT3D dataset <ref type="bibr" target="#b26">[27]</ref> instead of DAVIS. Both of them show a significantly lower performance than our method trained on DAVIS. This is due to the appearance of synthetic FT3D videos being very different from the real-world ones. The variant trained on ground truth flow (GT Flow) is inferior to that trained on LDOF flow because the motion network (MP-Net) achieves a high performance on FT3D with ground truth flow, and thus our visual memory module learns to simply follow the motion stream output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison to MP-Net variants</head><p>In <ref type="table">Table 2</ref> we compare our method to MP-Net and its variants presented in <ref type="bibr" target="#b43">[44]</ref> on the DAVIS validation set. Our visual memory-based approach ("Ours" in the table) outperforms the MP-Net baseline ("MP-Net"), which serves as the motion stream in our model, by 16.5%. This clearly demonstrates the value of the appearance stream and our memory unit for video segmentation. The post-processing variants in <ref type="bibr" target="#b43">[44]</ref>, using objectness cues, CRF, and video segmentation method <ref type="bibr" target="#b31">[32]</ref>, improve this baseline, but remain inferior to our result. Our full method ("Ours + CRF") is nearly 6% better than "MP-Net-Frame", which is the best performing MP-Net variant on DAVIS. Note that "MP-Net-Video" which combines MP-Net with objectness cues and the video segmentation method of <ref type="bibr" target="#b31">[32]</ref> is also inferior to our method, as it relies strongly on the tracking capabilities of <ref type="bibr" target="#b31">[32]</ref>, which is prone to segmentation leaking in case of errors in the flow estimation. The example in the first row in <ref type="figure">Figure 5</ref> shows a typical error of <ref type="bibr" target="#b31">[32]</ref>.</p><p>MP-Net-Video performs better than MP-Net-Frame on the FBMS dataset (see <ref type="table">Table 4</ref>) since the frame-only variant does not segment objects when they stop moving. The propagation of segment(s) over time with tracking in MP-Net-Video addresses this, but is less precise due to segmentation leaks, as shown by the comparison with precision measure CUT <ref type="bibr" target="#b21">[22]</ref> MP-Net-Video <ref type="bibr" target="#b43">[44]</ref> Ours <ref type="figure">Figure 6</ref>. Qualitative comparison with top-performing methods on FBMS. Left to right: results of CUT <ref type="bibr" target="#b21">[22]</ref>, MP-Net-Video <ref type="bibr" target="#b31">[32]</ref>, and our method.</p><p>in the table and the qualitative results in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.</head><p>Comparison to the state-of-the-art DAVIS. <ref type="table">Table 3</ref> compares our approach to the state-ofthe-art methods on DAVIS. In addition to comparing our results to the top-performing unsupervised approaches reported in <ref type="bibr" target="#b34">[35]</ref>, we evaluated two more recent methods: CUT <ref type="bibr" target="#b21">[22]</ref> and PCM <ref type="bibr" target="#b2">[3]</ref>, with the authors' implementation. Our method outperforms MP-Net-Frame, the previous state of the art, by 5.9% on the IoU measure, and is 20.1% better than the next best method <ref type="bibr" target="#b31">[32]</ref>. We also observe a 30.8% improvement in temporal stability over MP-Net-Frame. PCM <ref type="bibr" target="#b2">[3]</ref>, which performs well on a subset of the FBMS dataset (as shown in <ref type="bibr" target="#b43">[44]</ref>), is in fact significantly worse on DAVIS. <ref type="figure">Figure 5</ref> shows qualitative results of our approach, and the next three top-performing methods on DAVIS: MP-Net-Frame <ref type="bibr" target="#b43">[44]</ref>, FST <ref type="bibr" target="#b31">[32]</ref> and CUT <ref type="bibr" target="#b21">[22]</ref>. In the first row, both CUT and our method segment the dancer, but our result is more accurate. FST leaks to the people in the background and MP-Net misses parts of the person due to the incorrectly estimated objectness. Our approach does not include any heuristics, which makes it robust to this type of errors. In the second row, all the methods are able to segment the CUT <ref type="bibr" target="#b21">[22]</ref> FST <ref type="bibr" target="#b31">[32]</ref>  swan, but only our method segments it completely and also does not leak into the background. In the next row, our approach shows very high precision, being able to correctly separate the dog and the pole occluding it. In the last row, we illustrate a failure case of our method. The people in the background move in some of the frames in this example. CUT, MP-Net and our method segment them to varying extents. FST focuses on the foreground object, but leaks to the background partially nevertheless. <ref type="table">Table 4</ref> MP-Net-Frame <ref type="bibr" target="#b43">[44]</ref> is outperformed by most of the methods on this dataset. Our approach based on visual memory outperforms MP-Net-Frame by 15.6% on the test set and by 14.2% on the training set according to the F-measure. FST <ref type="bibr" target="#b31">[32]</ref> based post-processing ("MP-Net-V" in the table) significantly improves the results of MP-Net on FBMS, but it remains below our approach on both precision and F-measure. Overall, our method shows top results in terms of precision and F-measure but is outperformed by some methods on recall. This is due to very long static sequences present in FBMS, which our recurrent memory-based method can not handle as well as methods with explicit tracking components, such as CUT <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure">Figure 6</ref> shows qualitative results of our method and the two next-best methods on FBMS: MP-Net-Video <ref type="bibr" target="#b43">[44]</ref> and CUT <ref type="bibr" target="#b21">[22]</ref>. MP-Net-Video relies highly on FST's <ref type="bibr" target="#b31">[32]</ref> tracking capabilities, and thus demonstrates the same background leaking failure mode, as seen in all the three examples. CUT misses parts of objects and incorrectly assigns background regions to the foreground in some cases, whereas our method demonstrates very high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FBMS. As shown in</head><p>SegTrack-v2. Our approach achieves IoU of 57.3 on this dataset. The relatively lower IoU compared to DAVIS is mainly due to the low resolution of some of the SegTrack-v2 videos, which differ from the high resolution ones used for training. We have also evaluated the state-of-the-art approaches for comparison on SegTrack. As shown in Table 5, our method performs better than <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref> on SegTrack, but worse than NLC <ref type="bibr" target="#b10">[11]</ref>. Note that NLC was designed and evaluated on SegTrack; we outperform it on DAVIS by 20.8% (see <ref type="table">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">ConvGRU visualization</head><p>We present a visualization of the gate activity in our Con-vGRU unit on two videos from the DAVIS validation set.</p><p>We use the unidirectional model in the following for better clarity. The reset and update gates of the ConvGRU, r t and z t respectively, are 3D matrices of 64 × h/8 × w/8 dimension. The overall behavior of ConvGRU is determined by the interplay of these 128 components. We use a selection of the components of r t and (1 − z t ) to interpret the workings of the gates. Our analysis is shown on two frames which correspond to the middle of the goat and dance-twirl sequences in (a) and (b) of <ref type="figure">Figure 7</ref>.</p><p>The outputs of the motion stream alone (left) and the final segmentation result (right) of the two examples are shown in the top row in the figure. The five rows below correspond each to one of the 64 dimensions of r t and (1 − z t ). These activations are shown as a grayscale heat map. High values for either of the two activations increases the influence of the previous state of a ConvGRU unit in the computation of the new state matrix. If both values are low, the state in the corresponding locations is rewritten with a new value; see equations <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_0">(4)</ref>.</p><p>For i = 8, we observe the update gate being selective based on the appearance information, i.e., it updates the state for foreground objects and duplicates it for the background. Note that motion does not play a role in this case. This can be seen in the example of stationary people (in the background) on the right, that are treated as foreground by the update gate. In the second row, showing responses for i = 18, both heatmaps are uniformly close to 0.5, which implies that the new features for this dimension are obtained by combining the previous state and the input at the time step t.</p><p>In the third row for i = 28, the update gate is driven by motion. It keeps the state for regions that are predicted as moving, and rewrites it for other regions in the frame. For the fourth row, where i = 41, r t is uniformly close to 0, whereas (1 − z t ) is close to 1. As a result, the input is effectively ignored and the previous state is duplicated. In the last row showing i = 63, a more complex behavior can be observed, where the gates rewrite the memory for regions in object boundaries, and use both the previous state and the current input for other regions in the frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces a novel approach for video object segmentation. Our method combines two complementary sources of information: appearance and motion, with a visual memory module, realized as a bidirectional convolutional gated recurrent unit. The ConvGRU module encodes spatio-temporal evolution of objects in a video and uses this encoding to improve motion segmentation. The effectiveness of our approach is validated on the DAVIS and FBMS datasets, where it shows top performance. Instance-level video object segmentation is a promising direction for future work.  <ref type="figure">Figure 7</ref>. Visualization of the ConvGRU gate activations for two sequences from the DAVIS validation set. The first row in each example shows the motion stream output and the final segmentation result. The other rows are the reset (rt) and the inverse of the update (1 − zt) gate activations for the corresponding ith dimension. These activations are shown as grayscale heat maps, where white denotes a high activation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our segmentation approach. Each video frame is processed by the appearance (green) and the motion (yellow) networks to produce an intermediate two-stream representation. The ConvGRU module combines this with the learned visual memory to compute the final segmentation result. The width (w') and height (h') of the feature map and the output are w/8 and h/8 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of ConvGRU with details for the candidate hidden state module, whereht is computed with two convolutional operations and a tanh nonlinearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the bidirectional processing with our Con-vGRU module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) goat, t = 23(b) dance-twirl, t = 19</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with a learning rate of 10 −4 . The learning rate</figDesc><table><row><cell>Method</cell><cell>Mean IoU</cell></row><row><cell>Ours</cell><cell>70.1</cell></row><row><cell>Ours + CRF</cell><cell>75.9</cell></row><row><cell>MP-Net</cell><cell>53.6</cell></row><row><cell>MP-Net + Obj</cell><cell>63.3</cell></row><row><cell>MP-Net + Obj + FST (MP-Net-V)</cell><cell>55.0</cell></row><row><cell>MP-Net + Obj + CRF (MP-Net-F)</cell><cell>70.0</cell></row><row><cell>Table 2. Comparison to MP-Net</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.6% drop in Measure PCM<ref type="bibr" target="#b2">[3]</ref> CVOS<ref type="bibr" target="#b41">[42]</ref> KEY<ref type="bibr" target="#b24">[25]</ref> MSG<ref type="bibr" target="#b3">[4]</ref> NLC<ref type="bibr" target="#b10">[11]</ref> CUT<ref type="bibr" target="#b21">[22]</ref> FST<ref type="bibr" target="#b31">[32]</ref> MP-Net-F<ref type="bibr" target="#b43">[44]</ref> Ours</figDesc><table><row><cell></cell><cell>Mean</cell><cell>40.1</cell><cell>48.2</cell><cell>49.8</cell><cell>53.3</cell><cell>55.1</cell><cell>55.2</cell><cell>55.8</cell><cell>70.0</cell><cell>75.9</cell></row><row><cell>J</cell><cell>Recall</cell><cell>34.3</cell><cell>54.0</cell><cell>59.1</cell><cell>61.6</cell><cell>55.8</cell><cell>57.5</cell><cell>64.9</cell><cell>85.0</cell><cell>89.1</cell></row><row><cell></cell><cell>Decay</cell><cell>15.2</cell><cell>10.5</cell><cell>14.1</cell><cell>2.4</cell><cell>12.6</cell><cell>2.3</cell><cell>0.0</cell><cell>1.4</cell><cell>0.0</cell></row><row><cell></cell><cell>Mean</cell><cell>39.6</cell><cell>44.7</cell><cell>42.7</cell><cell>50.8</cell><cell>52.3</cell><cell>55.2</cell><cell>51.1</cell><cell>65.9</cell><cell>72.1</cell></row><row><cell>F</cell><cell>Recall</cell><cell>15.4</cell><cell>52.6</cell><cell>37.5</cell><cell>60.0</cell><cell>51.9</cell><cell>61.0</cell><cell>51.6</cell><cell>79.2</cell><cell>83.4</cell></row><row><cell></cell><cell>Decay</cell><cell>12.7</cell><cell>11.7</cell><cell>10.6</cell><cell>5.1</cell><cell>11.4</cell><cell>3.4</cell><cell>2.9</cell><cell>2.5</cell><cell>1.3</cell></row><row><cell>T</cell><cell>Mean</cell><cell>51.3</cell><cell>24.4</cell><cell>25.2</cell><cell>29.1</cell><cell>41.4</cell><cell>26.3</cell><cell>34.3</cell><cell>56.3</cell><cell>25.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison to state-of-the-art methods on SegTrack-v2 with mean IoU.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NLC [11] Ours</cell></row><row><cell>47.8</cell><cell>54.3</cell><cell>67.2</cell><cell>57.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by the ERC advanced grant ALLEGRO, the MSR-Inria joint project, a Google research award and a Facebook gift. We gratefully acknowledge the support of NVIDIA with the donation of GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<ptr target="http://thoth.inrialpes.fr/research/mpnet.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learned-Miller. It&apos;s moving! A probabilistic model for causal motion segmentation in moving camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-shot video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.3" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertain. Fuzziness Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. National Academy of Sciences</title>
		<meeting>National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop track</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">COURSERA: Lecture 6.5 -Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rmsprop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
