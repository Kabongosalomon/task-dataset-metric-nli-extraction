<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
							<email>colinwei@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<email>adrien.gaidon@tri.global</email>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
							<email>nikos.arechiga@tri.global</email>
							<affiliation key="aff3">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
							<email>tengyuma@stanford.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern real-world large-scale datasets often have long-tailed label distributions <ref type="bibr" target="#b50">[Van Horn and Perona, 2017</ref><ref type="bibr" target="#b27">, Krishna et al., 2017</ref><ref type="bibr" target="#b33">, Lin et al., 2014</ref><ref type="bibr" target="#b11">, Everingham et al., 2010</ref><ref type="bibr" target="#b14">, Guo et al., 2016</ref><ref type="bibr" target="#b49">, Thomee et al., 2015</ref>. On these datasets, deep neural networks have been found to perform poorly on less represented classes <ref type="bibr" target="#b16">[He and Garcia, 2008</ref><ref type="bibr" target="#b50">, Van Horn and Perona, 2017</ref><ref type="bibr" target="#b4">, Buda et al., 2018</ref>. This is particularly detrimental if the testing criterion places more emphasis on minority classes. For example, accuracy on a uniform label distribution or the minimum accuracy among all classes are examples of such criteria. These are common scenarios in many applications <ref type="bibr" target="#b6">[Cao et al., 2018</ref><ref type="bibr" target="#b41">, Merler et al., 2019</ref><ref type="bibr" target="#b19">, Hinnefeld et al., 2018</ref> due to various practical concerns such as transferability to new domains, fairness, etc.</p><p>The two common approaches for learning long-tailed data are re-weighting the losses of the examples and re-sampling the examples in the SGD mini-batch (see <ref type="bibr" target="#b4">[Buda et al., 2018</ref><ref type="bibr" target="#b9">, Cui et al., 2019</ref><ref type="bibr" target="#b16">, He and Garcia, 2008</ref><ref type="bibr" target="#b17">, He and Ma, 2013</ref><ref type="bibr" target="#b8">, Chawla et al., 2002</ref> and the references therein). They both devise a training loss that is in expectation closer to the test distribution, and therefore can achieve better trade-offs between the accuracies of the frequent classes and the minority classes. However, because we have fundamentally less information about the minority classes and the models γ " γ # <ref type="figure">Figure 1</ref>: For binary classification with a linearly separable classifier, the margin γ i of the i-th class is defined to be the the minimum distance of the data in the i-th class to the decision boundary. We show that the test error with the uniform label distribution is bounded by a quantity that scales in 1 γ1 √ n1 + 1 γ2 √ n2 . As illustrated here, fixing the direction of the decision boundary leads to a fixed γ 1 + γ 2 , but the trade-off between γ 1 , γ 2 can be optimized by shifting the decision boundary. As derived in Section 3.1, the optimal trade-off is γ i ∝ n −1/4 i where n i is the sample size of the i-th class.</p><p>deployed are often huge, over-fitting to the minority classes appears to be one of the challenges in improving these methods.</p><p>We propose to regularize the minority classes more strongly than the frequent classes so that we can improve the generalization error of minority classes without sacrificing the model's ability to fit the frequent classes. Implementing this general idea requires a data-dependent or label-dependent regularizer -which in contrast to standard 2 regularization depends not only on the weight matrices but also on the labels -to differentiate frequent and minority classes. The theoretical understanding of data-dependent regularizers is sparse (see <ref type="bibr" target="#b42">, Nagarajan and Kolter, 2019</ref><ref type="bibr" target="#b1">, Arora et al., 2018</ref> for a few recent works.)</p><p>We explore one of the simplest and most well-understood data-dependent properties: the margins of the training examples. Encouraging a large margin can be viewed as regularization, as standard generalization error bounds (e.g., <ref type="bibr" target="#b3">[Bartlett et al., 2017</ref><ref type="bibr" target="#b58">, Wei et al., 2018</ref>) depend on the inverse of the minimum margin among all the examples. Motivated by the question of generalization with respect to minority classes, we instead study the minimum margin per class and obtain per-class and uniform-label test error bounds. 2 Minimizing the obtained bounds gives an optimal trade-off between the margins of the classes. See <ref type="figure">Figure 1</ref> for an illustration in the binary classification case.</p><p>Inspired by the theory, we design a label-distribution-aware loss function that encourages the model to have the optimal trade-off between per-class margins. The proposed loss extends the existing soft margin loss <ref type="bibr" target="#b52">[Wang et al., 2018a]</ref> by encouraging the minority classes to have larger margins. As a label-dependent regularization technique, our modified loss function is orthogonal to the re-weighting and re-sampling approach. In fact, we also design a deferred re-balancing optimization procedure that allows us to combine the re-weighting strategy with our loss (or other losses) in a more efficient way.</p><p>In summary, our main contributions are (i) we design a label-distribution-aware loss function to encourage larger margins for minority classes, (ii) we propose a simple deferred re-balancing optimization procedure to apply re-weighting more effectively, and (iii) our practical implementation shows significant improvements on several benchmark vision tasks, such as artificially imbalanced CIFAR and Tiny ImageNet <ref type="bibr">[tin]</ref>, and the real-world large-scale imbalanced dataset iNaturalist'18 <ref type="bibr" target="#b51">[Van Horn et al., 2018]</ref>. that it discards a large portion of the data and thus is not feasible when data imbalance is extreme. Over-sampling is effective in a lot of cases but can lead to over-fitting of the minority classes <ref type="bibr" target="#b8">[Chawla et al., 2002</ref><ref type="bibr" target="#b9">, Cui et al., 2019</ref>. Stronger data augmentation for minority classes can help alleviate the over-fitting <ref type="bibr" target="#b8">[Chawla et al., 2002</ref><ref type="bibr" target="#b60">, Zou et al., 2018</ref>.</p><p>Re-weighting. Cost-sensitive re-weighting assigns (adaptive) weights for different classes or even different samples. The vanilla scheme re-weights classes proportionally to the inverse of their frequency <ref type="bibr" target="#b54">, Wang et al., 2017</ref>. Re-weighting methods tend to make the optimization of deep models difficult under extreme data imbalanced settings and large-scale scenarios <ref type="bibr" target="#b21">[Huang et al., , 2019</ref>. <ref type="bibr" target="#b9">Cui et al. [2019]</ref> observe that re-weighting by inverse class frequency yields poor performance on frequent classes, and thus propose re-weighting by the inverse effective number of samples. This is the main prior work that we empirically compare with.</p><p>Another line of work assigns weights to each sample based on their individual properties. Focal loss  down-weights the well-classified examples;  suggests an improved technique which down-weights examples with either very small gradients or large gradients because examples with small gradients are well-classified and those with large gradients tend to be outliers.</p><p>In a recent work <ref type="bibr" target="#b5">[Byrd and Lipton, 2019</ref>], Byrd and Lipton study the effect of importance weighting and show that empirically importance weighting does not have a significant effect when no regularization is applied, which is consistent with the theoretical prediction in <ref type="bibr" target="#b47">[Soudry et al., 2018]</ref> that logistical regression without regularization converges to the max margin solution. In our work, we explicitly encourage rare classes to have higher margin, and therefore we don't converge to a max margin solution. Moreover, in our experiments, we apply non-trivial 2 -regularization to achieve the best generalization performance. We also found deferred re-weighting (or deferred re-sampling) are more effective than re-weighting and re-sampling from the beginning of the training.</p><p>In contrast, and orthogonally to these papers above, our main technique aims to improve the generalization of the minority classes by applying additional regularization that is orthogonal to the re-weighting scheme. We also propose a deferred re-balancing optimization procedure to improve the optimization and generalization of a generic re-weighting scheme.</p><p>Margin loss. The hinge loss is often used to obtain a "max-margin" classifier, most notably in SVMs <ref type="bibr" target="#b48">[Suykens and Vandewalle, 1999]</ref>. Recently, Large-Margin Softmax <ref type="bibr" target="#b36">[Liu et al., 2016]</ref>, Angular Softmax <ref type="bibr" target="#b37">[Liu et al., 2017a]</ref>, and Additive Margin Softmax <ref type="bibr" target="#b52">[Wang et al., 2018a]</ref> have been proposed to minimize intra-class variation in predictions and enlarge the inter-class margin by incorporating the idea of angular margin. In contrast to the class-independent margins in these papers, our approach encourages bigger margins for minority classes. Uneven margins for imbalanced datasets are also proposed and studied in <ref type="bibr" target="#b31">[Li et al., 2002]</ref> and the recent work <ref type="bibr" target="#b24">[Khan et al., 2019</ref>. Our theory put this idea on a more theoretical footing by providing a concrete formula for the desired margins of the classes alongside good empirical progress.</p><p>Label shift in domain adaptation. The problem of learning imbalanced datasets can be also viewed as a label shift problem in transfer learning or domain adaptation (for which we refer the readers to the survey <ref type="bibr" target="#b53">[Wang and Deng, 2018]</ref> and the reference therein). In a typical label shift formulation, the difficulty is to detect and estimate the label shift, and after estimating the label shift, re-weighting or re-sampling is applied. We are addressing a largely different question: can we do better than re-weighting or re-sampling when the label shift is known? In fact, our algorithms can be used to replace the re-weighting steps of some of the recent interesting work on detecting and correcting label shift <ref type="bibr" target="#b35">[Lipton et al., 2018</ref><ref type="bibr" target="#b2">, Azizzadenesheli et al., 2019</ref>.</p><p>Distributionally robust optimization (DRO) is another technique for domain adaptation (see <ref type="bibr">[Duchi et al., Hashimoto et al., 2018</ref><ref type="bibr" target="#b7">, Carmon et al., 2019</ref> and the reference therein.) However, the formulation assumes no knowledge of the target label distribution beyond a bound on the amount of shift, which makes the problem very challenging. We here assume the knowledge of the test label distribution, using which we design efficient methods that can scale easily to large-scale vision datasets with significant improvements.</p><p>Meta-learning. Meta-learning is also used in improving the performance on imbalanced datasets or the few shot learning settings. We refer the readers to <ref type="bibr" target="#b54">[Wang et al., 2017</ref><ref type="bibr" target="#b46">, Shu et al., 2019</ref><ref type="bibr" target="#b55">, Wang et al., 2018b</ref> and the references therein. So far, we generally believe that our approaches that modify the losses are more computationally efficient than meta-learning based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical Motivations</head><p>Problem setup and notations. We assume the input space is R d and the label space is {1, . . . , k}. Let x denote the input and y denote the corresponding label. We assume that the class-conditional distribution P(x | y) is the same at training and test time. Let P j denote the class-conditional distribution, i.e. P j = P(x | y = j). We will use P bal to denote the balanced test distribution which first samples a class uniformly and then samples data from P j .</p><p>For a model f : R d → R k that outputs k logits, we use L bal [f ] to denote the standard 0-1 test error on the balanced data distribution:</p><formula xml:id="formula_0">L bal [f ] = Pr (x,y)∼Pbal [f (x) y &lt; max =y f (x) ]</formula><p>Similarly, the error L j for class j is defined as</p><formula xml:id="formula_1">L j [f ] = Pr (x,y)∼Pj [f (x) y &lt; max =y f (x) ]. Suppose we have a training dataset {(x i , y i )} n i=1 .</formula><p>Let n j be the number of examples in class j. Let S j = {i : y i = j} denote the example indices corresponding to class j.</p><p>Define the margin of an example (x, y) as</p><formula xml:id="formula_2">γ(x, y) = f (x) y − max j =y f (x) j<label>(1)</label></formula><p>Define the training margin for class j as:</p><formula xml:id="formula_3">γ j = min i∈Sj γ(x i , y i )<label>(2)</label></formula><p>We consider the separable cases (meaning that all the training examples are classified correctly) because neural networks are often over-parameterized and can fit the training data well. We also note that the minimum margin of all the classes, γ min = min{γ 1 , . . . , γ k }, is the classical notion of training margin studied in the past <ref type="bibr" target="#b26">[Koltchinskii et al., 2002]</ref>.</p><p>Fine-grained generalization error bounds. Let F be the family of hypothesis class. Let C(F) be some proper complexity measure of the hypothesis class F. There is a large body of recent work on measuring the complexity of neural networks (see <ref type="bibr" target="#b3">[Bartlett et al., 2017</ref><ref type="bibr" target="#b12">, Golowich et al., 2017</ref> and references therein), and our discussion below is orthogonal to the precise choices. When the training distribution and the test distribution are the same, the typical generalization error bounds scale in C(F)/ √ n. That is, in our case, if the test distribution is also imbalanced as the training distribution, then imbalanced test error 1 γ min</p><formula xml:id="formula_4">C(F) n<label>(3)</label></formula><p>Note that the bound is oblivious to the label distribution, and only involves the minimum margin across all examples and the total number of data points. We extend such bounds to the setting with balanced test distribution by considering the margin of each class. As we will see, the more fine-grained bound below allows us to design new training loss function that is customized to the imbalanced dataset. Theorem 1 (Informal and simplified version of Theorem 2). With high probability (1 − n −5 ) over the randomness of the training data, the error L j for class j is bounded by</p><formula xml:id="formula_5">L j [f ] 1 γ j C(F) n j + log n √ n j<label>(4)</label></formula><p>where we use to hide constant factors. As a direct consequence,</p><formula xml:id="formula_6">L bal [f ] 1 k k j=1 1 γ j C(F) n j + log n √ n j<label>(5)</label></formula><p>Class-distribution-aware margin trade-off. The generalization error bound (4) for each class suggests that if we wish to improve the generalization of minority classes (those with small n j 's), we should aim to enforce bigger margins γ j 's for them. However, enforcing bigger margins for minority classes may hurt the margins of the frequent classes. What is the optimal trade-off between the margins of the classes? An answer for the general case may be difficult, but fortunately we can obtain the optimal trade-off for the binary classification problem.</p><p>With k = 2 classes, we aim to optimize the balanced generalization error bound provided in <ref type="formula" target="#formula_6">(5)</ref>, which can be simplified to (by removing the low order term log n √ nj and the common factor C(F))</p><formula xml:id="formula_7">1 γ 1 √ n 1 + 1 γ 2 √ n 2<label>(6)</label></formula><p>At the first sight, because γ 1 and γ 2 are complicated functions of the weight matrices, it appears difficult to understand the optimal margins. However, we can figure out the relative scales between γ 1 and γ 2 . Suppose γ 1 , γ 2 &gt; 0 minimize the equation above, we observe that any γ 1 = γ 1 − δ and γ 2 = γ 2 + δ (for δ ∈ (−γ 2 , γ 1 )) can be realized by the same weight matrices with a shifted bias term (See <ref type="figure">Figure 1</ref> for an illustration). Therefore, for γ 1 , γ 2 to be optimal, they should satisfy</p><formula xml:id="formula_8">1 γ 1 √ n 1 + 1 γ 2 √ n 2 ≥ 1 (γ 1 − δ) √ n 1 + 1 (γ 2 + δ) √ n 2<label>(7)</label></formula><p>The equation above implies that</p><formula xml:id="formula_9">γ 1 = C n 1/4 1 , and γ 2 = C n 1/4 2<label>(8)</label></formula><p>for some constant C. Please see a detailed derivation in the Section A.</p><p>Fast rate vs slow rate, and the implication on the choice of margins. The bound in Theorem 1 may not necessarily be tight. The generalization bounds that scale in 1/ √ n (or 1/ √ n i here with imbalanced classes) are generally referred to the "slow rate" and those that scale in 1/n are referred to the "fast rate". With deep neural networks and when the model is sufficiently big enough, it is possible that some of these bounds can be improved to the fast rate. See  for some recent development. In those cases, we can derive the optimal trade-off of the margin to be n i ∝ n −1/3 i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label-Distribution-Aware Margin Loss</head><p>Inspired by the trade-off between the class margins in Section 3.1 for two classes, we propose to enforce a class-dependent margin for multiple classes of the form</p><formula xml:id="formula_10">γ j = C n 1/4 j<label>(9)</label></formula><p>We will design a soft margin loss function to encourage the network to have the margins above. Let (x, y) be an example and f be a model. For simplicity, we use z j = f (x) j to denote the j-th output of the model for the j-th class.</p><p>The most natural choice would be a multi-class extension of the hinge loss:</p><formula xml:id="formula_11">L LDAM-HG ((x, y); f ) = max(max j =y {z j } − z y + ∆ y , 0) (10) where ∆ j = C n 1/4 j for j ∈ {1, . . . , k}<label>(11)</label></formula><p>Here C is a hyper-parameter to be tuned. In order to tune the margin more easily, we effectively normalize the logits (the input to the loss function) by normalizing last hidden activation to 2 norm 1, and normalizing the weight vectors of the last fully-connected layer to 2 norm 1, following the previous work <ref type="bibr" target="#b52">[Wang et al., 2018a]</ref>. Empirically, the non-smoothness of hinge loss may pose difficulties for optimization. The smooth relaxation of the hinge loss is the following cross-entropy loss with enforced margins:</p><formula xml:id="formula_12">L LDAM ((x, y); f ) = − log e zy−∆y e zy−∆y + j =y e zj (12) where ∆ j = C n 1/4 j for j ∈ {1, . . . , k}<label>(13)</label></formula><p>In the previous work <ref type="bibr" target="#b36">[Liu et al., 2016</ref><ref type="bibr" target="#b37">, 2017a</ref><ref type="bibr" target="#b52">, Wang et al., 2018a</ref> where the training set is usually balanced, the margin ∆ y is chosen to be a label independent constant C, whereas our margin depends on the label distribution.</p><p>Remark: Attentive readers may find the loss L LDAM somewhat reminiscent of the re-weighting because in the binary classification case -where the model outputs a single real number which is passed through a sigmoid to be converted into a probability, -both the two approaches change the gradient of an example by a scalar factor. However, we remark two key differences: the scalar factor introduced by the re-weighting only depends on the class, whereas the scalar introduced by L LDAM also depends on the output of the model; for multiclass classification problems, the proposed loss L LDAM affects the gradient of the example in a more involved way than only introducing a scalar factor. Moreover, recent work has shown that, under separable assumptions, the logistical loss, with weak regularization <ref type="bibr" target="#b58">[Wei et al., 2018]</ref> or without regularization <ref type="bibr" target="#b47">[Soudry et al., 2018]</ref>, gives the max margin solution, which is in turn not effected by any re-weighting by its definition. This further suggests that the loss L LDAM and the re-weighting may complement each other, as we have seen in the experiments. (Re-weighting would affect the margin in the non-separable data case, which is left for future work.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deferred Re-balancing Optimization Schedule</head><p>Cost-sensitive re-weighting and re-sampling are two well-known and successful strategies to cope with imbalanced datasets because, in expectation, they effectively make the imbalanced training distribution closer to the uniform test distribution. The known issues with applying these techniques are (a) re-sampling the examples in minority classes often causes heavy over-fitting to the minority classes when the model is a deep neural network, as pointed out in prior work (e.g., <ref type="bibr" target="#b9">[Cui et al., 2019]</ref>), and (b) weighting up the minority classes' losses can cause difficulties and instability in optimization, especially when the classes are extremely imbalanced <ref type="bibr" target="#b9">[Cui et al., 2019</ref>. In fact, <ref type="bibr" target="#b9">Cui et al. [2019]</ref> develop a novel and sophisticated learning rate schedule to cope with the optimization difficulty.</p><p>We observe empirically that re-weighting and re-sampling are both inferior to the vanilla empirical risk minimization (ERM) algorithm (where all training examples have the same weight) before annealing the learning rate in the following sense. The features produced before annealing the learning rate by re-weighting and re-sampling are worse than those produced by ERM. (See <ref type="figure" target="#fig_4">Figure 6</ref> for an ablation study of the feature quality performed by training linear classifiers on top of the features on a large balanced dataset.)</p><p>Inspired by this, we develop a deferred re-balancing training procedure (Algorithm 1), which first trains using vanilla ERM with the LDAM loss before annealing the learning rate, and then deploys a re-weighted LDAM loss with a smaller learning rate. Empirically, the first stage of training leads to a good initialization for the second stage of training with re-weighted losses. Because the loss is non-convex and the learning rate in the second stage is relatively small, the second stage does not move the weights very far. Interestingly, with our LDAM loss and deferred re-balancing training, the vanilla re-weighting scheme (which re-weights by the inverse of the number of examples in each class) works as well as the re-weighting scheme introduced in prior work <ref type="bibr" target="#b9">[Cui et al., 2019]</ref>. We also found that with our re-weighting scheme and LDAM, we are less sensitive to early stopping than <ref type="bibr" target="#b9">[Cui et al., 2019]</ref>.</p><p>Algorithm 1 Deferred Re-balancing Optimization with LDAM Loss</p><formula xml:id="formula_13">Require: Dataset D = {(x i , y i )} n i=1 .</formula><p>A parameterized model f θ 1: Initialize the model parameters θ randomly 2: for t = 1 to T 0 do 3:</p><formula xml:id="formula_14">B ← SampleMiniBatch(D, m) a mini-batch of m examples 4: L(f θ ) ← 1 m (x,y)∈B L LDAM ((x, y); f θ ) 5: f θ ← f θ − α∇ θ L(f θ ) one SGD step 6:</formula><p>Optional: α ← α/τ anneal learning rate by a factor τ if necessary </p><formula xml:id="formula_15">f θ ← f θ − α 1 (x,y)∈B n −1 y ∇ θ L(f θ )</formula><p>one SGD step with re-normalized learning rate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed algorithm on artificially created versions of IMDB review <ref type="bibr" target="#b40">[Maas et al., 2011]</ref>, CIFAR-10, CIFAR-100 <ref type="bibr" target="#b28">[Krizhevsky and Hinton, 2009</ref>] and Tiny ImageNet <ref type="bibr">[Russakovsky et al., 2015, tin]</ref> with controllable degrees of data imbalance, as well as a real-world large-scale imbalanced dataset, iNaturalist 2018 <ref type="bibr" target="#b51">[Van Horn et al., 2018]</ref>. Our core algorithm is developed using PyTorch <ref type="bibr" target="#b43">[Paszke et al., 2017]</ref>.</p><p>Baselines. We compare our methods with the standard training and several state-of-the-art techniques and their combinations that have been widely adopted to mitigate the issues with training on imbalanced datasets: (1) Empirical risk minimization (ERM) loss: all the examples have the same weights; by default, we use standard cross-entropy loss.</p><p>(2) Re-Weighting (RW): we re-weight each sample by the inverse of the sample size of its class, and then re-normalize to make the weights 1 on average in the mini-batch.</p><p>(3) Re-Sampling (RS): each example is sampled with probability proportional to the inverse sample size of its class. (4) CB <ref type="bibr" target="#b9">[Cui et al., 2019]</ref>: the examples are re-weighted or re-sampled according to the inverse of the effective number of samples in each class, defined as (1 − β ni )/(1 − β), instead of inverse class frequencies. This idea can be combined with either re-weighting or re-sampling. (5) Focal: we use the recently proposed focal loss  as another baseline. (6) SGD schedule: by SGD, we refer to the standard schedule where the learning rates are decayed a constant factor at certain steps; we use a standard learning rate decay schedule.</p><p>Our proposed algorithm and variants. We test combinations of the following techniques proposed by us.</p><p>(1) DRW and DRS: following the proposed training Algorithm 1, we use the standard ERM optimization schedule until the last learning rate decay, and then apply re-weighting or resampling for optimization in the second stage.</p><p>(2) LDAM: the proposed Label-Distribution-Aware Margin losses as described in Section 3.2.</p><p>When two of these methods can be combined, we will concatenate the acronyms with a dash in between as an abbreviation. The main algorithm we propose is LDAM-DRW. Please refer to Section B for additional implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental results on IMDB review dataset</head><p>IMDB review dataset consists of 50,000 movie reviews for binary sentiment classification <ref type="bibr" target="#b40">[Maas et al., 2011]</ref>. The original dataset contains an evenly distributed number of positive and negative reviews. We manually created an imbalanced training set by removing 90% of negative reviews. We train a two-layer bidirectional LSTM with Adam optimizer <ref type="bibr" target="#b25">[Kingma and Ba, 2014]</ref>. The results are reported in <ref type="table" target="#tab_0">Table 1</ref>.  Imbalanced CIFAR-10 and CIFAR-100. The original version of CIFAR-10 and CIFAR-100 contains 50,000 training images and 10,000 validation images of size 32 × 32 with 10 and 100 classes, respectively. To create their imbalanced version, we reduce the number of training examples per class and keep the validation set unchanged. To ensure that our methods apply to a variety of settings, we consider two types of imbalance: long-tailed imbalance <ref type="bibr" target="#b9">[Cui et al., 2019]</ref> and step imbalance <ref type="bibr" target="#b4">[Buda et al., 2018]</ref>. We use imbalance ratio ρ to denote the ratio between sample sizes of the most frequent and least frequent class, i.e., ρ = max i {n i }/ min i {n i }. Long-tailed imbalance follows an exponential decay in sample sizes across different classes. For step imbalance setting, all minority classes have the same sample size, as do all frequent classes. This gives a clear distinction between minority classes and frequent classes, which is particularly useful for ablation study. We further define the fraction of minority classes as µ. By default we set µ = 0.5 for all experiments.</p><p>We report the top-1 validation error of various methods for imbalanced versions of CIFAR-10 and CIFAR-100 in <ref type="table" target="#tab_1">Table 2</ref>. Our proposed approach is LDAM-DRW, but we also include a various combination of our two techniques with other losses and training schedule for our ablation study.</p><p>We first show that the proposed label-distribution-aware margin cross-entropy loss is superior to pure cross-entropy loss and one of its variants tailored for imbalanced data, focal loss, while no data-rebalance learning schedule is applied. We also demonstrate that our full pipeline outperforms the previous state-of-the-arts by a large margin. To further demonstrate that the proposed LDAM loss is essential, we compare it with regularizing by a uniform margin across all classes under the setting of cross-entropy loss and hinge loss. We use M-DRW to denote the algorithm that uses a cross-entropy loss with uniform margin <ref type="bibr" target="#b52">[Wang et al., 2018a]</ref> to replace LDAM, namely, the ∆ j in equation <ref type="formula" target="#formula_2">(13)</ref> is chosen to be a tuned constant that does not depend on the class j. Hinge loss (HG) suffers from optimization issues with 100 classes so we constrain its experiment setting with CIFAR-10 only.  We anneal decay the learning rate at epoch 160 for all algorithms. Our DRW schedule uses ERM before annealing the learning rate and thus performs worse than RW and RS before that point, as expected. However, it outperforms the others significantly after annealing the learning rate. See Section 4.4 for more analysis. xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imbalanced but known test label distribution:</head><p>We also test the performance of an extension of our algorithm in the setting where the test label distribution is known but not uniform. Please see Section C.5 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual recognition on iNaturalist 2018 and imbalanced Tiny ImageNet</head><p>We further verify the effectiveness of our method on large-scale imbalanced datasets. The iNatualist species classification and detection dataset <ref type="bibr" target="#b51">[Van Horn et al., 2018</ref>] is a real-world large-scale imbalanced dataset which has 437,513 training images with a total of 8,142 classes in its 2018 version. We adopt the official training and validation splits for our experiments. The training datasets have a long-tailed label distribution and the validation set is designed to have a balanced label distribution. We use ResNet-50 as the backbone network across all experiments for iNaturalist 2018. <ref type="table" target="#tab_2">Table 3</ref> summarizes top-1 validation error for iNaturalist 2018. Notably, our full pipeline is able to outperform the ERM baseline by 10.86% and previous state-of-the-art by 6.88% in top-1 error. Please refer to Appendix C.2 for results on imbalanced Tiny ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Evaluating generalization on minority classes. To better understand the improvement of our algorithms, we show per-class errors of different methods in <ref type="figure">Figure 2</ref> on imbalanced CIFAR-10. Please see the caption there for discussions.</p><p>Evaluating deferred re-balancing schedule. We compare the learning curves of deferred rebalancing schedule with other baselines in <ref type="figure">Figure 3</ref>. In <ref type="figure" target="#fig_4">Figure 6</ref> of Section C.3, we further show that even though ERM in the first stage has slightly worse or comparable balanced test error compared to RW and RS, in fact the features (the last-but-one layer activations) learned by ERM are better than those by RW and RS. This agrees with our intuition that the second stage of DRW, starting from better features, adjusts the decision boundary and locally fine-tunes the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose two methods for training on imbalanced datasets, label-distribution-aware margin loss (LDAM), and a deferred re-weighting (DRW) training schedule. Our methods achieve significantly improved performance on a variety of benchmark vision tasks. Furthermore, we provide a theoretically-principled justification of LDAM by showing that it optimizes a uniform-label generalization error bound. For DRW, we believe that deferring re-weighting lets the model avoid the drawbacks associated with re-weighting or re-sampling until after it learns a good initial representation (see some analysis in <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 6</ref>). However, the precise explanation for DRW's success is not fully theoretically clear, and we leave this as a direction for future work.</p><p>A Missing Proofs and Derivations in Section 3.1</p><p>Let L γ,j denote the hard margin loss on examples from class j:</p><formula xml:id="formula_16">L γ,j [f ] = Pr x∼Pj [max j =j f (x) j &gt; f (x) j − γ]</formula><p>and letL γ,j denote its empirical variant. For a hypothesis class F, letR j (F) denote the empirical Rademacher complexity of its class j margin:</p><formula xml:id="formula_17">R j (F) = 1 n j E σ   sup f ∈F i∈Sj σ i [f (x i ) j − max j =j f (x i ) j ]  </formula><p>where σ is a vector of i.i.d. uniform {−1, +1} bits. The following formal versiom of Theorem 1 bounds the balanced-class generalization P bal using samples from P. Theorem 2. With probability 1 − δ over the randomness of the training data, for all choices of classdependent margins γ 1 , . . . , γ k &gt; 0, all hypotheses f ∈ F will have balanced-class generalization bounded by</p><formula xml:id="formula_18">L bal [f ] ≤ 1 k   k j=1L γj ,j [f ] + 4 γ jR j (F) + j (γ j )   where j (γ) log log 2 ( 2 max x∈X ,f ∈F |f (x)| γ )+log 2c δ nj</formula><p>is typically a low-order term in n j . Concretely, the Rademacher complexityR j (F) will typically scale as C(F ) nj for some complexity measure C(F), in which case</p><formula xml:id="formula_19">L bal [f ] ≤ 1 k   k j=1L γj ,j [f ] + 4 γ j C(F) n j + j (γ j )  </formula><p>Proof. We will prove generalization separately for each class j and then union bound over all classes.</p><p>Let L j [f ] denote the test 0 − 1 error of classifier f on examples drawn from P j . As the examples for class j is a set of n j i.i.d. draws from the conditional distribution P j , we can apply the standard margin-based generalization bound (Theorem 2 of <ref type="bibr" target="#b23">[Kakade et al., 2009]</ref>) to obtain with probability 1 − δ/c, for all choices of γ j &gt; 0 and f ∈ F,</p><formula xml:id="formula_20">L j [f ] ≤L γj ,j + 4 γ jR j (F) + log log 2 ( 2 max x∈X ,f ∈F |f (x)| γj ) n j + log 2c δ n j<label>(14)</label></formula><p>Now since L bal = 1 k k j=1 L j , we can union bound over all classes and average (14) to get the desired result.</p><p>We will now show that in the case of k = 2 classes, it is always possible to shift the margins in order to optimize the generalization bound of Theorem 2 by adding bias terms. Theorem 3. For binary classification, let F be a hypothesis class of neural networks with a bias term, i.e. F = {f + b} where f is a neural net function and b ∈ R 2 is a bias, with Rademacher complexity upper boundR j (F) ≤ C(F ) nj . Suppose some classifier f ∈ F can achieve a total sum of margins γ 1 + γ 2 = β with γ 1 , γ 2 &gt; 0. Then there exists a classifier f ∈ F with margins γ 1 = βn which with probability 1 − δ obtains the optimal generalization guarantees for Theorem 2:</p><formula xml:id="formula_21">L bal [f ] ≤ min γ1+γ2=β   2 γ 1 C(F) n 1 + 2 γ 2 C(F) n 2   + (γ 1 ) + (γ 2 )</formula><p>where is defined in Theorem 2. Furthermore, this f is obtained via f + b for some bias b .  <ref type="figure" target="#fig_2">Fig. 4a</ref> and <ref type="figure" target="#fig_2">Fig. 4b</ref> belong to long-tailed imbalance type and <ref type="figure" target="#fig_2">Fig. 4c</ref> is a step imbalance distribution.</p><p>Proof. For our bias b , we simply choose b 1 = (γ 1 − γ 1 )/2, b 2 = −(γ 1 − γ 1 )/2. Now note that adding a bias term simply shifts the margins for class 1 by b 1 − b 2 , giving a new margin of γ 2 . Likewise, the margin for class 2 becomes</p><formula xml:id="formula_22">b 2 − b 1 + γ 2 = γ 2 − γ 1 + γ 1 = β − γ 1 = γ 2</formula><p>Now we apply Theorem 2 to get with probability 1 − δ the generalization error bound</p><formula xml:id="formula_23">L bal [f ] ≤ 2 γ 1 C(F) n 1 + 2 γ 2 C(F) n 2 + (γ 1 ) + (γ 2 )</formula><p>To see that γ 1 , γ 2 indeed solve</p><formula xml:id="formula_24">min γ1+γ2=β 1 γ 1 1 n 1 + 1 γ 2 1 n 2</formula><p>we can substitute γ 2 = β − γ 1 into the expression and set the derivative to 0, obtaining 1 (β − γ 1 ) 2 √ n 2 − 1 γ 2 1 √ n 1 = 0</p><p>Solving gives γ 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation details</head><p>Label distributions. Some example distributions of our artificially created imbalance are shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Implementation details for CIFAR. For CIFAR-10 and CIFAR-100, we follow the simple data augmentation in  for training: 4 pixels are padded on each side, and a 32 × 32 crop is randomly sampled from the padded image or its horizontal flip. We use ResNet-32  as our base network, and use stochastic gradient descend with momentum of 0.9, weight decay of 2 × 10 −4 for training. The model is trained with a batch size of 128 for 200 epochs. For fair comparison, we use an initial learning rate of 0.1, then decay by 0.01 at the 160th epoch and again at the 180th epoch. We also use linear warm-up learning rate schedule  for the first 5 epochs for fair comparison. Notice that the warm-up trick is essential for the training of re-weighting, but it won't affect other algorithms in our experiments. We tune C to normalize ∆ j so that the largest enforced margin is 0.5.</p><p>Implementation details for Tiny ImageNet. For Tiny ImageNet, we perform simple horizontal flips, taking random crops of size 64 × 64 from images padded by 8 pixels on each side. We perform 1 crop test with the validation images. We use ResNet-18  as our base network, and use stochastic gradient descend with momentum of 0.9, weight decay of 2 × 10 −4 for training. We train the model using a batch size of 128 for 120 epochs with a initial learning rate of 0.1. We decay the learning rate by 0.1 at epoch 90. We tune C to normalize ∆ j so that the largest enforced margin is 0.5.</p><p>Implementation details for iNaturalist 2018. On iNaturalist 2018, we followed the same training strategy used by  and trained ResNet-50 with 4 Tesla V100 GPUs. Each image is first  <ref type="figure">Figure 5</ref>: Visualization of feature distribution of different methods. We constrain the feature dimension to be three and normalize it for better illustration. The top row has the feature distribution on the training set and the second row the feature distributions on the validation set. We can see that LDAM appears to have more separate training features compared to the other methods. We note this visualization is only supposed to provide qualitative intuitions, and the differences between our methods and other methods may be more significant for harder tasks with higher feature dimension.</p><p>(For example, here the accuracies of re-weighting and LDAM are very similar, whereas for large-scale datasets with higher feature dimensions, the gap is significantly larger.) resized by setting the shorter side to 256 pixels, and then a 224 × 224 crop is randomly sampled from an image or its horizontal flip. We train the network for 90 epochs with an initial learning rate of 0.1. We anneal the learning rate at epoch 30 and 60. For our two-stage training schedule, we rebalance the training data starting from epoch 60. We tune C to normalize ∆ j so that the largest enforced margin is 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Feature visualization</head><p>To have a better understanding of our proposed LDAM loss, we use a toy example to visualize feature distributions trained under different schemes. We train a 7-layer CNN as adopted in <ref type="bibr" target="#b38">[Liu et al., 2017b]</ref> on <ref type="bibr">MNIST [LeCun et al., 1998</ref>] with step imbalance setting (ρ = 100, µ = 0.5). For a more intuitive visualization, we constrain the feature dimension to 3 and normalize the feature before feeding it into the final fully-connected layer, allowing us to scatter the features on a unit hyper-sphere in a 3D frame. The visualization is shown in <ref type="figure">Figure 5</ref> with additional discussion in the caption. The left-5 classes are frequent and denoted with -F. The features obtained from ERM setting has the strongest performance, confirming our intuition that the second stage of DRW starts from better features. In the second stage, DRW re-weights the example again, adjusting the decision boundary and locally fine-tuning the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Visual Recognition on imbalanced Tiny ImageNet</head><p>In addition to artificial imbalanced CIFAR, we further verify the effectiveness of our method on artificial imbalanced Tiny ImageNet. The Tiny ImageNet dataset has 200 classes. Each class has 500 training images and 50 validation images of size 64 × 64. We use the same strategy described above to create long-tailed and step imbalance versions of Tiny ImageNet. The results are presented in <ref type="table" target="#tab_4">Table 4</ref>. While Class-Balanced Softmax performs worse than the ERM baseline, the proposed LDAM and DRW demonstrate consistent improvements over ERM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Comparing feature extractors trained by different schemes</head><p>As discussed in Section 4.4, we train a linear classifier on features extracted by backbone filters pretrained under different schemes. We could conclude that for highly imbalanced settings (step imbalance with ρ = 100, µ = 0.5), backbone networks trained by ERM learns the most expressive feature embedding compared with the other two methods, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Comparing DRW and DRS</head><p>Our proposed deferred re-balancing optimization schedule can be combined with either re-weighting or re-sampling. We use re-weighting as the default choice in the main paper. Here we demonstrate through <ref type="table" target="#tab_5">Table 5</ref> that re-weighting and re-sampling exhibit similar performance when combined with deferred re-balancing scheme. This result could be explained by the fact that the second stage does not move the weights far. Re-balancing in the second stage mostly re-adjusts the decision boundary and thus there is no significant difference between using re-weighting or re-sampling for the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Imbalanced Test Label Distributions</head><p>Though the majority of our experiments follow the uniform test distribution setting, it could be extended to imbalanced test distribution naturally. Suppose the number of training examples in class i is denoted by n i and the number of test examples in class i is denoted by n i , then we could adapt  As discussed in C.5 we run two random seeds for generating test distributions. Here <ref type="figure" target="#fig_5">Figure 7b</ref> denotes the left column in <ref type="table" target="#tab_6">Table 6.</ref> the LDAM simply by encouraging the margin ∆ i for class i with</p><formula xml:id="formula_25">∆ j ∝ n i n i 1/4<label>(15)</label></formula><p>To complement our main result, In <ref type="table" target="#tab_6">Table 6</ref>, we demonstrate that this extended algorithm can also work well when the test distribution is imbalanced. We use the same rule as described in Section 4 to generate imbalanced test label distribution and then permute randomly the frequency of the labels (so that the training label distribution is very different from the test label distribution.). For example, in the experiment shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the training label distribution of the column of "long-tailed with ρ = 100" follows <ref type="figure" target="#fig_5">Figure 7a</ref> (which is the same as <ref type="figure" target="#fig_2">Figure 4b</ref>) whereas the test label distribution is shown in <ref type="figure" target="#fig_5">Figure 7b</ref> and <ref type="figure" target="#fig_5">Figure 7c</ref>. For each of the settings reported in <ref type="table" target="#tab_6">Table 6</ref>, we have run it with two different random seeds for generating the test label distribution, and we see qualitatively similar results. We refer to our code for the precise label distribution generated in the experiments. 3 3 Code available at https://github.com/kaidic/LDAM-DRW. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>y)∈B n −1 y · L LDAM ((x, y); f θ ) standard re-weighting by frequency 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Per-class top-1 error on CIFAR-10 with step imbalance (ρ = 100, µ = 0.5). Classes 0-F to 4-F are frequent classes, and the rest are minority classes. Under this extremely imbalanced setting RW suffers from under-fitting, while RS over-fits on minority examples. On the contrary, the proposed algorithm exhibits great generalization on minority classes while keeping the performance on frequent classes almost unaffected. This suggests we succeeded in regularizing minority classes more strongly. Imbalanced training errors (dotted lines) and balanced test errors (solid lines) on CIFAR-10 with long-tailed imbalance (ρ = 100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Number of training examples per class in artificially created imbalanced CIFAR-10 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>In the setting of training mbalanced CIFAR-10 dataset with step imbalance of ρ = 100, µ = 0.5, to test the quality of the features obtained by the ERM, RW and RS before annealing the learning rate, we use a subset of the balanced validation dataset to train linear classifiers on top of the features, and evaluate the per-class validation error on the rest of the validation data. (Little over-fitting in training the linear classifier is observed.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Example distributions when train and test distributions are both imbalanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Top-1 validation errors on imbalanced IMDB review dataset. Our proposed approach LDAM-DRW outperforms the baselines.</figDesc><table><row><cell>Approach</cell><cell cols="3">Error on positive reviews Error on negative reviews Mean Error</cell></row><row><cell>ERM</cell><cell>2.86</cell><cell>70.78</cell><cell>36.82</cell></row><row><cell>RS</cell><cell>7.12</cell><cell>45.88</cell><cell>26.50</cell></row><row><cell>RW</cell><cell>5.20</cell><cell>42.12</cell><cell>23.66</cell></row><row><cell>LDAM-DRW</cell><cell>4.91</cell><cell>30.77</cell><cell>17.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Cui et al., 2019] 25.43 12.90 39.73 16.54 63.98 42.01 80.24 49.98    </figDesc><table><row><cell cols="9">: Top-1 validation errors of ResNet-32 on imbalanced CIFAR-10 and CIFAR-100. The</cell></row><row><cell cols="9">combination of our two techniques, LDAM-DRW, achieves the best performance, and each of them</cell></row><row><cell cols="6">individually are beneficial when combined with other losses or schedules.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Imbalanced CIFAR-10</cell><cell></cell><cell cols="4">Imbalanced CIFAR-100</cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>ERM</cell><cell cols="8">29.64 13.61 36.70 17.50 61.68 44.30 61.45 45.37</cell></row><row><cell>Focal [Lin et al., 2017]</cell><cell cols="8">29.62 13.34 36.09 16.36 61.59 44.22 61.43 46.54</cell></row><row><cell>LDAM</cell><cell cols="8">26.65 13.04 33.42 15.00 60.40 43.09 60.42 43.73</cell></row><row><cell>CB RS</cell><cell cols="8">29.45 13.21 38.14 15.41 66.56 44.94 66.23 46.92</cell></row><row><cell>CB RW [Cui et al., 2019]</cell><cell cols="8">27.63 13.46 38.06 16.20 66.01 42.88 78.69 47.52</cell></row><row><cell>CB Focal [HG-DRS</cell><cell cols="4">27.16 14.03 29.93 14.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LDAM-HG-DRS</cell><cell cols="4">24.42 12.72 24.53 12.82</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>M-DRW</cell><cell cols="8">24.94 13.57 27.67 13.17 59.49 43.78 58.91 44.72</cell></row><row><cell>LDAM-DRW</cell><cell cols="8">22.97 11.84 23.08 12.19 57.96 41.29 54.64 40.54</cell></row><row><cell cols="2">4.2 Experimental results on CIFAR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Validation errors on iNaturalist 2018 of various approaches. Our proposed method LDAM-DRW demonstrates significant improvements over the previous state-of-the-arts. We include ERM-DRW and LDAM-SGD for the ablation study.</figDesc><table><row><cell>Loss</cell><cell cols="2">Schedule Top-1 Top-5</cell></row><row><cell>ERM</cell><cell>SGD</cell><cell>42.86 21.31</cell></row><row><cell>CB Focal [Cui et al., 2019]</cell><cell>SGD</cell><cell>38.88 18.97</cell></row><row><cell>ERM</cell><cell>DRW</cell><cell>36.27 16.55</cell></row><row><cell>LDAM</cell><cell>SGD</cell><cell>35.42 16.48</cell></row><row><cell>LDAM</cell><cell>DRW</cell><cell>32.00 14.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Validation error on imbalanced Tiny ImageNet with different loss functions and training schedules.</figDesc><table><row><cell cols="2">Imbalance Type</cell><cell>long-tailed</cell><cell></cell><cell>step</cell></row><row><cell cols="2">Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>Loss</cell><cell cols="5">Schedule Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>ERM</cell><cell>SGD</cell><cell cols="4">66.19 42.63 50.33 26.68 63.82 44.09 50.89 27.06</cell></row><row><cell>CB SM</cell><cell>SGD</cell><cell cols="4">72.72 52.62 51.58 28.91 74.90 59.14 54.51 33.23</cell></row><row><cell>ERM</cell><cell>DRW</cell><cell cols="4">64.57 40.79 50.03 26.19 62.36 40.84 49.17 25.91</cell></row><row><cell>LDAM</cell><cell>SGD</cell><cell cols="4">64.04 40.46 48.08 24.80 62.54 39.27 49.08 24.52</cell></row><row><cell>LDAM</cell><cell>DRW</cell><cell cols="4">62.53 39.06 47.22 23.84 60.63 38.12 47.43 23.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Top-1 validation error of ResNet-32 trained with different training schedules on imbalanced CIFAR-10 and CIFAR-100. 13.61 36.70 17.50 61.68 44.30 61.05 45.37 DRW 25.14 13.12 28.40 14.49 59.34 42.68 58.86 42.78 DRS 25.50 13.28 27.97 14.83 59.67 42.74 58.65 43.21</figDesc><table><row><cell>Dataset Name</cell><cell cols="3">Imbalanced CIFAR-10</cell><cell></cell><cell cols="4">Imbalanced CIFAR-100</cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>ERM</cell><cell>29.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Top-1 validation error of ResNet-32 on imbalanced training and imbalanced validation scheme for CIFAR-10. See Section C.5 for details.</figDesc><table><row><cell>Imbalance Type</cell><cell></cell><cell cols="2">long-tailed</cell><cell></cell><cell></cell><cell>step</cell><cell></cell><cell></cell></row><row><cell>Imbalance Ratio Train</cell><cell>100</cell><cell></cell><cell>10</cell><cell></cell><cell>100</cell><cell></cell><cell>10</cell><cell></cell></row><row><cell>Imbalance Ratio Val</cell><cell>100</cell><cell>100</cell><cell>10</cell><cell>10</cell><cell>100</cell><cell>100</cell><cell>10</cell><cell>10</cell></row><row><cell>ERM</cell><cell cols="8">30.99 28.45 13.08 13.12 24.55 28.63 10.34 11.67</cell></row><row><cell>CB-RW</cell><cell cols="8">20.86 26.19 10.70 11.93 35.76 31.35 9.82 11.02</cell></row><row><cell>LDAM-DRW</cell><cell cols="6">14.40 12.95 10.12 10.62 10.30 9.54</cell><cell>7.51</cell><cell>7.82</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/kaidic/LDAM-DRW. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Toyota Research Institute ("TRI") provided funds and computational resources to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We thank Percy Liang and Michael Xie for helpful discussions in various stages of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<ptr target="https://tiny-imagenet.herokuapp.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regularized learning for domain adaptation under label shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl0r3R9KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6240" to="6249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose-robust face recognition via deep residual equivariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Variance reduction for matrix games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sidford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02056</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distributionally robust losses against mixture covariate shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namkoong</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Size-independent sample complexity of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06541</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairness without demographics in repeated loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1934" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imbalanced learning: foundations, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqian</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluating fairness metrics in the presence of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry Hinnefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Cooman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupert</forename><surname>Mammo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09245</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study. Intelligent data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the complexity of linear prediction: Risk bounds, margin bounds, and regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sham M Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Empirical margin distributions and bounding the generalization error of combined classifiers. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Koltchinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Panchenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05181</idno>
		<title level="m">Gradient harmonized single-stage detector</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The perceptron algorithm with uneven margins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaz</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overfitting of neural nets under class imbalance: Analysis and improvements for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeju</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3128" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nalini</forename><surname>Ratha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10436</idno>
		<title level="m">Diversity in faces</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavh</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hygn2o0qKX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07379</idno>
		<title level="m">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Least squares support vector machine classifiers. Neural processing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Johan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joos</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03684</idno>
		<title level="m">Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improved sample complexities for deep networks and robust classification via an all-layer margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04284</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05369</idno>
		<title level="m">On the margin theory of feedforward neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Towards good practices for recognition &amp; detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07911</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
