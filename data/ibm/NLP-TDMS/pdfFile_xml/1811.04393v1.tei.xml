<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian-Induced Convolution for Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Jiang</surname></persName>
							<email>jiatao@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
							<email>zhen.cui@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian-Induced Convolution for Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning representation on graph plays a crucial role in numerous tasks of pattern recognition. Different from gridshaped images/videos, on which local convolution kernels can be lattices, however, graphs are fully coordinate-free on vertices and edges. In this work, we propose a Gaussianinduced convolution (GIC) framework to conduct local convolution filtering on irregular graphs. Specifically, an edgeinduced Gaussian mixture model is designed to encode variations of subgraph region by integrating edge information into weighted Gaussian models, each of which implicitly characterizes one component of subgraph variations. In order to coarsen a graph, we derive a vertex-induced Gaussian mixture model to cluster vertices dynamically according to the connection of edges, which is approximately equivalent to the weighted graph cut. We conduct our multi-layer graph convolution network on several public datasets of graph classification. The extensive experiments demonstrate that our GIC is effective and can achieve the state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As witnessed by the widespread applications, graph is one of the most successful models to conduct structured and semistructured data, ranging from text <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref>, bioinformatics <ref type="bibr" target="#b21">(Yanardag and Vishwanathan 2015;</ref><ref type="bibr">Niepert, Ahmed, and Kutzkov 2016;</ref><ref type="bibr" target="#b19">Song et al. 2018</ref>) and social network <ref type="bibr" target="#b6">(Gomez, Chiem, and Delvenne 2017;</ref><ref type="bibr" target="#b15">Orsini, Baracchi, and Frasconi 2017)</ref> to images/videos <ref type="bibr" target="#b12">(Marino, Salakhutdinov, and Gupta 2016;</ref><ref type="bibr">Cui, Yang, and others 2017)</ref>. Among these applications, learning robust representations from structured graphs becomes the main topic. To this end, various methods have come forth in recent years. Graph kernels (Yanardag and Vishwanathan 2015) and recurrent neural networks (RNNs) <ref type="bibr" target="#b17">(Scarselli et al. 2009</ref>) are the most representative ones. Graph kernels usually take the classic Rconvolution strategy <ref type="bibr" target="#b8">(Haussler 1999)</ref> to recursively decompose graphs into atomic sub-structures and then define local similarities between them. RNNs based methods sequentially traverse neighbors with tied parameters in depth. With the increase of graph size, graph kernels would suffer diagonal dominance of kernels <ref type="bibr" target="#b18">(Schölkopf et al. 2002)</ref> while In contrast, graphs are with irregular structures and fully coordinate-free on vertices and edges. The vertices/edges are not strictly ordered, and can not be explicitly matched between two graphs. To generalize the idea of CNNs onto graphs, we need to solve this problem therein that the same responses should be produced for those homomorphic graphs/subgraphs when performing convolutional filtering. To this end, recent graph convolution methods <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b0">Atwood and Towsley 2016;</ref><ref type="bibr" target="#b7">Hamilton, Ying, and Leskovec 2017)</ref> attempted to aggregate neighbor vertices as shown in <ref type="figure" target="#fig_0">Fig. 1e</ref>. This kind of methods actually employ a fuzzy filtering (i.e., a tied/shared filter) on neighbor vertices because only firstorder statistics (mean) is used. Two examples are shown in <ref type="figure" target="#fig_0">Fig. 1a</ref> and <ref type="figure" target="#fig_0">Fig. 1b</ref>. Although they have different structures, the responses on them are fully equal. Oppositely, Niepert et.al <ref type="bibr">(Niepert, Ahmed, and Kutzkov 2016)</ref> ranked neighbor vertices according to weights of edges, and then used different filters on these sorted vertices, as shown in <ref type="figure" target="#fig_0">Fig. 1f</ref>. However, this rigid ranking method will suffer some limitations: i) probably consistent responses to different structures (e.g., <ref type="figure" target="#fig_0">Fig. 1b</ref> and <ref type="figure" target="#fig_0">Fig. 1c</ref>) because weights of edges are out of consideration after ranking; ii) information loss of node pruning for a fixed-size receptive field as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>; and iii) ranking ambiguity for equal connections as shown in <ref type="figure" target="#fig_0">Fig. 1d</ref>; and iv) ranking sensitivity to (slightly) changes of edge weights/connections.</p><p>In this paper we propose a Gaussian-induced graph convolution framework to learn graph representation. For a coordinate-free subgraph region, we design an edge-induced Gaussian mixture model (EI-GMM) to implicitly coordinate the vertices therein. Specifically, the edges are used to regularize Gaussian models such that variations of subgraph can be well-encoded. In analogy to the standard convolutional kernel as shown in <ref type="figure" target="#fig_0">Fig. 1h</ref>, EI-GMM can be viewed as a coordinate normalization by projecting variations of subgraph into several Gaussian components. For example, the four subgraphs w.r.t. <ref type="figure" target="#fig_0">Fig. 1a∼1d</ref> will have different repre-  </p><formula xml:id="formula_0">(d) Σ =1 5 � 0 = Σ =1 5 � 0 ( ) 0 weighted summation 2 1 3 4 5</formula><p>(e) Tied filtering</p><formula xml:id="formula_1">1 2 + 2 ( 1 ) + 3 ( 4 ) + 4 ( 5 )</formula><p>rank and select vertices where v 0 is the reference vertex and each vertex is assigned to a signal. The tied filtering (e) summarizes all neighbor vertices, and generates the same responses to (a) and (b) under the filter f , i.e., f ( w 0i x i ) = f (1.9), although the two graphs are completely different in structures. The ranking filtering (f) sorts/prunes neighbor vertices and then performs different filtering on them. It might result into the same responses f 1 (1) + f 2 (3) + f 3 (1) + f 4 (4) to different graphs such as (b) and (c), where the digits in red boxes denote the ranked indices and the vertex of dashed box in (b) is pruned. Moreover, the vertex ranking is uncertain/non-unique for equal connections in (d).To address these problems, we derive edge-induced GMM to coordinate subgraphs as shown in (g). Each of Gaussian model can be viewed as one variation component (or direction) of subgraph. Like the standard convolution (h), the Gaussian encoding is sensitive to different subgraphs, e.g., (a)-(d) will have different responses. Note f, f i are linear filters, and the non-linear activation functions are put on their responses. sentations 1 through our Gaussian encoding in <ref type="figure" target="#fig_0">Fig. 1g</ref>. To make the network inference forward, we transform Gaussian components of each subgraph into the gradient space of multivariate Gaussian parameters, instead of employing the sophisticated EM algorithm. Then the filters (or transform functions) are performed on different Gaussian components like latticed kernels on different directions in <ref type="figure" target="#fig_0">Fig. 1h</ref>. Further, we derive a vertex-induced Gaussian mixture model (VI-GMM) to favor dynamic coarsening of graph. We also theoretically analyze the approximate equivalency of VI-GMM to weighted graph cut <ref type="bibr" target="#b4">(Dhillon, Guan, and Kulis 2007)</ref>. Finally, EI-GMM and VI-GMM can be alternately stacked into an end-to-end optimization network. In summary, our main contributions are four folds: i) propose an end-to-end Gaussian-induced convolutional neural network for graph representation; ii) propose edge-induced GMM to encode variations of different subgraphs; iii) derive vertex-induced GMM to perform dynamic coarsening of graphs, which is an approximation to the weighted graph cut; iv) verify the effectiveness of our method and report state-of-the-art results on several graph datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Graph CNNs mainly fall into two categories: spectral and spatial methods. Spectral methods <ref type="bibr" target="#b1">(Bruna et al. 2014;</ref><ref type="bibr" target="#b17">Scarselli et al. 2009;</ref><ref type="bibr" target="#b8">Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b10">Li et al. 2018a;</ref><ref type="bibr" target="#b11">Li et al. 2018b</ref>) construct a series of spectral filters by decomposing graph Laplacian, which often suffers high-computational burden. To address this problem, the fast local spectral filtering method (Defferrard, Bresson, and Vandergheynst 2016) parameterizes the frequency responses as a Chebyshev polynomial approximation. However, as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>, after summarizing all nodes, this method will discard topology structures of a local receptive field. This kind of methods usually require equal sizes of graphs like the same sizes of images for CNNs <ref type="bibr" target="#b9">(Kipf and Welling 2017)</ref>. Spatial methods attempt to define spatial structures of adjacent vertices and then perform filtering on structured graphs. Diffusion CNNs (Atwood and Towsley 2016) scans a diffusion process across each node. PATCHY-SAN (Niepert, Ahmed, and Kutzkov 2016) linearizes neighbors by sorting weights of edges and deriving convolutional filtering on graphs, as shown in <ref type="figure" target="#fig_0">Fig. 1c</ref>. As an alternative, random walks based approach is also used to define the neighborhoods <ref type="bibr" target="#b16">(Perozzi, Al-Rfou, and Skiena 2014)</ref>. For the linearized neighbors, RNNs <ref type="bibr" target="#b10">(Li et al. 2016)</ref> could be used to model the structured sequences. Similarly, <ref type="bibr">NgramCNN (Luo et al. 2017</ref>) serializes each graph by introducing the concept of n-gram block. GAT <ref type="bibr" target="#b20">(Velickovic et al. 2018</ref>) attempts to weight edges through the attention mechanism. WSC (Jiang  Different from these tied filtering or ranking filtering methods, we use Gaussian models to encode local variations of graph. Also different from the recent mixture models <ref type="bibr" target="#b13">(Monti et al. 2017)</ref>, which uses GMM to only learn the importance of adjacent nodes, our method uses weighted GMM to encode the distributions of local graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The GIC Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute Graph</head><p>Here we consider an undirected attribute graph G =</p><formula xml:id="formula_2">(V, A, X) of m vertices (or nodes), where V = {v i } m i=1</formula><p>is the set of vertices, A is a (weighted) adjacency matrix, and X is a matrix of graph attributes (or signals). The adjacency matrix A ∈ R m×m records the connections between vertices.</p><formula xml:id="formula_3">If v i , v j are not connected, then A(v i , v j ) = 0, oth- erwise A(v i , v j ) = 0. We sometimes abbreviate A(v i , v j ) as A ij . The attribute matrix X ∈ R m×d is associated with the vertex set V, whose i-th row X i (or X vi ) denotes a d- dimension attribute of the i-th node (i.e., v i ). The graph Laplacian matrix Ł is defined as Ł = D − A, where D ∈ R m×m is the diagonal degree matrix with D ii = j A ij . The normalized version is written as Ł norm = D −1/2 ŁD −1/2 = I − D −1/2 AD −1/2 .</formula><p>where I is the identity matrix. Unless otherwise specified, we use the latter. We give the definition of subgraph used in the following.</p><formula xml:id="formula_4">Definition 1. Given an attribute graph G = (V, A, X), the attribute graph G = (V , A , X ) is a subgraph of G, de- noted G ⊆ G, if (i) V ⊆ V, (ii) A is the submatrix of A w.r.t. the subset V , and (iii) X = X V .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The GIC network architecture is shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. Given an attribute graph G (0) = (V (0) , A (0) , X (0) ), where the superscript denotes the layer number, we construct multi-scale re-ceptive fields for each vertex based on the adjacency matrix A (0) . Each receptive field records k-hop neighborhood relationships around the reference vertex, and forms a local centralized subgraph. To encode the centralized subgraph, we project it into edge-induced Gaussian models, each of which defines one variation "direction" of the subgraph. We perform different filtering operations on different Gaussian components and aggregate all responses as the convolutional output. After the convolutional filtering, the input graph</p><formula xml:id="formula_5">G (0) is transformed into a new graph G (1) = (V (1) , A (1) , X (1) ), where V (1) = V (0) and A (1) = A (0)</formula><p>. To further abstract graphs, we next stack a coarsening layer on the graph G (1) . The proposed vertex-induced GMM is used to downsample the graph G (1) into the low-resolution graph G (2) = (V (2) , A (2) , X (2) ). Taking the convolution and coarsening modules, we may alternately stack them into a multi-layer GIC network, With the increase of layers, the receptive field size of filters will become larger, so the higher layer can extract more global graph information. In the supervised case of graph classification, we finally concatenate with a fully connected layer followed by a softmax loss layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Receptive Fields</head><p>In the standard CNN, receptive fields may be conveniently defined as latticed spatial regions. Thus convolution kernels on grid-shaped structures are accessible. However, the construction of convolutional kernels on graphs are intractable due to coordinate-free graphs, e.g., unordered vertices, unfixed number of adjacent edges/vertices. To address this problem, we resort to the adjacent matrix A, which expresses connections between vertices. Since A k exactly records the k-step reachable vertices, we may construct a k-neighbor receptive field by using the k-order polynomial of A, denoted as ψ k (A). Taking the simplest case, ψ k (A) = A k reflects the k-hop neighborhood relationships. In order to remove the scale effect, we may normalize ψ k (A) as ψ k (A)diag(ψ k (A)1) −1 , which describes the reachable possibility in a k-hop walking. Formally, we define the k-th scale receptive field as a subgraph. Definition 2. The k-th scale receptive field around a ref-</p><formula xml:id="formula_6">erence vertex v i is a subgraph G k vi = (V , A , X ) of the k-order graph (V, A = ψ k (A), X), where V = {v j | A ij = 0} ∪ {v i }, A is the submatrix of A w.r.t. V , and X = X V .</formula><p>Convolution: Edge-Induced GMM Given a reference vertex v i , we can construct the centralized subgraph G k vi of the k-th scale. To coordinate the subgraph, we introduce Gaussian mixture models (GMMs), each of which may be understood as one principal direction of its variations. To encode the variations accurately, we jointly formulate attributes of vertices and connections of edges into Gaussian models. The edge weight A (v i , v j ) indicates the relevance of v j to the central vertex v i . The higher weight is, the stronger impact on v i is. So the weights can be incorporated into a Gaussian model by observing A (v i , v j ) times. As the likelihood function, it is equivalent to raise the power A (v i , v j ) on Gaussian function, which is proportional to N (X vj , µ, 1 A (vi,vj ) Σ). Formally, we estimate the probability density of the subgraph G k vi from the C 1component GMM,</p><formula xml:id="formula_7">p vi (X vj ; Θ 1 , A ij ) = C1 c=1 π c N (X vj ; µ c , 1 A ij Σ c ), s.t. π c &gt; 0, C1 c=1 π c = 1,<label>(1)</label></formula><p>where Θ 1 = {π 1 , · · · , π C1 , µ 1 , · · · , µ C1 , Σ 1 , · · · , Σ C1 } are the mixture parameters, {π c } are the mixture coefficients, {µ c , Σ c } are the parameters of the k-th component, and A ij &gt; 0 2 . Intuitively, edge weight A ij is, the stronger impact of the node v j w.r.t. the reference vertex v i is. We will refer to the model in Eqn.</p><p>(1) as the edge-induced Gaussian mixture model (EI-GMM).</p><p>In what follows, we assume all attributes of nodes are independent on each other, which is often used in signal processing. That means, the covariance matrix Σ c is diagonal, so we denote it as diag(σ 2 c ). To avoid the explicit constraints for π c in Eqn. (1), we adopt the soft-max normalization with the re-parameterization variable α c , i.e., π c = exp(α c )/ C1 k=1 exp(α k ). Thus, the entire subgraph log-likelihood can be written as</p><formula xml:id="formula_8">ζ(G k vi ) = m j=1 ln p vi (X vj ; Θ 1 , A ) = m j=1 ln C1 c=1 π c N (X vj ; µ c , 1 A ij Σ c ),<label>(2)</label></formula><p>To infer forward, instead of the expectation-maximization (EM) algorithm, we use the gradients of the subgraph with regard to the parameters of the EI-GMM model Θ 1 , motivated by the recent Fisher vector work <ref type="bibr" target="#b16">(Sanchez et al. 2013</ref>), which has been proven to be effective in representation. For a convenient calculation, we simplify the notations,</p><formula xml:id="formula_9">N jc = N (X vj , µ c , 1 A ij σ 2 c ) and Q jc = πcNjc C 1 k=1 π k N jk</formula><p>, then we 2 In practice, we normalize A into a non-negative matrix.</p><p>can derive the gradients of model parameters from Eqn.</p><p>(2) as follows</p><formula xml:id="formula_10">∂ζ(G k vi ) ∂µ c = m j=1 A ij Q jc (X vj − µ c ) σ 2 c , ∂ζ(G k vi ) ∂σ c = m j=1 Q jc (A ij (X vj − µ c ) 2 − σ 2 c ) σ 3 c ,<label>(3)</label></formula><p>where the division of vectors means a term-by-term operation. Note we do not use ∂ζ(G k vi )/∂α c due to no improvement in our experience. The gradients describe the contribution of the corresponding parameters to the generative process. The subgraph variations are adaptively allocated to C 1 Gaussian models. Finally, we ensemble all gradients w.r.t. Gaussian model (i.e., directions of graph) to analogize the collection of local square receptive field on image. Formally, for the k-scale receptive field G k vi around the vertex v i , the attributes produced from Gaussian models are filtered respectively and then concatenated,</p><formula xml:id="formula_11">F (G k vi , Θ 1 , f ) = ReLU( C1 c=1 f i (Cat[ ∂ζ(G k vi ) ∂µ c , ∂ζ(G k vi ) ∂σ c ]),<label>(4)</label></formula><p>where Cat[·, ·] is a concatenation operator, f i is a linear filtering function (i.e., a convolution function) and ReLU is the rectified linear unit. Therefore we can produce the feature vectors that have same dimensionality depending on the number of Gaussian models for different subgraphs. If the soft assignment distribution Q jc is sharply peaked on a single value of one certain Gaussian for the vertex v j , the vertex will be only projected onto one Gaussian direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarsening: Vertex-Induced GMM</head><p>Like the standard pooling in CNNs, we need to downsample graphs so as to abstract them as well as reduce the computational cost. However, the pooling on images are tailored for latticed structures, and cannot be used for irregular graphs. One solution is to use some clustering algorithms to partition vertices to several clusters, and then produce a new vertex from each cluster. However, we expect that two vertices should not fall into the same cluster with a larger possibility if there is a high transfer difficulty between them. To this end, we derive vertex-induced Gaussian mixture models (VI-GMM) to weight each vertex. To utilize the edge information, we construct a latent observation φ(v i ) w.r.t. each vertex v i from the graph Laplacian (or adjacent matrix if semi-positive definite), i.e., the kernel calculation φ(v i ), φ(v j ) = L ij . Moreover, for each vertex v i , we define an influence factor w i for Gaussian models. Formally, given C 2 Gaussian models, VI-GMM is written as</p><formula xml:id="formula_12">p(φ(v i ); Θ 2 , w i ) = C2 c=1 π c N (φ(v i ); µ c , 1 w i Σ c ), s.t. w i = h(X vi ) &gt; 0,<label>(5)</label></formula><p>where h is a mapping function to be learnt. To reduce the computation cost of matrix inverse on Σ, we specify it as an identity matrix. Then we have</p><formula xml:id="formula_13">p(φ(v i ); Θ 2 , w i ) = C2 c=1 π c ( 2π wi ) d/2 exp − w i 2 φ(vi)−µc 2 ,<label>(6)</label></formula><p>Given a graph with m vertices, the objective is to maximize the following log-likelihood:</p><formula xml:id="formula_14">arg max Θ2 ζ(Θ 2 ) = m i=1 ln C2 c=1 π c N (φ(v i ); µ c , 1 w i I)). (7)</formula><p>To solve above model in Eqn. <ref type="formula">(7)</ref>, we use the iterative expectation maximization algorithm, which has closed-form solution at each step. Meanwhile, the algorithm may automatically conduct the required constraints. The graphical clustering process is summarized as follows:</p><p>(1) E-Step: the posteriors, i.e., the i-th vertex for the cth cluster, are updated with p ic = πcp(φ(vi);θc,wi) C k=1 π k p(φ(vi);θ k ,wi) , where θ c is the c-th Gaussian parameters, and Θ 2 = {θ 1 , · · · , θ C2 }.</p><p>(2) M-Step: we optimize Gaussian parameters π, µ. The parameter estimatation is given by</p><formula xml:id="formula_15">π c = 1 m m i=1 r ic , µ c = v i ∈Gc wiφ(vi) v i ∈Gc wi</formula><p>. π c indicates the energy summation of all vertices assigned to the cluster c, and µ c may be understood as a doubly weighted (w i , r ic ) average on the cluster c.</p><p>After several iterations of the two steps, we perform hard quantification. The i-th vertex is assigned as the class with the maximum possibility, formally, r ic = 1 if c = arg max k p ik , otherwise 0. Thus we can obtain the cluster matrix P ∈ {0, 1} m×C2 , where P ic = 1 if the i-th vertex falls into the cluster c. During coarsening, we take maximal responses of each cluster as the attributes of new vertex, and derive a new adjacency matrix by using P AP.</p><p>It is worth noting that we need not compute the concrete φ during the clustering process. The main calculation φ(v i ) − µ c 2 in EM can be reduced to the kernel version:</p><formula xml:id="formula_16">K ii − 2 v j ∈Gc wj Kij v j ∈Gc wj + v j ,v k ∈Gc wj w k K jk ( v j ∈Gc wj ) 2 , where K ij = φ(v i ), φ(v j )</formula><p>. In practice, we can use the graph Laplacian Ł as the kernel. In this case, we can easily reach the following proposition, which is relevant to graph cut <ref type="bibr" target="#b4">(Dhillon, Guan, and Kulis 2007)</ref>. Proposition 1. In EM, if the kernel matrix takes the weightregularized graph Laplacian, i.e., K = diag(w)Łdiag(w), then VI-GMM is equal to an approximate optimization of graph cut, i.e., min C c=1</p><formula xml:id="formula_17">links(Vc,V\Vc) w(Vc)</formula><p>, where links(A, B) = vi∈A,vj ∈B A ij , and w(V c ) = j∈Vc w j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Graph Classification</head><p>For graph classification, each graph is annotated with one label. We use two types of datasets: Bioinformatics and Network datasets. The former contains <ref type="bibr">MUTAG (Debnath et al. 1991</ref><ref type="bibr">), PTC (Toivonen et al. 2003</ref>, NCI1 and NCI109 <ref type="bibr" target="#b21">(Wale, Watson, and Karypis 2008)</ref>, <ref type="bibr">ENZYMES (Borgwardt et al. 2005)</ref> and PROTEINS <ref type="bibr" target="#b0">(Borgwardt et al. 2005</ref>). The latter has COLLAB <ref type="bibr" target="#b9">(Leskovec, Kleinberg, and Faloutsos 2005)</ref>, REDDIT-BINARY, REDDIT-MULTI-5K, REDDIT-MULTI-12K, IMDB-BINARY and IMDB-MULTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>We verify our GIC on the above bioinformatics and social network datasets. In default, GIC mainly consists of three graph convolution layers, each of which is followed by a graph coarsening layer, and one fully connected layer with a final softmax layer as shown in <ref type="figure" target="#fig_3">Fig 2.</ref> Its configuration can simply be set as C(64)-P(0.25)-C(128)-P(0.25)-C(256)-P-FC(256), where C, P and FC denote the convolution, coarsening and fully connected layers respectively. The choices of hyperparameters are mainly inspired from the classic VGG net. For example, the coarsening factor is 0.25 (w.r.t. 0.5×0.5 in VGG), the attribute dimensions at three conv. layers are 64-128-256 (w.r.t. the channel numbers of conv1-3 in VGG). The scale of respective field and the number of Gaussian components are both set to 7. We train GIC network with stochastic gradient descent for roughly 300 epochs with a batch size of 100, where the learning rate is 0.1 and the momentum is 0.95.</p><p>In the bioinformatics datasets, we exploit labels and degrees of the vertices to generate initial attributes of each vertex. In the social network datasets, we use degrees of vertices. We closely follow the experimental setup in PSCN <ref type="bibr">(Niepert, Ahmed, and Kutzkov 2016)</ref>. We perform 10-fold cross-validation, 9-fold for training and 1-fold for testing. The experiments are repeated 10 times and the average accuracies are reported.</p><p>Comparisons with the State-of-the-arts We compare our GIC with several state-of-the-arts, which contain graph convolution networks (PSCN (Niepert, Ahmed, and Kutzkov 2016), DCNN (Atwood and Towsley 2016), Ngram-CNN (Luo et al. 2017)), neural networks (SAEN <ref type="bibr" target="#b15">(Orsini, Baracchi, and Frasconi 2017)</ref>), feature based algorithms (DyF (Gomez, Chiem, and Delvenne 2017), FB <ref type="bibr" target="#b1">(Bruna et al. 2014)</ref>), random walks based methods (RW <ref type="bibr" target="#b6">(Gärtner, Flach, and Wrobel 2003)</ref>), graph kernel approaches (GK <ref type="bibr" target="#b18">(Shervashidze et al. 2009</ref>), DGK (Yanardag and Vishwanathan 2015), WL <ref type="bibr" target="#b14">(Morris, Kersting, and Mutzel 2017)</ref>). We present the comparisons with the state-of-the-arts, as shown in <ref type="table" target="#tab_2">Table 1</ref>. All results come from the related literatures. We have the following observations.</p><p>Deep learning based methods on graphs (including DCNN, PSCN, NgramCNN, SAEN and ours) are superior to those conventional methods in most cases. The conventional kernel methods usually require the calculation on graph kernels with high-computational complexity. In contrast, these graph neural networks attempt to learn more abstract highlevel features by performing inference-forward, which need relatively low computation cost.</p><p>Compared with recent graph convolution methods, ours can achieve better performance on most datasets, such as PTC, NCI1, NCI109, ENZYMES and PROTEINS. The main reason should be that local variations of subgraphs are accurately described with Gaussian component analysis.</p><p>The proposed GIC achieves state-of-the-art results on most datasets. The best performance is gained in some bioin-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Classification</head><p>For node classification, one node is assigned one/multiple labels. It is challenging if the label set is large. During training, we only use a fraction of nodes and their labels. The task is to predict the labels for the remaining nodes. Following the setting in <ref type="bibr" target="#b7">(Hamilton, Ying, and Leskovec 2017)</ref>, we conduct the experiments on Reddit data and PPI data. For a fair comparison to graphSAGE <ref type="bibr" target="#b7">(Hamilton, Ying, and Leskovec 2017)</ref>, we use the same initial graph data, minibatch iterators, supervised loss function and neighborhood sample. The other network parameters are similar to graph classification except removing the coarsening layer. Tabel 2 summarizes the comparison results. Our GIC can obtain the best performance 0.661 on PPI data and a comparable result 0.952 on Reddit data. The raw features provide an important initial information for node multi-label classification. Based on the raw features, deep walk <ref type="bibr" target="#b16">(Perozzi, Al-Rfou, and Skiena 2014)</ref> improves about 0.36 (micro-F1 scores) on Reddit data. Meanwhile, we conduct an experiment of node2vec and use regression model to classification. Our method gains better performance than node2vec (Grover and Leskovec 2016). Comparing different aggregation methods like GCN <ref type="bibr" target="#b9">(Kipf and Welling 2017)</ref>, mean and LSTM, our GIC has a significant improvement about 0.16 on PPI data and gains a competitive performance on Reddit data. The results demonstrate our approach is robust to infer unknown labels of partial graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis</head><p>EI-GMM and VI-GMM: To directly analyze convolution filtering with EI-GMM, we compare our method with <ref type="bibr">Cheb-Net (Defferrard, Bresson, and Vandergheynst 2016)</ref> and GCN <ref type="bibr" target="#b9">(Kipf and Welling 2017)</ref> approaches by using the same coarsening mechanism VI-GMM. As shown in <ref type="table" target="#tab_4">Table 3</ref>, under the same coarsening operation, our GIC is superior to   ChebNet+VI-GMM and GCN+VI-GMM. It indicates EI-GMM can indeed encode the variations of subgraphs more effectively. On the other hand, we remove the coarsening layer from our GIC. For different size graphs, we pad new zero vertices into a fixed size and then concatenate attributes of all vertices for classification. As shown in this table, the performance of GIC still outperforms GIC without VI-GMM coarsening, which verifies the effectiveness of the coarsening layer VI-GMM.</p><p>K and C 1 : The kernel size K and the number of Gaussian components C 1 are the most crucial parameters. Generally, the C 1 is proportional to the K. The reason is that the larger receptive field usually contains more vertices (i.e., a relative large subgraph). Thus we simply take the equal values for them, K = C 1 = {1, 3, 5, 7}. The experimental results are shown in <ref type="table" target="#tab_5">Table 4</ref>. With the increase of K, C 1 , the performance improves at most cases. The reasons are two folds: i) with increasing receptive field size, the convolution will cover the farther hopping neighbors; ii) with the increase of C 1 , the variations of subgraphs are encoded more accurately. But for the larger values of K and C 1 will increase the computational burden. Moreover, the overfitting phenomenon might occur with the increase of model complexity. Take the example of NCI109, in the first convolution layer, the encoded attributes (in Eqn. (4)) will be 2 × 39 × 7 = 546 for each scale of receptive field, where 39 is the dimension of attributes (w.r.t the number of node labels) and 7 is the number of Gaussian components. Thus, for 7 scales of receptive field, the final encoded attributes will be 546 × 7 = 3822 dimensions, which will be mapping to 64 dimensions by the function f = [f 1 , · · · , f C1 ] in Eqn. (4). Thus the model parameter is 3822×64 = 244608 in the first layer. Similarly, if the number of node label is 2, the model parameter will sharply decrease into 18816. Besides, the parameter complexity is related to the number of classes and nodes. The comparison results in <ref type="table" target="#tab_5">Table 4</ref> demonstrate the trend of the parameters K and C 1 in our GIC framework.</p><p>Number of stacked layers: Here we test on the number of stacked network layers with N = 2, 4, 6, 8. When N = 2, only one fully connected layer and one softmax layer are preserved. When N = 4, we add two layers: the convolution layer and the coarsening layer. When continuing to stack both, the depth of network will be 6 and 8. The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Deeper networks can gain better performance in most cases, because the larger receptive field is observed and more abstract structures will be extracted in the topper layer. Of course, there is an extra risk of overfitting due to the increase of model complexity.</p><p>An analysis of computation complexity: In the convolution layer, the computational costs of receptive fields and Gaussian encoding are about O(Km 2 ) and O(C 1 d 2 ) respectively, where m, d are number of nodes and the feature dimensionality. Generally, K = C 1 d &lt; m. In the coarsening layer, the time complexity is about O(pm 2 + md), where p is iteration number of the EM algorithm. In all, suppose the whole GIC alternatively stacks n convolution and coarsening layers, the entire time complexity is O(n(K + p)m 2 + nC 1 d 2 + nmd).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a novel Gaussian-induced convolution network to handle with general irregular graph data. Considering the previous spectral and spatial methods do not well characterize local variations of graph, we derived edgeinduced GMM to adaptively encode subgraph structures by projecting them into several Gaussian components and then performing different filtering operations on each Gaussian direction like the standard CNN filters on images. Meanwhile, we formulated graph coarsening into vertex-induced GMM to dynamically partition a graph, which was also proven to be equal to graph cut. Extensive experiments in two graphic tasks (i.e. graph and node classification) demonstrated the effectiveness and superiority of our GIC compared with those baselines and state-of-the-art methods. In the future, we would like to extend our method into more applications to irregular data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different filters operations on graph vertices. Examples of one-hop subgraphs are given in (a)-(d),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>Suppose three Gaussian models are N (0, 1), N (0, 2) and N (0, 3), then we can compute the responses on (a)-(d) respectively as f1([0.49, −0.93]) + f2([0.17, −0.65]) + f3([0.07, −0.44]), f1([0.35, −0.73]) + f2([0.15, −0.58]) + f3([0.10, −0.64], f1([0.35, −0.71]) + f2([0.15, −0.39]) + f3([0.10, −0.43]), f1([0.46, −0.99]) + f2([0.18, −0.62]) + f3([0.08, −0.42]). Please refer to incoming section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The GIC network architecture. The GIC main contains two module: convolution layer (EI-GMM) and coarsening layer (VI-GMM). The GIC stacks several convolution and coarsening layers alternatively and iteratively. More details can be found in incoming section. et al. 2018) attempts to aggregate walk fields defined by random walks into Gaussian mixture models. Zhao<ref type="bibr" target="#b23">(Zhao et al. 2018</ref>) attempts to define a standard network with different graph convolutions. Besides, some variants<ref type="bibr" target="#b7">(Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b5">Duran and Niepert 2017;</ref>) employ the aggregation or propagation of local neighbor nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with state-of-the-art methods.</figDesc><table><row><cell>DATASET</cell><cell cols="3">PSCN DCNN NGRAMCNN</cell><cell>FB</cell><cell>DYF</cell><cell>WL</cell><cell>GK</cell><cell>DGK</cell><cell>RW</cell><cell>SAEN</cell><cell>GIC</cell></row><row><cell>MUTAG</cell><cell>92.63 ±4.21</cell><cell>66.98 -</cell><cell>94.99 ±5.63</cell><cell cols="7">84.66 ±2.01 ±2.37 ±1.9 ±2.11 ±1.45 ±1.50 ±1.82 ±4.30 88.00 78.3 81.66 82.66 83.72 84.99 94.44</cell></row><row><cell>PTC</cell><cell>60.00 ±4.82</cell><cell>56.60 -</cell><cell>68.57 ±1.72</cell><cell>55.58 2.30</cell><cell>57.15 ±1.47</cell><cell>--</cell><cell cols="4">57.26 ±1.41 ±1.13 ±1.30 ± 1.30 ± 6.98 57.32 57.85 57.04 77.64</cell></row><row><cell>NCI1</cell><cell>78.59 ±1.89</cell><cell>62.61 -</cell><cell>--</cell><cell cols="7">62.90 ±0.96 ±0.34 ±0.2 ±0.29 ±0.25 ±0.50 ± 0.42 ±1.77 68.27 83.1 62.28 62.48 48.15 77.80 84.08</cell></row><row><cell>NCI109</cell><cell>--</cell><cell>62.86 -</cell><cell>--</cell><cell cols="6">62.43 ±1.13 ± 0.20 ± 0.2 ± 0.19 ± 0.23 ± 0.60 66.72 85.2 62.60 62.69 49.75</cell><cell>--</cell><cell>82.86 ± 2.37</cell></row><row><cell>ENZYMES</cell><cell>--</cell><cell>18.10 -</cell><cell>--</cell><cell cols="6">29.00 ±1.16 ± 1.20 ± 1.4 ± 0.99 ± 0.79 ± 1.64 33.21 53.4 26.61 27.08 24.16</cell><cell>--</cell><cell>62.50 ± 5.12</cell></row><row><cell>PROTEINS</cell><cell>75.89 ± 2.76</cell><cell>--</cell><cell>75.96 ±2.98</cell><cell cols="7">69.97 ±1.34 ± 0.65 ± 0.5 ± 0.55 ± 0.50 ± 0.42 ± 0.70 ± 3.21 75.04 73.7 71.67 71.68 74.22 75.31 77.65</cell></row><row><cell>COLLAB</cell><cell>72.60 ± 2.15</cell><cell>--</cell><cell>--</cell><cell>76.35 1.64</cell><cell>80.61 ± 1.60</cell><cell>--</cell><cell cols="4">72.84 ± 0.28 ± 0.25 ± 0.09 ± 0.31 ± 1.44 73.09 69.01 75.63 81.24</cell></row><row><cell>REDDIT-B</cell><cell>86.30 ± 1.58</cell><cell>--</cell><cell>--</cell><cell cols="7">88.98 ±2.26 ± 1.96 ± 0.3 ± 0.18 ± 0.39 ± 1.01 ± 0.53 ± 1.60 89.51 75.3 77.34 78.04 67.63 86.08 88.45</cell></row><row><cell>REDDIT-5K</cell><cell>49.10 ± 0.70</cell><cell>--</cell><cell>--</cell><cell>50.83 1.83</cell><cell>50.31 ± 1.92</cell><cell>--</cell><cell cols="2">41.01 ± 0.17 ± 0.18 41.27</cell><cell>--</cell><cell>52.24 ± 0.38 ± 1.68 51.58</cell></row><row><cell>REDDIT-12K</cell><cell>41.32 ± 0.42</cell><cell>--</cell><cell>--</cell><cell>42.37 1.27</cell><cell>40.30 ± 1.41</cell><cell>--</cell><cell cols="2">31.82 ± 0.08 ± 0.10 32.22</cell><cell>--</cell><cell>46.72 ± 0.23 ± 0.87 42.98</cell></row><row><cell>IMDB-B</cell><cell>71.00 ± 2.29</cell><cell>--</cell><cell>71.66 ±2.71</cell><cell cols="7">72.02 ±4.71 ± 4.05 ± 0.5 ± 0.98 ± 0.56 ± 1.22 ± 0.74 ± 3.25 72.87 72.4 65.87 66.96 64.54 71.26 76.70</cell></row><row><cell>IMDB-M</cell><cell>45.23 ± 2.84</cell><cell>--</cell><cell>50.66 ±4.10</cell><cell>47.34 3.56</cell><cell>48.12 ± 3.56</cell><cell>--</cell><cell cols="4">43.89 ± 0.38 ± 0.52 ± 0.76 ± 0.64 ± 3.40 44.55 34.54 49.11 51.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Node label prediction on Reddit and PPI data (micro-averaged F1 score).</figDesc><table><row><cell>DATASET</cell><cell>REDDIT</cell><cell>PPI</cell></row><row><cell>RANDOM</cell><cell>0.042</cell><cell>0.396</cell></row><row><cell>RAW FEATURES</cell><cell>0.585</cell><cell>0.422</cell></row><row><cell>DEEP WALK</cell><cell>0.324</cell><cell>-</cell></row><row><cell>DEEP WALK + FEATURES</cell><cell>0.691</cell><cell>-</cell></row><row><cell>NODE2VEC + REGRESSION</cell><cell>0.934</cell><cell>-</cell></row><row><cell>GRAPHSAGE-GCN</cell><cell>0.930</cell><cell>0.500</cell></row><row><cell>GRAPHSAGE-MEAN</cell><cell>0.950</cell><cell>0.598</cell></row><row><cell>GRAPHSAGE-LSTM</cell><cell>0.954</cell><cell>0.612</cell></row><row><cell>GIC</cell><cell>0.952</cell><cell>0.661</cell></row><row><cell cols="3">formatics datasets and some social network datasets in-</cell></row><row><cell cols="3">cluding PTC, NCI1, ENZYMES, PROTEINS, COLLAB,</cell></row><row><cell cols="3">IMDB-BINARY and IMDB-MULTI. Although Ngram-</cell></row><row><cell cols="3">CNN, DyF, WL and SEAN approaches have obtained the</cell></row><row><cell cols="3">best performance on MUTAG, REDDIT-BINARY, NCI109,</cell></row><row><cell cols="3">REDDIT-MULTI-5K and REDDIT-MULTI-12K respec-</cell></row><row><cell cols="3">tively, our method is fully comparable to them.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The verification of our convolution and coarsening. VI-GMM W/ VI-GMM W/O VI-GMM MUTAG 89.44 ± 6.30 92.22 ± 5.66 93.33 ± 4.84 94.44 ± 4.30 PTC 68.23 ± 6.28 71.47 ± 4.75 68.23 ± 4.11 77.64 ± 6.98 NCI1 73.96 ± 1.87 76.39 ± 1.08 79.17 ± 1.63 84.08 ± 1.77 NCI109 72.88 ± 1.85 74.92 ± 1.70 77.81 ± 1.88 82.86 ± 2.37 ENZYMES 52.83 ± 7.34 51.50 ± 5.50 52.00 ± 4.76 62.50 ± 5.12 PROTEINS 78.10 ± 3.37 80.09 ± 3.20 78.19 ± 2.04 77.65 ± 3.21</figDesc><table><row><cell>DATASET</cell><cell>CHEBNET W/</cell><cell>GCN</cell><cell>GIC</cell><cell>GIC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on K and C 1 .</figDesc><table><row><cell>DATASET</cell><cell>K, C 1 = 1</cell><cell>K, C 1 = 3</cell><cell>K, C 1 = 5</cell><cell>K, C 1 = 7</cell></row><row><cell>MUTAG</cell><cell cols="4">67.77 ± 11.05 83.88 ± 5.80 90.55 ± 6.11 94.44 ± 4.30</cell></row><row><cell>PTC</cell><cell>72.05 ± 8.02</cell><cell cols="3">77.05 ± 4.11 76.47 ± 5.58 77.64 ± 6.98</cell></row><row><cell>NCI1</cell><cell>71.21 ± 1.94</cell><cell cols="3">83.26 ± 1.17 84.47 ± 1.64 84.08 ±1.77</cell></row><row><cell>NCI109</cell><cell>70.02 ± 1.57</cell><cell cols="3">81.74 ± 1.56 83.39 ± 1.65 82.86 ± 2.37</cell></row><row><cell>ENZYMES</cell><cell>33.83 ± 4.21</cell><cell cols="3">64.00 ± 4.42 63.66 ± 3.85 62.50 ± 5.12</cell></row><row><cell>PROTEINS</cell><cell>75.49 ± 4.00</cell><cell cols="3">77.47 ± 3.37 78.10 ± 2.96 77.65 ± 3.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on the layer number. MUTAG 86.66 ± 8.31 91.11 ± 5.09 93.88 ± 5.80 94.44 ± 4.30 PTC 64.11 ± 6.55 74.41 ± 6.45 75.29 ± 6.05 77.64 ± 6.98 NCI1 71.82 ± 1.85 81.36 ± 1.07 83.01 ± 1.54 84.08 ± 1.77 NCI109 71.09 ± 2.41 80.02 ± 1.67 81.60 ± 1.83 82.86 ± 2.37 ENZYMES 42.33 ± 4.22 61.83 ± 5.55 64.83 ± 6.43 62.50 ± 5.12 PROTEINS 77.38 ± 2.97 79.81 ± 3.84 78.37 ± 4.00 77.65 ± 3.21</figDesc><table><row><cell>DATASET</cell><cell>N = 2</cell><cell>N = 4</cell><cell>N = 6</cell><cell>N = 8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the Chairs and the anonymous reviewers for their critical and constructive comments and suggestions. This work was supported by the National Science Fund of China under Grant Nos. 61602244, 61772276, U1713208 and 61472187 and Program for Changjiang Scholars.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
	<note>Diffusion-convolutional neural networks. suppl</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-dependent diffusion network for visual relationship detection</title>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1475" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">;</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05553</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kulis ; Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flach</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Delvenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10817</idno>
	</analytic>
	<monogr>
		<title level="m">Dynamics based features for graph classification</title>
		<meeting><address><addrLine>Leskovec</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SIGKDD</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Walk-steered convolution for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<idno>arXiv:1804.05837</idno>
	</analytic>
	<monogr>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California at Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Convolution kernels on discrete structures. Jiang et al. 2018</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks. ICLR. [LeCun, Bengio, and Hinton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<meeting><address><addrLine>Leskovec, Kleinberg, and Faloutsos</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
	<note>Graphs over time: densification laws, shrinking diameters and possible explanations</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks. ICLR</title>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3657" to="3670" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Action-attending graphic neural network</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2125" to="2139" />
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note>Deep learning of graphs with ngram convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salakhutdinov</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta ; Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04844</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glocalized weisfeiler-lehman graph kernels: Global-local feature maps of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kersting</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baracchi</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05537</idno>
		<title level="m">Shift aggregate extract networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="222" to="245" />
		</imprint>
	</monogr>
	<note>Deepwalk: Online learning of social representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A kernel approach for learning from almost orthogonal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>Efficient graphlet kernels for large graph comparison</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eeg emotion recognition using dynamical graph convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
	<note>Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Velickovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Watson</forename><surname>Karypis ; Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
	<note>Deep graph kernels</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10071</idno>
		<title level="m">Tensor graph convolutional neural network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">When work matters: Transforming classical network structures to graph cnn</title>
		<idno type="arXiv">arXiv:1807.02653</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
