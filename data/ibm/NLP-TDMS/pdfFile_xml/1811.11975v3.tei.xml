<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Online Multi-Person 2D Pose Tracking with Recurrent Spatio-Temporal Affinity Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaadhav</forename><surname>Raaj</surname></persName>
							<email>raaj@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
							<email>hidrees@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Online Multi-Person 2D Pose Tracking with Recurrent Spatio-Temporal Affinity Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an online approach to efficiently and simultaneously detect and track 2D poses of multiple people in a video sequence. We build upon Part Affinity Fields (PAF) representation designed for static images, and propose an architecture that can encode and predict Spatio-Temporal Affinity Fields (STAF) across a video sequence. In particular, we propose a novel temporal topology cross-linked across limbs which can consistently handle body motions of a wide range of magnitudes. Additionally, we make the overall approach recurrent in nature, where the network ingests STAF heatmaps from previous frames and estimates those for the current frame. Our approach uses only online inference and tracking, and is currently the fastest and the most accurate bottom-up approach that is runtime-invariant to the number of people in the scene and accuracy-invariant to input frame rate of camera. Running at ∼30 fps on a single GPU at single scale, it achieves highly competitive results on the PoseTrack benchmarks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-person human pose estimation has received considerable attention in the past few years assisted by deep convolutional learning as well as COCO <ref type="bibr" target="#b19">[21]</ref> and MPII <ref type="bibr" target="#b1">[3]</ref> datasets. The recently introduced PoseTrack dataset <ref type="bibr" target="#b15">[17]</ref> has provided the community with a large scale corpus of video data with multiple people in the scenes. In this paper, our aim is to utilize these towards building a truly online and real-time multi-person 2D pose estimator and tracker that is deployable and scalable while achieving high performance and requiring minimal post-processing. The potential uses include real-time and closed-loop applications with low latency where the execution is in sync with frame rate of camera such as self-driving cars and augmented reality.</p><p>The real-time and online nature of such an approach introduces several challenges: i) scenes with multiple peo- ple demand handling of occlusion, proximity and contact as well as limb articulation, and ii) it should be runtimeinvariant to the number of people in the scene. Furthermore, iii) it must be capable of handling challenges induced from video data, such as large camera motion and motion blur across frames. We build upon the Part Affinity Fields (PAFs) <ref type="bibr" target="#b4">[6]</ref> to overcome these challenges, which represent connections across body keypoints in static images as normalized 2D vector fields with position and orientation. In this work, we propose Temporal Affinity Fields (TAFs) which encode connections between keypoints across frames, including a unique cross-linked limb topology as seen in bottom row of <ref type="figure" target="#fig_0">Figure 1</ref>. In the absence of motion or when there is not enough data from previous frames, TAFs constructed between same keypoints, e.g., wrist-wrist or elbow-elbow across frames lose all associative properties (see top row of <ref type="figure" target="#fig_0">Fig. 1</ref>). In this case, the nullification of magnitude and orientation provides no useful information to dis-cern between the case where a new person appears or where an existing person stops moving. This effect is compounded if these two cases occur in proximity together. However, the longer limb TAF connections allow information preservation even in the absence of motion or appearance of new people by preventing corruption of valid information with noise as the magnitude of motion becomes small. In the limiting case of zero motion, the TAF effectively collapses to a PAF. From the perspective of a network, TAF between keypoints destroys spatial information about keypoints as motion ceases, whereas TAF across keypoints simply learns to propagate the PAF, which is a much simpler task. Furthermore, we work on videos in a recurrent manner to make the approach real-time, where computation of each frame leverages information from previous frames thereby reducing overall computation. Where the single-image pose estimation methods use multiple stages to refine heatmaps <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b23">25]</ref>, we exploit the redundant information in the video frames and divert the resources towards efficient computation of both poses and tracks across multiple frames. Thus, the multi-stage computation over images is divided over multiple frames in a video. Overall, we call this Recurrent Spatio-Temporal Affinity Fields (STAF) and it achieves highly competitive results on the PoseTrack benchmarks: [64.6% mAP, 58.4% MOTA] on single scale at ∼30 FPS, and [71.5% mAP, 61.3% MOTA] on multiple scales at ∼7 FPS on the PoseTrack 2017 validation set using one GTX 1080 Ti. As of writing, our approach currently ranks second for accuracy and at third place for tracking on the 2017 challenge <ref type="bibr">[1]</ref>. Note that, our tracking approach is truly online on a per-frame basis with no post processing.</p><p>The rest of the paper is organized as follows. In Sec. 2, we discuss related work and situate the paper in the literature. In Sec. 3, we present details of our approach, training procedure as well as tracking and inference algorithm. Finally, we present results and ablation experiments in Sec. 7 and conclude the paper in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early methods for human pose estimation localized keypoints or body parts of individuals but did not consider multiple people simultaneously <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b32">34]</ref>. Hence, these methods were not adept at localizing keypoints of highly articulated or interacting people. Person detection was typically used which followed single-person keypoint detection <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b14">16]</ref>. With deep learning, human detection methods such as Mask-RCNN <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b12">14]</ref> were employed to directly predict multiple human bounding boxes through ROI-pooling followed by pose estimation per person <ref type="bibr" target="#b10">[12]</ref>. However, these methods suffered when people were in close proximity as bounding boxes got grouped together. Furthermore, these top-down methods required more computation as the number of people increased in the image, making them inadequate for real-time pose estimation and tracking.</p><p>The bottom-up Part Affinity Fields (PAF) method <ref type="bibr" target="#b4">[6]</ref> produced a spatial encoding of pair-wise body part connections in the image space, followed by greedy bipartite graph matching for inference permitting consistent computation speed irrespective of the number of people. Person Lab <ref type="bibr" target="#b24">[26]</ref> built upon these ideas to incorporate redundant connections on people with a less greedy inference approach getting highly competitive results on the COCO <ref type="bibr" target="#b20">[22]</ref> and MPII <ref type="bibr" target="#b1">[3]</ref> datasets. These methods work on single images and do not incorporate any keypoint tracking or past information.</p><p>Many offline methods have been proposed to enforce temporal consistency of poses in videos <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b33">35]</ref>. These require solving spatio-temporal graphs or incorporating data from future frames making them inadequate for online operation. Alternatively, Song et al. and Pfister et al. <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b30">32]</ref> demonstrate how optical flow fields could be predicted per keypoint by formulating the input to be multiframed. LSTM Pose Machines <ref type="bibr" target="#b21">[23]</ref> built upon previous work demonstrating use of single stage per frame for video sequences. However, these networks did not model spatial relationship between keypoints and were evaluated on the single person Penn Action <ref type="bibr" target="#b36">[38]</ref> and JHMDB <ref type="bibr" target="#b16">[18]</ref> datasets.</p><p>A different line of works explored maintaining temporal graphs in neural networks for handling multiple people <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8]</ref>. Rohit et al. demonstrated that a 3D extension of Mask-RCNN, called person tubes, can connect people across time. However, this required applying grouped convolutions over a stack of frames reducing speed, and did not achieve better results for tracking than the Hungarian Algorithm baseline. Joint Flow <ref type="bibr" target="#b6">[8]</ref> used the concept of Temporal Flow Field which connected keypoints across two frames. However, it did not use a recurrent structure and explicitly required a pair of images as input increasing run-time significantly. The flow representation also suffered from ambiguity when subjects moved slowly or were stationary and required special handling of such cases during tracking.</p><p>Top-down pose and tracking methods <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b12">14]</ref> have dominated the detection and tracking tasks <ref type="bibr" target="#b33">[35]</ref>  <ref type="bibr" target="#b34">[36]</ref> in PoseTrack but their speed suffered due to explicit human detection and follow-up keypoint detection for each person. Moreover, modeling long-term spatio-temporal graphs for tracking in an offline manner hurts real-time applications. None of these methods are able to report any significant runtime-to-performance measures as they cannot run in real time. In this work, we demonstrate this problem can be solved in a simple elegant single-stage network that incorporates recurrence by using the previous pose heatmaps to predict both keypoints and their spatio-temporal associations. We call this Recurrent Spatio-Temporal Affinity Fields (STAF) which not only represents the prediction of Spatial (PAF) and Temporal (TAF) Affinity Fields but also how they are refined through past information. <ref type="figure">Figure 2</ref>: Left: Training architecture for one of our models which ingests video sequences in a recurrent manner across time while generating keypoints and connections across keypoints in each frame as Part Affinity Fields (PAFs), and connections between keypoints across frames as Temporal Affinity Fields (TAFs). Together, we call this Recurrent Spatio-Temporal Affinity Fields (STAF). Each module ingests outputs from other modules in both previous and current frames (shown with arrows) and refines it. Center: During inference, our network operates on a single video frame at each time step using past information. Right: During inference, we use the predicted heatmaps to detect and track people. Keypoints (red) are extracted first, then associated into poses and tracklets using PAFs (green), TAFs (blue), and tracklets from previous frames.</p><formula xml:id="formula_0">P t-1 P t L t KP -ψ K PAF -ψ L TAF -ψ R Inference L t-1 V t VGG -ψ V L t K t-1 K t V t-1 R t-1 R t P t-1 P t (L t ,K t ,R t ) KP -ψ K VGG -ψ V PAF -ψ L KP -ψ K VGG -ψ V PAF -ψ L KP -ψ K VGG -ψ V PAF -ψ L TAF -ψ R TAF -ψ R TAF -ψ R Inference Inference Inference I 0 I 1 I 2 I t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>Our approach aims to solve the problems of keypoint estimation and tracking simultaneously in videos. We employ Recurrent Convolutional Neural Networks which we construct from four essential building blocks. Let P t represent the pose of a person in a particular frame or time t, consisting of keypoints K = {K 1 , K 2 , . . . K K }. The Part Affinity Fields (PAFs) L = {L 1 , L 2 , . . . L L } are synthesized from keypoints in each frame. For tracking keypoints across frames a video, we propose Temporal Affinity Fields (TAFs) given by R = {R 1 , R 2 , . . . R R } which capture the recurrence and connect the keypoints across frames. Together, they are referred to as Spatio-Temporal Affinity Fields (STAF). These blocks are visualized in <ref type="figure">Fig. 2</ref> where each block is shown with a different color: the raw convolutional feature from VGG backbone <ref type="bibr" target="#b29">[31]</ref> are shown in amber, PAFs in green, keypoints in red and TAFs in blue.</p><p>Thus, the output of VGG backbone, PAFs, keypoints and TAFs are given by V, L, K and R, respectively, and computed through CNNs by ψ V , ψ L , ψ K and ψ R , respectively. The keypoint heatmaps are constructed from ground truth by placing a Gaussian kernel at the location of the annotated keypoint, whereas the PAFs and TAFs are constructed from ground truth between pairs of keypoints for each person:</p><formula xml:id="formula_1">L t k→k := Ω K t k , K t k , R t k→k := Ω K t−1 k , K t k ,<label>(1)</label></formula><p>where ∼ denotes the ground truth and the function Ω(·) places a directional unit vector at every pixel within a predefined radius of the line connecting the two keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Models for Pose Estimation and Tracking</head><p>Next, we present the three models comprising the four blocks capable of estimating keypoints and STAF. The input to each network consists of a set of consecutive frames of a video. Each block in each network consists of five 7 × 7 and two 1 × 1 convolution layers. Each 7 × 7 layer is replaceable with the concatenation of three 3 × 3 convolution layers providing the same receptive field. The first stage has a unique set of weights from subsequent frames as it cannot incorporate any previous data and also has a lower depth which was found to improve results (see Sec. 7). The VGG features are computed for each frame. For frame I t at time t of the video, they are computed as V t = ψ V (I t ).</p><p>Model I: Given V t−1 and V t , the the following equations describe the first model:</p><formula xml:id="formula_2">L t = ψ L V t , ψ q−1 L (·) , K t = ψ K V t , ψ q L (·), ψ q−1 K (·) ,<label>(2)</label></formula><formula xml:id="formula_3">R t = ψ R V t−1 , V t , L t−1 , L t , R t−1 ,</formula><p>where ψ q means q recursive applications of ψ. In our experiments, we found that performance plateaus at q = 5. In Model I, PAFs are obtained by recursive application of ψ L on concatenated input from VGG features and PAFs from previous stage. Similarly, keypoints depend on VGG features, keypoints from the previous stage and PAFs from the current stage. Finally, TAFs are dependent on VGG features and PAFs from both the previous and current frames, as well as TAFs from previous frame. This model produces good results but is the slowest due to recursive stages. Model II: Unlike Model I with multiple applications of CNNs for PAFs and keypoints, Model II computes the PAFs and keypoints in a single pass as visualized in <ref type="figure">Fig. 2</ref>:</p><formula xml:id="formula_4">L t = ψ L V t , L t−1 , K t = ψ K V t , L t , K t−1 ,<label>(3)</label></formula><formula xml:id="formula_5">R t = ψ R V t−1 , V t , L t−1 , L t , R t−1 .</formula><p>Replacing five stages with a single stage is expected to drop performance. Therefore, the multi-stage computation of PAFs and keypoints in Model II is supplanted with output of PAFs and keypoints from the previous frames. This boosts up the speed significantly without major loss in performance as it takes advantage of the redundant information in videos, i.e., the PAFs and keypoints from previous frame are a reliable guide to the location of PAFs and keypoints in the current frame. Model III: Finally, the third model attempts to estimate Part and Temporal Affinity Fields through a single CNN:</p><formula xml:id="formula_6">[L, R] t = ψ [L,R] V t−1 , V t , [L, R] t−1 , K t = ψ K V t , L t , K t−1 ,<label>(4)</label></formula><p>where [L, R] implies simultaneous computation of Part and Temporal Affinity Fields through a single CNN. For Model III, the channels corresponding to PAFs are then passed for keypoint estimation along with VGG features from current frame and keypoints from previous frame. As Model III consists of only three blocks, it has the fastest inference, however it proved to be the most difficult to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Topology of Spatio-Temporal Affinity Fields</head><p>For our body model, we define K = 21 body parts or keypoints which is the union of body parts in COCO and MPII pose datasets. They include ears, nose and eyes from COCO; and head and neck from MPII. Next, there are several possible ways to associate and track the keypoints and STAF across frames as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. In this figure, solid circles represent keypoints while straight lines and arrows stand for PAFs and TAFs, respectively. <ref type="figure" target="#fig_1">Figure 3</ref>(a) consists of TAFs between same keypoints as well as PAFs.</p><p>For this topology, the number of TAFs and PAFs is 21 and 48, respectively. The TAFs capture temporal connections directly across keypoints similar to <ref type="bibr" target="#b6">[8]</ref>.</p><p>On the other hand, <ref type="figure" target="#fig_1">Figure 3</ref>(b) consists of TAFs between different limbs in a cross-linked manner across frames. The number of PAFs and TAFs is 48 and 96, respectively. We also tested the topology in <ref type="figure" target="#fig_1">Figure 3</ref>(c) which consists of 69 keypoints and limb TAFs only. This does not model any spatial links within frames across keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Training</head><p>During training, we unroll each model to handle multiple frames at once. Each model is first pre-trained in Image Mode where we present a single image or frame at each time instant to the model. This implies multiple applications of PAF and keypoint stages to the same frame. We train with COCO, MPII and PoseTrack datasets with a batch distribution of 0.7, 0.2 and 0.1, respectively, which corresponds to dataset sizes where each batch consists of images or frames from one dataset exclusively. For masking out un-annotated keypoints, we use the head bounding boxes available in MPII and PoseTrack datasets, and location of annotated keypoints for batches from COCO dataset. The net takes in 368 × 368 images and has scaling, rotation and translation augmentations. Heatmaps are computed with an Next, we proceed training in the Video Mode where we expose the network to video sequences. For static image datasets including COCO and MPII, we augment data with video sequences that have length equal to number of times the network is unrolled by synthesizing motion with scaling, rotation and translation. We train COCO, MPII and PoseTrack in Video Mode with a batch distribution of of 0.4, 0.1 and 0.5, respectively. Moreover, we also use skip-frame augmentation for video-based PoseTrack dataset where some of the randomly selected sequences skip up to 3 frames. We lock the weights of VGG module in Video Mode. For Model I, we only train the TAF module when training on videos. For Model II, we train keypoint, PAF and TAF modules for 5000 epochs, then lock all modules except TAF. In Model III, both STAF and keypoints remain unlocked throughout the 300k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference and Tracking</head><p>The method described till now predicts heatmaps of keypoints and STAF at every frame. Next, we present the framework to perform pose inference and tracking across frames given the predicted heatmaps. Let the inferred poses at time t be given by {P t,1 , P t,2 , . . . , P t,N } where the second superscript indexes over people at frame t. Each pose at a particular time consists of up to K keypoints that become part of a pose post inference, i.e., P t,n = {K t,n</p><formula xml:id="formula_7">1 , K t,n 2 , . . . , K t,n K }.</formula><p>The detection and tracking procedure begins with localization of keypoints at time t. The inferred keypoints K t are obtained by rescaling the heatmaps to original image resolution followed by non-maximal suppression. Then, we infer PAF weights, L t , and those for TAF, R t , between all pairs of keypoints in each frame defined by the given topology, i.e.,</p><formula xml:id="formula_8">L t k→k = ω K t k , K t k , R t k→k = ω K t−1 k , K t k ,<label>(5)</label></formula><p>where the function ω(·) samples points between the two keypoints, computes the dot product between the the mean vector of the sampled points and the directional vector from the first to the second keypoint. Both the inferred PAFs and TAFs are sorted by their scores before inferring the complete poses and associating them across frames with unique ids. We perform this in a bottom-up style where we utilize poses and inferred PAFs from the previous frame to determine the update, addition or deletion of tracklets. Going through each PAF in the sorted list, (i) we initialize a new pose if both keypoints in the PAF are unassigned, (ii) add to existing pose if one of the keypoints is assigned, (iii) update score of PAF in pose if both are assigned to the same pose, and (iv) merge two poses if keypoints belong to different poses with opposing keypoints unassigned. Finally, we assign id to each pose in the current We first select E, and select the best TAF linking it, going to F. We know F belongs to person A, so we go to G. Then we sample the TAF between G and A, since transitivity only exists between those limbs. We see the score is lower</p><p>We then select B, and select the best TAF linking it, going to C. We know C belongs to person B, so we go to D. Then we sample TAF between D and A. We see the score is higher.</p><p>Hence, we select B to be our next point in the graph. frame with the most frequent id of keypoints from the previous frame. For cases where we have ambiguous PAFs, i.e., multiple equally likely possibilities as seen in <ref type="figure" target="#fig_3">Figure 4</ref>, we use transitivity that reweighs PAFs with TAFs to disambiguate between them, using α as a biasing weight. In this figure, keypoint {A} -an elbow -is under consideration with wrists {B} and {E} as two possibilities. We select the strongest TAF where {A, B, C, D, A} has a higher weight than {A, E, F, G, A}, computed as:</p><formula xml:id="formula_9">L t,n k→k = (1 − α) * ω(K t−1,n k , K t,n k ) + α * ω(K t,n k , K t,n k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present results of our experiments. Input images to networks are resized at W×368 maintaining aspect ratio for single scale (SS); and W×736, W×368 and W×184 for multiple scales (MS). The heatmaps for multiple scales are re-sized back to W×736 and merged through averaging. This is followed by inference and tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We conducted a series of ablation studies to determine the construction of our network architecture: Filter Sizes: As discussed in Sec. 3, each block either consists of five 7 × 7 layers followed by two 1 × 1 layers <ref type="bibr" target="#b4">[6]</ref>, or each 7 × 7 layer is replaced with three 3 × 3 layers similar to <ref type="bibr" target="#b3">[5]</ref> in the alternate experiment. The results are shown in <ref type="table" target="#tab_10">Table 7</ref>. We run single frame inference on Model I and find the 3 × 3 filter size to be 2% more accurate than 7 × 7, with significant boosts in average precision of knee and ankle keypoints. It is also 40% faster while requiring 40% more memory. Video Mode / Depth of First Stage: Next, we report results when training in Image Mode (Im) using single images, and when we continue training beyond images while exposing the network to videos and augmenting with synthetic motion in the Video Mode (Vid). During testing, the network is run recurrently on video sequences with one frame per (a) (b) (c) (d) <ref type="figure">Figure 5</ref>: Improvement in quality of heatmaps before (a,c) and after (b,d) the network is exposed to videos and synthetic motion augmentation. We observe better peaks and less noise across both PAF and keypoint heatmaps.</p><p>stage. Model II is deployed for these experiments. We find that by exposing the network to video sequences for 5000 iterations, we were able to boost the mAP as seen in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure" target="#fig_12">Fig. 9</ref>. We also find that if we use the same depth, i.e., number of channels for the first frame as the other frames (128-128), the network was not able to generalize well to recurrent execution (56.6 mAP) when trained with Image Mode. When reducing the depth for the first frame to one-half, i.e. (64-128), we found that the generalization to videos was better (62.6 mAP). When trained with Video Mode, mAP increased further to 64.1. We reason that the 64-depth modules produced relatively vague outputs which gave sufficient room for the subsequent modules in the following frames to process and refine the heatmaps yielding a boost in performance. Furthermore, this also highlights the importance of incorporating shot change detection and running the first stage at each shot change.  Effect of Camera Frame Rate on mAP: For these experiments, we studied how the frame rate of the camera and number of stages affect the accuracy of pose estimation. With a high frame rate, the apparent motion between frames is smooth, which becomes relatively abrupt at low framerates. Therefore, the heatmaps from previous frames would not be as useful at low frame-rates. We tested this hypothesis with Model I (five stages of the same modules without ingesting previous frame heatmaps), and Model II (different number of stages with each ingesting heatmaps from previous frame). We also evaluate the influence of training with Image and Video modes in <ref type="figure" target="#fig_5">Figure 6</ref>. <ref type="figure" target="#fig_5">Fig. 6(a)</ref> shows results on a subset of ten sequences where the human subjects comprised at least 30% of the frame height in the PoseTrack 2017 validation set. <ref type="figure" target="#fig_5">Fig. 6(b)</ref> presents results on the entire validation set. The original videos were assumed to run at the film-standard 24 Hz, hence we ran experiments by varying frame rates at 24, 12 and 6 Hz through sub-sampling. The ground truth has been annotated at 6 Hz. As expected, accuracy is proportional to video frame rate and number of stages. When the Model II was trained in Image Mode, we observed small increments in accuracy until at four stages, it peaks at the same level as Model I. Upon training with Video Mode, it surpasses this accuracy peaking earlier at two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Hea Sho Elb Wri Hip Kne Ank mAP fps</head><p>When considering the entire validation set, the approach is still able to reap the benefits of more stages and training in Video Mode as can be seen in <ref type="figure" target="#fig_5">Fig. 6(b)</ref>. However, it was barely able to reach the accuracy of the much slower Model I. For the validation set, the accuracy reduced when including sequences with smaller apparent size of humans. These sequences usually were more crowded as well, and passing in the previous heatmaps seemed to hurt the performance. The body parts of small-sized humans only occupied a few pixels in the heatmaps and the normalized direction vectors were inconsistent and random across frames.</p><p>Influence of Topology / Model Type in Tracking: Next, we report experiments on different combinations of topology defined in <ref type="figure" target="#fig_1">Fig. 3</ref> with the three models presented in Sec. 3.1, both for pose estimation and tracking evaluated using mean Average Precision (mAP) and Multiple Object Tracking Accuracy (MOTA) metrics in <ref type="table" target="#tab_3">Table 3</ref>. We found an improvement in tracking using limb TAFs in Topology B versus keypoint TAFs in Topology A. As highlighted in <ref type="figure" target="#fig_0">Fig. 1</ref>, Topology A lacks associative properties when a keypoint has minimal motion or when a new person appears. Although we enforced spatial constraint that joint locations should be close in consecutive frames, and adjusted it according to scale (similar to <ref type="bibr" target="#b6">[8]</ref>), this still resulted in false positives since it is difficult to disambiguate between a newly detected person and some nearby stationary person. Furthermore, where motion of a person tended to be small, Topology A resulted in jittery and noisy vectors causing more reliance on pixel distances. This was further exacerbated by recurrence where accumulation of noisy vectors from previous frame heatmaps deteriorated associative ability of Temporal Affinity Fields. <ref type="table" target="#tab_3">Table 3</ref> also shows results for Topology C which significantly under-performed compared to Topology B. Since it exclusively consists of limb and joint TAFs without any spatial components, this makes keypoint localization and association rather difficult.</p><p>Topology B solves all of these problems elegantly. The longer cross-linked limb TAF connections preserve information even in the absence of motion or appearance of new people since the TAF effectively collapses to a PAF in such cases. This allows us to avoid association heuristics and makes the problem of new person identification trivial. With this representation, recurrence was observably beneficial due to true and consistent representation irrespective of magnitude of motion. As a side-advantage, this also allowed us to warm-start the TAF input with PAF providing more reliable initialization for tracking in the first frame.</p><p>For Model III, training beyond 5000 iterations gradually begins to harm the accuracy of the pose estimation resulting in reduced tracking performance as well. This is primarily due to the disparity in the amount of diverse data between COCO / MPII and PoseTrack datasets. For Model II, if we train on keypoints and PAFs modules and lock their weights afterwards, then follow with training only the TAF, this results in better performance with a significant boost in speed as well. Although Model I outperformed the other models with five stages for keypoints and PAFs; and a single recurrent stage for TAFs, however this comes at the expense of speed. Furthermore, we observe that an increase in mAP ends up sub-linearly increasing the MOTA as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Video Rate and Number of People on Tracking:</head><p>Finally, we performed a study on how the frame rate of the camera affects tracking accuracy, since a lower frame rate would require longer associations in pixel space.</p><p>We ran Lukas Kanade (LK) as a baseline tracker by replacing the TAF Module in Model I with LK (21 × 21 window size; 3 pyramid levels). Initially, we observe that there is roughly 2.0% improvement in MOTA as seen in    <ref type="figure" target="#fig_6">Fig. 7(a)</ref>. However, we note that around 20% of the sequences have significant articulation and camera movement, where TAFs outperformed LK as the latter was not able to match keypoints across large displacements whereas TAFs found matches due to stronger descriptive power. TAFs were able to maintain tracking accuracy even with low frame-rate cameras, but with LK the MOTA drops off significantly (see <ref type="figure" target="#fig_6">Fig. 7(a)</ref>). Furthermore, <ref type="figure" target="#fig_6">Fig. 7(b)</ref> suggests that our approach is nearly runtime-invariant to number of people in the frame making it suitable for crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison</head><p>We present results on PoseTrack dataset in <ref type="table" target="#tab_5">Table 4</ref> for 2017 validation set (top), 2017 test set (middle) and 2018 validation set (bottom). FlowTrack, JointFlow and Pose-Flow are included as comparison in this table. FlowTrack is a top-down approach which means human detection is performed first followed by pose estimation. Due to this reason, it is significantly slower than bottom-up approaches such as ours. Model II-B with single scale is competitive with other bottom-up approaches while being 270% faster. However, multi-scale (MS) processing boosts performance by ∼6% and ∼1.5% for mAP and MOTA, respectively. We are also able to achieve competitive results on the Pose-Track 2018 Validation set while maintaining the best speeds amongst all reported results. Note that PoseTrack 2018 Test set was not released to public at the time of submission of this paper. <ref type="figure" target="#fig_7">Figure 8</ref> shows some qualitative results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we first motivated recurrent Spatio-Temporal Affinity Fields (STAF) as the right approach for detection and tracking of articulated human pose in videos, especially for real-time reactive systems. We showed that leveraging the previous frame data within a recurrent structure and training on video sequences yields as good results as a multi-stage network albeit at much lower computation cost. We also demonstrated the stability of tracking accuracy at reduced frame rates for the TAF formulation, due to its ability to correlate keypoints over large pixel distances. This implies that our method can be deployed on low-power embedded systems which may not be able to run large networks at high frame rates, yet are able to maintain reasonable accuracy. Our new cross-linked limb temporal topology is able to generalize better than previous approaches due to strong associative power with PAF being a special case of TAF. We are also able to operate at the same consistent speed irrespective of the number of people due to bottom-up formulation. For future work, we plan to embed a re-identification module to handle cases of people leaving and reappearing in a camera view. Furthermore, detecting and triggering warm-start at every shot change has the potential to boost pose estimation and tracking performance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Model Training</head><p>During training, we unroll each model so that it can handle multiple frames at once. Each model is first pretrained in Image Mode where we present a single image or frame at each time instant to the model. This implies multiple applications of PAF/KP stages to the same frame. We train with COCO, MPII and PoseTrack datasets with a batch distribution of 0.7, 0.2 and 0.1, respectively, matching dataset sizes, where each batch consists of images or frames from one dataset exclusively. We use the head bounding box information in MPII and Posetrack datasets to mask out the eyes/nose/ears in the background heatmap channel when considering the MPII and PoseTrack batches, and mask out the neck and top head positions using the annotated eyes/nose/ears keypoints for COCO batches. The net is input images of 368 × 368 dimensions and has scaling, rotation and translation augmentations, where regions not annotated are masked out. Heatmaps are computed with an 2 loss with a stride of 8 resulting in 46 × 46 dimensional heatmaps. In topology (b) and (c), we initialize the TAF with PAF, and zeros for (a). We train the net for max of 400k iterations.  Next, we proceed training in the Video Mode where we expose the network to video sequences. For static image datasets including COCO and MPII, we augment data with video motion sequences by synthesizing motion with scaling, rotation and translation over the unroll count. We train COCO, MPII and PoseTrack in Video Mode with a batch distribution of of 0.4, 0.1 and 0.5, respectively. Moreover, we also use skip-frame augmentation for video-based Pose-Track dataset, where some of the randomly selected sequences skip up to 3 frames. We lock the weights of VGG module in Video Mode and only train the STAFs and keypoints blocks. For Model I, we only trained the TAFs block when training on videos. For Model II, we trained PAFs, keypoints and TAFs for 5000 epochs, then locked PAFs and keypoints before training TAFs only. In Model III, both STAFs and keypoints were kept unlocked and were trained for 300k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Inference and Tracking</head><p>The method described till now predicts heatmaps of keypoints and STAFs at every frame by running CNNs associated with each module while passing required data computed from previous frame. Next, we present the framework to perform pose inference as well as tracking across frames given the output heatmaps. Let the inferred poses at time t and t − 1 be given by: P t = {P t,1 , P t,2 , . . . , P t,N },</p><formula xml:id="formula_10">P t−1 = {P t−1,1 , P t−1,2 , . . . , P t−1,M },<label>(6)</label></formula><p>where the second superscript indexes over people in each frame. Each pose at a particular time P t,n consists of up to K keypoints post inference, i.e., P t,n includes only those keypoints that become part of a pose:</p><formula xml:id="formula_11">P t,n = {K t,n 1 , K t,n 2 , . . . , K t,n K }.<label>(7)</label></formula><p>The detection and tracking procedure begins with localization of keypoints at time t. The inferred keypoints K t are obtained by rescaling the heatmaps to match the original image resolution followed by non-maximal suppression. Then, we infer PAF and TAF weights between all possible pairs of keypoints in each frame defined by the given topology, i.e., </p><p>Algorithm 1 : Estimation and tracking of keypoints and STAFs Input: K t , L t , R t , and P t−1 with unique ids Output: P t with ids 1: procedure INFERPOSES()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Compute L t given K t , L t andL 3:</p><p>Compute R t given K t , R t , P t−1 andR 4:</p><p>Sort L t and R t by score <ref type="bibr">5:</ref> Initialize empty map of people P t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for every L t,n k→k in L t do 7:</p><p>If k and k unassigned; add new P t,n 8:</p><p>If k or k assigned; add to existing P t,n 9:</p><p>If k and k assigned; update score of P t,n 10:</p><p>If k assigned to P t,n and k assigned to P t,n 11:</p><p>&amp; P t,n and P t,n lack opposing points; <ref type="bibr">12:</ref> merge P t,n and P t,n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>end for <ref type="bibr">14:</ref> for P t,n in P t do <ref type="bibr">15:</ref> for K t,n in P t,n do 16:</p><p>Find R t with the highest score; copy id <ref type="bibr">17:</ref> end for 18:</p><p>Update P t,n with the most frequent id <ref type="bibr">19:</ref> end for 20:</p><p>If insufficient keypoint matches for P t,n ; <ref type="bibr">21:</ref> initialize tracklet <ref type="bibr">22:</ref> Remove P t−1,n if no association made 23: end procedure where the function ω(·) samples points between the two argument keypoints, computes the dot product between directional vector among the two points and the mean vector of the sampled points. The inference of PAF, L t , and TAF, R t , weights is constrained by the spatio-temporal topology,</p><p>where the spatial and temporal constraints are encoded in tablesL andR, respectively.</p><formula xml:id="formula_13">max N t K k L t,n k→k + R t k→k<label>(9)</label></formula><p>Overall, we wish to generate a set of people P t with ids that maximizes the connection scores between their keypoints pairs (K t−1,n k , K t,n k ), and temporal connections given previous keypoints K t−1,n from tracklets P t−1,n given an association.</p><p>To do this, both the inferred PAF and TAF weights are computed then sorted by their scores before inferring the complete poses, P t , and associating them across frames with unique ids. We perform this in a bottom-up style as described in Algorithm 1 where we utilize R t and P t−1 to determine the update, addition or deletion of tracklets. Go-ing through each PAF in the sorted list, (i) we initialize a new pose if both keypoints in the PAF are unassigned, (ii) add to existing pose if one of the keypoints is assigned, (iii) update score of PAF in pose if both are assigned to the same pose, and (iv) merge two poses if keypoints belong to different poses with opposing keypoints unassigned. Finally, we assign id to each pose in the current frame with the most frequent id of keypoints from the previous frame. This is done over all tracklets and people very quickly as it is done on the GPU. Furthermore, we make use of past poses P t−1 and TAFs R t to reweigh PAFs. For cases where we have an ambiguous PAFs (Alg. 1, 7:) as seen in <ref type="figure" target="#fig_3">Figure 4</ref>, we use transitivity that reweighs PAFs to disambiguate between them. In this figure, keypoint {A} -an elbow -is under consideration, with wrists {B}/{E} as two possibilities. We select the strongest TAFs where {A, B, C, D, A} has a higher weight than {A, E, F, G, A}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Additional Experiments</head><p>We present some experiments that were otherwise not displayed in detail in the main paper.</p><p>For the sake of completion, we first report results on the COCO dataset in <ref type="table" target="#tab_9">Table 6</ref>. Despite using single set of weights for all the stages, we were able to get close results. Our network is designed to be lightweight and work in a recurrent fashion, so our main reference point is still the Posetrack datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AP AP 50 AP 75 AP M AP L Top-Down Approaches Megvii <ref type="bibr" target="#b5">[7]</ref> 73.0 91.7 80.9 69.5 78.1 G-RMI <ref type="bibr" target="#b25">[27]</ref> 71.0 87.9 77.7 69.0 75.2 Mask R-CNN <ref type="bibr" target="#b12">[14]</ref> 69  Filter Sizes: We observed that having each 7 × 7 filter replaced with a three 3 × 3 filter resulted in better accuracies, especially for knees and ankles. The results are shown in <ref type="table" target="#tab_10">Table 7</ref>. We run single frame inference on Model I and find the 3 × 3 to be 2% more accurate than 7 × 7, with significant boosts in average precision of knee and ankle keypoints.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrence of Keypoints Module:</head><p>To verify that the network was indeed benefiting from ingesting previous frame heatmaps, we explicitly train a model where the keypoint module was connected to current PAF module in an auxiliary fashion and did not receive previous heatmaps (first row of <ref type="table">Table 8</ref>). The second row shows the case where keypoint module was connected to ingest output heatmaps from previous frames. The result is 2.1% improvement at single scale on the PoseTrack 2017 validation set using Model II with 7 × 7 filter size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Hea Sho Elb Wri Hip Kne Ank mAP fps KP Auxiliary 72.3 71.2 63.9 51.4 60.1 56.3 50.0 61.5 28 KP Connected 76.2 71.6 64.5 51.9 62.6 59.3 52.5 63. <ref type="bibr">6 27</ref>  <ref type="table">Table 8</ref>: Performance using Model II where the keypoint module does not take feedback from previous heatmaps (auxiliary), and when it does ingest previous heatmaps (connected).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STAF Topology:</head><p>We experimented with Topology A, B and C. Topology B proved to be better than A due to it's ability to preserve information even during minimal motion, lending itself better to the recurrent structure of our network. It especially performed better during jittery camera motion, or during crowded scenes with several people. Topology C, which does not consist of any current frame spatial information, was difficult to train and resulted in an MAP that was about 8% lower. This was mainly because we had to construct a person during the first frame, or during a new person appearance, and simply propagate it using the TAF, which proved to be less reliable than extracting poses on every frame with PAF/TAF, then propagating it with TAF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Implementation</head><p>Training: We train on 4 Titan XP in Caffe. Testing: We test on a single 1080 Ti, and i7 7800X. We write our own code in C++ and CUDA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We solve multi-person human pose tracking by encoding change in position and orientation of keypoints or limbs across time as Temporal Affinity Fields (TAFs) in a recurrent fashion. Top: Modeling TAFs (blue arrows) through keypoints works when motion occurs but fails during limited motion making temporal association difficult. Bottom: Cross-linked TAFs across limbs perform consistently for all kinds of motions providing redundancy and smoother encoding for further refinement and prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>This figure illustrates the three possible topology variations for Spatio-Temporal Affinity Fields including the new cross-linked limb topology (b). Keypoints, PAFs and TAFs are represented by solid circles, straight lines and arrows, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>H</head><label></label><figDesc>Assume we are constructing a person, starting at Node A. We are confused about moving either to B or E, since their scores were sorted closely in PAF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Ambiguity when selecting between two wrist locations B and E is resolved by reweighing PAFs through TAFs. (b)-(d): With transitivity, incorrect PAFs containing ankles (c) are resolved with past pose (b) resulting in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Im -7x7 -128-128 74.6 69.6 55.5 40.2 56.4 47.2 44.0 56.6 27 Vid -7x7 -128-128 76.2 71.6 64.5 51.9 62.6 59.3 52.5 63.6 27 Im -7x7 -64-128 73.5 72.2 63.8 52.1 62.7 57.3 51.1 62.6 27 Vid -7x7 -64-128 75.8 73.4 65.5 53.8 64.2 58.4 51.4 64.1 27 Im -3x3 -64-128 73.5 72.5 65.0 52.7 63.7 57.7 53.2 63.4 35 Vid -3x3 -64-128 75.4 73.2 67.4 55.0 63.9 58.4 53.5 64.6 35</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>1 s / 35 fps Model II: 2 s / 26 fps Model I: 5 s / 14 fps *Model II: 1 s / 35 fps *Model II: 2 s / 26 fps *Model II: 3 s / 20 fps *Model II: 4 s / 17 fps (a) Validation Subset (b) Validation Set These graphs show mAP curves as a function of frame rates of camera, i.e., the rate at which an original 24Hz video is input to the method. The flat black line shows the performance of five-stage Model I, while '*' in the legend indicates training using Image Mode only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>(a) This graph shows MOTA as a function of video frame rate for Temporal Affinity Fields (TAFs) and Lukas-Kanade (LK) tracker. The performance of TAFs is virtually invariant to frame rate or alternatively to the amount of motion between frames. (b) Our approach is effectively runtime-invariant to the number of people in the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Three example cases of tracking at ∼30 FPS on multiple targets. Top / Middle: Observe that tracking continues to function despite large motion displacements and occlusions. Bottom: A failure case where abrupt scene change causes ghosting, where previously tracked person appears in the new frame. This issue can be rectified through a warm-start.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Acknowledgment:</head><label></label><figDesc>Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number D17PC00340. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation/herein. Disclaimer: The views and conclusions contained herein</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>MethodWrist-AP Ankles-AP mAP MOTA fps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Types of filters usued Method Hea Sho Elb Wri Hip Kne Ank mAP fps Model I -3x3 75.7 73.9 67.8 56.3 66.8 62.3 56.9 66.3 14 Model I -7x7 76.0 73.3 66.4 54.0 63.4 59.2 52.2 64.3 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Method Hea Sho Elb Wri Hip Kne Ank mAP fps Model I -3x3 75.7 73.9 67.8 56.3 66.8 62.3 56.9 66.3 14 Model I -7x7 76.0 73.3 66.4 54.0 63.4 59.2 52.2 64.3 10 This table shows results for experiments with the two filter sizes on PoseTrack 2017 validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>This table shows single-scale performance usingModel II before and after training with videos, filter sizes, as well as different depths for first stage.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>This table shows pose estimation and tracking performance for combinations of model types and topologies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>This table shows comparison on the PoseTrack datasets. For our approach, we report results with Models I / II and Top. B. The last column shows the speed in frames per second (* excludes pose inference time). .edu, hidrees@andrew.cmu.edu, gines@cmu.edu, yaser@cs.cmu.edu}</figDesc><table><row><cell>FlowTrack is</cell></row></table><note>{raaj@cmu</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Training Parameters for both Image Mode and Video Mode</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results on the COCO test-dev dataset. Top: topdown results. Bottom: bottom-up results (top methods only). AP 50 is for OKS = 0.5, AP L is for large scale persons.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>This table shows results for experiments with the two filter sizes on PoseTrack 2017 validation set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">loss with a stride of 8 resulting in 46 × 46 dimensional heatmaps. We initialize the limb TAFs with PAFs in topology 3(b,c), and keypoint TAFs with zeros in topology 3(a,c). We train the net for a maximum of 400k iterations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: new benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3D pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>abs/1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Joint flow: Temporal flow fields for multi person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno>abs/1805.04596</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno>abs/1712.09184</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DensePose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-domain pose network for multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ArtTrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW: Crowd Understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PoseTrack: Joint multiperson pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards multiperson pose tracking: Bottom-up and top-down methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">LSTM pose machines</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">PersonLab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08225</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormahlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1804.06208</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
