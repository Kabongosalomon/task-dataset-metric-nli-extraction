<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Modality Learning for Video Action Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
							<email>stepan.komkov@intsys.msu.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University Huawei Moscow Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
							<email>dzabraev.maksim@intsys.msu.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University Huawei Moscow Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
							<email>petyushko.alexander1@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University Huawei Moscow Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mutual Modality Learning for Video Action Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Recognition</term>
					<term>Video Action Classification</term>
					<term>Video Labeling</term>
					<term>Mutual Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The construction of models for video action classification progresses rapidly. However, the performance of those models can still be easily improved by ensembling with the same models trained on different modalities (e.g. Optical flow). Unfortunately, it is computationally expensive to use several modalities during inference. Recent works examine the ways to integrate advantages of multi-modality into a single RGB-model. Yet, there is still a room for improvement. In this paper, we explore the various methods to embed the ensemble power into a single model. We show that proper initialization, as well as mutual modality learning, enhances single-modality models. As a result, we achieve state-of-the-art results in the Something-Something-v2 benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video Recognition has progressed a lot during the last several years. Datasets have enlarged from thousands of clips <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref> to hundreds of thousands <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref> and even to hundreds of millions <ref type="bibr" target="#b16">[17]</ref>. Neural network-based approaches for video processing evolved from simple 3D-convolutions <ref type="bibr" target="#b24">[25]</ref> to Parvo-and Magnocellular counterparts emulation <ref type="bibr" target="#b6">[7]</ref> and absorbed developments of classical Image Recognition <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Nevertheless, classical results in the domain of Video Processing are still useful: Optical Flow estimation for a video sequence can significantly improve the quality of video recognition <ref type="bibr" target="#b21">[22]</ref>. However, the common ways to estimate Optical Flow require an amount of calculation that is comparable to the whole further neural network inference. That is why a number of works are devoted to the implicit Optical Flow estimation during the RGB-based neural network inference <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>In our work, we target not only the improvement of RGBbased models but also simultaneous improvement of different single-modality models. To this end, we utilize Mutual Learning <ref type="bibr" target="#b33">[34]</ref> that enables us to share the knowledge between single-modality models in both directions. We combine it with proper initialization that develops the performance of trained models.</p><p>We show that our approach not only improves each singlemodality model but also boosts RGB-based models better than existing methods. Additionally, we examine how to use Mutual Learning to achieve the best results of multi-modality ensemble. Thus, we achieve state-of-the-art (SOTA) results among the ones reported previously in the Something-Something-v2 benchmark <ref type="bibr" target="#b9">[10]</ref>.</p><p>Our code is available as a fork from the code presented in <ref type="bibr" target="#b15">[16]</ref> (https://github.com/papermsucode/ mutual-modality-learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>First, we briefly describe the most common approaches for video action classification from the historical perspective. They can be divided into two groups: models with 3d-convolutions and models with 2d-convolutions.</p><p>Second, we describe the methods that improve singlemodality models using other modalities and highlight the differences from our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D-approaches</head><p>A video sequence is a 4d-tensor with the following parameters: height of frames, width of frames, number of frames, and number of channels per frame (3 in case of RGB input). Therefore, we can process it using Convolutional Neural Networks (CNNs) where 3d-convolutions are applied instead of 2d convolutions (with the new temporal dimension). <ref type="bibr">Tran et al.</ref> are the first to propose 3d convolutional networks based on this idea <ref type="bibr" target="#b24">[25]</ref>. Thereby, they achieved the SOTA results in a number of tasks. Although a video model has to obtain temporal and motion information from the sequence of frames, it still needs to recognize the spatial information contained in each frame. Carreira and Zisserman propose to inflate the trained weights of the Image Recognition network and to use them as initialization for 3d-CNN <ref type="bibr" target="#b1">[2]</ref>. Nowadays, this is a common approach for video model initialization.</p><p>Wang et al. implement an attention mechanism that helps to find dependencies between far positions on different frames. That is meaningful for fast-moving objects or quick movements of the camera <ref type="bibr" target="#b28">[29]</ref>.</p><p>The disadvantage of 3d-CNNs is that they require to work with much more parameters in comparison to their 2d analogs. To address this problem, the first convolutions can be replaced by the per-frame 2d-convolutions (top-heavy models) since those convolutions are mostly responsible for the evaluation of spatial features <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Also, 3d-convolutions can be decomposed as 2d spatial-convolutions plus 1d temporalconvolutions. This kind of decomposition reduces the number of parameters and operations and increases non-linearity at the same time <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>Feichtenhofer et al. present a SlowFast Network architecture that emulates Parvo-and Magnocellular counterparts by sampling video frames with two different framerates and by feeding them to two branches with different computational power <ref type="bibr" target="#b6">[7]</ref>. Thus, the lightweight Fast pathway captures motion and temporal dynamics while the Slow pathway captures the spatial semantics. This approach achieved the SOTA results on the Kinetics-400 Action Classification dataset <ref type="bibr" target="#b13">[14]</ref> among models without additional data.</p><p>The Temporal Pyramid Networks (TPN) of Yang et al. can be viewed as an extension of SlowFast networks <ref type="bibr" target="#b30">[31]</ref>. A thinned out frames sequence flows to the different branches from intermediate layers instead of entering from the input. This approach is an add-on to existing architectures and can be implemented for 3d-CNNs and 2d-CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D-approaches</head><p>The early CNN-based models for video with 2dconvolutions consist of two streams. The first stream called Spatial takes RGB frames as an input. The second stream called Temporal takes a stack of consecutive Optical Flow estimations <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The final prediction is an average of the predictions of both streams. Note that the authors use pretrained weights from Image Recognition models for the temporal stream as well as for the spatial stream.</p><p>Nowadays, the idea of features sharing between frames is used to simulate a 3d-inference using 2d-convolutions. The pioneering work in this scope is Temporal Shift Modules network (TSM) by Lin et al. that applies ordinary 2d-ResBlocks <ref type="bibr" target="#b10">[11]</ref> to each input frame <ref type="bibr" target="#b15">[16]</ref>. The single difference is that TSM replaces a one-eighth of channels with the same channels from the previous frame and another one-eighth of channels with the same channels from the future frame before each first convolution of the ResBlock. Thereby, the authors achieved the SOTA results on the Something-Something-v2 dataset <ref type="bibr" target="#b9">[10]</ref> and provided a powerful and efficient baseline for the future research in this area.</p><p>Based on the idea of feature sharing, Shao et al. present Temporal Interlacing Network <ref type="bibr" target="#b19">[20]</ref>. The authors add extra lightweight blocks that decide on distances and weights of channels sending within each ResBlock. This approach is used instead of a fixed replacement of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optical Flow distillation</head><p>Despite all the aforementioned progress, most of works can be improved by averaging of their predictions with the predictions of the same network trained on the Optical Flow modality <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>Since the Optical Flow calculation is a time-consuming operation, a number of works is devoted to incorporation of the motion-estimation blocks inside the CNN architecture <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, knowledge distillation from the Optical Flow modality to any RGB single-modality network seems to be of more interest.</p><p>Three basic works that should be mentioned are Knowledge Distillation (KD) <ref type="bibr" target="#b11">[12]</ref>, Mutual Learning (ML) <ref type="bibr" target="#b33">[34]</ref>, and Born-Again Networks (BAN) <ref type="bibr" target="#b7">[8]</ref>.</p><p>The first proposes to use soft-predictions of the model called Teacher network to train the smaller model called Student network. It turns out that this technique is helpful for video action classification task not as a neural network compression method but as a transfering of modality knowledge. Zhang et al. use KD to train a two-stream network with Motion Vector as the second modality <ref type="bibr" target="#b32">[33]</ref>. Stroud et al. confirm by constructing Distilled 3D Networks (D3D) <ref type="bibr" target="#b23">[24]</ref> that KD from the Optical Flow stream improves the quality of the RGB stream. In addition, the authors of D3D show that KD teaches implicit Optical Flow calculation inside the RGB stream. Motion-Augmented RGB Stream (MARS) of Crasto et al. distills the knowledge not from the prediction of the Optical Flow stream but from its feature maps before the global averaging operation <ref type="bibr" target="#b2">[3]</ref>.</p><p>In contrast to the mentioned works, we utilize the idea of ML to train jointly several single-modality networks and improve the quality of each of them. Motivated by BAN, we show that the relaunch of training procedure can further boost the performance of models. Additionally, we show that proper initialization improves our results as well as results for MARS and D3D works.</p><p>Note that we target on the single-modality model quality. The improvement of the average predictions of several streams is a different branch of research. An example of an approach that addresses this problem is Gradient-Blending <ref type="bibr" target="#b27">[28]</ref>. Nevertheless, we examine the ability of ML to improve the average prediction the multi-modality ensemble. It turns out that proposed initialization with relaunches of single-modality ML provides the best result for the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED SOLUTION</head><p>The proposition of the best single-modality model training pipeline is depicted in <ref type="figure" target="#fig_1">Figure 1</ref>. The pipeline for the best ensemble training is described in section V.</p><p>The pipeline consists of three parts: initialization preparation, ML implantation and Mutual Modality Learning (MML).</p><p>The importance of each part is confirmed in section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initialization preparation</head><p>The standard starting point for the Video Action Classification models training is an ImageNet <ref type="bibr" target="#b3">[4]</ref> pretrained model. Inflating of 2d-convolutions proposed in <ref type="bibr" target="#b1">[2]</ref> makes that possible for both 3d-models and 2d-models.</p><p>If we use the input modality different from RGB then we have to change the shape of the first convolution from (C, 3, K, K) to (C, N, K, K). Here, C is a number of output channels of the first convolution, K is a kernel size and N is a number of channels of the new input. The pseudocode for the weights of the new convolution is as follows:  Dashed arrows denote weights transferring for initialization. Green part: first, we train two networks with RGB input initialized by ImageNet weights using cross-entropy loss. Yellow part: next, we use weights from the first step as initialization for two networks with RGB input that are trained jointly using Mutual Learning. Red part: finally, we apply Mutual Modality Learning to obtain the best single-modality model for each modality. We use weights of the network from the second step as initialization for each model in the third part.</p><p>In the proposed pipeline, we use ImageNet initialization only for the first step. The next two steps use weights from the previous step (with a change in the first convolution shape if it is needed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mutual Learning implantation</head><p>ML is a technique of training two models together in a way that they help each other to reach better convergence. To achieve that, we modify the loss functions of the networks as follows:</p><formula xml:id="formula_0">L 1 = L CE (p 1 , y) + L KL (p 2 ||p 1 ),<label>(1)</label></formula><formula xml:id="formula_1">L 2 = L CE (p 2 , y) + L KL (p 1 ||p 2 ).<label>(2)</label></formula><p>Here, L i is a loss of the i-th network, p i is a vector of the predicted class probabilities by the i-th network, y is a groundtrue class label, L CE is a cross-entropy loss and L KL is the Kullback Leibler (KL) Divergence loss given by the formula</p><formula xml:id="formula_2">L KL (p i ||p j ) = N n=1 p n i · log p n i p n j .<label>(3)</label></formula><p>In this formula p n i stands for a probability for the n-th class predicted by the i-th model. Thus, models teach each other using dependencies that they found during training and thereby improve their performance.</p><p>If there are more than two models involved in ML then the loss function is</p><formula xml:id="formula_3">L i = L CE (p i , y) + L KL j =i p j M − 1 || p i<label>(4)</label></formula><p>where M is a number of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mutual Modality Learning</head><p>In the original ML, both models use the same modality as an input. We propose to use different modalities of the video obtained from the same frames as inputs for different models. Thus, we share the knowledge obtained from one modality to other modalities.</p><p>Note that we need two consecutive frames to calculate the Optical Flow. Thus, if there are N RGB frames in total then there are only N − 1 Optical Flow frames in total.</p><p>So, suppose that the model requires T input-frames for the prediction and we have two representations of the video by different modalities: one representation with n frames and another with N frames (N &gt; n).</p><p>For this and similar cases in our work, we first sample frames with numbers (i 1 , . . . , i T ) for the modality with the least number of frames, and then we use frames with numbers (i 1 + ξ, . . . , i T + ξ) for the modality with the biggest number of frames. Here ξ ∼ unif{0, . . . , N − n}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ABLATION STUDIES</head><p>There are several conclusions that we make:</p><p>• Initialization with the RGB model trained on the same video dataset significantly enhances the performance for various modalities and training scenarios (not only ML but MARS and D3D also). • MML performs better than MARS or D3D approaches. • Two iterations of ML are better than one and there is no need for more. • MML performs better than ML as a final step. • The behavior described above preserves when we use modalities different from the Optical Flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments setup</head><p>For the ablation studies, we use several models and benchmarks: TSM <ref type="bibr" target="#b19">[20]</ref> on Something-Something-v2 <ref type="bibr" target="#b9">[10]</ref> with the code provided by the authors (the main setup, we use it unless otherwise specified) and I3D <ref type="bibr" target="#b1">[2]</ref> on Charades <ref type="bibr" target="#b20">[21]</ref> with the code provided in https://github.com/facebookresearch/ SlowFast.</p><p>We obtain the Optical Flow using TV-L1 algorithm <ref type="bibr" target="#b31">[32]</ref> and combine 5 consecutive evaluations of the Optical Flow by the x-and y-axes as one input-frame.</p><p>For the RGBDiff modality, we take 6 consecutive RGB frames to obtain 5 consecutive differences between them. Obtained differences are concatenated and considered as one input-frame.  1) TSM on Something-Something-v2: We use the standard setup for the TSM+ResNet-50 <ref type="bibr" target="#b10">[11]</ref> training proposed by the authors with batch size 64, ImageNet pretrain, 0.025 initial learning rate. The only difference is the frames sampling strategy. Instead of using one sampling strategy, we use both uniform sampling and dense sampling. The first one works as follows: we split video into T equal parts and take a random frame from each of them. Dense sampling requires taking each τ -th frame starting from a random position. We apply each of the two sampling strategies with 50% probability. See Appendix A as an explanation for this strategy.</p><p>We use single uniform sampling with one spatial 224x224 center crop during testing for the ablation studies. That is why the baseline result is worse than the same in <ref type="bibr" target="#b15">[16]</ref> where 256x256 central crop is used during testing.</p><p>2) I3D on Charades: This setup is used to show the advantages of MML regarding other approaches. Both D3D <ref type="bibr" target="#b23">[24]</ref> and MARS <ref type="bibr" target="#b2">[3]</ref> deal with 3d-models, that is why we use the I3D ResNet-50 model <ref type="bibr" target="#b1">[2]</ref> to make a fair comparison with the mentioned methods.</p><p>Besides, Charades is the dataset with multiple corresponding classes per one clip, so we show how to extend the proposed MML to the multi-label task.</p><p>Optimizer, the number of epochs and other hyperparameters are taken from the standard config-file for the Charades training in https://github.com/facebookresearch/SlowFast without any changes. We use model trained on Kinetics-400 <ref type="bibr" target="#b1">[2]</ref> as a standard initialization instead of ImageNet initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Initialization</head><p>An abbreviation "Flow from ImageNet" means that we initialize a model that takes Optical Flow as an input with the weights of the model trained on ImageNet. An abbreviation "Diff from RGB" means that we initialize a model that takes differences between RGB frames as an input with the weights of the model with RGB input trained on the current dataset using the cross-entropy loss and initialized by a model trained on ImageNet. We make other abbreviations in a similar way.</p><p>We do not include training from scratch into the ablation studies since this is a well-known fact that ImageNet initialization outperforms random initialization for the training of one-stream video models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>We can see from <ref type="table" target="#tab_0">Table I</ref> that RGB initialization significantly outperforms ImageNet initialization in the case of ordinary cross-entropy training of the Flow and Diff models. At the same time, Flow initialization is useless for RGB models.</p><p>We apply MARS <ref type="bibr" target="#b2">[3]</ref> and D3D <ref type="bibr" target="#b23">[24]</ref> approaches in both directions for RGB and Flow models. <ref type="table" target="#tab_0">Table II</ref> shows that RGB initialization improves results in each scenario. It should be noted that both MARS and D3D approaches mainly target 3dmodels. That is why the results of MARS training of "RGB from ImageNet" may be worse than the baseline ("RGB from ImageNet" using cross-entropy) since we use 2d-models. <ref type="table" target="#tab_0">Table III</ref> and IV are more representative. First, we train "RGB from ImageNet" and "Flow from ImageNet" models using cross-entropy. Then we train Flow and RGB models together using MML with all possible pairs of the initialization. The results of the RGB models trained using MML are presented in <ref type="table" target="#tab_0">Table III</ref>. The results of the Flow models trained using MML are presented in <ref type="table" target="#tab_0">Table IV.</ref> As we can see, the middle values of each column in <ref type="table" target="#tab_0">Table III</ref> are the best as well as the right values of each row in <ref type="table" target="#tab_0">Table IV</ref>. Thus, the consistency of better initialization is preserved in the case of MML.</p><p>Finally, even if we train models on one modality using ML then RGB initialization is still the best. The first three  rows and the next two rows of <ref type="table" target="#tab_2">Table V</ref> confirm that. We use abbreviations ImageNet2 and RGB2 to point out that we use different initialization obtained in the same way (KL loss is equal to zero otherwise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MML versus MARS and D3D</head><p>Since MARS <ref type="bibr" target="#b2">[3]</ref> and D3D <ref type="bibr" target="#b23">[24]</ref> works target mainly 3dmodels, we use the I3D on Charades setup in this subsection.</p><p>It should be noted that ordinary KL loss implementation uses the "batchmean" regime of averaging, i.e. we divide the sum of losses by the number of instances in one batch. However, we have to use the "mean" regime of averaging when we train the multi-label model using Binary Cross-Entropy losses (BCE), i.e. we divide the sum of losses by the multiplication of two factors: the number of instances in one batch and the number of classes. See Appendix B as an explanation of this point.</p><p>By similar reasoning, we divide additional loss functions of MARS and D3D by the number of classes.</p><p>The mean Average Precision (mAP) results of all approaches are presented in <ref type="table" target="#tab_0">Table VI</ref>. We can see again that RGB initialization improves the performance of each method. D3D is still better than MARS and MML is the best.</p><p>The right column in <ref type="table" target="#tab_0">Table VI</ref> is empty for MARS and D3D approaches since these approaches do not modify the Optical Flow model during training.</p><p>We assume that performance correlates negatively with the strength of the supervision signal. Since we apply KL loss to probits, then any l 2 distance between logits is possible during MML. Thus, we weakly bound the feature extraction strategy of a network. In the case of D3D training, we minimize l 2 distance between logits only. Thus, D3D does not force a network to estimate the same features in contrast to MARS.</p><p>We want to stress that we can significantly improve a singlemodality model different from RGB, e.g. MML improves mAP of the Flow model by about 2 times. With some further research these findings may be very helpful for video recognition by event cameras <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Relaunch of the ML and MML versus ML</head><p>An abbreviation "RGB from A(RGB)" in <ref type="table" target="#tab_0">Table VII</ref> means that we initialize an RGB model with the weights of the RGB Rows number 1 and number 3 from <ref type="table" target="#tab_0">Table VII</ref> demonstrate that relaunch of MML can improve the performance. At the same time, row number 4 demonstrates that the second relaunch is probably useless. Rows number 5 and number 6 demonstrate that the consistency is preserved for singlemodality ML.</p><p>As we can see, the results of the RGB model trained using MML are better than the results of both RGB models trained using ML: row number 1 versus row number 5 from <ref type="table" target="#tab_0">Table VII</ref>. This consistency is preserved for the relaunch of ML: row number 6 versus row number 7.</p><p>Rows number 1 and number 2 demonstrate that initialization with different RGB weights does not significantly affect the performance of MML.</p><p>Finally, row number 7 compared to row number 3 demonstrates that it is better to use MML only as a second step. We believe that the reason for that the separation of advantages of ML itself and additional information from another modality. We extensively examine this effect in section V. We use the weights from the first step as initialization for each launch of Mutual Learning. We have to use two launches for the second step because we need to obtain two models for which KL loss has not been optimized yet. Red part: finally, we apply single-modality Mutual Learning to each modality that we want to use in the ensemble. We use the weight from one model from each pair from the previous step as the initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other modalities</head><p>We expand our experiments to the Diff modality to examine the preservation of the consistency.</p><p>The last row from <ref type="table" target="#tab_0">Table I</ref> confirms that RGB initialization is also useful for Diff model. Row number 5 compared to rows number 4 and number 1 from <ref type="table" target="#tab_0">Table VIII</ref> confirms that MML is not worse than or even better than single-modality ML in case of RGB and Diff modalities.</p><p>Finally, the comparison of row number 6 to rows 1-5 demonstrates that MML with all three modalities outperforms or is not worse than any other ML in terms of individual results for each modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ENSEMBLE PERFORMANCE</head><p>The predictions of RGB and Flow models can be highly correlated since we train them using KL loss. Thus, an averaging of the predictions may perform worse than the averaging of ordinary RGB and Flow models trained using cross-entropy. The same logic is applicable to MARS or D3D training.</p><p>We show results of ensembles of two models in Appendix C-A and some results of ensembles of three different models with RGB, Flow and Diff input modalities in Appendix C-B.</p><p>The main conclusions are as follows:</p><p>• RGB models that do not use Optical Flow during training perform the best in ensemble with Flow models. Models trained using ML with RGB only are the first, RGB models trained using MML with RGBDiff are the second. • RGB models that use Optical Flow during training are the worst in the ensemble with Flow models. Performance in the ensemble with Flow models from better to worse: MML, D3D, MARS. We believe that this order is caused by the same reasons that are mentioned in the subsection IV-C. • The same behavior preserves when we combine Flow models with/without RGB signals in loss function during training with RGB models. The only point we want to stress is that "Flow from RGB" models still perform better than "Flow from ImageNet" models in ensembles with RGB models. • It is also better to combine models trained using singlemodality ML when we average the predictions of the RGB and Diff models. • An ensemble of RGB and Diff models can achieve results that are similar to the results of the RGB and Flow ensemble. • Models trained using single-modality ML achieve the best results in the ensemble of three different modalities in our experiments. See Appendix C-B for more details. Thus, although MML provides the best single-modality models, ordinary ML performs better for ensembles. Considering the aforementioned observations, we propose a pipeline for the best ensemble training that is depicted in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>First, we train two "RGB from ImageNet" models using cross-entropy. Second, we launch two single-modality ML procedures for the RGB models from the previous step. Finally, we train models using single-modality ML for each of three modalities (RGB, Flow, Diff) that we want to use in the ensemble. We use weights of the RGB models from the second step as an initialization for the third step. This is the reason why we have to launch two training procedures on the second step. KL loss is already optimized otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. COMPARISON TO STATE-OF-THE-ART</head><p>Followed by the observations made above, we train TSM+ResNet-101 with 16 input frames per clip on Something-Something-v2 using the described pipelines. Thus, we obtain an enhanced RGB model trained by MML and three models with RGB, Flow and Diff inputs for the best ensemble according to section V. The results are available in <ref type="table" target="#tab_0">Table IX.</ref> Our pipeline for the best ensemble achieves the SOTA results among the ones reported previously in the Something-Something-v2 benchmark.</p><p>We also make a comparison with other single-model solutions. There is only one single-model solution that outperforms our solution in one of two testing metrics in the Something-Something-v2 benchmark. This is a Temporal Pyramid Network <ref type="bibr" target="#b30">[31]</ref> that is several times heavier than TSM and uses more launches per one prediction.</p><p>For the simplest scenario, when we use ResNet-50 as a base architecture with 8 input frames and one launch per prediction, we achieve +2.77% improvement of the top-1 performance without adding complexity for the inference.</p><p>We exclude STM <ref type="bibr" target="#b12">[13]</ref> model from the comparison since it uses the average of predictions for three spatial crops. That is a more accurate but also a more computationally expensive approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We present Mutual Modality Learning, the approach that enhances the performance of single-modality model by joint training with models based on other modalities. In addition, we show that the proper initialization of network weights boosts the performance of various training scenarios. We check that our proposal works for different models and datasets, even for multi-label tasks. Our experiments lead to state-of-the-art results in the Something-Something-v2 benchmark.  for the second prediction, where T is a total number of frames for current modality and N is the shape of the temporal dimension of the input. Note that uniform sampling, unlike dense sampling, allows any period between input frames and depends on the total length of the video.</p><p>We found out that the use of more than two temporal crops with the same sampling strategy or more number of spatial crops insignificantly improves the validation results. At the same time, the use of different sampling strategies during testing significantly improves results regardless of the sampling strategy during training. That is why we incorporate both samplings into training. The median testing results for full-resolution central crops testing are shown in <ref type="table" target="#tab_7">Table X</ref>. Label "Dense k + Uniform m" means that we use k + m predictions per video using frames with numbers i·T k , i·T k + τ , . . . , i·T k + τ · (N − 1) , i ∈ {0, . . . , k − 1} when k &gt; 1 or frames with numbers Here T is a total number of frames for current modality, N is the shape of the temporal dimension of the input, τ &lt; T N −1 is a dense for the dense sampling and T = T − τ · (N − 1). Note that there is no random nature in frame numbers during testing.</p><p>We make the next conclusions based on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B LOSS MODIFICATION FOR THE BCE TRAINING</head><p>The ordinary implementation of the KL loss divides the sum of B · N terms by the B, where B is a batch size and N is a number of classes. The reason for that is that ordinary Cross-Entropy loss also divides the sum of B · N terms by the B, which can be unobvious:</p><formula xml:id="formula_4">L CE = 1 B · B b=1 − log e l gt b b N j=1 e l j b = = 1 B · B b=1 N i=1 −y i b · log e l i b N j=1 e l j b = (5) = 1 B · B b=1 N i=1 −y i b · log p i b = 1 B</formula><p>· H(y, p).</p><p>Here l i b is a predicted logit for the class number i for the instance number b, gt b -ground truth class for the instance So the magnitudes of the CE loss and KL loss are the same. Since the multi-label BCE loss is divided by the B · N :</p><formula xml:id="formula_5">L mlBCE = (6) 1 B · N · B b=1 N i=1 − y i b · log σ(l i b ) + (1 − y i b ) · log 1 − σ(l i b ) ,</formula><p>then we divide the KL loss by the B · N to make the magnitudes the same again. The authors of the MARS and D3D approaches found the best weights for their loss functions in the case of the Cross-Entropy training (50 for MARS and 1 for D3D). Our experiments confirm that additional division of the loss by the number of classes improves the performance of these two methods in the case of multi-label training according to the reasoning made above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C ENSEMBLES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ensembles of two models</head><p>Results of the ensembles of RGB and Flow models are depicted in <ref type="table" target="#tab_0">Table XI</ref>. Results of the ensembles of RGB and Diff models are depicted in <ref type="table" target="#tab_0">Table XII</ref>. "MML with Flow from RGB (A)" in the first row means that we use the model with RGB input that was jointly trained using Mutual Modality Learning with the model with Optical Flow input using RGB initialization for both models. Tag A means that we use the weights of this model as initialization for other models in the table.</p><p>"ML from B 2" in the first row means that we use the second model with RGB input that was jointly trained using Mutual Learning with the other (the first) model with RGB input. Both models were initialized by the weights obtained by the procedure with tag B</p><p>We color the cell on the intersection of the column and the row that are marked "CE from ImageNet" in <ref type="table" target="#tab_0">Table XI</ref> as white since it is the baseline ensemble. The more intense red color is, the higher the top-1 value for the ensemble is. The more intense light blue color is, the lower the top-1 value for the ensemble is.</p><p>The analysis of the tables is in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ensembles of three models</head><p>We evaluate the validation results for each combination of three models with different input modalities. We sort all the results of the ensembles of three models by the descending order. We show the sum of all indexes of positions for each model in <ref type="table" target="#tab_0">Table XIII</ref>. So, the smaller value stands in </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>for i in 1:N do W_new[:,i] = (W[:,1]+W[:,2]+W[:,3])/3 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Best viewed in color. Solid arrows denote flows of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>N , . . . , (N −0.5)·T N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>T 2 ,</head><label>2</label><figDesc>T 2 + τ , . . . , T 2 + τ · (N − 1) when k = 1 and frames with numbers i/m·T N , . . . , (N −1+i/m)·T N , i ∈ {0, . . . , m − 1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>number b, y i b = I gt b (i) and p i b =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ONE</head><label>I</label><figDesc>RGB from Ima-geNet 58.10 / 84.61 RGB from Flow 57.53 / 84.42 Flow from Ima-geNet 52.32 / 81.84 Flow from RGB 55.19 / 84.14 Diff from Ima-geNet 58.74 / 84.39 Diff from RGB 58.98 / 86.33</figDesc><table><row><cell></cell><cell>MODEL TRAINING</cell><cell></cell></row><row><cell>Model</cell><cell>Top-1 / Top-5 Model</cell><cell>Top-1 / Top-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MARS</head><label>II</label><figDesc>AND D3D TRAINING OF TSM</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell>Teacher modal-ity</cell><cell>MARS training Top-1 / Top-5</cell><cell>D3D training Top-1 / Top-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RGB from Ima-geNet</cell><cell>Flow</cell><cell>57.56 / 84.39</cell><cell>58.99 / 85.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RGB from RGB Flow</cell><cell cols="2">59.11 / 85.24 59.95 / 85.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Flow from Ima-geNet</cell><cell>RGB</cell><cell>57.46 / 85.01</cell><cell>55.04 / 83.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Flow from RGB RGB</cell><cell cols="2">58.23 / 85.37 56.41 / 83.98</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell></cell><cell cols="2">RGB RESULTS OF MML</cell><cell></cell><cell></cell><cell cols="2">FLOW RESULTS OF MML</cell><cell></cell></row><row><cell>RGB results</cell><cell>Flow from Ima-geNet</cell><cell cols="2">Flow from Flow Flow from RGB</cell><cell>Flow results</cell><cell>Flow from Ima-geNet</cell><cell cols="2">Flow from Flow Flow from RGB</cell></row><row><cell>RGB from Ima-geNet</cell><cell>56.25 / 84.07</cell><cell>60.02 / 86.08</cell><cell>58.70 / 85.18</cell><cell>RGB from Ima-geNet</cell><cell>54.94 / 83.82</cell><cell>57.06 / 84,87</cell><cell>57.84 / 85.15</cell></row><row><cell cols="2">RGB from RGB 60.80 / 86.47</cell><cell>60.94 / 86.67</cell><cell>60.82 / 86.75</cell><cell cols="2">RGB from RGB 54.76 / 83.61</cell><cell>56.74 / 84.78</cell><cell>57.95 / 85.44</cell></row><row><cell cols="2">RGB from Flow 58.37 / 84.82</cell><cell>58.56 / 85.28</cell><cell>58.62 / 85.36</cell><cell cols="2">RGB from Flow 55.85 / 84.33</cell><cell>56.86 / 84.80</cell><cell>57.79 / 85.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V SAME</head><label>V</label><figDesc>Flow from RGB 57.58 / 85.17 Flow from RGB2 57.71 / 85.26</figDesc><table><row><cell></cell><cell cols="2">MODALITY ML</cell><cell></cell></row><row><cell>First model</cell><cell cols="2">Top-1 / Top-5 Second model</cell><cell>Top-1 / Top-5</cell></row><row><cell>RGB from Ima-geNet</cell><cell>57.76 / 84.42</cell><cell>RGB from Ima-geNet2</cell><cell>58.15 / 84.64</cell></row><row><cell cols="2">RGB from RGB 57.84 / 84.55</cell><cell>RGB from Ima-geNet</cell><cell>60.20 / 86.33</cell></row><row><cell cols="4">RGB from RGB 60.54 / 86.23 RGB from RGB2 60.47 / 86.08</cell></row><row><cell>Flow from Ima-geNet</cell><cell>52.94 / 82.21</cell><cell>Flow from Ima-geNet2</cell><cell>53.44 / 82.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="2">I3D ON CHARADES</cell><cell></cell></row><row><cell>Training pipeline</cell><cell cols="2">RGB model mAP Flow model mAP</cell></row><row><cell>Ordinary training from Kinet-ics</cell><cell>33.72</cell><cell>15.81</cell></row><row><cell>MARS training from Kinetics</cell><cell>28.74</cell><cell></cell></row><row><cell>MARS training from RGB</cell><cell>34.40</cell><cell></cell></row><row><cell>D3D training from Kinetics</cell><cell>33.03</cell><cell></cell></row><row><cell>D3D training from RGB</cell><cell>35.48</cell><cell></cell></row><row><cell>MML training from Kinetics</cell><cell>33.84</cell><cell>17.34</cell></row><row><cell>MML training from RGB</cell><cell>35.96</cell><cell>29.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII MML</head><label>VII</label><figDesc>AND ML FROM ML</figDesc><table><row><cell></cell><cell cols="2">First model</cell><cell cols="4">Top-1 / Top-5 Second model Top-1 / Top-5 Tag</cell></row><row><cell cols="4">1 RGB from RGB 60.82 / 86.75</cell><cell>Flow RGB</cell><cell>from</cell><cell>57.95 / 85.44 A</cell></row><row><cell cols="4">2 RGB from RGB 60.88 / 86.86</cell><cell>Flow RGB2</cell><cell>from</cell><cell>57,87 / 85.53</cell></row><row><cell>3</cell><cell>RGB A(RGB)</cell><cell>from</cell><cell>61.18 / 86.81</cell><cell>Flow A(RGB)</cell><cell>from</cell><cell>58.02 / 85.49 B</cell></row><row><cell>4</cell><cell>RGB B(RGB)</cell><cell>from</cell><cell>61.15 / 86.81</cell><cell>Flow B(RGB)</cell><cell>from</cell><cell>57.96 / 85.30</cell></row><row><cell cols="4">5 RGB from RGB 60.54 / 86.23</cell><cell>RGB RGB2</cell><cell>from</cell><cell>60.47 / 86.08 C</cell></row><row><cell>6</cell><cell>RGB C(RGB)</cell><cell>from</cell><cell>60.68 / 86.35</cell><cell>RGB C(RGB2)</cell><cell>from</cell><cell>60.88 / 86.44</cell></row><row><cell>7</cell><cell>RGB C(RGB)</cell><cell>from</cell><cell>61.30 / 86.99</cell><cell>Flow C(RGB)</cell><cell>from</cell><cell>58.36 / 85.49</cell></row></table><note>model that was trained by ML tagged as A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VIII RGBDIFF</head><label>VIII</label><figDesc>MODALITY Row number First model Top-1 / Top-5 Second model Top-1 / Top-5 Third model Top-1 / Top-5 1 RGB from RGB 60.54 / 86.23 RGB from RGB2 60.47 / 86.08 2 RGB from RGB 60.82 / 86.75 Flow from RGB 57.95 / 85.44 3 Flow from RGB 57.58 / 85.17 Flow from RGB 57.71 / 85.26 4 Diff from RGB 60.66 / 87.65 Diff from RGB2 61.07 / 87.73 5 RGB from RGB 60.52 / 86.52 Diff from RGB 62.13 / 87.57 6 RGB from RGB 61.03 / 86.71 Flow from RGB 58.03 / 85.61 Diff from RGB 62.51 / 87.95 Fig. 2. Best viewed in color. Solid arrows denote flows of data. Dashed arrows denote weights transferring for initialization. Green part: first, we train two networks with RGB input initialized by ImageNet weights using cross-entropy loss. Yellow part: next, we launch RGB-only Mutual Learning for two times.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IX</head><label>IX</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">SOTA ON SOMETHING-SOMETHING-V2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Solution</cell><cell cols="2">Ensemble Base architecture</cell><cell>Number of input frames</cell><cell>Spatial crops × Temporal clips for prediction</cell><cell>Top-1 on validation</cell><cell>Top-5 on validation</cell><cell>Top-1 on test</cell><cell>Top-5 on test</cell></row><row><cell>TSM [16]</cell><cell>No</cell><cell>ResNet-50</cell><cell>8</cell><cell>1 × 1</cell><cell>59.1</cell><cell>85.6</cell><cell>−</cell><cell>−</cell></row><row><cell>TIN [20]</cell><cell>No</cell><cell>ResNet-50</cell><cell>8</cell><cell>1 × 1</cell><cell>60.0</cell><cell>85.5</cell><cell>−</cell><cell>−</cell></row><row><cell>TPN [31]</cell><cell>No</cell><cell>ResNet-50</cell><cell>8</cell><cell>1 × 1</cell><cell>62.0</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>MML (ours)</cell><cell>No</cell><cell>ResNet-50</cell><cell>8</cell><cell>1 × 1</cell><cell>61.87</cell><cell>87.32</cell><cell>−</cell><cell>−</cell></row><row><cell>STM [13]</cell><cell>No</cell><cell>ResNet-50</cell><cell>8</cell><cell>3×?</cell><cell>62.3</cell><cell>88.8</cell><cell>61.3</cell><cell>88.4</cell></row><row><cell>W3 [18]</cell><cell>No</cell><cell>ResNet-50</cell><cell>16</cell><cell>? × 2</cell><cell>66.5</cell><cell>90.4</cell><cell>−</cell><cell>−</cell></row><row><cell>STM [13]</cell><cell>No</cell><cell>ResNet-50</cell><cell>16</cell><cell>3×?</cell><cell>64.2</cell><cell>89.8</cell><cell>63.5</cell><cell>89.6</cell></row><row><cell>TPN [31]</cell><cell>No</cell><cell>ResNet-101</cell><cell>16</cell><cell>3 × 2</cell><cell>−</cell><cell>−</cell><cell>67.72</cell><cell>91.28</cell></row><row><cell>MML (ours)</cell><cell>No</cell><cell>ResNet-101</cell><cell>16</cell><cell>1 × 3</cell><cell>65.9</cell><cell>90.15</cell><cell>66.83</cell><cell>91.30</cell></row><row><cell>bLVNet-TAM RGB+Flow [6]</cell><cell>Yes</cell><cell>ResNet-101</cell><cell>32+32</cell><cell>3 × 10</cell><cell>68.5</cell><cell>91.4</cell><cell>67.1</cell><cell>91.4</cell></row><row><cell>TSM RGB+Flow [16]</cell><cell>Yes</cell><cell>ResNet-50</cell><cell>16+16</cell><cell>?×?</cell><cell>66.0</cell><cell>90.5</cell><cell>66.55</cell><cell>91.25</cell></row><row><cell>RGB-only ensemble (9702 10347) by Anonymous</cell><cell>Yes</cell><cell>−</cell><cell>−</cell><cell>?×?</cell><cell>−</cell><cell>−</cell><cell>68.18</cell><cell>91.26</cell></row><row><cell>TSM ResNet-101, RGB+Flow by Anonymous</cell><cell>Yes</cell><cell>ResNet-101</cell><cell>−</cell><cell>?×?</cell><cell>−</cell><cell>−</cell><cell>67.71</cell><cell>91.95</cell></row><row><cell>ML RGB+Flow (ours)</cell><cell>Yes</cell><cell>ResNet-101</cell><cell>16+16</cell><cell>1 × 3</cell><cell>68.16</cell><cell>91.69</cell><cell>−</cell><cell>−</cell></row><row><cell>ML RGB+Flow+Diff (ours)</cell><cell>Yes</cell><cell>ResNet-101</cell><cell cols="2">16+16+16 1 × 3</cell><cell>69.07</cell><cell>92.07</cell><cell>69.02</cell><cell>92.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X TESTING</head><label>X</label><figDesc>WITH DIFFERENT SAMPLING STRATEGIES Dense sampling 57.33 / 84.51 58.79 / 85.68 57.61 / 84.98 58.70 / 85.66 59.71 / 86.57 60.07 / 86.47 60.11 / 86.56 60.27 / 86.61</figDesc><table><row><cell>Sampling dur-</cell><cell>Dense 0 + Uni-</cell><cell>Dense 0 + Uni-</cell><cell>Dense 1 + Uni-</cell><cell>Dense 2 + Uni-</cell><cell>Dense 1 + Uni-</cell><cell>Dense 2 + Uni-</cell><cell>Dense 1 + Uni-</cell><cell>Dense 2 + Uni-</cell></row><row><cell>ing training</cell><cell>form 1</cell><cell>form 2</cell><cell>form 0</cell><cell>form 0</cell><cell>form 1</cell><cell>form 1</cell><cell>form 2</cell><cell>form 2</cell></row><row><cell>Uniform</cell><cell>59.86 / 86.14</cell><cell>61.16 / 87.03</cell><cell>56.25 / 83.72</cell><cell>56.20 / 84.18</cell><cell>60.64 / 85.58</cell><cell>59.69 / 86.38</cell><cell>61.50 / 87.32</cell><cell>61.03 / 87.10</cell></row><row><cell>sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Both samplings 60.11 / 85.79</cell><cell>61.38 / 86.82</cell><cell cols="2">57.80 / 85.05 58.66 / 85.32</cell><cell cols="4">61.10 / 86.66 61.01 / 86.57 61.71 / 87.40 61.59 / 86.97</cell></row><row><cell></cell><cell cols="2">APPENDIX A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SAMPLING STRATEGY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">The common procedure for the Something-Something-v2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">final testing is an averaging of two predictions for each</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">video. For each prediction, we use central full-resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">crop and uniform sampling: we use frames with numbers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table X :</head><label>X</label><figDesc>• Dense sampling training is not suitable for the Something-Something-v2. Uniform sampling training and Both samplings training are nearly equal if we use prediction for Uniform sampling.</figDesc><table /><note>•• Both samplings training outperforms Uniform sampling strategy by up to one percent when tested with both strategies.• It is better to average predictions for two Uniform sam- plings and one Dense sampling during testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XI ENSEMBLE</head><label>XI</label><figDesc>OF RGB AND FLOW MODELS</figDesc><table><row><cell>RGB</cell><cell>CE from</cell></row><row><cell>Flow</cell><cell>ImageNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XII ENSEMBLE</head><label>XII</label><figDesc>OF RGB AND DIFF MODELS</figDesc><table><row><cell>RGB</cell><cell>CE from</cell></row><row><cell>Diff</cell><cell>ImageNet</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the table the better model is in ensemble with two other modalities. Note that the magnitude of sums vary across the input modalities since there are different numbers of models for each modality are tested.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mars: Motion-augmented rgb stream for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tschannen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<title level="m">Laurent Itti, and Anima Anandkumar. Born again neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video to events: Recycling video datasets for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hidalgo-Carrió</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3586" to="3595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a textvideo embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Knowing what, where and when to look: Efficient video action modeling with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01278</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9945" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06499</idno>
		<title level="m">Temporal interlacing network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">What makes training multimodal networks hard?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03548</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
