<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Version Space Reduction for Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Chiotellis</surname></persName>
							<email>chiotell@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Triebel</surname></persName>
							<email>triebel@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
							<email>cremers@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Version Space Reduction for Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>active learning · deep learning · version space · diameter reduction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In active learning, sampling bias could pose a serious inconsistency problem and hinder the algorithm from finding the optimal hypothesis. However, many methods for neural networks are hypothesis space agnostic and do not address this problem. We examine active learning with convolutional neural networks through the principled lens of version space reduction. We identify the connection between two approaches-prior mass reduction and diameter reduction-and propose a new diameter-based querying method-the minimum Gibbs-vote disagreement. By estimating version space diameter and bias, we illustrate how version space of neural networks evolves and examine the realizability assumption. With experiments on MNIST, Fashion-MNIST, SVHN and STL-10 datasets, we demonstrate that diameter reduction methods reduce the version space more effectively and perform better than prior mass reduction and other baselines, and that the Gibbs vote disagreement is on par with the best query method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Active learning is a supervised learning framework in which the learner is given access to a pool or stream of unlabeled samples and is allowed to selectively query labels from an oracle (e.g., a human annotator). In each query round, the learner queries the labels of some unlabeled samples and trains on the augmented labeled set to obtain new classifiers. The goal is to learn a good classifier or hypothesis using as few labels as possible. This setting is relevant in many real-world problems, where labeled data are scarce or expensive to obtain, but unlabeled data are cheap and abundant.</p><p>Many active learning methods for neural networks rely on measures of the "informativeness" of a query, in the form of classifier uncertainty, margin <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref> or information gain <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>. Other methods capture the informativeness by representativeness of the query set using geometry-based <ref type="bibr" target="#b26">[27]</ref> or discriminative <ref type="bibr" target="#b13">[14]</ref> methods. However, most of these methods ignore the notion of the hypothesis space and do not address the problem of sampling bias <ref type="bibr" target="#b8">[9]</ref>, which plague many <ref type="figure">Fig. 1</ref>: Version space reduction for binary classification. Upon observing the label of x, the current version space V is split into subspaces V 0</p><formula xml:id="formula_0">(x i , y i = 1) V 0 xi V 1 xi (x j , y j = 0) V 0 xj V 1 xj (x k , y k = 0) V 0 xk V 1 xk</formula><p>x and V 1 x , one of which will be removed and the other remains. Left: Prior mass reduction methods remove approximately half of the mass. Middle: Diameter reduction methods, like pairwise disagreement, query a sample that lead to sub-spaces of small diameter. Right: Proposed method, the Gibbs-vote disagreement, measures diameter by the expected distance between random hypotheses and their majority vote. active learning methods. Without carefully handling this problem, an active learning algorithm is not guaranteed to be consistent, i.e., capable of finding the optimal classifier in the hypothesis space.</p><p>We consider the hypothesis space of convolutional neural networks (Con-vNets) and study version space reduction methods. Version space reduction works by removing hypotheses that are inconsistent with the observed labels from a predefined hypothesis space and maintaining the consistent sub-space, the version space. A key condition called the realizability assumption is that the hypothesis space contains the classifier that provides the ground truth-if not, there are no guarantees that the best hypothesis will not be removed, because a hypothesis might make mistakes on the queried samples but perform well on the data distribution.</p><p>For neural networks, the realizability assumption may not hold for all cases. For instance, no neural networks can achieve arbitrarily small test error on some classification datasets. A workaround is to consider the effective labelings on a set of i.i.d. pool samples. To avoid the problem of an unreasonably large effective hypothesis space, as implied by the result of <ref type="bibr" target="#b32">[33]</ref>, we only consider the labelings achievable by training on unaltered samples and correct labels. We examine experimentally whether the realizability holds with this restriction and analyze its implications on version space reduction methods.</p><p>Prior mass reduction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref> and diameter reduction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref> are two widely used version space reduction approaches. See <ref type="figure">Fig. 1</ref> for illustration. However, prior mass reduction is not an appropriate objective for active learning <ref type="bibr" target="#b29">[30]</ref> since any intermediate version spaces containing more than one hypothesis may still have a large diameter, i.e., large error rate in the worst-case scenario, despite having substantially reduced mass. We derive connections between prior mass and diameter reduction and introduce a new interpretation of diameter reduction as prior mass "reducibility reduction".</p><p>We propose a new diameter measure called the Gibbs-vote disagreement, which equals the expected distance between the random hypotheses and their majority vote classifier. We show its relation to a common diameter measure, the pairwise disagreement, and discuss under which situations the former may be advantageous. We show experimentally on four image classification datasets that diameter reduction methods perform better than all baselines and that prior mass reduction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref> and other baselines like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref> do not perform consistently better than random query and sometimes fail completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A lot of research has been conducted to study the label complexity for active learning and optimality guarantees for greedy version space reduction. Hanneke <ref type="bibr" target="#b15">[16]</ref> and Balcan et al. <ref type="bibr" target="#b0">[1]</ref> prove upper-bounds on the label complexity in the realizable and non-realizable cases, using a parameter called the disagreement coefficient. Tosh and Dasgupta <ref type="bibr" target="#b29">[30]</ref> propose a diameter-based active learning algorithm and characterize its sample complexity using a parameter called the splitting index. Dasgupta <ref type="bibr" target="#b6">[7]</ref> shows that a greedy strategy maximizing the worstcase prior mass reduction is approximately as good as the optimal strategy. Golovin and Krause <ref type="bibr" target="#b14">[15]</ref> show that the prior mass reduction utility function is adaptive submodular and a greedy algorithm is guaranteed to obtain nearoptimal solutions in the average-case scenario. Cuong et al. <ref type="bibr" target="#b4">[5]</ref> prove a worst-case optimality guarantee for pointwise submodular functions.</p><p>A variety of methods relying on the informativeness of a query have been proposed for neural networks. Gal et al. <ref type="bibr" target="#b12">[13]</ref> use the Monte Carlo dropout to approximate the mutual information between predictions and model posterior <ref type="bibr" target="#b17">[18]</ref> in a Bayesian setting. Kirsch et al. <ref type="bibr" target="#b20">[21]</ref> extend <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref> to a batch query method. Ducoffe and Precioso <ref type="bibr" target="#b10">[11]</ref> use adversarial attacks to generate samples close to the decision boundaries. Sener and Savarese <ref type="bibr" target="#b26">[27]</ref> adopt a core-set approach to select representative samples for query. Gissin and Shalev-Shwartz <ref type="bibr" target="#b13">[14]</ref> use a discriminative method to select samples such that the labeled and the unlabeled set are indistinguishable. Pinsler et al. <ref type="bibr" target="#b25">[26]</ref> formulate batch query as a sparse approximation to the expected complete data posterior of model parameters in a Bayesian setting. Beluch et al. <ref type="bibr" target="#b2">[3]</ref> show that ensemble methods consistently outperform geometry-based methods <ref type="bibr" target="#b26">[27]</ref> and the Monte Carlo dropout method <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Let X be the input feature space and Y the label space. Let H be a hypothesis space of functions h : X → Y and assume a prior π over H. A hypothesis randomly drawn from the prior is called a Gibbs classifier.</p><formula xml:id="formula_1">Denote S = {(x i , y i )} n i=1</formula><p>a pool of i.i.d. samples from the data distribution P XY and Q ⊆ S the set of queried labeled samples. Define the version space V corresponding to Q as</p><formula xml:id="formula_2">V := {h ∈ H : h(x) = y, ∀(x, y) ∈ Q} .<label>(1)</label></formula><p>Denote the subset of V that is consistent with x being labeled as y as</p><formula xml:id="formula_3">V y x := {h ∈ H : h(x) = y, h ∈ V } .<label>(2)</label></formula><p>The disagreement probability induced by the marginal distribution P X is defined as</p><formula xml:id="formula_4">d(h, h ) := Pr x (h(x) = h (x))<label>(3)</label></formula><p>which is a pseudo-metric on the hypothesis space. The disagreement and agreement region are defined as</p><formula xml:id="formula_5">DIS(V ) := {x ∈ X : ∃h, h ∈ V, h(x) = h (x)} ,<label>(4)</label></formula><formula xml:id="formula_6">AGR(V ) := X \ DIS(V ).<label>(5)</label></formula><p>4 Prior Mass Reduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gibbs Error</head><p>The Gibbs error <ref type="bibr" target="#b5">[6]</ref> of an unlabeled sample x is the average-case relative prior mass reduction:</p><formula xml:id="formula_7">GE(x|V ) := E y 1 − Pr h∼π| V (h(x) = y) = E y [1 − π| V (V y x )] ,<label>(6)</label></formula><p>where π| V (h) = π(h)/π(V ) is the conditional distribution of H restricted to V . Gibbs error measures the proportion of inconsistent hypotheses taking expectation over all possible labelings of x, achievable by hypotheses in the version space. A greedy strategy that considers maximizing the average-case absolute prior mass reduction in each query can equivalently select the unlabeled sample that maximizes the Gibbs error arg max</p><p>x GE(x|V ).</p><p>Define the prior mass reduction utility function as</p><formula xml:id="formula_9">f (Q) := 1 − Pr ({h ∈ H : h(x) = y, ∀(x, y) ∈ Q}) = 1 − π(V ).<label>(8)</label></formula><p>The optimization problem in <ref type="bibr" target="#b6">(7)</ref> can be written, up to a scaling factor, as arg max</p><formula xml:id="formula_10">x π(V )GE(x|V ) = arg max x E y [f (Q ∪ {(x, y)}) − f (Q)]<label>(9)</label></formula><p>= arg max</p><formula xml:id="formula_11">x ∆ avg (x|Q),<label>(10)</label></formula><p>where the notation ∆ avg (x|Q) denotes the expected marginal gain of x in terms of prior mass reduction given the labeled samples in Q.</p><p>A closely related objective for active learning is the label entropy given x. It can be shown that the Gibbs error lower bounds the entropy. However, a greedy strategy that maximizes the entropy is not guaranteed to be near-optimal in the adaptive case <ref type="bibr" target="#b4">[5]</ref>. Furthermore, empirically this criterion performs similarly or worse than the maximum Gibbs error. For the sake of simplicity, we do not consider this method in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Variation Ratio</head><p>The variation ratio of an unlabeled sample x is the worst-case relative prior mass reduction upon the reveal of its label:</p><formula xml:id="formula_12">VR(x|V ) := min y 1 − Pr h∼π| V (h(x) = y) = min y [1 − π| V (V y x )] .<label>(11)</label></formula><p>It measures the proportion of inconsistent hypotheses considering the worst-case labeling of x and is a lower bound on the Gibbs error. A greedy strategy that considers maximizing the worst-case absolute prior mass reduction in each query selects the unlabeled sample that maximizes the variation ratio arg max</p><formula xml:id="formula_13">x VR(x|V ),<label>(12)</label></formula><p>which can be expressed in terms of the prior mass reduction utility function, up to a scaling factor, as arg max</p><formula xml:id="formula_14">x π(V )VR(x|V ) = arg max x min y [f (Q ∪ {(x, y)}) − f (Q)]<label>(13)</label></formula><p>= arg max</p><formula xml:id="formula_15">x ∆ wc (x|Q) ,<label>(14)</label></formula><p>where the notation ∆ wc (x|Q) denotes the worst-case marginal gain of x in terms of prior mass reduction given the labeled samples in Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Diameter Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Worst-Case Pairwise Disagreement</head><p>The size of the version space can be measured by the expected pairwise disagreement between hypotheses drawn from the conditional distribution:</p><formula xml:id="formula_16">PWD(V ) := E h,h ∼π| V [d(h, h )] .<label>(15)</label></formula><p>It is the average diameter of the version space. A greedy strategy selects the unlabeled sample that minimizes the worst-case pairwise disagreement arg min</p><formula xml:id="formula_17">x max y PWD(V y x ) = arg min x max y E h,h ∼π| V y x [d(h, h )] .<label>(16)</label></formula><p>Other measures of diameter based on the supremum distance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8]</ref> are not amenable to implementation because evaluation of such diameters involves optimization. The pairwise disagreement can be estimated from a finite set of sample hypotheses from the version space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Worst-Case Gibbs-Vote Disagreement</head><p>We propose to use a new diameter measure called the Gibbs-vote disagreement.</p><p>It is the expected disagreement between random hypotheses and their majority vote:</p><formula xml:id="formula_18">GVD(V ) := E h∼π| V [d(h, h vote | V )] ,<label>(17)</label></formula><p>where h vote | V is the majority vote classifier of hypotheses from V . For each x, it induces a prediction</p><formula xml:id="formula_19">h vote | V (x) = arg max y E h∼π| V [p(y|x; h)] ,<label>(18)</label></formula><p>where p(y|x; h) is the predicted probability of x belonging to class y given by a hypothesis h. The majority vote classifier is the deterministic classifier that has the smallest expected distance to the Gibbs classifier <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>:</p><formula xml:id="formula_20">E h [d(h , h vote )] = min h E h [d(h , h)] .<label>(19)</label></formula><p>Hence the Gibbs-vote disagreement measures the size of the version space by the expected distance of the random hypotheses to their "center". Further, the following relation holds</p><formula xml:id="formula_21">1 2 PWD(V ) ≤ GVD(V ) ≤ PWD(V )<label>(20)</label></formula><p>We defer the proof to the appendix. Essentially, Equation <ref type="bibr" target="#b19">(20)</ref> reveals that the Gibbs-vote disagreement is sandwiched between the average radius and diameter. A greedy strategy selects the unlabeled sample that minimizes the worst-case Gibbs-vote disagreement arg min</p><formula xml:id="formula_22">x max y GVD(V y x ) = arg min x max y E h∼π| V y x d(h, h vote | V y x ) ,<label>(21)</label></formula><p>where h vote | V y x is the majority vote of hypotheses from the subspace V y x of the current version space if x is labeled y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Diameter Reduction as Reducibility Reduction</head><p>Pairwise disagreement shares a simple relation with Gibbs error-it is the expected Gibbs error:</p><formula xml:id="formula_23">PWD(V ) = E h,h ∼π| V [ E x [1(h(x) = h (x))] ]<label>(22)</label></formula><formula xml:id="formula_24">= E x E h∼π| V Pr h ∼π| V (h(x) = h (x))<label>(23)</label></formula><formula xml:id="formula_25">= E x [ GE(x|V )] .<label>(24)</label></formula><p>A similar relation holds between Gibbs-vote disagreement and the variation ratio:</p><formula xml:id="formula_26">GVD(V ) = E h∼π| V [ E x [1(h(x) = h vote | V (x))] ]<label>(25)</label></formula><formula xml:id="formula_27">= E x E h∼π| V [1(h(x) = h vote | V (x))]<label>(26)</label></formula><formula xml:id="formula_28">= E x [ VR(x|V )] ,<label>(27)</label></formula><p>where the last equality holds because the predictions of the majority vote classifier are always the worst-case labels for prior mass reduction. Diameter reduction selects samples such that, upon revealing their labels, the induced subspaces have minimum possibility to be further reduced by a potential random query. Thus, it can be thought of as reducing the expected prior mass "reducibility". Prior mass reduction finds splits in directions that evenly partition the version space, but could result in version spaces that have irregular shapes, in the sense that the space can be whittled down finely in some directions while being undersplit in others. The worst-case error rate of the resulted version space could still be large. Diameter reduction correctly resolve this issue. <ref type="figure">Fig. 1</ref> illustrates the differences between prior mass and diameter reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Weighted Diameter Reduction</head><p>Tosh and Dasgupta <ref type="bibr" target="#b29">[30]</ref> show that in general average diameter cannot be decreased at steady rate and propose to query the unlabeled samples that minimize the diameter weighted by the squared prior mass in the worst-case scenario arg min</p><formula xml:id="formula_29">x max y E h,h ∼π [1(h, h ∈ V y x ) d(h, h )]<label>(28)</label></formula><p>= arg min</p><formula xml:id="formula_30">x max y π(V y x ) 2 E h,h ∼π| V y x [d(h, h )] .<label>(29)</label></formula><p>The potential to be minimized is a surrogate for the "amount" of edges between hypotheses and is closely related to the splittablity of version space <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Realizability Assumption</head><p>Even though neural networks are capable of fitting an arbitrary pool set, we show experimentally that the version space obtained by training on a subset of the pool set with stochastic gradient descent-the "samplable" version space-is biased and not likely to contain the correct labeling of the pool set. Indeed, the distance from the Bayes classifier, which provides the ground truth labeling, to the "boundary" of the version space is non-negligible. Let h * ⊥ be the projection of the Bayes classifier h * to the set of hypotheses V that agree with V on AGR(V ) (see the left plot of <ref type="figure" target="#fig_0">Fig. 2</ref></p><formula xml:id="formula_31">), i.e., h * ⊥ := arg min h∈Ṽ d(h, h * ),<label>(30)</label></formula><formula xml:id="formula_32">V := {h : h(AGR(V)) = h (AGR(V )), h ∈ V } .<label>(31)</label></formula><p>It is easy to see that h * ⊥ provides the ground truth on DIS(V ) and predicts the same labels on AGR(V ) as hypotheses in V do, hence where d(h, h * ; AGR(V)) is the disagreement probability restricted to AGR(V), or equivalently the wrong agreement of hypotheses in V .</p><formula xml:id="formula_33">d(h * ⊥ , h * ) = d(h, h * ; AGR(V)) = E x [1(x ∈ AGV(V ))1(h(x) = h * (x))] , ∀h ∈ V. (32) h * h * ⊥ h Ṽ V d(h, h * ; AGR(V )) d(h, h * ; DIS(V ))</formula><p>We show the evolution of wrong agreement in the right plot of <ref type="figure" target="#fig_0">Fig. 2</ref>. As more random samples are queried, the wrong agreement decreases for all datasets, but for some much slower than the others. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we show for MNIST a 2-D embedding of version spaces using Multi-Dimensional Scaling (MDS) <ref type="bibr" target="#b21">[22]</ref>, which finds a low-dimensional representation of potentially high-dimensional data by preserving pairwise distances between the data points. The Bayes classifier is not contained in any of the samplable version spaces although the distances between them decrease steadily.</p><p>In general neural networks trained with a random subset do not automatically predict all labels in the pool set correctly, unless a relatively large proportion of samples are used for training. However, this fact does not render version space reduction inconsistent, because the samplable version space is not fixed, but it shifts towards the correct labeling and finally covers it when the whole pool set has been used.</p><p>We conjecture that the dynamics of active learning with neural networks have two major components: (1) shrinkage of the samplable version space, which is explicitly optimized by the learning algorithm and (2) reduction of bias, which is not directly controllable. Empirical evidence is provided in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Datasets and Architectures We conduct active learning experiments 3 on four image classification datasets: MNIST, Fashion-MNIST, SVHN and STL-10. Neural network architectures are chosen to be competent for each dataset but as simple as possible in the hope of controlling the model complexity and mitigating the effect of overfitting. See <ref type="table" target="#tab_0">Table 1</ref> for the complete experiment settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Active Learning Methods</head><p>We compare nine querying methods: Random, variation ratio (VR), Gibbs error (GE), Bayesian Active Learning by Disagreement with Monte Carlo dropout (BALD-MCD) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref>, Core-Set <ref type="bibr" target="#b26">[27]</ref>, Deep-Fool Active Learning (DFAL) <ref type="bibr" target="#b10">[11]</ref>, pairwise disagreement (PWD), Gibbs-vote disagreement (GVD), and double-weighted pairwise disagreement (M 2 -PWD) <ref type="bibr" target="#b29">[30]</ref>. For each method on each dataset, at least three runs of active learning with different random balanced initial training set are performed.</p><p>Ensemble Size We train networks multiple times from scratch to obtain sample hypotheses and use them for prior mass and diameter estimation. Since diame-  <ref type="figure">Fig. 4</ref>: Accuracy over number of queried labels on the test set. Direct diameter reduction methods PWD and GVD are consistently better than Random and are among the best methods. Weighted diameter reduction M 2 -PWD is on par with Random. Other baselines are effective on some datasets but inferior to Random on the others. Note that PWD, GVD and M 2 -PWD exhibit smaller variances than the others.</p><p>ters are estimated by considering partitioned version spaces, the ensemble size should be at least in the order of number of classes. We set the size to 20. Larger ensemble improves estimation but at the cost of longer training time.</p><p>In preliminary experiments, we tried larger ensembles (40) and did not observe significant differences. Hence we do not include experiments on changing this hyper-parameter in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Size</head><p>We set a small query budget for each round to reduce the correlation between queries. Larger budget may alleviate the pressure of frequent retraining, but the effect of each query can not be estimated and examined reliably. We observed in preliminary experiments that using larger budget (one or two orders larger) hides the differences between methods. <ref type="figure">Fig. 4</ref> and <ref type="table" target="#tab_1">Table 2</ref> show that direct diameter reduction methods PWD and GVD are consistently better than Random and achieve higher accuracy than other baselines while weighted diameter reduction M2-PWD is on par with Random. Diameter reduction methods usually exhibit less variances because training on samples queried by PWD, GVD and M2-PWD yields version spaces with smaller diameters and less diverse sample hypotheses. Prior mass reduction is not always effective and even fails on SVHN. This failure is an example of prior mass reduction being incapable of reducing the diameter, and provides empirical evidence that it may not be an appropriate objective for active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Diameter Reduction is More Effective Than Prior Mass Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Comparison to Other Baselines</head><p>BALD-MCD, Core-Set and DFAL are not consistently better than Random although each of them achieves comparative test accuracy on certain dataset. Their   <ref type="figure" target="#fig_2">Fig. 5</ref> and <ref type="table" target="#tab_2">Table 3</ref>). BALD-MCD and DFAL are highly related to prior mass reduction methods in that BALD <ref type="bibr" target="#b17">[18]</ref> seeks samples for which the model parameters under the posterior disagree the most about the prediction <ref type="bibr" target="#b17">[18]</ref>, and that DFAL, inspired by margin-based active learning <ref type="bibr" target="#b1">[2]</ref>, tries to locate the decision boundary with fewer labels which is essentially removing inconsistent hypotheses in the realizable case. However, none of them explicitly minimize the diameter, neither does Core-Set. Note that for a fair comparison, we do not augment the training set by also adding the adversarial samples as the original DFAL paper <ref type="bibr" target="#b10">[11]</ref> does. Samples with minimum adversarial perturbation are then verified reliably to be less effective than those lead to minimum diameter. The original Core-Set paper <ref type="bibr" target="#b26">[27]</ref> uses a large query batch size (in the order of 1000). However, many baselines rely on greedy selection and do not perform any batch optimization. To reduce query correlation, we adopt as small batch size as possible. This allows reliable evaluation of the effectiveness of queried samples as in the online setting. We are therefore able to identify one major cause of inferiority to Random as failing to effectively reduce the version space diameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evolution of Samplable Version Space and its Implications</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 5 and 3</ref>, the samplable version space shifts closer to the correct labeling while reducing its diameter as more labels are queried. These two processes together result in smaller test error.</p><p>No Direct Control Over Reduction of Version Space Bias Interestingly, the Core-Set method, which queries representative samples from the pool set by solving a k-center problem in the feature space learned by neural networks, is incapable of achieving negligible wrong agreement on the learned version spaces. Indeed, it suffers larger version space bias than the direct diameter reduction methods. After all, random queries which are i.i.d. by assumption fail to achieve this goal as concluded in Section 6 and other attempts without augmenting the training data seem doomed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Mass Induced by Stochastic Gradient Descent May Not Be a</head><p>Reliable Surrogate Measure The continued decline in wrong agreement indicates that the distribution over labelings changes over time. This fact of shifting density over samplable labelings renders the notion of prior mass problematic, hence all notions relying on prior mass may not be well-defined. A direct consequence is that an estimate of the worst-case version space reduction would be more reliable than the average-case one. For example, VR provides a more reliable estimate of version space reduction than GE does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inferiority of Weighted Diameter Reduction Method</head><p>The estimation of weighted diameter involves estimating the prior mass. Hence, the inferiority of M 2 -PWD to PWD and GVD can be attributed to the intrinsic difficulty of obtaining unbiased samplable version spaces and the resulted density shift. A supportive evidence can be seen by noting that on MNIST and Fashion-MNIST, where the wrong agreement is large (hence large density shift), the weighted variant performs worse, while on SVHN and STL-10, where the wrong agreement is small (hence small shift), the gap is less significant. <ref type="figure">Fig. 6</ref>: Distance from the Gibbs and the majority vote classifier to the projection of h * . On four datasets, the majority vote classifier has a smaller distance, hence smaller error rate. See description of <ref type="figure" target="#fig_0">Fig. 2</ref> for total numbers of random samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Gibbs-Vote Disagreement</head><p>The Gibbs-vote disagreement is among the best methods on all datasets, except for the early learning stage on SVHN. Its effectiveness can be ascribed to an interesting phenomenon-majority voting reduces mistakes. Although it need not necessarily be the case, this phenomenon occurs in many situations and the boost to accuracy depends on the variance of errors of Gibbs classifiers <ref type="bibr" target="#b22">[23]</ref>. We show empirically that the majority vote classifier indeed has smaller error rate than random hypotheses in the version space in <ref type="figure">Fig. 6</ref>. Hence, optimizing the Gibbs-vote disagreement not only reduces the diameter but also implicitly moves the consistent hypotheses closer to the correct labeling, which is useful when the samplable version spaces are biased and do not contain the Bayes classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we studied version space reduction for convolutional neural networks. We revealed the differences and connections between prior mass and diameter reduction methods and proposed the Gibbs-vote disagreement as a new effective diameter-reduction method. With experiments on four datasets, we shed light into how version space reduction works in the deep active learning setting and demonstrated the superiority of diameter reduction over prior mass reduction methods and other baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Estimators and Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Effective Hypothesis Space</head><p>Let X S be the set of unlabeled samples in the pool set S and X Q the set of queried unlabeled samples. The effective hypothesis spaceĤ is the restriction of H to X S , or equivalently all possible labelings of X S :</p><formula xml:id="formula_34">H := H| X S = {((h(x 1 ), h(x 2 ), · · · , h(x n )) : ∀h ∈ H, x i ∈ X S , 1 ≤ i ≤ n)} (33)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Estimators of Diameters</head><p>LetV be a set of M sample hypotheses from π| V</p><formula xml:id="formula_35">V := {h m : h m i.i.d. ∼ π| V , 1 ≤ m ≤ M }<label>(34)</label></formula><p>Assuming M = |V | ≥ 2, an unbiased estimator of the pairwise disagreement can be constructed by computing average pairwise distances between hypotheses in V</p><formula xml:id="formula_36">1 |V |(1 − |V |) h∈V h ∈V , h =h d(h, h )<label>(35)</label></formula><p>Similarly, an unbiased estimator of the Gibbs vote disagreement can be constructed by computing average distances between hypotheses inV and the empirical majority vote h vote |V</p><formula xml:id="formula_37">1 |V | h∈V d(h, h vote |V )<label>(36)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Algorithm</head><p>A (batch mode) greedy algorithm that selects the unlabeled samples which induce the minimum version space in terms of a given diameter measure in the worst-case scenario is shown in Algorithm 1. Other active learning methods (e.g., prior mass reduction methods) can also be described using this algorithm with line 11 in the algorithm replaced by the corresponding objective functions. Note that in line 2 and 15 the version space is maintained explicitly while in practice we only need to sample from the new version space by training neural networks on the updated set of queried samples. Hence, the version space is always implicitly maintained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Worst-Case Diameter Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Equation 20</head><p>Proof.</p><formula xml:id="formula_38">GVD(V ) = E h ∼π| V d(h , h vote| V ) = 1 2 min h E h ,h ∼π| V [d(h , h) + d(h , h)] ≥ 1 2 E h,h ∼π| V [d(h, h )] = 1 2 PWD(V ).</formula><p>where the last inequality is by triangular inequality. By definition, it holds that VR(x|V ) ≤ GE(x|V ). Using the relations derived in Section 5.3</p><formula xml:id="formula_39">PWD(V ) = E x [ GE(x|V )] , GVD(V ) = E x [ VR(x|V )] ,</formula><p>it holds that</p><formula xml:id="formula_40">GVD(V ) ≤ PWD(V ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Singly-Weighted Diameter Reduction</head><p>Besides the doubly-weighted diameter reduction method mentioned in Section 5.4, there are singly-weighted variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Pairwise Disagreement</head><p>arg min</p><formula xml:id="formula_41">x max y π(V y x ) E h,h ∼π| V y x [d(h, h )] ,<label>(37)</label></formula><p>which is equivalent to minimizing the potential expected marginal gain on the prior mass reduction utility function arg min</p><formula xml:id="formula_42">x max y E x [π(V y x ) GE(x|V y x )] = arg min x max y E x [∆ avg (x|Q ∪ {(x, y)})] .<label>(38)</label></formula><p>Weighted Gibbs-Vote Disagreement arg min</p><formula xml:id="formula_43">x max y π(V y x ) E h∼π| V y x d(h, h vote | V y x ) ,<label>(39)</label></formula><p>which is equivalent to minimizing the potential worst-case marginal gain arg min</p><formula xml:id="formula_44">x max y E x [π(V y x ) VR(x|V y x )] = arg min x max y E x [∆ wc (x|Q ∪ {(x, y)})] .</formula><p>(40)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Evaluation on the Pool Set</head><p>In addition to the evaluation results on the test set shown in the paper, we show the results on the pool set as well in <ref type="figure" target="#fig_3">Figure 7</ref>. On MNIST, Fashion-MNIST and SVHN, diameter reduction are more effective than prior mass reduction methods and other baselines in finding the true labeling on the pool set using as few labels as possible. On STL-10, prior mass reduction performs better at reducing the diameter. An explanation is that we use unlabeled samples in the validation set to estimate relative prior mass and diameters when selecting each query. So the diameter reduction methods are not explicitly optimizing the diameter measured on the pool set, but rather on an unbiased validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Embedding of Version Spaces</head><p>To better illustrate the evolution of version spaces and the existence of bias in version spaces, we show a 2-D embedding of sample hypotheses during the active learning process for each dataset using Multi-Dimensional Scaling (MDS) <ref type="bibr" target="#b21">[22]</ref> in <ref type="figure" target="#fig_4">Figure 8</ref>. MDS finds a low-dimensional representation of potentially highdimensional data by preserving pairwise distances between the data points. We show sample hypotheses from the first (purple) and the last (red) version spaces as well as intermediate version spaces obtained by training on randomly queried labels (approximately) amount to 25%, 50% and 75% of the total budget. To achieve better visualization, we first compute the embedding of the five Gibbs (random) classifiers and the Bayes classifier and then compute the embedding of each version space separately and center them at the corresponding Gibbs classifier. We use the disagreement probability evaluated on the test set as the distance metric.</p><p>As more labels are queried, the version spaces move closer to the Bayes classifier while reducing their diameters. The bias in the version spaces is nonnegligible for the four datasets. An active learning algorithm contributes to the shrinkage of the samplable version space but does not have direct control over the reduction of bias. How to efficiently reduce the bias remains an open problem for designing active learning algorithms for neural networks. The 2-D embedding of version spaces obtained by training on random samples is calculated through MDS for each dataset. The purple dots in the largest clusters illustrate a set of sample hypotheses from the version space at the beginning of the active learning experiments, while the red dots in the smallest clusters illustrate hypotheses at the end, being closer to the Bayes classifier (star marker) than those from other version spaces. The blue, green and orange dots represent version spaces obtained by training with (approximately) 25%, 50% and 75% labels of the total budget, respectively. The Gibbs classifier (triangle marker) corresponding to each version space is a random classifier that predicts by randomly sampling a hypothesis from the version space and making the same prediction as the sample hypothesis does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Datasets Selection</head><p>The four image classification datasets MNIST, Fashion-MNIST, SVHN and STL-10 are chosen based on several considerations: (1) relatively balanced label distribution; (2) there exist neural network models that can train fast on the original training set of the datasets; (3) no data augmentations are needed. Since active learning methods query highly biased samples, a balanced label distribution help mitigate the problem of query label imbalance. The second point helps reduce the time needed to run active learning experiments. The last point guarantees that the samples used for training are exactly those that have been queried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Implementation Details</head><p>For the four datasets we consider, no data augmentation is used. Unless otherwise stated, the neural network models are trained using SGD with initial learning rate 1e−2 and momentum 0.8. The learning rate decays by a factor of 0.1 when there are no improvements on the validation accuracy for any consecutive 10 training epochs until it is smaller than 1e−4. The maximum training epochs are 200. The batch size for training is set to 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 MNIST [24]</head><p>We select a random balanced set of 50000 samples from the original 60000 training samples as the training/validation set and use the original 10000 test samples as the test set. The 2-conv-layer ConvNet is trained using RMSProp <ref type="bibr" target="#b28">[29]</ref> with learning rate 1e−4. The learning rate decays by a factor of 0.5 when there are no improvements on the validation accuracy for any consecutive 10 training epochs until it is smaller than 1e−5. Dropout <ref type="bibr" target="#b27">[28]</ref> of rate 0.5 is applied to the output of the fully-connected layer which lies between the last convolution layer and the output layer. The batch size for training is set to 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Fashion-MNIST [31]</head><p>We used the original balanced 60000 training and 10000 test samples as the training/validation and test sets. A 3-layer-conv ConvNet is used as the classifier model. Dropout of rate 0.5 is applied to the output of the fully-connected layer which lies between the last convolution layer and the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 SVHN [25]</head><p>We select a random balanced set of 45000 samples from the original 73257 training samples as the training/validation set and a random balanced 15000 samples from the original 26032 test samples as the test set. A 6-layer-conv ConvNet is used as the classifier model. Dropouts of rate 0.3 are applied to the output of every two convolution layers and that of the fully-connected layer which lies between the last convolution layer and the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 STL-10 [4]</head><p>We used the original balanced 5000 training and 8000 test samples as the training/validation and test sets. ResNet18 <ref type="bibr" target="#b16">[17]</ref> is used as the classifier model. Dropout of rate 0.5 is applied between all convolution layers in each convolutional block <ref type="bibr" target="#b31">[32]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Left: Projection of h * to the samplable version space. Right: Wrong agreement of version spaces trained on random samples. Total numbers of samples are 400, 1000, 3000 and 2580 for MNIST, Fashion-MNIST, SVHN and STL-10 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Embedding of version spaces on MNIST using MDS. As more random samples are used for training, the samplable version spaces move closer to the Bayes classifier but hardly cover it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Pairwise disagreement and wrong agreement over number of queried labels on the test set. Except direct diameter reduction methods PWD and GVD, other baselines are not consistently better than or on par with Random at reducing version space diameter. Performing worse than Random: GE, VR and BALD-MCD on datasets except MNIST, Core-Set on Fashion-MNIST and SVHN, and DFAL on MNIST and SVHN, and M 2 -PWD on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Error rate, pairwise disagreement and wrong agreement over number of queried labels on the pool set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Embedding of version spaces using Multi-Dimensional Scaling (MDS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Settings for each dataset used in the active learning experiments.</figDesc><table><row><cell>Dataset</cell><cell>Pool/Val/Test</cell><cell>Model</cell><cell cols="3">Ensemble Size Init/Query/Total Runs</cell></row><row><cell>MNIST</cell><cell cols="2">45000/5000/10000 2-conv-layer ConvNet</cell><cell>20</cell><cell>10/5/400</cell><cell>4</cell></row><row><cell cols="3">Fashion-MNIST 55000/5000/10000 3-conv-layer ConvNet</cell><cell>20</cell><cell>10/10/1000</cell><cell>4</cell></row><row><cell>SVHN</cell><cell cols="2">40000/5000/15000 6-conv-layer ConvNet</cell><cell>20</cell><cell>100/20/3000</cell><cell>4</cell></row><row><cell>STL-10</cell><cell>4000/1000/8000</cell><cell>ResNet18</cell><cell>20</cell><cell>100/40/2580</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on the test set in percentage.</figDesc><table><row><cell></cell><cell cols="2">MNIST Fashion-MNIST</cell><cell>SVHN</cell><cell>STL-10</cell></row><row><cell>#labels</cell><cell>400</cell><cell>1000</cell><cell>3000</cell><cell>2580</cell></row><row><cell>Random</cell><cell>93.47 ± 0.38</cell><cell>83.90 ± 0.38</cell><cell cols="2">85.60 ± 0.23 58.15 ± 0.54</cell></row><row><cell>VR</cell><cell>96.74 ± 0.15</cell><cell>83.05 ± 1.09</cell><cell cols="2">63.23 ± 1.99 59.13 ± 0.21</cell></row><row><cell>GE</cell><cell>96.79 ± 0.10</cell><cell>80.01 ± 0.94</cell><cell cols="2">64.08 ± 3.77 58.84 ± 0.34</cell></row><row><cell cols="2">BALD-MCD 96.51 ± 0.22</cell><cell>84.67 ± 0.41</cell><cell cols="2">85.26 ± 0.34 57.35 ± 0.64</cell></row><row><cell>Core-Set</cell><cell>95.38 ± 0.28</cell><cell>79.08 ± 0.82</cell><cell cols="2">84.91 ± 0.20 58.93 ± 0.33</cell></row><row><cell>DFAL</cell><cell>92.88 ± 1.19</cell><cell>85.38 ± 0.60</cell><cell cols="2">86.34 ± 0.33 58.81 ± 0.37</cell></row><row><cell>PWD</cell><cell>96.92 ± 0.12</cell><cell>85.92 ± 0.10</cell><cell cols="2">86.41 ± 0.12 59.45 ± 0.11</cell></row><row><cell>GVD</cell><cell cols="4">97.02 ± 0.06 86.01 ± 0.15 86.44 ± 0.20 59.33 ± 0.37</cell></row><row><cell>M 2 -PWD</cell><cell>93.24 ± 0.09</cell><cell>84.33 ± 0.03</cell><cell cols="2">85.42 ± 0.16 57.81 ± 0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Diameter (pairwise disagreement) on the test set in percentage. ± 0.13 33.23 ± 0.18 inferiority to Random in terms of test accuracy usually correlates with higher diameter (See description in</figDesc><table><row><cell></cell><cell cols="2">MNIST Fashion-MNIST</cell><cell>SVHN</cell><cell>STL-10</cell></row><row><cell>#labels</cell><cell>400</cell><cell>1000</cell><cell>3000</cell><cell>2580</cell></row><row><cell>Random</cell><cell>2.86 ± 0.18</cell><cell>7.55 ± 0.26</cell><cell cols="2">13.13 ± 0.29 32.88 ± 0.43</cell></row><row><cell>VR</cell><cell>2.27 ± 0.18</cell><cell>10.64 ± 0.72</cell><cell cols="2">46.88 ± 2.76 34.21 ± 0.08</cell></row><row><cell>GE</cell><cell>2.30 ± 0.04</cell><cell>11.38 ± 1.52</cell><cell cols="2">44.87 ± 4.25 34.25 ± 0.09</cell></row><row><cell cols="2">BALD-MCD 2.39 ± 0.15</cell><cell>8.11 ± 0.51</cell><cell cols="2">16.58 ± 0.42 33.55 ± 0.44</cell></row><row><cell>Core-Set</cell><cell>2.91 ± 0.18</cell><cell>10.79 ± 1.34</cell><cell cols="2">14.66 ± 0.47 33.13 ± 0.64</cell></row><row><cell>DFAL</cell><cell>3.79 ± 0.60</cell><cell>7.06 ± 0.60</cell><cell cols="2">13.98 ± 0.31 32.41 ± 0.27</cell></row><row><cell>PWD</cell><cell>1.93 ± 0.04</cell><cell>6.91 ± 0.16</cell><cell cols="2">12.80 ± 0.08 32.25 ± 0.26</cell></row><row><cell>GVD</cell><cell>1.98 ± 0.05</cell><cell>6.98 ± 0.26</cell><cell cols="2">12.88 ± 0.25 32.96 ± 0.48</cell></row><row><cell>M 2 -PWD</cell><cell>3.37 ± 0.13</cell><cell>7.22 ± 0.08</cell><cell>13.31</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Input: T query rounds, k batch size of query, M size of ensemble, XS pool of</figDesc><table><row><cell></cell><cell cols="3">unlabeled samples, diam diamter measure, π prior over hypotheses</cell></row><row><cell cols="2">2: V0 ←Ĥ</cell><cell></cell><cell></cell></row><row><cell cols="2">3: Q ← ∅</cell><cell></cell><cell></cell></row><row><cell cols="2">4: for t ← 1 to T do</cell><cell></cell><cell></cell></row><row><cell cols="2">5:V ← ∅</cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell cols="2">for m ← 1 to M do</cell><cell></cell></row><row><cell>7:</cell><cell cols="2">sample hm ∼ π|V t</cell><cell></cell></row><row><cell cols="2">8:V ←V ∪ {hm}</cell><cell></cell><cell></cell></row><row><cell>9:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell cols="2">for j ← 1 to k do</cell><cell></cell></row><row><cell>11:</cell><cell>xj ← arg min x∈X S \X Q</cell><cell>max y∈Y</cell><cell>diam V y x</cell></row><row><cell>12:</cell><cell>query yj</cell><cell></cell><cell></cell></row><row><cell>13:</cell><cell cols="2">Q ← Q ∪ {(xj, yj)}</cell><cell></cell></row><row><cell>14:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>15:</cell><cell cols="3">Vt ← {h ∈ Vt−1 : h(x) = y, ∀ (x, y) ∈ Q}</cell></row><row><cell cols="2">16: end for</cell><cell></cell><cell></cell></row><row><cell cols="3">17: return any h ∼ π|V t</cell><cell></cell></row></table><note>1:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Source code is available at https://github.com/jiayu-liu/effective-version-spacereduction-for-convnets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the BMBF project MLWin and the Munich Center for Machine Learning (MCML).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Margin based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference on Learning Theory</title>
		<meeting>the 20th Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Köhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Near-optimal adaptive pool-based active learning with general loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 30th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active learning for probabilistic hypotheses using the maximum gibbs error criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1457" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of a greedy active learning strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coarse sample complexity bounds for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two faces of active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Algorithmic Learning Theory</title>
		<meeting>the 20th International Conference on Algorithmic Learning Theory</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A probabilistic theory of pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Györfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adversarial active learning for deep networks: a margin based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ducoffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09841</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gissin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06347</idno>
		<title level="m">Discriminative active learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive submodularity: A new approach to active learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="427" to="486" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A bound on the label complexity of agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<idno>abs/1112.5745</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalization error bounds using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kääriäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Conference on Learning Theory</title>
		<meeting>the 18th Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="127" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7024" to="7035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multidimensional scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<pubPlace>Sage</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pac-bayes bounds for the risk of the majority vote and the variance of the gibbs classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian batch active learning as sparse subset approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6356" to="6367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A coreset approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th the International Conference on Learning Representations</title>
		<meeting>6th the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lecture 6.5 -RMSProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diameter-based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>1-87.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th British Machine Vision Conference</title>
		<meeting>the 27th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
