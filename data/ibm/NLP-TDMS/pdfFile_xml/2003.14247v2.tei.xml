<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DPGN: Distribution Propagation Graph Network for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
							<email>yangling0818@163.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Li</surname></persName>
							<email>liliangliang@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilun</forename><surname>Zhang</surname></persName>
							<email>zilun.zhang@mail.utoronto.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<email>liuyu@megvii.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DPGN: Distribution Propagation Graph Network for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most graph-network-based meta-learning approaches model instance-level relation of examples. We extend this idea further to explicitly model the distribution-level relation of one example to all other examples in a 1-vs-N manner. We propose a novel approach named distribution propagation graph network (DPGN) for few-shot learning. It conveys both the distribution-level relations and instancelevel relations in each few-shot learning task. To combine the distribution-level relations and instance-level relations for all examples, we construct a dual complete graph network which consists of a point graph and a distribution graph with each node standing for an example. Equipped with dual graph architecture, DPGN propagates label information from labeled examples to unlabeled examples within several update generations. In extensive experiments on few-shot learning benchmarks, DPGN outperforms stateof-the-art results by a large margin in 5% ∼ 12% under supervised settings and 7% ∼ 13% under semi-supervised settings. support classification distribution-level inference instance-level inference</p><p>Figure 1: Our proposed DPGN adopts contrastive comparisons between each sample with support samples to produce distribution representation. Then it incorporates distribution-level comparisons with instance-level comparisons when classifying the query sample.</p><p>An episode is one round of model training, where in each episode, only few examples (e.g., 1 or 5) are randomly sampled from each class in training data. Meta-learning methods adopt a trainer (also called meta-learner) which takes the few-shot training data and outputs a classifier. This process is called episodic training <ref type="bibr" target="#b40">[41]</ref>. Under the framework of meta-learning, a diverse hypothesis was made to build an efficient meta-learner.</p><p>A rising trend in recent researches was to process the training data with Graph Networks [2], which is a powerful model that generalizes many data structures (list, trees) while introduces a combinatorial prior over data.  is proposed to build a complete graph network where each node feature is concatenated with the corresponding class label, then node features are updated via the attention mechanism of graph network to propagate the label information. To further exploit intra-cluster similarity and inter-cluster dissimilarity in the graph-based network, EGNN [18]  demonstrates an edge-labeling graph neural network under the episodic training framework. It is noted that previous GNN studies in few-shot learning mainly fo-1 arXiv:2003.14247v2 [cs.CV] 1 Apr 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of deep learning is rooted in a large amount of labeled data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>, while humans generalize well after having seen few examples. The contradiction between these two facts brings great attention to the research of few-shot learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>. Few-shot learning task aims at predicting unlabeled data (query set) given a few labeled data (support set).</p><p>Fine-tuning <ref type="bibr" target="#b3">[4]</ref> is the defacto method in obtaining a predictive model from a small training dataset in practice nowadays. However, it suffers from overfitting issues <ref type="bibr" target="#b10">[11]</ref>. Meta-learning <ref type="bibr" target="#b7">[8]</ref> methods introduces the concept of episode to address the few-shot problem explicitly. cused on pair-wise relations like node labeling or edge labeling, and ignored a large number of substantial distribution relations. Additionally, other meta-learning approaches claim to make use of the benefits of global relations by episodic training, but in an implicitly way.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, firstly, we extract the instance feature of support and query samples. Then, we obtain the distribution feature for each sample by calculating the instance-level similarity over all support samples. To leverage both instance-level and distribution-level representation of each example and process the representations at different levels independently, we propose a dual-graph architecture: a point graph (PG) and a distribution graph (DG). Specifically, a PG generates a DG by gathering 1-vs-n relation on every example, while the DG refines the PG by delivering distribution relations between each pair of examples. Such cyclic transformation adequately fuses instancelevel and distribution-level relations and multiple generations (rounds) of this Gather-Compare process concludes our approach. Furthermore, it is easy to extend DPGN to semi-supervised few-shot learning task where support set containing both labeled and unlabeled samples for each class. DPGN builds a bridge connection between labeled and unlabeled samples in the form of similarity distribution, which leads to a better propagation for label information in semi-supervised few-shot classification.</p><p>Our main contributions are summarized as follows:</p><p>• To the best of our knowledge, DPGN is the first to explicitly incorporate distribution propagation in graph network for few-shot learning. The further ablation studies have demonstrated the effectiveness of distribution relations.</p><p>• We devise the dual complete graph network that combines instance-level and distribution-level relations. The cyclic update policy in this framework contributes to enhancing instance features with distribution information.</p><p>• Extensive experiments are conducted on four popular benchmark datasets for few-shot learning. By comparing with all state-of-the-art methods, the DPGN achieves a significant improvement of 5%∼12% on average in few-shot classification accuracy. In semisupervised tasks, our algorithm outperforms existing graph-based few-shot learning methods by 7%∼13 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Neural Network</head><p>Graph neural networks were first designed for tasks on processing graph-structured data <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>. Graph neural networks mainly refine the node representations by aggregating and transforming neighboring nodes recursively. Recent approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref> are proposed to exploit GNN in the field of few-shot learning task. TPN <ref type="bibr" target="#b24">[25]</ref> brings the transductive setting into graph-based few-shot learning, which performs a Laplacian matrix to propagate labels from support set to query set in the graph. It also considers the similarity between support and query samples through the process of pairwise node features affinities to propagate labels. EGNN <ref type="bibr" target="#b17">[18]</ref> uses the similarity/dissimilarity between samples and dynamically update both node and edge features for complicated interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Metric Learning</head><p>Another category of few-shot learning approaches focus on optimizing feature embeddings of input data using metric learning methods. Matching Networks <ref type="bibr" target="#b40">[41]</ref> produces a weighted nearest neighbor classifier through computing embedding distance between support and query set. Prototypical Networks <ref type="bibr" target="#b35">[36]</ref> firstly build a prototype representation of each class in the embedding space. As an extension of Prototypical Networks, IMF <ref type="bibr" target="#b0">[1]</ref> constructs infinite mixture prototypes by self-adaptation. RelationNet <ref type="bibr" target="#b39">[40]</ref> adopts a distance metric network to learn pointwise relations in support and query samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Distribution Learning</head><p>Distribution Learning theory was first introduced in <ref type="bibr" target="#b16">[17]</ref> to find an efficient algorithm that determines the distribution from which the samples are drawn. Different methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> are proposed to efficiently estimate the target distributions. DLDL <ref type="bibr" target="#b8">[9]</ref> is one of the researches that has assigned the discrete distribution instead of one-hot label for each instance in classification and regression tasks. CPNN <ref type="bibr" target="#b43">[44]</ref> takes both features and labels as the inputs and produces the label distribution with only one hidden layer in its framework. LDLFs <ref type="bibr" target="#b34">[35]</ref> devises a distribution learning method based on the decision tree algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Meta Learning</head><p>Some few-shot approaches adopt a meta-learning framework that learns meta-level knowledge across batches of tasks. MAML <ref type="bibr" target="#b7">[8]</ref> are gradient-based approaches that design the meta-learner as an optimizer that could learn to update the model parameters (e.g., all layers of a deep network) within few optimization steps given novel examples. Reptile <ref type="bibr" target="#b27">[28]</ref> simplifies the computation of meta-loss by incorporating an L2 loss which updates the meta-model parameters towards the instance-specific adapted models. SNAIL <ref type="bibr" target="#b26">[27]</ref> learn a parameterized predictor to estimate the parameters in models. MetaOptNet <ref type="bibr" target="#b20">[21]</ref> advocates the use of linear classifier instead of nearest-neighbor methods which can be optimized as convex learning problems. LEO <ref type="bibr" target="#b32">[33]</ref> utilizes an encoder-decoder architecture to mine the latent genera-  <ref type="figure">Figure 2</ref>: The overall framework of DPGN. In this illustration, we take a 2way-1shot task as an example. The support and query embeddings obtained from feature extractor are delivered to the dual complete graph (a point graph and a distribution graph) for transductive propagation generation after generation. The green arrow represents a edge-to-node transformation (P2D, described in Section 3.2.1) which aggregates instance similarities to construct distribution representations and the blue arrow represents another edge-to-node transformation (D2P, described in Section 3.2.2) which aggregates distribution similarities with instance features. DPGN makes the prediction for the query sample at the end of generation l.</p><p>tive representations and predicts high-dimensional parameters in extreme low-data regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first provide the background of fewshot learning task, then introduce the proposed algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>The goal of few-shot learning tasks is to train a model that can perform well in the case where only few samples are given. Each few-shot task has a support set S and a query set Q. Given training data D train , the support set S ⊂ D train contains N classes with K samples for each class (i.e., the N -way K-shot setting), it can be denoted as S = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x N × K , y N ×K )}. The query set Q ⊂ D train hasT samples and can be denoted</p><formula xml:id="formula_0">as Q = {(x N ×K+1 , y N ×K+1 ), . . . , (x N ×K+T , y N ×K+T )}.</formula><p>Specifically, in the training stage, data labels are provided for both support set S and query set Q. Given testing data D test , our goal is to train a classifier that can map the query sample from Q ∈ D test to the corresponding label accurately with few support samples from S ∈ D test . Labels of support sets and query sets are mutually exclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Distribution Propagation Graph Networks</head><p>In this section, we will explain the DPGN that we proposed for few-shot learning in detail. As shown in <ref type="figure">Figure  2</ref>. The DPGN consists of l generations and each generation consists of a point graph G p l = (V p l , E p l ) and a distribu-</p><formula xml:id="formula_1">tion graph G d l = (V d l , E d l )</formula><p>. Firstly, the feature embeddings of all samples are extracted by a convolutional backbone, these embeddings are used to compute the instance similarities E p l . Secondly, the instance relations E p l are delivered to construct the distribution graph G d l . The node features V d l are initialized by aggregating E p l following the position order in G p l and the edge features E d l stand for the distribution similarities between the node features V d l . Finally, the obtained E d l is delivered to G p l for constructing more discriminative representations of nodes V p l and we repeat the above procedure generation by generation. A brief introduction of generation update for the DPGN can be ex-</p><formula xml:id="formula_2">pressed as E p l − → V d l − → E d l − → V p l − → E p l+1 , where l denotes the l-th generation.</formula><p>For further explanation, we formulate V p l , E p l , V d l and E d l as follows:</p><formula xml:id="formula_3">V p l = {v p l,i }, E p l = {e p l,ij }, V d l = {v d l,i }, E d l = {e d l,ij } where i, j = 1, · · · T . T = N × K +T denotes the total number of examples in a training episode. v p 0,i is first initialized by the output of the feature extractor f emb . For each sample x i : v p 0,i = f emb (x i ) ,<label>(1)</label></formula><p>where v p 0,i ∈ R m and m denotes the dimension of the feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Point-to-Distribution Aggregation</head><p>Point Similarity Each edge in the point graph stands for the instance (point) similarity and the edge e p 0,ij of the first generation is initialized as follows:</p><formula xml:id="formula_4">e p 0,ij = f e p 0 ((v p 0,i − v p 0,j ) 2 ) ,<label>(2)</label></formula><p>where e p 0,ij ∈ R. f e p 0 : R m − → R is the encoding network that transforms the instance similarity to a certain scale. f e p  contains two Conv-BN-ReLU <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref> blocks with the parameter set θ e p 0 and a sigmoid layer. For generation l &gt; 0, given e p l−1,ij , v p l−1,i and v p l−1,j , e p l,ij can be updated as follows:</p><formula xml:id="formula_5">0 2-d 4-d 2-d 2-d , 1 , 1 1 , 1 2 − 1 , 1 MLP-1 P2D Agg ,</formula><formula xml:id="formula_6">e p l,ij = f e p l ((v p l−1,i − v p l−1,j ) 2 ) · e p l−1,ij .<label>(3)</label></formula><p>In order to use edge information with a holistic view of the graph G p l , a normalization operation is conducted on the e p l,ij .</p><p>P2D Aggregation After edge features E p l in point graph G p l are produced or updated, the distribution graph</p><formula xml:id="formula_7">G d l = (V d l , E d l )</formula><p>is the next to be constructed. As shown in <ref type="figure" target="#fig_0">Figure  3</ref>, G d l aims at integrating instance relations from the point graph G p l and process the distribution-level relations. Each distribution feature v d l,i in G d l is a N K dimension feature vector where the value in j-th entry represents the relation between sample x i and sample x j and N K stands for the total number of support samples in a task. For first initialization:</p><formula xml:id="formula_8">v d 0,i = N K j=1 δ(y i , y j ) if x i is labeled, [ 1 N K , · · · , 1 N K ] otherwise,<label>(4)</label></formula><p>where v d 0,i ∈ R N K and is the concatenation operator. δ(·) is the Kronecker delta function which outputs one when y i = y j and zero otherwise (y i and y j are labels).</p><p>For generations l &gt; 0, the distribution node v d l,i can be updated as follows:</p><formula xml:id="formula_9">v d l,i = P2D( N K j=1 e p l,ij , v d l−1,i ) ,<label>(5)</label></formula><p>where P2D : (R N K , R N K ) − → R N K is the aggregation network for distribution graph. P2D applies a concatenation operation between two features. Then, P2D performs a transformation : R 2N K − → R N K on the concatenated features which is composed of a fully-connected layer and ReLU <ref type="bibr" target="#b12">[13]</ref>, with the parameter set θ v d l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Distribution-to-Point Aggregation</head><p>Distribution Similarity Each edge in distribution graph stands for the similarity between distribution features of different samples. For generation l = 0, the distribution similarity e d 0,ij is initialized as follows:</p><formula xml:id="formula_10">e d 0,ij = f e d 0 ((v d 0,i − v d 0,j ) 2 ) ,<label>(6)</label></formula><p>where e d 0,ij ∈ R. The encoding network f e d 0 : R N K − → R transforms the distribution similarity using two Conv-BN-ReLU blocks with the parameter set θ e d 0 and a sigmoid layer in the end. For generation l &gt; 0, the update rule for e d l,ij in G d l is formulated as follows:</p><formula xml:id="formula_11">e d l,ij = f e d l ((v d l,i − v d l,j ) 2 ) · e d l−1,ij .<label>(7)</label></formula><p>Also, we apply a normalization to e d l,ij .</p><p>D2P Aggregation As illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, the encoded distribution information in G d l flows back into the point graph G p l at the end of each generation. Then node features v p l,i in G p l captures the distribution relations through aggregating all the node features in G p l with edge features e d l,i as follows:</p><formula xml:id="formula_12">v p l,i = D2P( T j=1 (e d l,ij · v p l−1,j ), v p l−1,i ) ,<label>(8)</label></formula><p>where v p l,i ∈ R m and D2P : (R m , R m ) − → R m is the aggregation network for point graph in G p l with the parameter set θ v p l . D2P concatenates the feature which is computed by T j=1 (e d l,ij · v p l−1,j ) with the node features v p l−1,i in previous generation and update the concatenated feature with two Conv-BN-ReLU blocks. After this process, the node features can integrate the distribution-level information into the instance-level feature and prepares for computing instance similarities in the next generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Objective</head><p>The class prediction of each node can be computed by feeding the corresponding edges in the final generation l of DPGN into softmax function:</p><formula xml:id="formula_13">P (ŷ i |x i ) = Softmax( N K j=1 e p l,ij · one-hot(y j )) ,<label>(9)</label></formula><p>where P (ŷ i |x i ) is the probability distribution over classes given sample x i , and y j is the label of jth sample in the support set. e p l,ij stands for the edge feature in the point graph at the final generation.</p><p>Point Loss It is noted that we make classification predictions in the point graph for each sample. Therefore, the point loss at generation l is defined as follows:</p><formula xml:id="formula_14">L p l = L CE (P (ŷ i |x i ), y i ) ,<label>(10)</label></formula><p>where L CE is the cross-entropy loss function, T stands for the number of samples in each task (S, Q) ∈ D train . P (ŷ i |x i ) and y i are model probability predictions of sample x i and the ground-truth label respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution Loss</head><p>To facilitate the training process and learn discriminative distribution features , we incorporate the distribution loss which plays a significant role in contributing to faster and better convergence. We define the distribution loss for generation l as follows:</p><formula xml:id="formula_15">L d l = L CE (Softmax( N K j=1 e d l,ij · one-hot(y j )), y i ) ,<label>(11)</label></formula><p>where e d l,ij stands for the edge feature in the distribution graph at generation l.</p><p>The total objective function is a weighted summation of all the losses mentioned above:</p><formula xml:id="formula_16">L =l l=1 (λ p L p l + λ d L d l ) ,<label>(12)</label></formula><p>wherel denotes total generations of DPGN and the weights λ p and λ d of each loss are set to balance their importance. In most of our experiments, λ p and λ d are set to 1.0 and 0.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datesets</head><p>We evaluate DPGN on four standard few-shot learning benchmarks: miniImageNet <ref type="bibr" target="#b40">[41]</ref>, tieredImageNet <ref type="bibr" target="#b30">[31]</ref>, CUB-200-2011 <ref type="bibr" target="#b41">[42]</ref> and CIFAR-FS <ref type="bibr" target="#b2">[3]</ref>. The miniImageNet and tieredImageNet are the subsets of ImageNet <ref type="bibr" target="#b31">[32]</ref>. CUB-200-2011 is initially designed for fine-grained classification and CIFAR-FS is a subset of CIFAR-100 for fewshot classification. As shown in <ref type="table" target="#tab_2">Table 1</ref>, we list details for images number, classes number, images resolution and train/val/test splits following the criteria of previous works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>.  <ref type="bibr" target="#b17">[18]</ref>, MetaOptNet <ref type="bibr" target="#b20">[21]</ref>, CloserLook <ref type="bibr" target="#b3">[4]</ref> and LEO <ref type="bibr" target="#b32">[33]</ref> respectively. ConvNet mainly consists of four Conv-BN-ReLU blocks. The last two blocks also contain a dropout layer <ref type="bibr" target="#b36">[37]</ref>. ResNet12 and ResNet18 are the same as the one described in <ref type="bibr" target="#b13">[14]</ref>. They mainly have four blocks, which include one residual block for ResNet12 and two residual blocks for ResNet18 respectively. WRN was firstly proposed in <ref type="bibr" target="#b45">[46]</ref>. It mainly has three residual blocks and the depth of the network is set to 28 as in <ref type="bibr" target="#b32">[33]</ref>. The last features of all backbone networks are processed by a global average pooling, then followed by a fully-connected layer with batch normalization <ref type="bibr" target="#b14">[15]</ref> to obtain a 128-dimensions instance embedding.</p><p>Training Schema We perform data augmentation before training, such as horizontal flip, random crop, and color jitter (brightness, contrast, and saturation), which are mentioned in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">43]</ref>. We randomly sample 28 meta-task episodes in each iteration for meta-training. The Adam optimizer is used in all experiments with the initial learning rate of 10 −3 . We decay the learning rate by 0.1 per 15000 iterations and set the weight decay to 10 −5 .</p><p>Evaluation Protocols We evaluate DPGN in 5way-1shot/5shot settings on standard few-shot learning datasets, miniImageNet, tieredImageNet, CUB-200-2011 and CIFAR-FS. We follow the evaluation process of previous approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref>. We randomly sample 10,000 tasks then report the mean accuracy (in %) as well as the 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Results</head><p>Main Results We compare the performance of DPGN with several state-of-the-art models including graph and non-graph methods. For fair comparisons, we employ DPGN on miniImageNet, tieredImageNet, CIFAR-FS and CUB-200-2011 datasets, which is compared with other methods in the same backbones. As shown in <ref type="table" target="#tab_3">Table 2</ref>, 3 and 4, the proposed DPGN is superior to other existing methods and achieves the state-of-the-art performance, especially compared with the graph-based methods. Semi-supervised Few-shot Learning We employ DPGN on semi-supervised few-shot learning. Following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref>, we use the same criteria to split miniImageNet dataset into <ref type="table">Table 3</ref>: Few-shot classification accuracies on tieredIma-geNet. † denotes that it is implemented by public code. * denotes that it is reported from <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref> and DPGN are tested in transduction.</p><p>Method backbone 5way-1shot 5way-5shot MAML* <ref type="bibr" target="#b7">[8]</ref> ConvNet 51.67±1.81 70.30±1.75 ProtoNet* <ref type="bibr" target="#b35">[36]</ref> ConvNet   <ref type="figure">Figure 4</ref>: Semi-supervised few-shot learning accuracy in 5way-10shot on miniImageNet. DPGN surpass TPN and EGNN by a large margin consistently.</p><p>port samples, which could propagate label information from labeled samples to queries sufficiently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Transduction 5way-5shot Reptile <ref type="bibr" target="#b27">[28]</ref> No 62.74 GNN <ref type="bibr" target="#b9">[10]</ref> No 66.41 Edge-label <ref type="bibr" target="#b17">[18]</ref> No In <ref type="figure">Figure 4</ref>, DPGN shows the superiority to exsisting semi-supervised few-shot methods and the result demonstrates the effectiveness to exploit the relations between labeled and unlabeled data when the label ratio decreases. Notably, DPGN surpasses TPN <ref type="bibr" target="#b24">[25]</ref> and EGNN <ref type="bibr" target="#b17">[18]</ref> by 11% ∼ 16% and 7% ∼ 13% respectively in few-shot average classification accuracy on miniImageNet.</p><p>Transductive Propagation To validate the effectiveness of the transductive setting in our framework, we conduct the transductive and non-transductive experiments on miniImageNet dataset in 5way-5shot setting. <ref type="table" target="#tab_6">Table 5</ref> shows that the accuracy of DPGN increases by a large margin in the transductive setting (comparing with non-transductive setting). Compared to TPN and EGNN which consider instance-level features only, DPGN utilizes distribution similarities between query samples and adopts dual graph architecture to propagate label information in a sufficient way.</p><p>High-way classification Furthermore, the performance of DPGN in high-way few-shot scenarios is evaluated on miniImageNet dataset and its results are shown in <ref type="figure" target="#fig_1">Figure 5</ref>. The observed results show that DPGN not only exceeds the powerful graph-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref> but also surpasses the state-of-the-art non-graph methods significantly. As the number of ways increasing in few-shot tasks, it can broaden the horizons of distribution utilization and make it possible for DPGN to collect more abundant distribution-level information for queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Impact of Distribution Graph The distribution graph G d l works as an important component of DPGN by propagating distribution information, so it is necessary to investigate the effectiveness of G d l quantitatively. We design the experiment by limiting the distribution similarities which flow to G p l for performing aggregation in each generation during the inference process. Specifically, we mask out the edge features E d l through keeping a different number of feature dimensions and set the value of rest dimensions to zero, since zero gives no contribution. <ref type="figure" target="#fig_2">Figure 6</ref> shows the result for our experiment in 5way-1shot on miniImageNet. It is obvious that test accuracy and the number of feature dimensions kept in E d l have positive correlations and accuracy increment (area in blue) decreases with more feature dimen-  sions. Keeping dimensions from 0 to 5, DPGN boosts the performance nearly by 10% in absolute value and the result shows that the distribution graph has a great impact on our framework.</p><p>Generation Numbers DPGN has a cyclic architecture that includes point graph and distribution graph, each graph has node-update and edge-update modules respectively. The total number of generations is an important ingredient for DPGN, so we perform experiments to obtain the trend of test accuracy with different generation numbers in DPGN on miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS. In <ref type="figure" target="#fig_3">Figure 7</ref>, with the generation number changing from 0 to 1, the test accuracy has a significant rise. When the generation number changes from 1 to 10, the test accuracy increases by a small margin and the curve becomes to fluctuate in the last several generations. Consider- ing that more generations need more iterations to converge, we choose generation 6 as a trade-off between the test accuracy and convergence time. Additionally, to visualize the procedure of cyclic update, we choose a test scenario where the ground truth classes of five query images are <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> and visualize instance-level similarities which is used for predictions of five query samples as shown in <ref type="figure" target="#fig_4">Figure 8</ref>. The heatmap shows DPGN refines the instance-level similarity matrix after several generations and makes the right predictions for five query samples in the final generation. Notably, DPGN not only contributes to predicting more accurately but also enlarge the similarity distances between the samples in different classes through making instance features more discriminative, which cleans the prediction heatmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented the Distribution Propagation Graph Network for few-shot learning, a dual complete graph network that combines instance-level and distribution-level relations in an explicit way equipped with label propagation and transduction. The point and distribution losses are used to jointly update the parameters of the DPGN with episodic training. Extensive experiments demonstrate that our method outperforms recent state-ofthe-art algorithms by 5%∼12% in the supervised task and 7%∼13% in semi-supervised task on few-shot learning benchmarks. For future work, we aim to focus on the highorder message propagation through encoding more complicated information which is linked with task-level relations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Details about P2D aggregation and D2P aggregation in DPGN. A 2way-1shot task is presented as an example. MLP-1 is the FC-ReLU blocks mentioned in P2D Aggregation and MLP-2 is the Conv-BN-ReLU blocks mentioned in D2P Aggregation. The green arrow denotes the P2D aggregation while the blue arrow denotes the D2P aggregation. Both aggregation processes integrate the node or edge features of their previous generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>High-way few-shot classification accuracies on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Effectiveness of G d l through keeping n dimensions in 5way-1shot on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Generation number in DPGN on miniImageNet, tieredImageNet, CUB-200-2011 and CIFAR-FS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>The visualization of edge prediction in each generation of DPGN. (a) to (f) denotes generation 1 to 6. The dark denotes higher score and the shallow denotes lower confidence. The left axis stands for the index of 5 query images and the bottom axis stands for 5 support class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This research was supported by National Key R&amp;D Program of China (No. 2017YFA0700800).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Details for few-shot learning benchmarks.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Images Classes Train-val-test Resolution</cell></row><row><cell cols="2">miniImageNet 60000 100</cell><cell>64/16/20</cell><cell>84x84</cell></row><row><cell cols="2">tieredImageNet 779165 608</cell><cell>351/97/160</cell><cell>84x84</cell></row><row><cell cols="2">CUB-200-2011 11788 200</cell><cell>100/50/50</cell><cell>84x84</cell></row><row><cell>CIFAR-FS</cell><cell>60000 100</cell><cell>64/16/20</cell><cell>32x32</cell></row><row><cell cols="2">4.1.2 Experiment Setups</cell><cell></cell><cell></cell></row><row><cell cols="4">Network Architecture We use four popular networks for</cell></row><row><cell cols="4">fair comparison, which are ConvNet, ResNet12, ResNet18</cell></row><row><cell cols="2">and WRN that are used in EGNN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Few-shot classification accuracies on miniIma-geNet. † denotes thatit is implemented by public code.<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref> and DPGN are tested in transduction.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone 5way-1shot 5way-5shot</cell></row><row><cell cols="3">MatchingNet [41] ConvNet 43.56±0.84 55.31± 0.73</cell></row><row><cell>ProtoNet [36]</cell><cell cols="2">ConvNet 49.42±0.78 68.20±0.66</cell></row><row><cell cols="3">RelationNet [40] ConvNet 50.44±0.82 65.32±0.70</cell></row><row><cell>R2D2 [3]</cell><cell cols="2">ConvNet 51.20±0.60 68.20±0.60</cell></row><row><cell>MAML [8]</cell><cell cols="2">ConvNet 48.70±1.84 55.31±0.73</cell></row><row><cell>Dynamic [11]</cell><cell cols="2">ConvNet 56.20±0.86 71.94±0.57</cell></row><row><cell>GNN [10]</cell><cell cols="2">ConvNet 50.33±0.36 66.41±0.63</cell></row><row><cell>TPN [25]</cell><cell cols="2">ConvNet 55.51±0.86 69.86±0.65</cell></row><row><cell>Global [26]</cell><cell cols="2">ConvNet 53.21±0.40 72.34±0.32</cell></row><row><cell cols="3">Edge-label [18] ConvNet 59.63±0.52  † 76.34±0.48</cell></row><row><cell>DPGN</cell><cell cols="2">ConvNet 66.01±0.36 82.83±0.41</cell></row><row><cell>LEO [33]</cell><cell>WRN</cell><cell>61.76±0.08 77.59±0.12</cell></row><row><cell>wDAE [12]</cell><cell>WRN</cell><cell>61.07±0.15 76.75±0.11</cell></row><row><cell>DPGN</cell><cell cols="2">WRN 67.24±0.51 83.72±0.44</cell></row><row><cell cols="3">CloserLook [4] ResNet18 51.75±0.80 74.27±0.63</cell></row><row><cell>CTM [22]</cell><cell cols="2">ResNet18 62.05±0.55 78.63±0.06</cell></row><row><cell>DPGN</cell><cell cols="2">ResNet18 66.63±0.51 84.07±0.42</cell></row><row><cell cols="3">MetaGAN [47] ResNet12 52.71±0.64 68.63±0.67</cell></row><row><cell>SNAIL [27]</cell><cell cols="2">ResNet12 55.71±0.99 68.88±0.92</cell></row><row><cell>TADAM [29]</cell><cell cols="2">ResNet12 58.50±0.30 76.70±0.30</cell></row><row><cell cols="3">Shot-Free [30] ResNet12 59.04±0.43 77.64±0.39</cell></row><row><cell cols="3">Meta-Transfer [39] ResNet12 61.20±1.80 75.53±0.80</cell></row><row><cell>FEAT [43]</cell><cell cols="2">ResNet12 62.96±0.02 78.49±0.02</cell></row><row><cell>TapNet [45]</cell><cell cols="2">ResNet12 61.65±0.15 76.36±0.10</cell></row><row><cell>Dense [24]</cell><cell cols="2">ResNet12 62.53±0.19 78.95±0.13</cell></row><row><cell cols="3">MetaOptNet [21] ResNet12 62.64±0.61 78.63±0.46</cell></row><row><cell>DPGN</cell><cell cols="2">ResNet12 67.77±0.32 84.60±0.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Few-shot classification accuracies on CUB-200-</cell></row><row><cell cols="4">2011 and CIFAR-FS. * denotes that it is reported from [21]</cell></row><row><cell cols="3">or [4]. DPGN are tested in transduction.</cell></row><row><cell>Method</cell><cell>backbone</cell><cell cols="2">CUB-200-2011 5way-1shot 5way-5shot</cell></row><row><cell>ProtoNet* [36]</cell><cell cols="3">ConvNet 51.31±0.91 70.77±0.69</cell></row><row><cell>MAML* [8]</cell><cell cols="3">ConvNet 55.92±0.95 72.09±0.76</cell></row><row><cell cols="4">MatchingNet* [41] ConvNet 61.16±0.89 72.86±0.70</cell></row><row><cell cols="4">RelationNet* [40] ConvNet 62.45±0.98 76.11±0.69</cell></row><row><cell>CloserLook [4]</cell><cell cols="3">ConvNet 60.53±0.83 79.34±0.61</cell></row><row><cell>DN4 [23]</cell><cell cols="3">ConvNet 53.15±0.84 81.90±0.60</cell></row><row><cell>DPGN</cell><cell cols="3">ConvNet 76.05±0.51 89.08±0.38</cell></row><row><cell>FEAT [43]</cell><cell cols="3">ResNet12 68.87±0.22 82.90±0.15</cell></row><row><cell>DPGN</cell><cell cols="3">ResNet12 75.71±0.47 91.48±0.33</cell></row><row><cell>Method</cell><cell>backbone</cell><cell cols="2">CIFAR-FS 5way-1shot 5way-5shot</cell></row><row><cell>ProtoNet* [36]</cell><cell cols="2">ConvNet 55.5±0.7</cell><cell>72.0±0.6</cell></row><row><cell>MAML* [8]</cell><cell cols="2">ConvNet 58.9±1.9</cell><cell>71.5±1.0</cell></row><row><cell cols="3">RelationNet* [40] ConvNet 55.0±1.0</cell><cell>69.3±0.8</cell></row><row><cell>R2D2 [3]</cell><cell cols="2">ConvNet 65.3±0.2</cell><cell>79.4±0.1</cell></row><row><cell>DPGN</cell><cell cols="2">ConvNet 76.4±0.5</cell><cell>88.4±0.4</cell></row><row><cell>Shot-Free [30]</cell><cell cols="2">ResNet12 69.2±0.4</cell><cell>84.7±0.4</cell></row><row><cell cols="3">MetaOptNet [21] ResNet12 72.0±0.7</cell><cell>84.2±0.5</cell></row><row><cell>DPGN</cell><cell cols="2">ResNet12 77.9±0.5</cell><cell>90.2±0.4</cell></row><row><cell cols="4">labeled and unlabeled parts with different ratios. For a 20%</cell></row><row><cell cols="4">labeled semi-supervised scenario, we split the support sam-</cell></row><row><cell cols="4">ples with a ratio of 0.2/0.8 for labeled and unlabeled data</cell></row><row><cell cols="4">in each class. In semi-supervised few-shot learning, DPGN</cell></row><row><cell cols="4">uses unlabeled support samples to explicitly construct sim-</cell></row><row><cell cols="4">ilarity distributions over all other samples and the distribu-</cell></row><row><cell cols="4">tions work as a connection between queries and labeled sup-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Trasductive/non-transductive experiments on miniImageNet. "BN" means information is shared among test examples using batch normalization. † denotes that it is implemented by public code released by authors.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04552</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning mixtures of gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual Symposium on Foundations of Computer Science (Cat. No. 99CB37039)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning poisson binomial distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="316" to="357" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic fewshot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficiently learning mixtures of two gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Adam Tauman Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-second ACM symposium on Theory of computing</title>
		<meeting>the forty-second ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the learnability of discrete distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronitt</forename><surname>Rubinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Sellie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9258" to="9267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05257</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04398</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Label distribution learning forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="834" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial age estimation by conditional probability neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06549</idno>
		<title level="m">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2365" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
