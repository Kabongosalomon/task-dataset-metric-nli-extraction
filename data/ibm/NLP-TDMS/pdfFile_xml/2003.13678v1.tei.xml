<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Designing Network Design Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Designing Network Design Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular Effi-cientNet models while being up to 5× faster on GPUs.</p><p>The general strategy we adopt is to progressively design simplified versions of an initial, relatively unconstrained, design space while maintaining or improving its quality ( <ref type="figure">Figure 1</ref>). The overall process is analogous to manual design, elevated to the population level and guided via distribution estimates of network design spaces <ref type="bibr" target="#b21">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exploring network structure (e.g., width, depth, groups, etc.) assuming standard model families including VGG [26], ResNet [8], and ResNeXt [31]. We start with a relatively unconstrained design space we call AnyNet (e.g., widths and depths vary freely across stages) and apply our humanin-the-loop methodology to arrive at a low-dimensional design space consisting of simple "regular" networks, that we call RegNet. The core of the RegNet design space is simple: stage widths and depths are determined by a quantized linear function. Compared to AnyNet, the RegNet design space has simpler models, is easier to interpret, and has a higher concentration of good models.</p><p>We design the RegNet design space in a low-compute, low-epoch regime using a single network block type on Im-ageNet <ref type="bibr" target="#b2">[3]</ref>. We then show that the RegNet design space generalizes to larger compute regimes, schedule lengths, and network block types. Furthermore, an important property of the design space design is that it is more interpretable and can lead to insights that we can learn from. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. For example, we find that the depth of the best models is stable across compute regimes (∼20 blocks) and that the best models do not use either a bottleneck or inverted bottleneck.</p><p>We compare top REGNET models to existing networks in various settings. First, REGNET models are surprisingly effective in the mobile regime. We hope that these simple models can serve as strong baselines for future work. Next, REGNET models lead to considerable improvements over standard RESNE(X)T <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b31">31]</ref> models in all metrics. We highlight the improvements for fixed activations, which is of high practical interest as the number of activations can strongly influence the runtime on accelerators such as GPUs. Next, we compare to the state-of-the-art EFFICIENT-NET [29] models across compute regimes. Under comparable training settings and flops, REGNET models outperform EFFICIENTNET models while being up to 5× faster on GPUs. We further test generalization on ImageNetV2 <ref type="bibr" target="#b24">[24]</ref>.</p><p>We note that network structure is arguably the simplest form of a design space design one can consider. Focusing on designing richer design spaces (e.g., including operators) may lead to better networks. Nevertheless, the structure will likely remain a core component of such design spaces.</p><p>In order to facilitate future research we will release all code and pretrained models introduced in this work. 2 2 https://github.com/facebookresearch/pycls 2. Related Work Manual network design. The introduction of AlexNet [13] catapulted network design into a thriving research area. In the following years, improved network designs were proposed; examples include VGG [26], Inception [27, 28], ResNet [8], ResNeXt [31], DenseNet [11], and Mo-bileNet <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b25">25]</ref>. The design process behind these networks was largely manual and focussed on discovering new design choices that improve accuracy e.g., the use of deeper models or residuals. We likewise share the goal of discovering new design principles. In fact, our methodology is analogous to manual design but performed at the design space level.</p><p>Automated network design. Recently, the network design process has shifted from a manual exploration to more automated network design, popularized by NAS. NAS has proven to be an effective tool for finding good models, e.g., <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29]</ref>. The majority of work in NAS focuses on the search algorithm, i.e., efficiently finding the best network instances within a fixed, manually designed search space (which we call a design space). Instead, our focus is on a paradigm for designing novel design spaces. The two are complementary: better design spaces can improve the efficiency of NAS search algorithms and also lead to existence of better models by enriching the design space.</p><p>Network scaling. Both manual and semi-automated network design typically focus on finding best-performing network instances for a specific regime (e.g., number of flops comparable to ResNet-50). Since the result of this procedure is a single network instance, it is not clear how to adapt the instance to a different regime (e.g., fewer flops). A common practice is to apply network scaling rules, such as varying network depth [8], width [32], resolution <ref type="bibr" target="#b9">[9]</ref>, or all three jointly <ref type="bibr" target="#b29">[29]</ref>. Instead, our goal is to discover general design principles that hold across regimes and allow for efficient tuning for the optimal network in any target regime.</p><p>Comparing networks. Given the vast number of possible network design spaces, it is essential to use a reliable comparison metric to guide our design process. Recently, the authors of [21] proposed a methodology for comparing and analyzing populations of networks sampled from a design space. This distribution-level view is fully-aligned with our goal of finding general design principles. Thus, we adopt this methodology and demonstrate that it can serve as a useful tool for the design space design process.</p><p>Parameterization. Our final quantized linear parameterization shares similarity with previous work, e.g. how stage widths are set <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b9">9]</ref>. However, there are two key differences. First, we provide an empirical study justifying the design choices we make. Second, we give insights into structural design choices that were not previously understood (e.g., how to set the number of blocks in each stages).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks are the engine of visual recognition. Over the past several years better architectures have resulted in considerable progress in a wide range of visual recognition tasks. Examples include LeNet <ref type="bibr" target="#b15">[15]</ref>, AlexNet <ref type="bibr" target="#b13">[13]</ref>, VGG <ref type="bibr" target="#b26">[26]</ref>, and ResNet <ref type="bibr" target="#b8">[8]</ref>. This body of work advanced both the effectiveness of neural networks as well as our understanding of network design. In particular, the above sequence of works demonstrated the importance of convolution, network and data size, depth, and residuals, respectively. The outcome of these works is not just particular network instantiations, but also design principles that can be generalized and applied to numerous settings.</p><p>While manual network design has led to large advances, finding well-optimized networks manually can be challenging, especially as the number of design choices increases. A popular approach to address this limitation is neural architecture search (NAS). Given a fixed search space of possible A B C B C A <ref type="figure">Figure 1</ref>. Design space design. We propose to design network design spaces, where a design space is a parametrized set of possible model architectures. Design space design is akin to manual network design, but elevated to the population level. In each step of our process the input is an initial design space and the output is a refined design space of simpler or better models. Following <ref type="bibr" target="#b21">[21]</ref>, we characterize the quality of a design space by sampling models and inspecting their error distribution. For example, in the figure above we start with an initial design space A and apply two refinement steps to yield design spaces B then C. In this case C ⊆ B ⊆A (left), and the error distributions are strictly improving from A to B to C (right). The hope is that design principles that apply to model populations are more likely to be robust and generalize.</p><p>networks, NAS automatically finds a good model within the search space. Recently, NAS has received a lot of attention and shown excellent results <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29]</ref>. Despite the effectiveness of NAS, the paradigm has limitations. The outcome of the search is a single network instance tuned to a specific setting (e.g., hardware platform). This is sufficient in some cases; however, it does not enable discovery of network design principles that deepen our understanding and allow us to generalize to new settings. In particular, our aim is to find simple models that are easy to understand, build upon, and generalize.</p><p>In this work, we present a new network design paradigm that combines the advantages of manual design and NAS. Instead of focusing on designing individual network instances, we design design spaces that parametrize populations of networks. <ref type="bibr" target="#b0">1</ref> Like in manual design, we aim for interpretability and to discover general design principles that describe networks that are simple, work well, and generalize across settings. Like in NAS, we aim to take advantage of semi-automated procedures to help achieve these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Design Space Design</head><p>Our goal is to design better networks for visual recognition. Rather than designing or searching for a single best model under specific settings, we study the behavior of populations of models. We aim to discover general design principles that can apply to and improve an entire model population. Such design principles can provide insights into network design and are more likely to generalize to new settings (unlike a single model tuned for a specific scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type="bibr" target="#b21">[21]</ref>. A design space is a large, possibly infinite, population of model architectures. The core insight from <ref type="bibr" target="#b21">[21]</ref> is that we can sample models from a design space, giving rise to a model distribution, and turn to tools from classical statistics to analyze the design space. We note that this differs from architecture search, where the goal is to find the single best model from the space.</p><p>In this work, we propose to design progressively simplified versions of an initial, unconstrained design space. We refer to this process as design space design. Design space design is akin to sequential manual network design, but elevated to the population level. Specifically, in each step of our design process the input is an initial design space and the output is a refined design space, where the aim of each design step is to discover design principles that yield populations of simpler or better performing models.</p><p>We begin by describing the basic tools we use for design space design in §3.1. Next, in §3.2 we apply our methodology to a design space, called AnyNet, that allows unconstrained network structures. In §3.3, after a sequence of design steps, we obtain a simplified design space consisting of only regular network structures that we name RegNet. Finally, as our goal is not to design a design space for a single setting, but rather to discover general principles of network design that generalize to new settings, in §3.4 we test the generalization of the RegNet design space to new settings.</p><p>Relative to the AnyNet design space, the RegNet design space is: (1) simplified both in terms of its dimension and type of network configurations it permits, (2) contains a higher concentration of top-performing models, and (3) is more amenable to analysis and interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tools for Design Space Design</head><p>We begin with an overview of tools for design space design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type="bibr" target="#b21">[21]</ref>, who propose to quantify the quality of a design space by sampling a set of models from that design space and characterizing the resulting model error distribution. The key intuition behind this approach is that comparing distributions is more robust and informative than using search (manual or automated) and comparing the best found models from two design spaces.  <ref type="figure">Figure 2</ref>. Statistics of the AnyNetX design space computed with n = 500 sampled models. Left: The error empirical distribution function (EDF) serves as our foundational tool for visualizing the quality of the design space. In the legend we report the min error and mean error (which corresponds to the area under the curve).</p><p>Middle: Distribution of network depth d (number of blocks) versus error. Right: Distribution of block widths in the fourth stage (w4) versus error. The blue shaded regions are ranges containing the best models with 95% confidence (obtained using an empirical bootstrap), and the black vertical line the most likely best value.</p><p>To obtain a distribution of models, we sample and train n models from a design space. For efficiency, we primarily do so in a low-compute, low-epoch training regime. In particular, in this section we use the 400 million flop 3 (400MF) regime and train each sampled model for 10 epochs on the ImageNet dataset <ref type="bibr" target="#b2">[3]</ref>. We note that while we train many models, each training run is fast: training 100 models at 400MF for 10 epochs is roughly equivalent in flops to training a single ResNet-50 <ref type="bibr" target="#b8">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type="bibr" target="#b21">[21]</ref>, our primary tool for analyzing design space quality is the error empirical distribution function (EDF). The error EDF of n models with errors e i is given by:</p><formula xml:id="formula_0">F (e) = 1 n n i=1 1[e i &lt; e].<label>(1)</label></formula><p>F (e) gives the fraction of models with error less than e. We show the error EDF for n = 500 sampled models from the AnyNetX design space (described in §3.2) in <ref type="figure">Figure 2</ref> (left). Given a population of trained models, we can plot and analyze various network properties versus network error, see <ref type="figure">Figure 2</ref> (middle) and (right) for two examples taken from the AnyNetX design space. Such visualizations show 1D projections of a complex, high-dimensional space, and can help obtain insights into the design space. For these plots, we employ an empirical bootstrap 4 <ref type="bibr" target="#b4">[5]</ref> to estimate the likely range in which the best models fall.</p><p>To summarize: (1) we generate distributions of models obtained by sampling and training n models from a design space, (2) we compute and plot error EDFs to summarize design space quality, (3) we visualize various properties of a design space and use an empirical bootstrap to gain insight, and (4) we use these insights to refine the design space.  <ref type="figure">Figure 3</ref>. General network structure for models in our design spaces. (a) Each network consists of a stem (stride-two 3×3 conv with w0 = 32 output channels), followed by the network body that performs the bulk of the computation, and then a head (average pooling followed by a fully connected layer) that predicts n output classes. (b) The network body is composed of a sequence of stages that operate at progressively reduced resolution ri. (c) Each stage consists of a sequence of identical blocks, except the first block which uses stride-two conv. While the general structure is simple, the total number of possible network configurations is vast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The AnyNet Design Space</head><p>We next introduce our initial AnyNet design space. Our focus is on exploring the structure of neural networks assuming standard, fixed network blocks (e.g., residual bottleneck blocks). In our terminology the structure of the network includes elements such as the number of blocks (i.e. network depth), block widths (i.e. number of channels), and other block parameters such as bottleneck ratios or group widths. The structure of the network determines the distribution of compute, parameters, and memory throughout the computational graph of the network and is key in determining its accuracy and efficiency.</p><p>The basic design of networks in our AnyNet design space is straightforward. Given an input image, a network consists of a simple stem, followed by the network body that performs the bulk of the computation, and a final network head that predicts the output classes, see <ref type="figure">Figure 3a</ref>. We keep the stem and head fixed and as simple as possible, and instead focus on the structure of the network body that is central in determining network compute and accuracy.</p><p>The network body consists of 4 stages operating at progressively reduced resolution, see <ref type="figure">Figure 3b</ref> (we explore varying the number of stages in §3.4). Each stage consists of a sequence of identical blocks, see <ref type="figure">Figure 3c</ref>. In total, for each stage i the degrees of freedom include the number of blocks d i , block width w i , and any other block parameters. While the general structure is simple, the total number of possible networks in the AnyNet design space is vast.</p><p>Most of our experiments use the standard residual bottlenecks block with group convolution <ref type="bibr" target="#b31">[31]</ref>, shown in <ref type="figure">Figure 4</ref>. We refer to this as the X block, and the AnyNet design space built on it as AnyNetX (we explore other blocks in §3.4). While the X block is quite rudimentary, we show it can be surprisingly effective when network structure is optimized. <ref type="figure">Figure 4</ref>. The X block is based on the standard residual bottleneck block with group convolution <ref type="bibr" target="#b31">[31]</ref>. (a) Each X block consists of a 1×1 conv, a 3×3 group conv, and a final 1×1 conv, where the 1×1 convs alter the channel width. BatchNorm <ref type="bibr" target="#b12">[12]</ref> and ReLU follow each conv. The block has 3 parameters: the width wi, bottleneck ratio bi, and group width gi. (b) The stride-two (s = 2) version.</p><formula xml:id="formula_1">1×1, s=2 w i , r i , r i ⨁ w i , r i , r i w i /b i , r i , r i w i /b i , r i , r i 1×1, s=1 3×3, g i , s=1 1×1, s=1 (a) X block, s=1 w i , r i , r i ⨁ w i-1 , 2r i , 2r i w i /b i , 2r i , 2r i w i /b i , r i , r i 1×1, s=1 3×3, g i , s=2 1×1, s=1 (b) X block, s=2</formula><p>The AnyNetX design space has 16 degrees of freedom as each network consists of 4 stages and each stage i has 4 parameters: the number of blocks d i , block width w i , bottleneck ratio b i , and group width g i . We fix the input resolution r = 224 unless otherwise noted. To obtain valid models, we perform log-uniform sampling of d i ≤ 16, w i ≤ 1024 and divisible by 8, b i ∈ {1, 2, 4}, and g i ∈ {1, 2, . . . , 32} (we test these ranges later). We repeat the sampling until we obtain n = 500 models in our target complexity regime (360MF to 400MF), and train each model for 10 epochs. <ref type="bibr" target="#b4">5</ref> Basic statistics for AnyNetX were shown in <ref type="figure">Figure 2</ref>.</p><p>There are (16·128·3·6) 4 ≈ 10 18 possible model configurations in the AnyNetX design space. Rather than searching for the single best model out of these ∼10 18 configurations, we explore whether there are general design principles that can help us understand and refine this design space. To do so, we apply our approach of designing design spaces. In each step of this approach, our aims are: 1. to simplify the structure of the design space, 2. to improve the interpretability of the design space, 3. to improve or maintain the design space quality, 4. to maintain model diversity in the design space. We now apply this approach to the AnyNetX design space.</p><p>AnyNetXA. For clarity, going forward we refer to the initial, unconstrained AnyNetX design space as AnyNetXA.</p><p>AnyNetXB. We first test a shared bottleneck ratio b i = b for all stages i for the AnyNetXA design space, and refer to the resulting design space as AnyNetXB. As before, we sample and train 500 models from AnyNetXB in the same settings. The EDFs of AnyNetXA and AnyNetXB, shown in <ref type="figure" target="#fig_1">Figure 5</ref> (left), are virtually identical both in the average and best case. This indicates no loss in accuracy when coupling the b i . In addition to being simpler, the AnyNetXB is more amenable to analysis, see for example <ref type="figure" target="#fig_1">Figure 5</ref> (right). <ref type="bibr" target="#b4">5</ref> Our training setup in §3 exactly follows <ref type="bibr" target="#b21">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 on 1 GPU, and a half-period cosine schedule with initial learning rate of 0.05 and weight decay of 5·10 −5 . Ten epochs are usually sufficient to give robust population statistics.    AnyNetXC. Our second refinement step closely follows the first. Starting with AnyNetXB, we additionally use a shared group width g i = g for all stages to obtain AnyNetXC. As before, the EDFs are nearly unchanged, see <ref type="figure" target="#fig_1">Figure 5</ref> (middle). Overall, AnyNetXC has 6 fewer degrees of freedom than AnyNetXA, and reduces the design space size nearly four orders of magnitude. Interestingly, we find g &gt; 1 is best (not shown); we analyze this in more detail in §4.</p><p>AnyNetXD. Next, we examine typical network structures of both good and bad networks from AnyNetXC in <ref type="figure" target="#fig_3">Figure 6</ref>. A pattern emerges: good network have increasing widths. We test the design principle of w i+1 ≥ w i , and refer to the design space with this constraint as AnyNetXD. In <ref type="figure">Figure 7</ref> (left) we see this improves the EDF substantially. We return to examining other options for controlling width shortly.</p><p>AnyNetXE. Upon further inspection of many models (not shown), we observed another interesting trend. In addition to stage widths w i increasing with i, the stage depths d i likewise tend to increase for the best models, although not necessarily in the last stage. Nevertheless, we test a design space variant AnyNetXE with d i+1 ≥ d i in <ref type="figure">Figure 7</ref> (right), and see it also improves results. Finally, we note that the constraints on w i and d i each reduce the design space by 4!, with a cumulative reduction of O(10 7 ) from AnyNetXA.  <ref type="figure">Figure 7</ref>. AnyNetXD (left) and AnyNetXE (right). We show various constraints on the per stage widths wi and depths di. In both cases, having increasing wi and di is beneficial, while using constant or decreasing values is much worse. Note that AnyNetXD = AnyNetXC + wi+1 ≥ wi, and AnyNetXE = AnyNetXD + di+1 ≥ di. We explore stronger constraints on wi and di shortly.  <ref type="figure">Figure 8</ref>. Linear fits. Top networks from the AnyNetX design space can be well modeled by a quantized linear parameterization, and conversely, networks for which this parameterization has a higher fitting error efit tend to perform poorly. See text for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The RegNet Design Space</head><p>To gain further insight into the model structure, we show the best 20 models from AnyNetXE in a single plot, see While there is significant variance in the individual models (gray curves), in the aggregate a pattern emerges. In particular, in the same plot we show the line w j = 48·(j+1) for 0 ≤ j ≤ 20 (solid black curve, please note that the y-axis is logarithmic). Remarkably, this trivial linear fit seems to explain the population trend of the growth of network widths for top models. Note, however, that this linear fit assigns a different width w j to each block, whereas individual models have quantized widths (piecewise constant functions).</p><p>To see if a similar pattern applies to individual models, we need a strategy to quantize a line to a piecewise constant function. Inspired by our observations from AnyNetXD and AnyNetXE, we propose the following approach. First, we introduce a linear parameterization for block widths:</p><formula xml:id="formula_2">u j = w 0 + w a · j for 0 ≤ j &lt; d<label>(2)</label></formula><p>This parameterization has three parameters: depth d, initial width w 0 &gt; 0, and slope w a &gt; 0, and generates a different block width u j for each block j &lt; d. To quantize u j , we introduce an additional parameter w m &gt; 0 that controls quantization as follows. First, given u j from Eqn. <ref type="formula" target="#formula_2">(2)</ref>, we compute s j for each block j such that the following holds:</p><formula xml:id="formula_3">u j = w 0 · w sj m</formula><p>(3) Then, to quantize u j , we simply round s j (denoted by s j ) and compute quantized per-block widths w j via:</p><formula xml:id="formula_4">w j = w 0 · w sj m (4)</formula><p>We can convert the per-block w j to our per-stage format by simply counting the number of blocks with constant width, that is, each stage i has block width w i = w 0 ·w i m and number of blocks</p><formula xml:id="formula_5">d i = j 1[ s j = i].</formula><p>When only considering four stage networks, we ignore the parameter combinations that give rise to a different number of stages.</p><p>We test this parameterization by fitting to models from AnyNetX. In particular, given a model, we compute the fit by setting d to the network depth and performing a grid search over w 0 , w a and w m to minimize the mean log-ratio (denoted by e fit ) of predicted to observed per-block widths. Results for two top networks from AnyNetXE are shown in <ref type="figure">Figure 8</ref> (top-right). The quantized linear fits (dashed curves) are good fits of these best models (solid curves).</p><p>Next, we plot the fitting error e fit versus network error for every network in AnyNetXC through AnyNetXE in <ref type="figure">Figure</ref>  <ref type="bibr">8 (bottom)</ref>. First, we note that the best models in each design space all have good linear fits. Indeed, an empirical bootstrap gives a narrow band of e fit near 0 that likely contains the best models in each design space. Second, we note that on average, e fit improves going from AnyNetXC to AnyNetXE, showing that the linear parametrization naturally enforces related constraints to w i and d i increasing.</p><p>To further test the linear parameterization, we design a design space that only contains models with such linear structure. In particular, we specify a network structure via 6 parameters: d, w 0 , w a , w m (and also b, g). Given these, we generate block widths and depths via Eqn. (2)- <ref type="bibr" target="#b3">(4)</ref>. We refer to the resulting design space as RegNet, as it contains only simple, regular models. We sample d &lt; 64, w 0 , w a &lt; 256, 1.5 ≤ w m ≤ 3 and b and g as before (ranges set based on e fit on AnyNetXE).</p><p>The error EDF of RegNetX is shown in <ref type="figure">Figure 9</ref> (left). Models in RegNetX have better average error than AnyNetX while maintaining the best models. In <ref type="figure">Figure 9</ref> (middle) we test two further simplifications. First, using w m = 2 (doubling width between stages) slightly improves the EDF, but we note that using w m ≥ 2 performs better (shown later). Second, we test setting w 0 = w a , further  simplifying the linear parameterization to u j = w a · (j + 1). Interestingly, this performs even better. However, to maintain the diversity of models, we do not impose either restriction. Finally, in <ref type="figure">Figure 9</ref> (right) we show that random search efficiency is much higher for RegNetX; searching over just ∼32 random models is likely to yield good models. <ref type="table" target="#tab_4">Table 1</ref> shows a summary of the design space sizes (for RegNet we estimate the size by quantizing its continuous parameters). In designing RegNetX, we reduced the dimension of the original AnyNetX design space from 16 to 6 dimensions, and the size nearly 10 orders of magnitude. We note, however, that RegNet still contains a good diversity of models that can be tuned for a variety of settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Design Space Generalization</head><p>We designed the RegNet design space in a low-compute, low-epoch training regime with only a single block type. However, our goal is not to design a design space for a single setting, but rather to discover general principles of network design that can generalize to new settings.</p><p>In <ref type="figure" target="#fig_6">Figure 10</ref>, we compare the RegNetX design space to AnyNetXA and AnyNetXE at higher flops, higher epochs, with 5-stage networks, and with various block types (described in the appendix). In all cases the ordering of the design spaces is consistent, with RegNetX &gt; AnyNetXE &gt; AnyNetXA. In other words, we see no signs of overfitting. These results are promising because they show RegNet can generalize to new settings. The 5-stage results show the regular structure of RegNet can generalize to more stages, where AnyNetXA has even more degrees of freedom. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analyzing the RegNetX Design Space</head><p>We next further analyze the RegNetX design space and revisit common deep network design choices. Our analysis yields surprising insights that don't match popular practice, which allows us to achieve good results with simple models.</p><p>As the RegNetX design space has a high concentration of good models, for the following results we switch to sampling fewer models (100) but training them for longer (25 epochs) with a learning rate of 0.1 (see appendix). We do so to observe more fine-grained trends in network behavior.</p><p>RegNet trends. We show trends in the RegNetX parameters across flop regimes in <ref type="figure">Figure 11</ref>. Remarkably, the depth of best models is stable across regimes (top-left), with an optimal depth of ∼20 blocks (60 layers). This is in contrast to the common practice of using deeper models for higher flop regimes. We also observe that the best models use a bottleneck ratio b of 1.0 (top-middle), which effectively removes the bottleneck (commonly used in practice). Next, we observe that the width multiplier w m of good models is ∼2.5 (top-right), similar but not identical to the popular recipe of doubling widths across stages. The remaining parameters (g, w a , w 0 ) increase with complexity (bottom).  <ref type="figure">Figure 13</ref>. We refine RegNetX using various constraints (see text). The constrained variant (C) is best across all flop regimes while being more efficient in terms of parameters and activations. <ref type="bibr" target="#b32">32</ref> 34 Complexity analysis. In addition to flops and parameters, we analyze network activations, which we define as the size of the output tensors of all conv layers (we list complexity measures of common conv operators in <ref type="figure">Figure 12</ref>, top-left). While not a common measure of network complexity, activations can heavily affect runtime on memory-bound hardware accelerators (e.g., GPUs, TPUs), for example, see  <ref type="figure">Figure 12</ref> (bottom), we observe that for the best models in the population, activations increase with the square-root of flops, parameters increase linearly, and runtime is best modeled using both a linear and a square-root term due to its dependence on both flops and activations.</p><p>RegNetX constrained. Using these findings, we refine the RegNetX design space. First, based on <ref type="figure">Figure 11</ref> (top), we set b = 1, d ≤ 40, and w m ≥ 2. Second, we limit parameters and activations, following <ref type="figure">Figure 12</ref> (bottom). This yields fast, low-parameter, low-memory models without affecting accuracy. In <ref type="figure">Figure 13</ref>, we test RegNetX with theses constraints and observe that the constrained version is superior across all flop regimes. We use this version in §5, and further limit depth to 12 ≤ d ≤ 28 (see also Appendix D).</p><p>Alternate design choices. Modern mobile networks often employ the inverted bottleneck (b &lt; 1) proposed in <ref type="bibr" target="#b25">[25]</ref> along with depthwise conv <ref type="bibr" target="#b0">[1]</ref> (g = 1). In <ref type="figure">Figure 14</ref> (left), we observe that the inverted bottleneck degrades the EDF slightly and depthwise conv performs even worse relative to b = 1 and g ≥ 1 (see appendix for further analysis). Next, motivated by <ref type="bibr" target="#b29">[29]</ref> who found that scaling the input image resolution can be helpful, we test varying resolution in <ref type="figure">Figure 14</ref> (middle). Contrary to <ref type="bibr" target="#b29">[29]</ref>, we find that for RegNetX a fixed resolution of 224×224 is best, even at higher flops.</p><p>SE. Finally, we evaluate RegNetX with the popular Squeeze-and-Excitation (SE) op <ref type="bibr" target="#b10">[10]</ref> (we abbreviate X+SE as Y and refer to the resulting design space as RegNetY). In <ref type="figure">Figure 14</ref> (right), we see that RegNetY yields good gains.   <ref type="figure" target="#fig_1">Figure 15</ref>. Top REGNETX models. We measure inference time for 64 images on an NVIDIA V100 GPU; train time is for 100 epochs on 8 GPUs with the batch size listed. Network diagram legends contain all information required to implement the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison to Existing Networks</head><p>We now compare top models from the RegNetX and RegNetY design spaces at various complexities to the stateof-the-art on ImageNet <ref type="bibr" target="#b2">[3]</ref>. We denote individual models using small caps, e.g. REGNETX. We also suffix the models with the flop regime, e.g. 400MF. For each flop regime, we pick the best model from 25 random settings of the RegNet parameters (d, g, w m , w a , w 0 ), and re-train the top model 5 times at 100 epochs to obtain robust error estimates.</p><p>Resulting top REGNETX and REGNETY models for each flop regime are shown in <ref type="figure" target="#fig_1">Figures 15 and 16</ref>, respectively. In addition to the simple linear structure and the trends we analyzed in §4, we observe an interesting pattern. Namely, the higher flop models have a large number of blocks in the third stage and a small number of blocks in the last stage. This is similar to the design of standard RESNET models. Moreover, we observe that the group width g increases with complexity, but depth d saturates for large models.</p><p>Our goal is to perform fair comparisons and provide simple and easy-to-reproduce baselines. We note that along with better architectures, much of the recently reported gains in network performance are based on enhancements to the training setup and regularization scheme (see <ref type="table" target="#tab_15">Table 7</ref>). As our focus is on evaluating network architectures, we perform carefully controlled experiments under the same training setup. In particular, to provide fair comparisons to classic work, we do not use any training-time enhancements.  <ref type="figure" target="#fig_3">Figure 16</ref>. Top REGNETY models (Y=X+SE). The benchmarking setup and the figure format is the same as in <ref type="figure" target="#fig_1">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">State-of-the-Art Comparison: Mobile Regime</head><p>Much of the recent work on network design has focused on the mobile regime (∼600MF). In <ref type="table">Table 2</ref>, we compare REGNET models at 600MF to existing mobile networks. We observe that REGNETS are surprisingly effective in this regime considering the substantial body of work on finding better mobile networks via both manual design <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b19">19]</ref> and NAS <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>We emphasize that REGNET models use our basic 100 epoch schedule with no regularization except weight decay, while most mobile networks use longer schedules with various enhancements, such as deep supervision <ref type="bibr" target="#b16">[16]</ref>, Cutout <ref type="bibr" target="#b3">[4]</ref>, DropPath <ref type="bibr" target="#b14">[14]</ref>, AutoAugment <ref type="bibr" target="#b1">[2]</ref>, and so on. As such, we hope our strong results obtained with a short training schedule without enhancements can serve as a simple baseline for future work.  <ref type="table">Table 2</ref>. Mobile regime. We compare existing models using originally reported errors to RegNet models trained in a basic setup. Our simple RegNet models achieve surprisingly good results given the effort focused on this regime in the past few years.  <ref type="table">Table 3</ref>. RESNE(X)T comparisons. (a) Grouped by activations, REGNETX show considerable gains (note that for each group GPU inference and training times are similar). (b) REGNETX models outperform RESNE(X)T models under fixed flops as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Standard Baselines Comparison: ResNe(X)t</head><p>Next, we compare REGNETX to standard RESNET <ref type="bibr" target="#b8">[8]</ref> and <ref type="bibr">RESNEXT</ref>  <ref type="bibr" target="#b31">[31]</ref> models. All of the models in this experiment come from the exact same design space, the former being manually designed, the latter being obtained through design space design. For fair comparisons, we compare REGNET and RESNE(X)T models under the same training setup (our standard REGNET training setup). We note that this results in improved RESNE(X)T baselines and highlights the importance of carefully controlling the training setup.</p><p>Comparisons are shown in <ref type="figure">Figure 17</ref> and <ref type="table">Table 3</ref>. Overall, we see that REGNETX models, by optimizing the network structure alone, provide considerable improvements under all complexity metrics. We emphasize that good REG-NET models are available across a wide range of compute regimes, including in low-compute regimes where good RESNE(X)T models are not available. <ref type="table">Table 3a</ref> shows comparisons grouped by activations (which can strongly influence runtime on accelerators such as GPUs). This setting is of particular interest to the research community where model training time is a bottleneck and will likely have more real-world use cases in the future, especially as accelerators gain more use at inference time (e.g., in self-driving cars). REGNETX models are quite effective given a fixed inference or training time budget.  is about 5× faster than EFFICIENTNET-B5. Note that originally reported errors for EFFICIENTNET (shown grayed out), are much lower but use longer and enhanced training schedules, see <ref type="table" target="#tab_15">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">State-of-the-Art Comparison: Full Regime</head><p>We focus our comparison on EFFICIENTNET <ref type="bibr" target="#b29">[29]</ref>, which is representative of the state of the art and has reported impressive gains using a combination of NAS and an interesting model scaling rule across complexity regimes.</p><p>To enable direct comparisons, and to isolate gains due to improvements solely of the network architecture, we opt to reproduce the exact EFFICIENTNET models but using our standard training setup, with a 100 epoch schedule and no regularization except weight decay (effect of longer schedule and stronger regularization are shown in <ref type="table" target="#tab_15">Table 7</ref>). We optimize only lr and wd, see <ref type="figure">Figure 22</ref> in appendix. This is the same setup as REGNET and enables fair comparisons.</p><p>Results are shown in <ref type="figure">Figure 18</ref> and <ref type="table" target="#tab_11">Table 4</ref>. At low flops, EFFICIENTNET outperforms the REGNETY. At intermediate flops, REGNETY outperforms EFFICIENTNET, and at higher flops both REGNETX and REGNETY perform better.</p><p>We also observe that for EFFICIENTNET, activations scale linearly with flops (due to the scaling of both resolution and depth), compared to activations scaling with the square-root of flops for REGNETs. This leads to slow GPU training and inference times for EFFICIENTNET. E.g., REGNETX-8000 is 5× faster than EFFICIENTNET-B5, while having lower error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present a new network design paradigm. Our results suggest that designing network design spaces is a promising avenue for future research.   <ref type="table">Table 6</ref>. EFFICIENTNET comparisons on ImageNetV2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Test Set Evaluation</head><p>In the main paper we perform all experiments on the Im-ageNet <ref type="bibr" target="#b2">[3]</ref> validation set. Here we evaluate our models on the ImageNetV2 <ref type="bibr" target="#b24">[24]</ref> test set (original test set unavailable).</p><p>Evaluation setup. To study generalization of models developed on ImageNet, the authors of <ref type="bibr" target="#b24">[24]</ref> collect a new test set following the original procedure (ImageNetV2). They find that the overall model ranks are preserved on the new test set. The absolute errors, however, increase. We repeat the comparisons from §5 on the ImageNetV2 test set. RESNE(X)T comparisons. We compare to RESNE(X)T models in <ref type="table" target="#tab_13">Table 5</ref>. We observe that while model ranks are generally consistent, the gap between them decreases. Nevertheless, REGNETX models still compare favorably, and provide good models across flop regimes, including in lowcompute regimes where good RESNE(X)T models are not available. Best results can be achieved using REGNETY.</p><p>EFFICIENTNET comparisons. We compare to EFFICIENT-NET models in <ref type="table">Table 6</ref>. As before, we observe that the model ranks are generally consistent but the gap decreases. Overall, the results confirm that the REGNET models perform comparably to state-of-the-art EFFICIENTNET while being up to 5× faster on GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Additional Ablations</head><p>In this section we perform additional ablations to further support or supplement the results of the main text. Fixed depth. In §5 we observed that the depths of our top models are fairly stable (∼20 blocks). In <ref type="figure">Figure 19</ref> (left) we compare using fixed depth (d = 20) across flop regimes. To compare to our best results, we trained each model for 100 epochs. Surprisingly, we find that fixed-depth networks can match the performance of variable depth networks for all flop regimes, in both the average and best case. Indeed, these fixed depth networks match our best results in §5.</p><p>Fewer stages. In §5 we observed that the top REGNET models at high flops have few blocks in the fourth stage (one or two). Hence we tested 3 stage networks at 6.4GF, trained for 100 epochs each. In <ref type="figure">Figure 19</ref> (middle), we show the results and observe that the three stage networks perform considerably worse. We note, however, that additional changes (e.g., in the stem or head) may be necessary for three stage networks to perform well (left for future work).</p><p>Inverted Bottleneck. In §4 we observed that using the inverted bottleneck (b &lt; 1) degrades performance. Since our results were in a low-compute regime, in <ref type="figure">Figure 19</ref> (right) we re-test at 6.4GF and 100 epochs. Surprisingly, we find that in this regime b &lt; 1 degrades results further.</p><p>Swish vs. ReLU Many recent methods employ the Swish <ref type="bibr" target="#b22">[22]</ref> activation function, e.g. <ref type="bibr" target="#b29">[29]</ref>. In <ref type="figure">Figure 20</ref>, we study RegNetY with Swish and ReLU. We find that Swish outperforms ReLU at low flops, but ReLU is better at high flops. Interestingly, if g is restricted to be 1 (depthwise conv), Swish performs much better than ReLU. This suggests that depthwise conv and Swish interact favorably, although the underlying reason is not at all clear.  <ref type="figure">Figure 21</ref>. Optimization settings. For these results, we generate a population of RegNetX models while also randomly varying the initial learning rate (lr) and weight decay (wd) for each model. These results use a batch size of 128 and are trained on 1 GPU.</p><p>Top: Distribution of model error versus lr, wd, and also lr·wd (at 10 epochs and 400MF). Applying an empirical bootstrap, we see that clear trends emerge, especially for lr and lr·wd. Middle:</p><p>We repeat this experiment but across various flop regimes (trained for 10 epochs each); the trends are stable. Bottom: Similarly, we repeat the above while training for various number of epochs (in the 400MF regime), and observe the same trends. Based on these results, we use an lr = 0.1 and wd = 5·10 −5 starting with §4 across all training schedules and flop regimes.  <ref type="figure">Figure 22</ref>. We repeat the sweep over lr and wd for EFFICIENT-NET training each model for 25 epochs. The lr (with reference to a batch size of 128) and wd is stable across regimes. We use these values for all EFFICIENTNET experiments in the main text (adjusting for batch size accordingly). See <ref type="figure">Figure 21</ref> for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type="bibr" target="#b21">[21]</ref> as discussed in §3. To tune the learning rate lr and weight decay wd for REG-NET models, we perform a study, described in <ref type="figure">Figure 21</ref>. Based on this, we set lr = 0.1 and wd = 5·10 −5 for all models in §4 and §5. To enable faster training of our final models at 100 epochs, we increase the number of GPUs to 8, while keeping the number of images per GPU fixed. When scaling the batch size, we adjust lr using the linear scaling rule and apply 5 epoch gradual warmup <ref type="bibr" target="#b5">[6]</ref>.</p><p>To enable fair comparisons, we repeat the same optimization for EFFICIENTNET in <ref type="figure">Figure 22</ref>. Interestingly, learning rate and weight decay are again stable across complexity regimes. Finally, in <ref type="table" target="#tab_15">Table 7</ref> we report the sizable effect of training enhancement on EFFICIENTNET-B0. The gap may be even larger for larger models (see <ref type="table" target="#tab_11">Table 4</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Implementation Details</head><p>We conclude with additional implementation details.</p><p>Group width compatibility. When sampling widths w and groups widths g for our models, we may end up with incompatible values (i.e. w not divisible by g). To address this, we employ a simple strategy. Namely, we set g = w if g &gt; w and round w to be divisible by g otherwise. The final w can be at most 1/3 different from the original w (proof omitted). For models with bottlenecks, we apply this strategy to the bottleneck width instead (and adjust widths accordingly).</p><p>Group width ranges. As discussed in §4, we notice the general trend that the group widths of good models are larger in higher compute regimes. To account for this, we gradually adjust the group width ranges for higher compute regimes. For example, instead of sampling g ≤ 32, at 3.2GF we use 16 ≤ g ≤ 64 and allow any g divisible by 8.</p><p>Block types. In §3, we showed that the RegNet design space generalizes to different block types. We describe these additional block types, shown in <ref type="figure" target="#fig_13">Figure 23</ref>, next:</p><p>1. R block: same as the X block except without groups, 2. V block: a basic block with only a single 3×3 conv, 3. VR block: same as V block plus residual connections.</p><p>We note that good parameter values may differ across block types. E.g., in contrast to the X block, for the R block using b &gt; 1 is better than b = 1. Our approach is robust to this.</p><p>Y block details. To obtain the Y block, we add the SE op after the 3×3 conv of the X block, and we use an SE reduction ratio of 1/4. We experimented with these choices but found that they performed comparably (not shown).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>AnyNetXB (left) and AnyNetXC (middle) introduce a shared bottleneck ratio bi = b and shared group width gi = g, respectively. This simplifies the design spaces while resulting in virtually no change in the error EDFs. Moreover, AnyNetXB and AnyNetXC are more amendable to analysis. Applying an empirical bootstrap to b and g we see trends emerge, e.g., with 95% confidence b ≤ 2 is best in this regime(right). No such trends are evident in the individual bi and gi in AnyNetXA (not shown).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Example good and bad AnyNetXC networks, shown in the top and bottom rows, respectively. For each network, we plot the width wj of every block j up to the network depth d. These per-block widths wj are computed from the per-stage block depths di and block widths wi (listed in the legends for reference).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>wi = [64, 128, 272, 704] g = 4, b = 2, e = 38.7% wa = 30, w0 = 80, wm = 1wi = [32, 144, 256, 448] g = 16, b = 1, e = 39.7% wa = 61, w0 = 48, wm = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 8 (top-left). For each model, we plot the per-block width w j of every block j up to the network depth d (we use i and j to index over stages and blocks, respectively). See Figure 6 for reference of our model visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>RegNetX generalization. We compare RegNetX to AnyNetX at higher flops (top-left), higher epochs (top-middle), with 5-stage networks (top-right), and with various block types (bottom). In all cases the ordering of the design spaces is consistent and we see no signs of design space overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .w 2 r 2 w 2 wr 2 3×3 conv 3 2 w 2 r 2 3 2 w 2 wr 2 3×3 gr conv 3 2 wgr 2 3 2 wg wr 2 3×3 dw conv 3 2 wr 2 3 2 w wr 2 0Figure 12 .</head><label>11222212</label><figDesc>RegNetX parameter trends. For each parameter and each flop regime we apply an empirical bootstrap to obtain the range that contains best models with 95% confidence (shown with blue shading) and the likely best model (black line), see also Figure 2. We observe that for best models the depths d are remarkably stable across flops regimes, and b = 1 and wm ≈ 2.5 are best. Block and groups widths (wa, w0, g) tend to increase with flops. flops params acts.1×1 conv Complexity metrics. Top: Activations can have a stronger correlation to runtime on hardware accelerators than flops (we measure inference time for 64 images on an NVIDIA V100 GPU). Bottom: Trend analysis of complexity vs. flops and best fit curves (shown in blue) of the trends for best models (black curves).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 12 (top). In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 23 .</head><label>23</label><figDesc>Block types used in our generalization experiments (see §3.4 and Figure 10). Left: Block diagrams. Right: Comparison of RegNet with the four block types. The X block performs best. Interestingly, for the V block, residual connections give no gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>38.9|49.4] AnyNetXC [38.7|43.2] + wi+1 wi [47.5|56.1] + wi+1 = wi [52.2|63.2] + wi+1 wi 38.7|43.2] AnyNetXD [38.7|42.7] + di+1 di [39.0|46.6] + di+1 = di [41.0|48.8] + di+1 di</figDesc><table><row><cell>cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>40</cell><cell>50</cell><cell>60 error</cell><cell>70</cell><cell>cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>40</cell><cell>45</cell><cell>50 error</cell><cell>55</cell></row></table><note>[[</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>38.2|41.0] RegNetX [38.0|40.7] RegNetX wm = 2 [38.2|40.1] RegNetX w0 = wa RegNetX design space. See text for details.</figDesc><table><row><cell>cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>40 45 50 55 60 65 error [39.0|49.0] AnyNetXA [38.7|42.7] AnyNetXE [38.2|41.0] RegNetX cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>38</cell><cell>40</cell><cell>42 error</cell><cell>44</cell><cell>46</cell><cell>error</cell><cell>40 45 50 55</cell><cell>2</cell><cell>8 sample size 32 AnyNetXA 128 RegNetX</cell></row><row><cell></cell><cell></cell><cell>Figure 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>[</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Design space summary. See text for details.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>restriction</cell><cell></cell><cell>dim.</cell><cell>combinations</cell><cell>total</cell></row><row><cell></cell><cell></cell><cell cols="3">AnyNetXA</cell><cell>none</cell><cell></cell><cell>16</cell><cell>(16·128·3·6) 4</cell><cell>∼1.8·10 18</cell></row><row><cell></cell><cell></cell><cell cols="3">AnyNetXB</cell><cell>+ bi+1 = bi</cell><cell></cell><cell>13</cell><cell>(16·128·6) 4 ·3</cell><cell>∼6.8·10 16</cell></row><row><cell></cell><cell></cell><cell cols="3">AnyNetXC</cell><cell>+ gi+1 = gi</cell><cell></cell><cell>10</cell><cell>(16·128) 4 ·3·6</cell><cell>∼3.2·10 14</cell></row><row><cell></cell><cell></cell><cell cols="3">AnyNetXD</cell><cell cols="2">+ wi+1 ≥ wi</cell><cell>10</cell><cell>(16·128) 4 ·3·6/(4!)</cell><cell>∼1.3·10 13</cell></row><row><cell></cell><cell></cell><cell cols="3">AnyNetXE</cell><cell cols="2">+ di+1 ≥ di</cell><cell>10</cell><cell>(16·128) 4 ·3·6/(4!) 2</cell><cell>∼5.5·10 11</cell></row><row><cell></cell><cell></cell><cell cols="3">RegNet</cell><cell cols="2">quantized linear</cell><cell>6</cell><cell>∼64 4 ·6·3</cell><cell>∼3.0·10 8</cell></row><row><cell>cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>35</cell><cell>40</cell><cell cols="2">45 [35.8|44.5] AnyNetXA 50 55 60 flops=800M [35.1|38.5] AnyNetXE [34.6|36.8] RegNetX</cell><cell cols="2">30 35 40 45 50 55 60 epochs=50 [30.0|38.8] AnyNetXA [30.0|32.5] AnyNetXE [29.4|31.5] RegNetX</cell><cell>40 45 50 55 60 65 70 stages=5 [40.4|49.9] AnyNetXA [38.4|42.8] AnyNetXE [37.9|41.4] RegNetX</cell></row><row><cell>cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell cols="3">45 50 55 60 65 70 error [42.9|53.1] AnyNetRA [42.0|45.7] AnyNetRE [41.9|44.3] RegNetR</cell><cell>50</cell><cell>60 error [47.0|61.2] AnyNetVA 70 80 [46.8|56.4] AnyNetVE [46.0|49.2] RegNetV</cell><cell>50</cell><cell>55 [48.1|58.9] AnyNetVRA 60 65 error 70 [47.4|53.3] AnyNetVRE [46.6|49.3] RegNetVR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>= 1) is even worse. Middle: Varying resolution r harms results. Right: RegNetY (Y=X+SE) improves the EDF.</figDesc><table><row><cell>cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>36 [30.7|32.6] b = 1, g 32 38 error [30.5|33.3] b 1, g 32 [33.2|35.5] b 1, g = 1 cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>30</cell><cell>35 error [30.7|32.6] 400MF r = 224 40 [31.2|34.1] 400MF r 448 [28.0|29.3] 800MF r = 224 [28.2|30.8] 800MF r 448 cumulative prob.</cell><cell>45 0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>30</cell><cell>31</cell><cell>32 error [30.7|32.6] RegNetX 33 34 [29.8|31.3] RegNetY</cell></row><row><cell cols="10">Figure 14. We evaluate RegNetX with alternate design choices.</cell></row><row><cell cols="10">Left: Inverted bottleneck ( 1 8 ≤ b ≤ 1) degrades results and depth-</cell></row><row><cell cols="3">wise conv (g</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>di = [1, 1, 4, 7] wi = [24, 56, 152, 368] g = 8, b = 1, e = 30.8% wa = 36, w0 = 24, wm = 2.5 = [1, 2, 7, 12] wi = [32, 64, 160, 384] g = 16, b = 1, e = 27.2% wa = 24, w0 = 24, wm = 2.5 = 24, b = 1, e = 25.5% wa = 37, w0 = 48, wm = 2.2 = 16, b = 1, e = 24.8% wa = 36, w0 = 56, wm = 2.3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>512 1024</cell><cell></cell><cell>RegNetX-400MF</cell><cell></cell><cell>512 1024</cell><cell cols="2">RegNetX-600MF</cell><cell>512 1024</cell><cell>RegNetX-800MF</cell></row><row><cell></cell><cell></cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell>256</cell><cell></cell><cell>256</cell></row><row><cell>width</cell><cell>512 1024 2048 32 64 128 256</cell><cell cols="8">0 3 6 9 12 15 18 21 RegNetX-3.2GF g 0 2 4 6 8 10 12 14 16 16 32 64 128 0 2 4 6 8 10 12 14 16 32 64 128 di = [1, 3, 5, 7] wi = [48, 96, 240, 528] g 0 2 4 6 8 10 12 14 16 32 64 128 di = [1, 3, 7, 5] wi = [64, 128, 288, 672] RegNetX-1.6GF 512 1024 2048 512 1024 2048 RegNetX-4.0GF 512 1024 2048 RegNetX-6.4GF di = [2, 4, 10, 2] wi = [72, 168, 408, 912] g = 24, b = 1, e = 22.9% wa = 34, w0 = 80, wm = 2.2 0 3 6 9 12 15 18 21 24 32 64 128 256 di = [2, 6, 15, 2] wi = [96, 192, 432, 1008] g = 48, b = 1, e = 21.6% wa = 26, w0 = 88, wm = 2.2 0 3 6 9 12 15 18 21 32 64 128 256 di = [2, 5, 14, 2] wi = [80, 240, 560, 1360] g = 40, b = 1, e = 21.3% wa = 39, w0 = 96, wm = 2.4 0 2 4 6 8 10 12 14 16 32 64 128 256 di = [2, 4, 10, 1] wi = [168, 392, 784, 1624] g = 56, b = 1, e = 20.7% wa = 61, w0 = 184, wm = 2.1</cell></row><row><cell></cell><cell>2048 4096</cell><cell>RegNetX-8.0GF</cell><cell>2048 4096</cell><cell></cell><cell>RegNetX-12GF</cell><cell></cell><cell>2048 4096</cell><cell cols="2">RegNetX-16GF</cell><cell>2048 4096</cell><cell>RegNetX-32GF</cell></row><row><cell></cell><cell>1024</cell><cell></cell><cell>1024</cell><cell></cell><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell>1024</cell></row><row><cell>width</cell><cell>64 128 256 512</cell><cell>0 3 6 9 12 15 18 21 block index di = [2, 5, 15, 1] wi = [80, 240, 720, 1920] g = 120, b = 1, e = 20.5% wa = 50, w0 = 80, wm = 2.9</cell><cell>64 128 256 512</cell><cell cols="3">0 2 4 6 8 10 12 14 16 18 block index di = [2, 5, 11, 1] wi = [224, 448, 896, 2240] g = 112, b = 1, e = 20.3% wa = 73, w0 = 168, wm = 2.4</cell><cell>64 128 256 512</cell><cell cols="2">0 3 6 9 12 15 18 21 block index di = [2, 6, 13, 1] wi = [256, 512, 896, 2048] g = 128, b = 1, e = 20.0% wa = 56, w0 = 216, wm = 2.1</cell><cell>64 128 256 512</cell><cell>block index 0 3 6 9 12 15 18 21 di = [2, 7, 13, 1] wi = [336, 672, 1344, 2520] g = 168, b = 1, e = 19.5% wa = 70, w0 = 320, wm = 2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">flops</cell><cell>params</cell><cell cols="2">acts</cell><cell>batch</cell><cell>infer</cell><cell>train</cell><cell>error</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(B)</cell><cell>(M)</cell><cell cols="2">(M)</cell><cell>size</cell><cell>(ms)</cell><cell>(hr)</cell><cell>(top-1)</cell></row><row><cell cols="3">REGNETX-200MF</cell><cell cols="2">0.2</cell><cell>2.7</cell><cell></cell><cell>2.2</cell><cell>1024</cell><cell>10</cell><cell>2.8</cell><cell>31.1±0.09</cell></row><row><cell cols="3">REGNETX-400MF</cell><cell cols="2">0.4</cell><cell>5.2</cell><cell></cell><cell>3.1</cell><cell>1024</cell><cell>15</cell><cell>3.9</cell><cell>27.3±0.15</cell></row><row><cell cols="3">REGNETX-600MF</cell><cell cols="2">0.6</cell><cell>6.2</cell><cell></cell><cell>4.0</cell><cell>1024</cell><cell>17</cell><cell>4.4</cell><cell>25.9±0.03</cell></row><row><cell cols="3">REGNETX-800MF</cell><cell cols="2">0.8</cell><cell>7.3</cell><cell></cell><cell>5.1</cell><cell>1024</cell><cell>21</cell><cell>5.7</cell><cell>24.8±0.09</cell></row><row><cell cols="3">REGNETX-1.6GF</cell><cell cols="2">1.6</cell><cell>9.2</cell><cell></cell><cell>7.9</cell><cell>1024</cell><cell>33</cell><cell>8.7</cell><cell>23.0±0.13</cell></row><row><cell cols="3">REGNETX-3.2GF</cell><cell cols="2">3.2</cell><cell>15.3</cell><cell cols="2">11.4</cell><cell>512</cell><cell>57</cell><cell>14.3</cell><cell>21.7±0.08</cell></row><row><cell cols="3">REGNETX-4.0GF</cell><cell cols="2">4.0</cell><cell>22.1</cell><cell cols="2">12.2</cell><cell>512</cell><cell>69</cell><cell>17.1</cell><cell>21.4±0.19</cell></row><row><cell cols="3">REGNETX-6.4GF</cell><cell cols="2">6.5</cell><cell>26.2</cell><cell cols="2">16.4</cell><cell>512</cell><cell>92</cell><cell>23.5</cell><cell>20.8±0.07</cell></row><row><cell cols="3">REGNETX-8.0GF</cell><cell cols="2">8.0</cell><cell>39.6</cell><cell cols="2">14.1</cell><cell>512</cell><cell>94</cell><cell>22.6</cell><cell>20.7±0.07</cell></row><row><cell cols="3">REGNETX-12GF</cell><cell cols="2">12.1</cell><cell>46.1</cell><cell cols="2">21.4</cell><cell>512</cell><cell>137</cell><cell>32.9</cell><cell>20.3±0.04</cell></row><row><cell cols="3">REGNETX-16GF</cell><cell cols="2">15.9</cell><cell>54.3</cell><cell cols="2">25.5</cell><cell>512</cell><cell>168</cell><cell>39.7</cell><cell>20.0±0.11</cell></row><row><cell cols="3">REGNETX-32GF</cell><cell cols="2">31.7</cell><cell>107.8</cell><cell cols="2">36.3</cell><cell>256</cell><cell>318</cell><cell>76.9</cell><cell>19.5±0.12</cell></row></table><note>di</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>ResNe(X)t comparisons. REGNETX models versus RESNE(X)T-(50,101,152) under various complexity metrics. As all models use the identical components and training settings, all observed gains are from the design of the RegNetX design space.</figDesc><table><row><cell>error</cell><cell>21 22 23 24 25</cell><cell cols="2">RegNetX ResNeXt ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResNet ResNeXt RegNetX</cell><cell></cell><cell>ResNet ResNeXt RegNetX</cell></row><row><cell></cell><cell>0.8</cell><cell>1.6</cell><cell>3.2 flops (B)</cell><cell>6.4</cell><cell>12.8</cell><cell>8</cell><cell>16 params (M) 32</cell><cell>64</cell><cell cols="2">8 activations (M) 16</cell><cell>32</cell></row><row><cell cols="5">Figure 17. flops</cell><cell></cell><cell>params</cell><cell>acts</cell><cell>infer</cell><cell>train</cell><cell>top-1 error</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(B)</cell><cell></cell><cell>(M)</cell><cell>(M)</cell><cell>(ms)</cell><cell>(hr)</cell><cell>ours±std [orig]</cell></row><row><cell cols="3">RESNET-50</cell><cell></cell><cell>4.1</cell><cell></cell><cell>22.6</cell><cell>11.1</cell><cell>53</cell><cell>12.2</cell><cell>23.2±0.09 [23.9]</cell></row><row><cell cols="4">REGNETX-3.2GF</cell><cell>3.2</cell><cell></cell><cell>15.3</cell><cell>11.4</cell><cell>57</cell><cell>14.3</cell><cell>21.7±0.08</cell></row><row><cell cols="3">RESNEXT-50</cell><cell></cell><cell>4.2</cell><cell></cell><cell>25.0</cell><cell>14.4</cell><cell>78</cell><cell>18.0</cell><cell>21.9±0.10 [22.2]</cell></row><row><cell cols="3">RESNET-101</cell><cell></cell><cell>7.8</cell><cell></cell><cell>44.6</cell><cell>16.2</cell><cell>90</cell><cell>20.4</cell><cell>21.4±0.11 [22.0]</cell></row><row><cell cols="4">REGNETX-6.4GF</cell><cell>6.5</cell><cell></cell><cell>26.2</cell><cell>16.4</cell><cell>92</cell><cell>23.5</cell><cell>20.8±0.07</cell></row><row><cell cols="4">RESNEXT-101</cell><cell>8.0</cell><cell></cell><cell>44.2</cell><cell>21.2</cell><cell>137</cell><cell>31.8</cell><cell>20.7±0.08 [21.2]</cell></row><row><cell cols="3">RESNET-152</cell><cell></cell><cell>11.5</cell><cell></cell><cell>60.2</cell><cell>22.6</cell><cell>130</cell><cell>29.2</cell><cell>20.9±0.12 [21.6]</cell></row><row><cell cols="4">REGNETX-12GF</cell><cell>12.1</cell><cell></cell><cell>46.1</cell><cell>21.4</cell><cell>137</cell><cell>32.9</cell><cell>20.3±0.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Comparisons grouped by activations.</cell><cell></cell></row><row><cell cols="3">RESNET-50</cell><cell></cell><cell>4.1</cell><cell></cell><cell>22.6</cell><cell>11.1</cell><cell>53</cell><cell>12.2</cell><cell>23.2±0.09 [23.9]</cell></row><row><cell cols="3">RESNEXT-50</cell><cell></cell><cell>4.2</cell><cell></cell><cell>25.0</cell><cell>14.4</cell><cell>78</cell><cell>18.0</cell><cell>21.9±0.10 [22.2]</cell></row><row><cell cols="4">REGNETX-4.0GF</cell><cell>4.0</cell><cell></cell><cell>22.1</cell><cell>12.2</cell><cell>69</cell><cell>17.1</cell><cell>21.4±0.19</cell></row><row><cell cols="3">RESNET-101</cell><cell></cell><cell>7.8</cell><cell></cell><cell>44.6</cell><cell>16.2</cell><cell>90</cell><cell>20.4</cell><cell>21.4±0.11 [22.0]</cell></row><row><cell cols="4">RESNEXT-101</cell><cell>8.0</cell><cell></cell><cell>44.2</cell><cell>21.2</cell><cell>137</cell><cell>31.8</cell><cell>20.7±0.08 [21.2]</cell></row><row><cell cols="4">REGNETX-8.0GF</cell><cell>8.0</cell><cell></cell><cell>39.6</cell><cell>14.1</cell><cell>94</cell><cell>22.6</cell><cell>20.7±0.07</cell></row><row><cell cols="3">RESNET-152</cell><cell></cell><cell>11.5</cell><cell></cell><cell>60.2</cell><cell>22.6</cell><cell>130</cell><cell>29.2</cell><cell>20.9±0.12 [21.6]</cell></row><row><cell cols="4">RESNEXT-152</cell><cell>11.7</cell><cell></cell><cell>60.0</cell><cell>29.7</cell><cell>197</cell><cell>45.7</cell><cell>20.4±0.06 [21.1]</cell></row><row><cell cols="4">REGNETX-12GF</cell><cell>12.1</cell><cell></cell><cell>46.1</cell><cell>21.4</cell><cell>137</cell><cell>32.9</cell><cell>20.3±0.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Comparisons grouped by flops.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Figure 18. EFFICIENTNET comparisons. REGNETs outperform the state of the art, especially when considering activations.</figDesc><table><row><cell>error</cell><cell>22 24 26</cell><cell></cell><cell></cell><cell cols="2">EfficientNet RegNetX RegNetY</cell><cell></cell><cell></cell><cell></cell><cell cols="3">EfficientNet RegNetX RegNetY</cell><cell></cell><cell></cell><cell>EfficientNet RegNetX RegNetY</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell>0.8</cell><cell>1.6 flops (B) 3.2</cell><cell>6.4</cell><cell>12.8</cell><cell>4</cell><cell>8</cell><cell cols="2">16 params (M)</cell><cell>32</cell><cell>64</cell><cell>4</cell><cell cols="2">8 activations (M) 16 32</cell><cell>64</cell><cell>128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">flops params</cell><cell></cell><cell>acts</cell><cell cols="2">batch</cell><cell>infer</cell><cell></cell><cell>train</cell><cell>top-1 error</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(B)</cell><cell></cell><cell>(M)</cell><cell></cell><cell>(M)</cell><cell cols="2">size</cell><cell>(ms)</cell><cell></cell><cell>(hr)</cell><cell>ours±std [orig]</cell></row><row><cell cols="4">EFFICIENTNET-B0</cell><cell>0.4</cell><cell></cell><cell>5.3</cell><cell></cell><cell>6.7</cell><cell cols="2">256</cell><cell>34</cell><cell></cell><cell>11.7</cell><cell>24.9±0.03 [23.7]</cell></row><row><cell cols="4">REGNETY-400MF</cell><cell>0.4</cell><cell></cell><cell>4.3</cell><cell></cell><cell>3.9</cell><cell cols="2">1024</cell><cell>19</cell><cell></cell><cell>5.1</cell><cell>25.9±0.16</cell></row><row><cell cols="4">EFFICIENTNET-B1</cell><cell>0.7</cell><cell></cell><cell>7.8</cell><cell></cell><cell>10.9</cell><cell cols="2">256</cell><cell>52</cell><cell></cell><cell>15.6</cell><cell>24.1±0.16 [21.2]</cell></row><row><cell cols="4">REGNETY-600MF</cell><cell>0.6</cell><cell></cell><cell>6.1</cell><cell></cell><cell>4.3</cell><cell cols="2">1024</cell><cell>19</cell><cell></cell><cell>5.2</cell><cell>24.5±0.07</cell></row><row><cell cols="4">EFFICIENTNET-B2</cell><cell>1.0</cell><cell></cell><cell>9.2</cell><cell></cell><cell>13.8</cell><cell cols="2">256</cell><cell>68</cell><cell></cell><cell>18.4</cell><cell>23.4±0.06 [20.2]</cell></row><row><cell cols="4">REGNETY-800MF</cell><cell>0.8</cell><cell></cell><cell>6.3</cell><cell></cell><cell>5.2</cell><cell cols="2">1024</cell><cell>22</cell><cell></cell><cell>6.0</cell><cell>23.7±0.03</cell></row><row><cell cols="4">EFFICIENTNET-B3</cell><cell>1.8</cell><cell></cell><cell>12.0</cell><cell></cell><cell>23.8</cell><cell cols="2">256</cell><cell>114</cell><cell></cell><cell>32.1</cell><cell>22.5±0.05 [18.9]</cell></row><row><cell cols="4">REGNETY-1.6GF</cell><cell>1.6</cell><cell></cell><cell>11.2</cell><cell></cell><cell>8.0</cell><cell cols="2">1024</cell><cell>39</cell><cell></cell><cell>10.1</cell><cell>22.0±0.08</cell></row><row><cell cols="4">EFFICIENTNET-B4</cell><cell>4.2</cell><cell></cell><cell>19.0</cell><cell></cell><cell>48.5</cell><cell cols="2">128</cell><cell>240</cell><cell></cell><cell>65.1</cell><cell>21.2±0.06 [17.4]</cell></row><row><cell cols="4">REGNETY-4.0GF</cell><cell>4.0</cell><cell></cell><cell>20.6</cell><cell></cell><cell>12.3</cell><cell cols="2">512</cell><cell>68</cell><cell></cell><cell>16.8</cell><cell>20.6±0.08</cell></row><row><cell cols="4">EFFICIENTNET-B5</cell><cell>9.9</cell><cell></cell><cell>30.0</cell><cell></cell><cell>98.9</cell><cell cols="2">64</cell><cell>504</cell><cell></cell><cell>135.1</cell><cell>21.5±0.11 [16.7]</cell></row><row><cell cols="4">REGNETY-8.0GF</cell><cell>8.0</cell><cell></cell><cell>39.2</cell><cell></cell><cell>18.0</cell><cell cols="2">512</cell><cell>113</cell><cell></cell><cell>28.1</cell><cell>20.1±0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 .</head><label>4</label><figDesc>EFFICIENTNET comparisons using our standard training schedule. Under comparable training settings, REGNETY outperforms EFFICIENTNET for most flop regimes. Moreover, REGNET models are considerably faster, e.g., REGNETX-F8000</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 .</head><label>5</label><figDesc>RESNE(X)T comparisons on ImageNetV2.</figDesc><table><row><cell></cell><cell cols="2">flops params</cell><cell>acts</cell><cell cols="2">batch infer</cell><cell>train</cell><cell>error</cell></row><row><cell></cell><cell>(B)</cell><cell>(M)</cell><cell>(M)</cell><cell>size</cell><cell>(ms)</cell><cell>(hr)</cell><cell>(top-1)</cell></row><row><cell>EFFICIENTNET-B0</cell><cell>0.4</cell><cell>5.3</cell><cell>6.7</cell><cell>256</cell><cell>34</cell><cell>11.7</cell><cell>37.1±0.22</cell></row><row><cell>REGNETY-400MF</cell><cell>0.4</cell><cell>4.3</cell><cell>3.9</cell><cell>1024</cell><cell>19</cell><cell>5.1</cell><cell>38.3±0.26</cell></row><row><cell>EFFICIENTNET-B1</cell><cell>0.7</cell><cell>7.8</cell><cell>10.9</cell><cell>256</cell><cell>52</cell><cell>15.6</cell><cell>36.4±0.10</cell></row><row><cell>REGNETY-600MF</cell><cell>0.6</cell><cell>6.1</cell><cell>4.3</cell><cell>1024</cell><cell>19</cell><cell>5.2</cell><cell>36.9±0.17</cell></row><row><cell>EFFICIENTNET-B2</cell><cell>1.0</cell><cell>9.2</cell><cell>13.8</cell><cell>256</cell><cell>68</cell><cell>18.4</cell><cell>35.3±0.25</cell></row><row><cell>REGNETY-800MF</cell><cell>0.8</cell><cell>6.3</cell><cell>5.2</cell><cell>1024</cell><cell>22</cell><cell>6.0</cell><cell>35.7±0.40</cell></row><row><cell>EFFICIENTNET-B3</cell><cell>1.8</cell><cell>12.0</cell><cell>23.8</cell><cell>256</cell><cell>114</cell><cell>32.1</cell><cell>34.4±0.27</cell></row><row><cell>REGNETY-1.6GF</cell><cell>1.6</cell><cell>11.2</cell><cell>8.0</cell><cell>1024</cell><cell>39</cell><cell>10.1</cell><cell>33.9±0.19</cell></row><row><cell>EFFICIENTNET-B4</cell><cell>4.2</cell><cell>19.0</cell><cell>48.5</cell><cell>128</cell><cell>240</cell><cell>65.1</cell><cell>32.5±0.23</cell></row><row><cell>REGNETY-4.0GF</cell><cell>4.0</cell><cell>20.6</cell><cell>12.3</cell><cell>512</cell><cell>68</cell><cell>16.8</cell><cell>32.3±0.28</cell></row><row><cell>EFFICIENTNET-B5</cell><cell>9.9</cell><cell>30.0</cell><cell>98.9</cell><cell>64</cell><cell>504</cell><cell>135.1</cell><cell>31.5±0.17</cell></row><row><cell>REGNETY-8.0GF</cell><cell>8.0</cell><cell>39.2</cell><cell>18.0</cell><cell>512</cell><cell>113</cell><cell>28.1</cell><cell>31.3±0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Figure 19.Additional ablations. Left: Fixed depth networks (d = 20) are effective across flop regimes. Middle: Three stage networks perform poorly at high flops. Right: Inverted bottleneck (b &lt; 1) is also ineffective at high flops. See text for more context.Figure 20. Swish vs. ReLU. Left: RegNetY performs better with Swish than ReLU at 400MF but worse at 6.4GF. Middle: Results across wider flop regimes show similar trends. Right: If, however, g is restricted to be 1 (depthwise conv), Swish is much better.</figDesc><table><row><cell cols="2">error</cell><cell>20 22 24 26 28 30</cell><cell>0.2</cell><cell>0.4</cell><cell cols="2">0.8 flops (B) 1.6 12 d 28 3.2 6.4 d = 20 cumulative prob.</cell><cell cols="2">0.0 0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell cols="2">21.0 21.5 22.0 22.5 23.0 23.5 error [21.0|21.7] 3 stage [20.7|21.0] 4 stage cumulative prob.</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>error 20.8 21.0 21.2 21.4 21.6 [21.0|21.2] b = 1/2 [20.7|21.0] b = 1</cell></row><row><cell>cumulative prob.</cell><cell cols="2">0.0 0.2 0.4 0.6 0.8 1.0</cell><cell cols="3">22 24 26 28 30 32 34 error [29.8|31.3] 400MF ReLU [28.7|30.4] 400MF Swish [22.1|22.7] 6.4GF ReLU [22.6|23.0] 6.4GF Swish</cell><cell cols="2">error</cell><cell>22 24 26 28 30 32 34 36</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8 flops (B) 1.6 ReLU g 1 3.2 error 6.4 22 24 26 28 30 Swish g 1 32 34 36</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8 flops (B) 1.6 ReLU g = 1 3.2 Swish g = 1 6.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 .</head><label>7</label><figDesc>Training enhancements to EFFICIENTNET-B0. Our EFFICIENTNET-B0 reproduction with DropPath<ref type="bibr" target="#b14">[14]</ref> and a 250 epoch training schedule (third row), achieves results slightly inferior to original results (bottom row), which additionally used RM-SProp<ref type="bibr" target="#b30">[30]</ref>, AutoAugment<ref type="bibr" target="#b1">[2]</ref>, etc. Without these enhancements to the training setup results are ∼2% lower (top row), highlighting the importance of carefully controlling the training setup.</figDesc><table><row><cell></cell><cell cols="6">flops (B) params (M) acts (M) epochs enhance error</cell></row><row><cell>EFFICIENTNET-B0</cell><cell></cell><cell>0.39</cell><cell>5.3</cell><cell>6.7</cell><cell>100</cell><cell>25.6</cell></row><row><cell>EFFICIENTNET-B0</cell><cell></cell><cell>0.39</cell><cell>5.3</cell><cell>6.7</cell><cell>250</cell><cell>25.0</cell></row><row><cell>EFFICIENTNET-B0</cell><cell></cell><cell>0.39</cell><cell>5.3</cell><cell>6.7</cell><cell>250</cell><cell>24.4</cell></row><row><cell cols="2">EFFICIENTNET-B0 [29]</cell><cell>0.39</cell><cell>5.3</cell><cell>6.7</cell><cell>350</cell><cell>23.7</cell></row><row><cell>wi, ri, ri</cell><cell>wi, ri, ri</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>⨁</cell><cell>⨁</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1×1</cell><cell>1×1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wi/bi, ri, ri</cell><cell>wi/bi, ri, ri</cell><cell></cell><cell>wi, ri, ri</cell><cell></cell><cell></cell></row><row><cell>3×3, gi</cell><cell>3×3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>wi, ri, ri</cell><cell></cell><cell></cell></row><row><cell>wi/bi, ri, ri</cell><cell>wi/bi, ri, ri</cell><cell></cell><cell>⨁</cell><cell></cell><cell></cell></row><row><cell>1×1</cell><cell>1×1</cell><cell></cell><cell>3×3</cell><cell></cell><cell></cell></row><row><cell>wi, ri, ri</cell><cell>wi, ri, ri</cell><cell></cell><cell>wi, ri, ri</cell><cell></cell><cell></cell></row><row><cell>(a) X block</cell><cell cols="2">(b) R block</cell><cell>(d) VR block</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the term design space following<ref type="bibr" target="#b21">[21]</ref>, rather than search space, to emphasize that we are not searching for network instances within the space. Instead, we are designing the space itself.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Following common practice, we use flops to mean multiply-adds. Moreover, we use MF and GF to denote 10 6 and 10 9 flops, respectively.<ref type="bibr" target="#b3">4</ref> Given n pairs (x i , e i ) of model statistic x i (e.g. depth) and corresponding error e i , we compute the empirical bootstrap by: (1) sampling with replacement 25% of the pairs, (2) selecting the pair with min error in the sample, (3) repeating this 10 4 times, and finally (4) computing the 95% CI for the min x value. The median gives the most likely best value.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On network design spaces for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks. ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
