<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Parametric Continuous Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
							<email>slwang@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
							<email>suo@uber.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
							<email>andrei@uber.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Parametric Continuous Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-ofthe-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Discrete convolutions are the most fundamental building block of modern deep learning architectures. Its efficiency and effectiveness relies on the fact that the data appears naturally in a dense grid structure (e.g., 2D grid for images, 3D grid for videos). However, many real world applications such as visual perception from 3D point clouds, mesh registration and non-rigid shape correspondences rely on making statistical predictions from non-grid structured data. Unfortunately, standard convolutional operators cannot be directly applied in these cases.</p><p>Multiple approaches have been proposed to handle nongrid structured data. The simplest approach is to voxelize the space to form a grid where standard discrete convolutions can be performed <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24]</ref>. However, most of the volume is typically empty, and thus this results in both memory inefficiency and wasted computation. Geometric deep learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> and graph neural network approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> exploit the graph structure of the data and model the relationship between nodes. Information is then propagated through the graph edges. However, they either have difficulties generalizing well or require strong feature representations as input to perform competitively. End-to-end learning is typically performed via back-propagation through time, but it is difficult to learn very deep networks due to the memory limitations of modern GPUs.</p><p>In contrast to the aforementioned approaches, in this paper we propose a new learnable operator, which we call parametric continuous convolution. The key idea is a parameterized kernel function that spans the full continuous vector space. In this way, it can handle arbitrary data structures as long as its support relationship is computable. This is a natural extension since objects in the real-world such as point clouds captured from 3D sensors are distributed unevenly in continuous domain. Based upon this we build a new family of deep neural networks that can be applied on generic non-grid structured data. The proposed networks are both expressive and memory efficient.</p><p>We demonstrate the effectiveness of our approach in both semantic labeling and motion estimation of point clouds. Most importantly, we show that very deep networks can be learned over raw point clouds in an end-to-end manner. Our experiments show that the proposed approach outperforms the state-of-the-art by a large margin in both outdoor and indoor 3D point cloud segmentation tasks, as well as lidar motion estimation in driving scenes. Importantly, our outdoor semantic labeling and lidar flow experiments are conducted on a very large scale dataset, containing 223 billion points captured by a 3D sensor mounted on the roof of a self-driving car. To our knowledge, this is 2 orders of magnitude larger than any existing benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning for 3D Geometry: Deep learning approaches that exploit 3D geometric data have recently become populer in the computer vision community. Early approaches convert the 3D data into a two-dimensional RGB + depth image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref> and exploit conventional convolutional neural networks (CNNs). Unfortunately, this representation does not capture the true geometric relationships between 3D points (i.e. neighboring pixels could be potentially far away geometrically). Another popular approach is to conduct 3D convolutions over volumetric representations <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18]</ref>. Voxelization is employed to convert point clouds into a 3D grid that encodes the geo-metric information. These approaches have been popular in medical imaging and indoor scene understanding, where the volume is relatively small. However, typical voxelization approaches sacrifice precision and the 3D volumetric representation is not memory efficient. Sparse convolutions <ref type="bibr" target="#b8">[9]</ref> and advanced data structures such as oct-trees <ref type="bibr" target="#b23">[24]</ref> have been used to overcome these difficulties. Learning directly over point clouds has only been studied very recently. The pioneer work of PointNet <ref type="bibr" target="#b19">[20]</ref>, learns an MLP over individual points and aggregates global information using pooling. PointNet++ <ref type="bibr" target="#b21">[22]</ref>, the follow-up, improves the ability to capture local structures through a multi-scale grouping strategy.</p><p>Graph Neural Networks: Graph neural networks (GNNs) <ref type="bibr" target="#b24">[25]</ref> are generalizations of neural networks to graph structured data. Early approaches apply neural networks either over the hidden representation of each node or the messages passed between adjacent nodes in the graph, and use back-propagation through time to conduct learning. Gated graph neural networks (GGNNs) <ref type="bibr" target="#b15">[16]</ref> exploit gated recurrent units along with modern optimization techniques, resulting in improved performance. In <ref type="bibr" target="#b22">[23]</ref>, GGNNs are applied to point cloud segmentation, achieving significant improvements over the state-of-the-art. One of the major difficulties of graph neural networks is that propagation is conducted in a synchronous manner and thus it is hard to scale up to graphs with millions of nodes. Inference in graphical models as well as recurrent neural networks can be seen as special cases of graph neural networks.</p><p>Graph Convolution Networks: An alternative formulation is to learn convolution operations over graphs. These methods can be categorized into spectral and spatial approaches depending on which domain the convolutions are applied to. For spectral methods, convolutions are converted to multiplication by computing the graph Laplacian in Fourier domain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref>. Parameterized spectral filters can be incorporated to reduce overfitting <ref type="bibr" target="#b3">[4]</ref>. These methods are not feasible for large scale data due to the expensive computation, since there is no FFT-like trick over generic graph. Spatial approaches directly propagate information along the node neighborhoods in the graph. This can be implemented either through low-order approximation of spectral filtering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>, or diffusion in a support domain <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26]</ref>. Our approach generalizes spatial approaches in two ways: first, we use more expressive convolutional kernel functions; second, the output of the convolution could be any point in the whole continuous domain.</p><p>Other Approaches: Edge-conditioned filter networks <ref type="bibr" target="#b26">[27]</ref> use a weighting network to communicate between adjacent nodes on the graph <ref type="bibr" target="#b12">[13]</ref> conditioned on edge labels, which is primarily formulated as relative point locations. In  <ref type="figure">Figure 1</ref>: Unlike grid convolution, parametric continuous convolution uses kernel functions that are defined for arbitrary points in the continuous support domain. As a result, it is possible to output features at points not seen in the input.</p><p>contrast, our approach is not constrained to a fixed graph structure, and has the flexibility to output features at arbitrary points over the continuous domain. In a concurrent work, <ref type="bibr" target="#b25">[26]</ref> uses similar parametric function form f (x i −x j ) to aggregate information between points. However, they only use shallow isotropic gaussian kernels to represent the weights, while we use expressive deep networks to parameterize the continuous filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Parametric Continuous CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parametric Continuous Convolutions</head><p>Standard CNNs use discrete convolutions (i.e., convolutions defined over discrete domain) as basic operations.</p><formula xml:id="formula_0">h[n] = (f * g)[n] = M m=−M f [n − m]g[m]</formula><p>where f : G → R and g : S → R are functions defined over the support domain of finite integer set:</p><formula xml:id="formula_1">G = Z D and S = {−M, −M + 1, ..., M − 1, M } D respectively.</formula><p>In contrast, continuous convolutions can be defined as  Motivated by monte-carlo integration <ref type="bibr" target="#b4">[5]</ref> we derive our continuous convolution operator. In particular, given continuous functions f and g with a finite number of input points y i sampled from the domain, the convolution at an arbitrary point x can be approximated as:</p><formula xml:id="formula_2">h(x) = (f * g)(x) = ∞ −∞ f (y)g(x − y)dy<label>(1)</label></formula><formula xml:id="formula_3">h(x) = ∞ −∞ f (y)g(x − y)dy ≈ N i 1 N f (y i )g(x − y i )</formula><p>The next challenge we need to solve is constructing the continuous convolutional kernel function g. Conventional 2D and 3D discrete convolution kernels are parameterized in a way that each point in the support domain is assigned a value (i.e. the kernel weight). Such a parameterization is infeasible for continuous convolutions, since the kernel function g is defined over an infinite number of points (i.e., has infinite support). Instead, in this paper we propose to use parametric continuous functions to model g. We name our approach Parametric Continuous Convolutions. In particular, we use a multi-layer perceptron (MLP) as the approximator. With reference to the universal approximation theorem of <ref type="bibr" target="#b11">[12]</ref>, MLPs are expressive and capable of approximating continuous functions over R n . Thus we define:</p><formula xml:id="formula_4">g(z; θ) = M LP (z; θ)</formula><p>The kernel function g(z; θ) : R D → R spans the full continuous support domain while remaining parametrizable by a finite number of parameters. Note that other choices such as polynomials are possible, however low-order polynomials are not expressive, whereas learning high-order polynomials can be numerically unstable for back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From Convolutions to Deep Networks</head><p>In this section, we first design a new convolution layer based on the parametric continuous convolutions derived in the previous subsection. We then propose a deep learning architecture using this new convolution layer.</p><p>Parametric Continuous Convolution Layer: Note that, unlike standard discrete convolutions which are conducted over the same point set, the input and output points of our parametric continuous convolution layer can be different. This is important for many practical applications, where we want to make dense predictions based on partial observations. Furthermore, this allow us to abstract information from redundant input points (i.e., pooling). As a consequence, the input of each convolution layer contains three parts: the input feature vector F = {f in,j ∈ R F }, the associated locations in the support domain S = {y j }, as well as the output domain locations O = {x i }. For each layer, we first evaluate the kernel function g d,k (y i − x j ; θ) for all x j ∈ S and all y i ∈ O, given the parameters θ. Each element of the output feature vector is then computed as:</p><formula xml:id="formula_5">h k,i = F d N j g d,k (y i − x j )f d,j</formula><p>Let N be the number of input points, M be the number of output points, and D the dimensionality of the support domain. Let F and O be predefined input and output feature dimensions respectively. Note that these are hyperparameters of the continuous convolution layer analogous to input and output feature dimensions in standard grid convolution layers. <ref type="figure">Fig. 1</ref> depicts our parametric continuous convolutions in comparison with conventional grid convolution. Two major differences are highlighted: 1) the kernel function is continuous given the relative location in support domain; 2) the input/ouput points could be any points in the continuous domain as well and can be different.  can construct a new family of deep networks which operates on unstructured data defined in a topological group under addition. In the following discussions, we will focus on multi-diumensional euclidean space, and note that this is a special case. The network takes the input features and their associated positions in the support domain as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Parametric Continuous CNNs: Using the parametric continuous convolution layers as building blocks, we</head><p>Then the hidden representations are generated from successive parametric continuous convolution layers. Following standard CNN architectures, we can add batch normalization, non-linearities and residual connections between lay-ers. Pooling can also be employed over the support domain to aggregate information. In practice, we find adding residual connection between parametric continuous convolution layers is critical to help convergence. Please refer to <ref type="figure" target="#fig_1">Fig. 2</ref> for an example of the computation graph of a single layer, and to <ref type="figure">Fig. 3</ref> for an example of the network architecture employed for our indoor semantic segmentation task.</p><p>Learning: All of our building blocks are differentiable, thus our networks can be learned through back-prop: convolutions. If the support domain is defined as concatenation of the spatial vector and feature vector with a gaussian kernel g(·), we recover the bilateral filter. If the support domain is defined as the neighboring vertices of a node we recover the first-order spatial graph convolution <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_6">∂h ∂θ = ∂h ∂g · ∂g ∂θ = F d N j f d,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We demonstrate the effectiveness of our approach in the tasks of semantic labeling and motion estimation of 3D point clouds, and show state-of-the-art performance. We conduct point-wise semantic labeling experiments over two datasets: a very large-scale outdoor lidar semantic segmentation dataset that we collected and labeled in house and a large indoor semantic labeling dataset. To our knowledge, these are the largest real-world outdoor and indoor datasets that are available for this task. The datasets are fully labeled and contain 137 billion and 629 million points respectively. The lidar flow experiment is also conducted on this dataset with ground-truth 3D motion label for each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Segmentation of Indoor Scenes</head><p>Dataset: We use the Stanford large-scale 3D indoor scene dataset <ref type="bibr" target="#b0">[1]</ref> and follow the training and testing procedure used in <ref type="bibr" target="#b28">[29]</ref>. We report the same metrics, i.e., mean-IOU, mean class accuracy (TP / (TP + FN)) and class-wise IOU. The input is six dimensional and is composed of the xyz coordinates and RGB color intensity. Each point is labeled with one of 13 classes shown in Tab. 1.</p><p>Competing Algorithms: We compare our approach to PointNet <ref type="bibr" target="#b19">[20]</ref> and SegCloud <ref type="bibr" target="#b28">[29]</ref>. We evaluate the proposed end-to-end continuous convnet with eight continuous convolution layers (Ours PCCN). The kernels are defined over the continuous support domain of 3D Euclidean space. Each intermediate layer except the last has 32 dimensional hidden features followed by batchnorm and ReLU nonlinearity. The dimension of the last layer is 128. We observe that the distribution of semantic labels within a room is highly correlated with the room type (e.g. office, hallway, conference room, etc.). Motivated by this, we apply max pooling over all the points in the last layer to obtain a global feature, which is then concatenated to the output feature of each points in the last layer, resulting in a 256 dimensional feature. A fully connected layer with softmax activation is used to produce the final logits. Our network is trained endto-end with cross entropy loss, using Adam optimizer.</p><p>Results: As shown in Tab. 1 our approach outperforms the state-of-the-art by 9.3% mIOU and 9.6% mACC. <ref type="figure">Fig. 4</ref> shows qualitative results. Despite the diversity of geometric structures, our approach works very well. Confusion mainly occurs between columns vs walls and window vs bookcase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D-FCN</head><p>Ours PCCN Ours 3D-FCN+PCCN <ref type="figure">Figure 5</ref>: Semenatic Segmentation Results on Driving Scene Dataset; Colored: correct prediciton; white: wrong prediciton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6: Semantic Labeling on KITTI Dataset without Retraining</head><p>It is also worth noting that our approach captures visual information encoded in RGB channels. The last row shows two failure cases. In the first one, the door in the washroom is labeled as clutter whearas our algorithm thinks is door. In the second one, the board on the right has a window-like texture, which makes the algorithm predict the wrong label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Segmentation of Driving Scenes</head><p>Dataset: We first conduct experiments on the task of point cloud segmentation in the context of autonomous driving.</p><p>Each point cloud is produced by a full sweep of a roofmounted Velodyne-64 lidar sensor driving in several cities in North America. The dataset is composed of snippets each having 300 consecutive frames. The training and validation set contains 11,337 snippets in total while the test set contains 1,644 snippets. We report metrics on a subset of the test set which is generated by sampling 10 frames from each snippet to avoid bias brought due to scenes where the egocar is static (e.g., when waiting at a traffic light). Each point is labeled with one of seven classes defined in Tab. <ref type="bibr" target="#b1">2</ref>   Baselines: We compare our approach to the point cloud segmentation network (PointNet) <ref type="bibr" target="#b19">[20]</ref> and a 3D fully convolutional network (3D-FCN) conducted over a 3D occupancy grid. We use a resolution of 0.2m for each voxel over a 160mx80mx6.4m range. This results in an occupancy grid encoded as a tensor of size 800x400x32. We define a voxel to be occupied if it contains at least one point. We use ResNet-50 as the backbone and replace the last average pooling and fully connected layer with two fully convolutional layers and a trilinear upsampling layer to obtain dense voxel predictions. The model is trained from scratch with the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> to minimize the class-reweighted cross-entropy loss. Finally, the voxel-wise predictions are mapped back to the original points and metrics are computed over points.</p><p>We adapted the open-sourced PointNet model onto our dataset and trained from scratch. The architecture and loss function remain the same with the original paper, except that we removed the point rotation layer since it negatively impacts validation performance on this dataset.</p><p>Our Approaches: We evaluate two versions of our approach. Our first instance conducts continuous convolutions directly over the raw xyz-intensity lidar points (Ours PCCN). Our second version (Ours 3D-FCN+PCCN) performs continuous convolutions over the features extracted from 3D-FCN. Ours PCCN has 16 continuous conv layers with residual connections, batchnorm and ReLU nonlinearities. We use the spatial support in R 3 to define our kernel. We train the network with point-wise cross-entropy loss and Adam <ref type="bibr" target="#b13">[14]</ref> optimizer. In contrast, Ours 3D-FCN+PCCN model has 7 residual continuous convolutional layers on top of the trained 3D-FCN model and performs end-to-end fine-tuning using Adam optimizer.</p><p>Results: As shown in Tab. 2, by exploiting sophisticated feature via 3D convolutions, 3D-FCN+PCCN results in the best performance. <ref type="figure">Fig. 5</ref> shows qualitative comparison between models. As shown in the figure, all models produce good results. Performance differences often result from ambiguous regions. In particular, we can see that the 3D-FCN model oversegements the scene: it mislabels a background pole as vehicle (red above egocar), nearby spurirous points as bicyclist (green above egocar), and a wall as pedestrian (purple near left edge). This is reflected in the confidence map (as bright regions). We observe a significant improvement in our 3D-CNN + PCCN model, with all of the above corrected with high confidence. For more results and videos please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Sizes:</head><p>We also compare the model sizes of the competing algorithms in Tab. 2. In comparison to the 3D-FCN approach, the end-to-end continuous convolution network's model size is eight times smaller , while achieving comparable results. And the 3D-FCN+PCCN is just 0.01MB larger than 3D-FCN, but the performance is improved by a large margin in terms of mean IOU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity and Runtime</head><p>We benchmark the proposed model's runtime over a GTX 1080 Ti GPU and Xeon E5-2687W CPU with 32 GB Memory. The forward pass of a 8-layer PCCN model (32 feature dim in each layer with 50 neighbours) takes 33ms. The KD-Tree neighbour search takes 28 ms. The end-to-end computation takes 61ms. The number of operations of each layer is 1.32GFLOPs.</p><p>Generalization: To demonstrate the generalization ability of our approach, we evaluate our model, trained with only North American scenes, on the KITTI dataset <ref type="bibr" target="#b7">[8]</ref>, which was captured in Europe. As shown in <ref type="figure">Fig. 6</ref>, the model achieves good results, with well segmented dynamic objects, such as vehicles and pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Lidar Flow</head><p>Dataset: We also validate our proposed method over the task of lidar based motion estimation, refered to as lidar   <ref type="figure">Figure 8</ref>: Lidar Flow Results on Driving Scene Dataset flow. In this task, the input is two consecutive frames of lidar sweep. The goal is to estimation the 3D motion field for each point in the first frame, to undo both ego-motion and the motion of dynamic objects. The ground-truth egomotion is computed through a comprehensive filters that take GPS, IMU as well as ICP based lidar alignment against pre-scaned 3D geometry of the scene as input. And the ground-truth 6DOF dynamics object motion is estimated from the temporal coherent 3D object tracklet, labeled by in-house annotators. Combining both we are able to get the ground-truth motion field. <ref type="figure" target="#fig_3">Fig. 7</ref> shows the colormapped flow field and the overlay between two frames after undoing per-point motion. This task is crucial for many applications, such as multi-rigid transform alignment, object tracking, global pose estimation, etc. The training and validation set contains 11,337 snippets while the test set contains 1,644 snippets. We use 110k frame pairs for training and validation, and 16440 frame pairs for testing. End-point error, and outlier percentage at 10 cm and 20 cm are used as metric.</p><p>Competing Algorithms: We compare against the 3D-FCN baseline using the same architecture and volumetric representation as used in Sec. 4.2. We also adopt a similar 3D-FCN + PCCN architecture with 7 residual continuous convolution layers added as a polishing network. In this task, we remove the ReLU nonlinearity and supervise the PCCN layers with MSE loss at every layer. The training objective function is mean square error loss between the ground-truth flow vector and the prediction.   <ref type="figure">Fig. 18</ref> shows sample flow predictions compared with ground truth labels. As shown in the figure, our algorithm is able to capture both global motion of the ego-car including self rotation, and the motion of each dynamic objects in the scene. For more results please refer to our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a new learnable convolution layer that operates over non-grid structured data. Our convolution kernel function is parameterized by multi-layer perceptrons and spans the full continuous domain. This allows us to design a new deep learning architecture that can be applied to arbitrary structured data, as long as the support relationships between elements are computable. We validate the performance on point cloud segmentation and motion estimation tasks, over very large-scale datasets with up to 200 bilion points. The proposed network achieves state-of-the-art performance on all the tasks and datasets.</p><p>In this supplementary material, we first validate our method's generalization ability by testing the proposed models on KITTI dataset without re-training. We then provide more details about the lidar flow task. We also show additional results for all tasks, with analysis of failure modes. Lastly, we conduct a point cloud classification experiment using the deep parameteric continuous convolutional networks. In the supplementary video, we show our results on semantic segmentation and lidar flow results over a sequential data. We also show the generalization ability of the proposed method by training on our dataset and test it on KITTI as well as a truck sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generalization</head><p>We show the generalization ability of our proposed model by training over one dataset and test it over another in our supplementary video. To be specific, we have used the following configurations:</p><p>• Train our proposed semantic labeling network on the driving scene data (several north America cities), and test it on KITTI (Europe).</p><p>• Train our proposed semantic labeling network on the driving scene data (non-highway road), and test it on a lidar sequence mounted on top of a high-way driving truck (highway road).</p><p>• Train our proposed lidar flow network on the driving scene data (several north America cities), and test it on KITTI (Europe).</p><p>Under all the settings, our algorithm is able to generalize well. <ref type="figure">Fig. 9</ref> shows our lidar flow model's performance on KITTI. As shown in <ref type="figure">Fig. 9</ref>, our Lidar flow model generalizes well to the unseen KITTI dataset. From left to right, the figures show the most common scenarios with moving, turning, and stationary ego-car. The model produces covincing flow predictions in all three cases. <ref type="figure">Fig. 10</ref> includes some additional segmentation results on KITTI.</p><p>For more results over several sequences, please refer to our supplementary video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lidar Flow Data and Analysis</head><p>Ground-truth Generation In this paragraph, we describe how we generate the ground-truth lidar flow data in detail. For each consecutive frame, we first get the global vehicle pose transform from frame 0 to frame 1: R ego , t ego with the help of additional sensors and prior information. This global vehicle pose transform represents how far away the vehicle moves and how the vehicle turns. This localization accuracy is at centi-meter scale. Therefore, the motion per each static point is:</p><formula xml:id="formula_7">f (0) static−gt = R T ego (x (0) static − t ego ) − x (0) static where f (k)</formula><p>gt is the ground-truth flow at the frame k under the ego-car centered coordinate, x (k) is the point's location at the frame k under the ego-car centered coordinate.</p><p>For dynamic objects in the scene, e.g. other vehicles and pedestrians, the motion between each lidar frame in the vehicle coordinate is not only due to the self-driving car's ego-motion. The movement of the dynamic objects themselves are also contributing to the motion. In this project, we assume rigid motion for all the objects. The labeling of the dynamics objects include two steps. Firstly, using the global pose that we get, we visualize the point cloud of 3D objects from two frames in the same reference coordinate and label the pose changes R obj , t obj between the objects at the different time. Secondly, both ego-motion and object motion are considered in order to generate the ground-truth flow vector:</p><formula xml:id="formula_8">f (0) dynamic−gt = R T obj (R T ego (x (0) dynamic −t ego )−t obj )−x (0) dynamic</formula><p>Please refer to <ref type="figure" target="#fig_5">Fig. 11</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth Motion Analysis</head><p>We also conduct an analysis over the ground-truth motion distribution. In <ref type="figure" target="#fig_1">Fig. 12</ref> we show the 2D histogram of the GT 3D translation component along x and y axis respectively. We also show the motion distribution across different object types, e.g. static background, vehicle and pedestrian. As we can see, different semantic types have different motion patterns. And the heaviest density of distribution is on the y-axis, which suggests the forward motion is the major motion pattern of our ego-car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth Validation and Visualization</head><p>We validate the quality of our ground-truth motion labels by overlaying target frame points (x <ref type="bibr" target="#b0">(1)</ref> ) and source frame points warped with ground-truth motion (x (0) + f (0) gt ). <ref type="figure" target="#fig_7">Fig. 13</ref> shows overlays of entire scenes and <ref type="figure">Fig. 14</ref> shows overlays of individual dynamic objects. For vehicles, points align perfectly across frames. For pedestrians, the correspondence is also near perfect: the only discrepancy is caused by non-rigid motion (e.g. changing posture).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results</head><p>In this section, we show additional qualitative results of the proposed algorithm over all the tasks.  <ref type="figure">Fig. 15</ref> and <ref type="figure">Fig. 16</ref> show more qualitatitive results over the stanford dataset. As the figure shown, in most cases our model is able to predict the semantic labels correctly. <ref type="figure" target="#fig_3">Fig. 17</ref> shows additional results for semantic labeling in driving scenes. As shown, the results capture very small dynamics, e.g. pedestrians and bicyclists. This suggests our model's potential in object detection and tracking. The model is also able to distinguish between road and non-road through lidar intensity and subtle geometry structure such as road curbs. This validates our model's potential in map automation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Semantic Segmentation for Driving Scenes</head><p>More specifically, we see that most error occur on road boundaries (bright curves in error map).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Lidar Flow</head><p>We show additional results on Lidar flow estimation in <ref type="figure">Fig. 18</ref>. Unlike the visualization in the main submission, we visualize the colored vector in order to better depicts the magnitudes of the motion vector. As shown in the figure, our model is able to capture majority flow field. The majority of the error happens at the object boundary. This suggests that a better support domain that includes both space and intensity features could be potentially used to boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Activations</head><p>We visualize the activation maps of the trained PCCN network over a single lidar frame from the driving scene dataset for segmentation. <ref type="figure">Fig. 19</ref> depicts the activation map at layer 1 of PCCN. As we can see, at the early conv layer the method mainly captures low-level geometry details, e.g. the z coordinate, the intensity peak, etc. <ref type="figure" target="#fig_1">Fig. 20</ref> shows the activation map at layer 8 of PCCN. The conv layers begin to capture information with more semantic meaning, e.g. the road curb and the dynamic objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Point Cloud Classification</head><p>To verify the applicability of the proposed parameteric continuous convolution over global prediction task, we conduct a simple point cloud classification task on the Model-Net40 benchmark. This dataset contains CAD models from 40 categories. The state-of-the-art and most representative algorithms conducted on ModelNet40 are compared <ref type="bibr" target="#b27">[28]</ref>. We randomly sampled 2048 points for each training and testing sample over the 3D meshes and feed the point cloud into our neural network. The architecture contains 6 continuous convolution layers with 32-dimensional hidden features, followed by two layers with 128-dimensions and 512 dimensions respectively. The output of the last continuous convolution layer is fed into a max pooling layer to generate the global 512-dimensional feature, followed by two fc layers to output the final logits. Tab. 4 reports the classification   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Accuracy MVCNN <ref type="bibr" target="#b27">[28]</ref> Multi-view Image 90.1% 3DShapeNet <ref type="bibr" target="#b29">[30]</ref> Volume 84.7% VoxNet <ref type="bibr" target="#b17">[18]</ref> Volume 85.9% Subvolume <ref type="bibr" target="#b20">[21]</ref> Volume 89.2% ECC <ref type="bibr" target="#b26">[27]</ref> Point 87.4% PointNet vanilla <ref type="bibr" target="#b19">[20]</ref> Point 87.2% PointNet <ref type="bibr" target="#b19">[20]</ref> Point 89.2% PointNet++ <ref type="bibr" target="#b21">[22]</ref> Point 91.9% Ours Point 88.9%</p><p>performance. As we can see in the table, the performance is comparable with PointNet and slightly below PointNet++.</p><p>Here we use a naive global max pooling to aggregate global information for our method. We expect to achieve better results with more comprehensive and hierachical pooling strategies.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Detailed Computation Block for the Parametric Continuous Convolution Layer. possible for real-world applications, where the input features are complicated and non-parametric, and the observations are sparse points sampled over the continuous domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 4 :</head><label>34</label><figDesc>Architecture of the Deep Parametric Continuous CNNs for Semantic Labeling Task. Semenatic Segmentation Results on Stanford Indoor3D Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Right: purple shows target frame, yellow shows source frame warped to target frame using ground truth flow adopt mean intersection-over-union (meanIOU) and pointwise accuracy (pointAcc) as our evaluation metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Lidar Flow Results on KITTI Dataset Semantic Segmentation on KITTI Dataset C.1. Semantic Segmentation for Indoor Scenes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Flow data generation. The source of motion comes from two components: motion of the ego-car and motion of the dynamic objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>Ground-truth Motion Distribution of the Lidar Flow Dataset (unit in meters)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Flow Ground Truth Overlay of Entire Scene. Yellow: target frame, purple: warped source frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :</head><label>14151617181920</label><figDesc>Flow Ground Truth Overlay of Individual Dynamic Objects. Green: target frame, red: warped source frame. Input GT Ours Semantic Segmentation Results on Stanford Indoor Dataset Input GT Ours Semantic Segmentation Results on Stanford Indoor Dataset Ground Truth Ours Error Map Semantic Segmentation Results on Driving Scenes Dataset Ground Truth Ours Error Map Lidar Flow Results on Driving Scenes Dataset Activation Map of PCCN at Layer 1 Activation Map of PCCN at Layer 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This can be achieved in differently. First, we can constrain the cardinality of its local support domain and only keep nonzero kernel values for its K-nearest neighbors: w(z) = 1 z∈KNN(S,x) . Alternatively we can keep non-zero kernel values for points within a fixed radius r: w(z) = 1 ||z||2&lt;r .Efficient Continuous Convolution:For each continuous convolution layer, the kernel function is evaluated N ×|S|× F × O times, where |S| is the cardinality of the support domain, and the intermediate weight tensor is stored for backpropagation. This is expensive in practice, especially when both the number of points and the feature dimension are large. With the locality enforcing formulation, we can constrain the cardinality of S. Furthermore, motivated by the idea of separable filters, we use the fact that this computation can be factorized if the kernel function value across different output dimensionality is shared. That is to say, we can decompose the weight tensor W ∈ R N ×|S|×F ×O into two tensors W 1 = R F ×O and W 2 = R N ×|S|×O , where W 1 is a linear weight matrix and W 2 is evaluated through the MLP. With this optimization, only N × |S| × O kernel evaluations need to be computed and stored. Lastly, in inference stage, through merging the operations of batchnorm and fc layer in MLP, 3x speed boosting can be achieved.</figDesc><table><row><cell>j ·</cell><cell>∂g ∂θ</cell></row><row><cell>3.3. Discussions</cell><cell></cell></row></table><note>Locality Enforcing Continuous Convolution: Standard grid convolution are computed over a limited kernel size M to keep locality. Similarly, locality can be enforced in our parametric continuous convolutions by constraining the influence of the function g to points close to x, i.e., g(z) = M LP (z)w(z) where w(·) is a modulating window function.Special Cases: Many previous convolutional layers are special cases of our approach. For instance, if the points are sampled over the finite 2D grid we recover conventional 2D</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. We Method mIOU mAcc ceiling floor wall beam column window door chair table bookcase sofa board clutter PointNet [20] 41.09 48.98 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52.61 58.93 40.28 5.85 26.38 33.22 3D-FCN-TI [29] 47.46 54.91 90.17 96.48 70.16 0.00 11.40 33.36 21.12 76.12 70.07 57.89 37.46 11.16 41.61 SEGCloud [29] 48.92 57.35 90.06 96.05 69.86 0.00 18.37 38.35 23.12 75.89 70.40 58.42 40.88 12.96 41.60 Ours PCCN 58.27 67.01 92.26 96.20 75.89 0.27 5.98 69.49 63.45 66.87 65.63 47.28 68.91 59.10 46.22</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Semantic Segmentation Results on Stanford Large-Scale 3D Indoor Scene Dataset</figDesc><table><row><cell>Flow Field</cell><cell>Overlay of Target and Warped Source</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Semenatic Segmentation Results on Driving Scenes Dataset</figDesc><table><row><cell>Ground Truth</cell><cell>Ours 3D-FCN+PCCN</cell><cell>Ground Truth</cell><cell>Ours 3D-FCN+PCCN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Lidar Flow Results on Driving Scenes Dataset Results: Tab. 3 reports the quantitative results. As shown in the table, our 3D-FCN+PCCN model outperforms the 3D-FCN by 0.351cm in end-point error and our method reduces approximately 20% of the outliers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>ModelNet40 Point Cloud Classification</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of largescale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Monte carlo and quasi-monte carlo methods. Acta numerica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Caflisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Submanifold sparse convolutional networks. arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07563</idno>
		<title level="m">Semantic segmentation of 3d point clouds</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
