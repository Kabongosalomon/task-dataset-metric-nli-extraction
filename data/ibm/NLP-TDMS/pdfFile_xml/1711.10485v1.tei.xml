<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<email>han.zhang@cs.rutgers.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@duke.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the At-tnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatically generating images according to natural language descriptions is a fundamental problem in many applications, such as art generation and computer-aided design. It also drives research progress in multimodal learning and inference across vision and language, which is one of the most active research areas in recent years <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">30]</ref> Most recently proposed text-to-image synthesis methods are based on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b5">[6]</ref>. A commonly used approach is to encode the whole text description into a global sentence vector as the condition for GAN-based image generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Although * work was performed when was an intern with Microsoft Research this bird is red with white and has a very short beak 10:short 3:red 11:beak 9:very 8:a 3:red 5:white 1:bird 10:short 0:this <ref type="figure">Figure 1</ref>. Example results of the proposed AttnGAN. The first row gives the low-to-high resolution images generated by G0, G1 and G2 of the AttnGAN; the second and third row shows the top-5 most attended words by F attn 1 and F attn 2 of the AttnGAN, respectively. Here, images of G0 and G1 are bilinearly upsampled to have the same size as that of G2 for better visualization.</p><p>impressive results have been presented, conditioning GAN only on the global sentence vector lacks important finegrained information at the word level, and prevents the generation of high quality images. This problem becomes even more severe when generating complex scenes such as those in the COCO dataset <ref type="bibr" target="#b13">[14]</ref>.</p><p>To address this issue, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attentiondriven, multi-stage refinement for fine-grained text-toimage generation. The overall architecture of the AttnGAN is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The model consists of two novel components. The first component is an attentional gener-ative network, in which an attention mechanism is developed for the generator to draw different sub-regions of the image by focusing on words that are most relevant to the sub-region being drawn (see <ref type="figure">Figure 1</ref>). More specifically, besides encoding the natural language description into a global sentence vector, each word in the sentence is also encoded into a word vector. The generative network utilizes the global sentence vector to generate a low-resolution image in the first stage. In the following stages, it uses the image vector in each sub-region to query word vectors by using an attention layer to form a word-context vector. It then combines the regional image vector and the corresponding word-context vector to form a multimodal context vector, based on which the model generates new image features in the surrounding sub-regions. This effectively yields a higher resolution picture with more details at each stage. The other component in the AttnGAN is a Deep Attentional Multimodal Similarity Model (DAMSM). With an attention mechanism, the DAMSM is able to compute the similarity between the generated image and the sentence using both the global sentence level information and the fine-grained word level information. Thus, the DAMSM provides an additional fine-grained image-text matching loss for training the generator.</p><p>The contribution of our method is threefold. (i) An Attentional Generative Adversarial Network is proposed for synthesizing images from text descriptions. Specifically, two novel components are proposed in the At-tnGAN, including the attentional generative network and the DAMSM. (ii) Comprehensive study is carried out to empirically evaluate the proposed AttnGAN. Experimental results show that the AttnGAN significantly outperforms previous state-of-the-art GAN models. (iii) A detailed analysis is performed through visualizing the attention layers of the AttnGAN. For the first time, it is demonstrated that the layered conditional GAN is able to automatically attend to relevant words to form the condition for image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generating high resolution images from text descriptions, though very challenging, is important for many practical applications such as art generation and computeraided design. Recently, great progress has been achieved in this direction with the emergence of deep generative models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. Mansimov et al. <ref type="bibr" target="#b14">[15]</ref> built the align-DRAW model, extending the Deep Recurrent Attention Writer (DRAW) <ref type="bibr" target="#b6">[7]</ref> to iteratively draw image patches while attending to the relevant words in the caption. Nguyen et al. <ref type="bibr" target="#b15">[16]</ref> proposed an approximate Langevin approach to generate images from captions. Reed et al. <ref type="bibr" target="#b20">[21]</ref> used conditional PixelCNN <ref type="bibr" target="#b25">[26]</ref> to synthesize images from text with a multi-scale model structure. Compared with other deep generative models, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b5">[6]</ref> have shown great performance for generating sharper samples <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref>. Reed et al. <ref type="bibr" target="#b19">[20]</ref> first showed that the conditional GAN was capable of synthesizing plausible images from text descriptions. Their followup work <ref type="bibr" target="#b17">[18]</ref> also demonstrated that GAN was able to generate better samples by incorporating additional conditions (e.g., object locations). Zhang et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> stacked several GANs for text-to-image synthesis and used different GANs to generate images of different sizes. However, all of their GANs are conditioned on the global sentence vector, missing fine-grained word level information for image generation.</p><p>The attention mechanism has recently become an integral part of sequence transduction models. It has been successfully used in modeling multi-level dependencies in image captioning <ref type="bibr" target="#b28">[29]</ref>, image question answering <ref type="bibr" target="#b29">[30]</ref> and machine translation <ref type="bibr" target="#b1">[2]</ref>. Vaswani et al. <ref type="bibr" target="#b26">[27]</ref> also demonstrated that machine translation models could achieve stateof-the-art results by solely using an attention model. In spite of these progress, the attention mechanism has not been explored in GANs for text-to-image synthesis yet. It is worth mentioning that the alignDRAW <ref type="bibr" target="#b14">[15]</ref> also used LAP-GAN <ref type="bibr" target="#b2">[3]</ref> to scale the image to a higher resolution. However, the GAN in their framework was only utilized as a post-processing step without attention. To our knowledge, the proposed AttnGAN for the first time develops an attention mechanism that enables GANs to generate fine-grained high quality images via multi-level (e.g., word level and sentence level) conditioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attentional Generative Adversarial Network</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed Attentional Generative Adversarial Network (AttnGAN) has two novel components: the attentional generative network and the deep attentional multimodal similarity model. We will elaborate each of them in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attentional Generative Network</head><p>Current GAN-based models for text-to-image generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> typically encode the whole-sentence text description into a single vector as the condition for image generation, but lack fine-grained word level information. In this section, we propose a novel attention model that enables the generative network to draw different subregions of the image conditioned on words that are most relevant to those sub-regions.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed attentional generative network has m generators (G 0 , G 1 , ..., G m−1 ), which take the hidden states (h 0 , h 1 , ..., h m−1 ) as input and generate images of small-to-large scales (x 0 ,x 1 , ...,x m−1 ).   Specifically,</p><formula xml:id="formula_0">F 2 ca F F0 F0 F1 F1 F2 F2 G2 G1 G0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Encoder</head><formula xml:id="formula_1">h 0 = F 0 (z, F ca (e)); h i = F i (h i−1 , F attn i (e, h i−1 )) for i = 1, 2, ..., m − 1; x i = G i (h i ).</formula><p>(1) Here, z is a noise vector usually sampled from a standard normal distribution. e is a global sentence vector, and e is the matrix of word vectors. F ca represents the Conditioning Augmentation <ref type="bibr" target="#b30">[31]</ref> that converts the sentence vector e to the conditioning vector. F attn i is the proposed attention model at the i th stage of the AttnGAN. F ca , F attn i , F i , and G i are modeled as neural networks.</p><p>The attention model F attn (e, h) has two inputs: the word features e ∈ R D×T and the image features from the previous hidden layer h ∈ RD ×N . The word features are first converted into the common semantic space of the image features by adding a new perceptron layer, i.e., e = U e, where U ∈ RD ×D . Then, a word-context vector is computed for each sub-region of the image based on its hidden features h (query). Each column of h is a feature vector of a sub-region of the image. For the j th sub-region, its wordcontext vector is a dynamic representation of word vectors relevant to h j , which is calculated by</p><formula xml:id="formula_2">c j = T −1 i=0 β j,i e i , where β j,i = exp(s j,i ) T −1 k=0 exp(s j,k ) ,<label>(2)</label></formula><p>s j,i = h T j e i , and β j,i indicates the weight the model attends to the i th word when generating the j th sub-region of the image. We then donate the word-context matrix for image feature set h by F attn (e, h) = (c 0 , c 1 , ..., c N −1 ) ∈ RD ×N .</p><p>Finally, image features and the corresponding word-context features are combined to generate images at the next stage.</p><p>To generate realistic images with multiple levels (i.e., sentence level and word level) of conditions, the final objective function of the attentional generative network is defined as</p><formula xml:id="formula_3">L = L G + λL DAM SM , where L G = m−1 i=0 L Gi . (3)</formula><p>Here, λ is a hyperparameter to balance the two terms of Eq. (3). The first term is the GAN loss that jointly approximates conditional and unconditional distributions <ref type="bibr" target="#b31">[32]</ref>. At the i th stage of the AttnGAN, the generator G i has a corresponding discriminator D i . The adversarial loss for G i is defined as</p><formula xml:id="formula_4">L G i = − 1 2 Ex i ∼p G i [log(D i (x i )] unconditional loss − 1 2 Ex i ∼p G i [log(D i (x i , e)] conditional loss ,<label>(4)</label></formula><p>where the unconditional loss determines whether the image is real or fake while the conditional loss determines whether the image and the sentence match or not.</p><p>Alternately to the training of G i , each discriminator D i is trained to classify the input into the class of real or fake by minimizing the cross-entropy loss defined by</p><formula xml:id="formula_5">L D i = − 1 2 Ex i ∼p data i [log Di(xi)] − 1 2 Ex i ∼p G i [log(1 − Di(xi)] unconditional loss + − 1 2 Ex i ∼p data i [log Di(xi, e)] − 1 2 Ex i ∼p G i [log(1 − Di(xi, e)] conditional loss ,<label>(5)</label></formula><p>where x i is from the true image distribution p datai at the i th scale, andx i is from the model distribution p Gi at the same scale. Discriminators of the AttnGAN are structurally disjoint, so they can be trained in parallel and each of them focuses on a single image scale. The second term of Eq. <ref type="formula">(3)</ref>, L DAM SM , is a word level fine-grained image-text matching loss computed by the DAMSM, which will be elaborated in Subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Attentional Multimodal Similarity Model</head><p>The DAMSM learns two neural networks that map subregions of the image and words of the sentence to a common semantic space, thus measures the image-text similarity at the word level to compute a fine-grained loss for image generation.</p><p>The text encoder is a bi-directional Long Short-Term Memory (LSTM) <ref type="bibr" target="#b23">[24]</ref> that extracts semantic vectors from the text description. In the bi-directional LSTM, each word corresponds to two hidden states, one for each direction. Thus, we concatenate its two hidden states to represent the semantic meaning of a word. The feature matrix of all words is indicated by e ∈ R D×T . Its i th column e i is the feature vector for the i th word. D is the dimension of the word vector and T is the number of words. Meanwhile, the last hidden states of the bi-directional LSTM are concatenated to be the global sentence vector, denoted by e ∈ R D .</p><p>The image encoder is a Convolutional Neural Network (CNN) that maps images to semantic vectors. The intermediate layers of the CNN learn local features of different sub-regions of the image, while the later layers learn global features of the image. More specifically, our image encoder is built upon the Inception-v3 model <ref type="bibr" target="#b24">[25]</ref> pretrained on ImageNet <ref type="bibr" target="#b21">[22]</ref>. We first rescale the input image to be 299×299 pixels. And then, we extract the local feature matrix f ∈ R 768×289 (reshaped from 768×17×17) from the "mixed 6e" layer of Inception-v3. Each column of f is the feature vector of a sub-region of the image. 768 is the dimension of the local feature vector, and 289 is the number of sub-regions in the image. Meanwhile, the global feature vector f ∈ R 2048 is extracted from the last average pooling layer of Inception-v3. Finally, we convert the image features to a common semantic space of text features by adding a perceptron layer:</p><formula xml:id="formula_6">v = W f , v = W f ,<label>(6)</label></formula><p>where v ∈ R D×289 and its i th column v i is the visual feature vector for the i th sub-region of the image; and v ∈ R D is the global vector for the whole image. D is the dimension of the multimodal (i.e., image and text modalities) feature space. For efficiency, all parameters in layers built from the Inception-v3 model are fixed, and the parameters in newly added layers are jointly learned with the rest of the network.</p><p>The attention-driven image-text matching score is designed to measure the matching of an image-sentence pair based on an attention model between the image and the text.</p><p>We first calculate the similarity matrix for all possible pairs of words in the sentence and sub-regions in the image by</p><formula xml:id="formula_7">s = e T v,<label>(7)</label></formula><p>where s ∈ R T ×289 and s i,j is the dot-product similarity between the i th word of the sentence and the j th sub-region of the image. We find that it is beneficial to normalize the similarity matrix as follows</p><formula xml:id="formula_8">s i,j = exp(s i,j ) T −1 k=0 exp(s k,j ) .<label>(8)</label></formula><p>Then, we build an attention model to compute a regioncontext vector for each word (query). The region-context vector c i is a dynamic representation of the image's subregions related to the i th word of the sentence. It is computed as the weighted sum over all regional visual vectors, i.e.,</p><formula xml:id="formula_9">c i = 288 j=0 α j v j , where α j = exp(γ 1 s i,j ) 288 k=0 exp(γ 1 s i,k )</formula><p>. <ref type="formula">(9)</ref> Here, γ 1 is a factor that determines how much attention is paid to features of its relevant sub-regions when computing the region-context vector for a word. Finally, we define the relevance between the i th word and the image using the cosine similarity between c i and e i , i.e., R(c i , e i ) = (c T i e i )/(||c i ||||e i ||). Inspired by the minimum classification error formulation in speech recognition (see, e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref>), the attention-driven image-text matching score between the entire image (Q) and the whole text description (D) is defined as</p><formula xml:id="formula_10">R(Q, D) = log T −1 i=1 exp(γ 2 R(c i , e i )) 1 γ 2 ,<label>(10)</label></formula><p>where γ 2 is a factor that determines how much to magnify the importance of the most relevant word-to-regioncontext pair. When γ 2 → ∞, R(Q, D) approximates to</p><formula xml:id="formula_11">max T −1 i=1 R(c i , e i ).</formula><p>The DAMSM loss is designed to learn the attention model in a semi-supervised manner, in which the only supervision is the matching between entire images and whole sentences (a sequence of words). Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, for a batch of image-sentence pairs</p><formula xml:id="formula_12">{(Q i , D i )} M i=1</formula><p>, the posterior probability of sentence D i being matching with image Q i is computed as</p><formula xml:id="formula_13">P (D i |Q i ) = exp(γ 3 R(Q i , D i )) M j=1 exp(γ 3 R(Q i , D j )) ,<label>(11)</label></formula><p>where γ 3 is a smoothing factor determined by experiments. In this batch of sentences, only D i matches the image Q i , and treat all other M − 1 sentences as mismatching descriptions. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, we define the loss function as the negative log posterior probability that the images are matched with their corresponding text descriptions (ground truth), i.e.,</p><formula xml:id="formula_14">L w 1 = − M i=1 log P (D i |Q i ),<label>(12)</label></formula><p>where 'w' stands for "word". Symmetrically, we also minimize</p><formula xml:id="formula_15">L w 2 = − M i=1 log P (Q i |D i ),<label>(13)</label></formula><p>where P (Q i |D i ) = exp(γ3R(Qi,Di)) M j=1 exp(γ3R(Qj ,Di)) is the posterior probability that sentence D i is matched with its corresponding image Q i . If we redefine Eq. (10) by R(Q, D) = v T e / ||v||||e|| and substitute it to Eq. (11), <ref type="bibr" target="#b11">(12)</ref> and <ref type="formula" target="#formula_15">(13)</ref>, we can obtain loss functions L s 1 and L s 2 (where 's' stands for "sentence") using the sentence vector e and the global image vector v.</p><p>Finally, the DAMSM loss is defined as</p><formula xml:id="formula_16">L DAM SM = L w 1 + L w 2 + L s 1 + L s 2 .<label>(14)</label></formula><p>Based on experiments on a held-out validation set, we set the hyperparameters in this section as: γ 1 = 5, γ 2 = 5, γ 3 = 10 and M = 50. Our DAMSM is pretrained 1 by minimizing L DAM SM using real image-text pairs. Since the size of images for pretraining DAMSM is not limited by the size of images that can be generated, real images of size 299×299 are utilized. In addition, the pretrained textencoder in the DAMSM provides visually-discriminative word vectors learned from image-text paired data for the attentional generative network. In comparison, conventional word vectors pretrained on pure text data are often not visually-discriminative, e.g., word vectors of different colors, such as red, blue, yellow, etc., are often clustered together in the vector space, due to the lack of grounding them to the actual visual signals.</p><p>In sum, we propose two novel attention models, the attentional generative network and the DAMSM, which play different roles in the AttnGAN. (i) The attention mechanism in the generative network (see Eq. 2) enables the AttnGAN to automatically select word level condition for generating different sub-regions of the image. (ii) With an attention mechanism (see Eq. 9), the DAMSM is able to compute the fine-grained text-image matching loss L DAM SM . It is worth mentioning that, L DAM SM is applied only on the output of the last generator G m−1 , because the eventual goal of the AttnGAN is to generate large images by the last <ref type="bibr" target="#b0">1</ref> We also finetuned the DAMSM with the whole network, however the performance was not improved. generator. We tried to apply L DAM SM on images of all resolutions generated by (G 0 , G 1 , ..., G m−1 ). However, the performance was not improved but the computational cost was increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Extensive experimentation is carried out to evaluate the proposed AttnGAN. We first study the important components of the AttnGAN, including the attentional generative network and the DAMSM. Then, we compare our At-tnGAN with previous state-of-the-art GAN models for textto-image synthesis <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Datasets. Same as previous text-to-image methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>, our method is evaluated on CUB <ref type="bibr" target="#b27">[28]</ref> and COCO <ref type="bibr" target="#b13">[14]</ref> datasets. We preprocess the CUB dataset according to the method in <ref type="bibr" target="#b30">[31]</ref>. <ref type="table" target="#tab_1">Table 1</ref> lists the statistics of datasets.</p><p>Evaluation. Following Zhang et al. <ref type="bibr" target="#b30">[31]</ref>, we use the inception score <ref type="bibr" target="#b22">[23]</ref> as the quantitative evaluation measure. Since the inception score cannot reflect whether the generated image is well conditioned on the given text description, we propose to use R-precision, a common evaluation metric for ranking retrieval results, as a complementary evaluation metric for the text-to-image synthesis task. If there are R relevant documents for a query, we examine the top R ranked retrieval results of a system, and find that r are relevant, and then by definition, the R-precision is r/R. More specifically, we conduct a retrieval experiment, i.e., we use generated images to query their corresponding text descriptions. First, the image and text encoders learned in our pretrained DAMSM are utilized to extract global feature vectors of the generated images and the given text descriptions. And then, we compute cosine similarities between the global image vectors and the global text vectors. Finally, we rank candidate text descriptions for each image in descending similarity and find the top r relevant descriptions for computing the R-precision. To compute the inception score and the R-precision, each model generates 30,000 images from randomly selected unseen text descriptions. The candidate text descriptions for each query image consist of one ground truth (i.e., R = 1) and 99 randomly selected mismatching descriptions.</p><p>Besides quantitative evaluation, we also qualitatively examine the samples generated by our models. Specifically, we visualize the intermediate results with attention learned by the attention models F attn . As defined in Eq. (2), weights β j,i indicates which words the model at- </p><formula xml:id="formula_17">Epoch AttnGAN1, =0.1 AttnGAN1, =1 AttnGAN1, =10 AttnGAN1, =50</formula><p>AttnGAN1, =100 AttnGAN2, =50 <ref type="figure">Figure 3</ref>. Inception scores and R-precision rates by our AttnGAN and its variants at different epochs on CUB (top) and COCO (bottom) test sets. For the text-to-image synthesis task, R = 1. tends to when generating a sub-region of the image, and T −1 i=0 β j,i = 1. We suppress the less-relevant words for an image's sub-region viâ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><formula xml:id="formula_18">β j,i = β j,i , if β j,i &gt; 1/T, 0, otherwise.<label>(15)</label></formula><p>For better visualization, we fix the word and compute its attention weights with N different sub-regions of an image, β 0,i ,β 1,i , ...,β N −1,i . We reshape the N attention weights to √ N × √ N pixels, which are then upsampled with Gaussian filters to have the same size as the generated images. Limited by the length of the paper, we only visualize the top-5 most attended words (i.e., words with top-5 highest N −1 j=0β j,i values) for each attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Component analysis</head><p>In this section, we first quantitatively evaluate the At-tnGAN and its variants. The results are shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure">Figure 3</ref>. Our "AttnGAN1" architecture has one attention model and two generators, while the "AttnGAN2" architecture has two attention models stacked with three generators (see <ref type="figure" target="#fig_1">Figure 2</ref>). In addition, as illustrated in <ref type="figure">Figure 4</ref>, <ref type="figure" target="#fig_3">Figure 5</ref>, <ref type="figure">Figure 6</ref>, and <ref type="figure">Figure 7</ref>, we qualitatively examine the images generated by our AttnGAN.</p><p>The DAMSM loss. To test the proposed L DAM SM , we adjust the value of λ (see Eq. <ref type="formula">(3)</ref>). As shown in <ref type="figure">Figure 3</ref>, a larger λ leads to a significantly higher R-precision rate on both CUB and COCO datasets. On the CUB dataset, when the value of λ is increased from 0.1 to 5, the inception score of the AttnGAN1 is improved from 4.19 to 4.35 and the corresponding R-precision rate is increased from 16.55% to 58.65% (see <ref type="table" target="#tab_2">Table 2</ref>). On the COCO dataset, by increasing the value of λ from 0.1 to 50, the AttnGAN1 achieves both high inception score and R-precision rate (see <ref type="figure">Figure 3</ref>). This comparison demonstrates that properly increasing the weight of L DAM SM helps to generate higher quality images that are better conditioned on given text descriptions. The reason is that the proposed fine-grained image-text matching loss L DAM SM provides additional supervision (i.e., word level matching information) for training the generator. Moreover, in our experiments, we do not observe any collapsed nonsensical mode in the visualization of AttnGAN-generated images. It indicates that, with extra supervision, the fine-grained image-text matching loss also helps to stabilize the training process of the AttnGAN. In addition, if we replace the proposed DAMSM sub-network with the text encoder used in <ref type="bibr" target="#b18">[19]</ref>, on the CUB dataset, the inception score and R-precision drops to 3.98 and 10.37%, respectively (i.e., the "AttnGAN1, no DAMSM" entry in table 2), which further demonstrates the effectiveness of the proposed L DAM SM .</p><p>The attentional generative network. As shown in Table 2 and <ref type="figure">Figure 3</ref>, stacking two attention models in the generative networks not only generates images of a higher resolution (from 128×128 to 256×256 resolution), but also yields higher inception scores on both CUB and COCO datasets. In order to guarantee the image quality, we find the best value of λ for each dataset by increasing the value of λ until the overall inception score is starting to drop on a held-out validation set. "AttnGAN1" models are built for searching the best λ, based on which a "AttnGAN2" model is built to generate higher resolution images. Due to GPU memory constraints, we did not try the AttnGAN with three attention models. As the result, our final model for CUB and COCO is "AttnGAN2, λ=5" and "AttnGAN2, λ=50", respectively. The final λ of the COCO dataset turns out to be much larger than that of the CUB dataset, indicating that the proposed L DAM SM is especially important for generat- <ref type="table">Table 3</ref>. Inception scores by state-of-the-art GAN models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref> and our AttnGAN on CUB and COCO test sets.</p><p>ing complex scenarios like those in the COCO dataset.</p><p>To better understand what has been learned by the At-tnGAN, we visualize its intermediate results with attention. As shown in <ref type="figure">Figure 4</ref>, the first stage of the AttnGAN (G 0 ) just sketches the primitive shape and colors of objects and generates low resolution images. Since only the global sentence vectors are utilized in this stage, the generated images lack details described by exact words, e.g., the beak and eyes of a bird. Based on word vectors, the following stages (G 1 and G 2 ) learn to rectify defects in results of the previous stage and add more details to generate higher-resolution images. Some sub-regions/pixels of G 1 or G 2 images can be inferred directly from images generated by the previous stage. For those sub-regions, the attention is equally allo-this bird has wings that are black and has a white belly this bird has wings that are red and has a yellow belly this bird has wings that are blue and has a red belly  cated to all words and shown to be black in the attention map (see <ref type="figure">Figure 4</ref>). For other sub-regions, which usually have semantic meaning expressed in the text description such as the attributes of objects, the attention is allocated to their most relevant words (bright regions in <ref type="figure">Figure 4</ref>). Thus, those regions are inferred from both word-context features and previous image features of those regions. As shown in <ref type="figure">Figure 4</ref>, on the CUB dataset, the words the, this, bird are usually attended by the F attn models for locating the object; the words describing object attributes, such as colors and parts of birds, are also attended for correcting defects and drawing details. On the COCO dataset, we have similar observations. Since there are usually more than one object in each COCO image, it is more visible that the words describing different objects are attended by different subregions of the image, e.g., bananas, kiwi in the bottom-right block of <ref type="figure">Figure 4</ref>. Those observations demonstrate that the AttnGAN learns to understand the detailed semantic meaning expressed in the text description of an image. Another observation is that our second attention model F attn 2 is able to attend to some new words that were omitted by the first attention model F attn 1 (see <ref type="figure">Figure 4</ref>). It demonstrates that, to provide richer information for generating higher resolution images at latter stages of the AttnGAN, the corresponding attention models learn to recover objects and attributes omitted at previous stages.</p><p>Generalization ability. Our experimental results above have quantitatively and qualitatively shown the generalization ability of the AttnGAN by generating images from unseen text descriptions. Here we further test how sensitive the outputs are to changes in the input sentences by changing some most attended words in the text descriptions. Some examples are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. It illustrates that the generated images are modified according to the changes in the input sentences, showing that the model can catch subtle semantic differences in the text description. Moreover, as shown in <ref type="figure">Figure 6</ref>, our AttnGAN can generate images to reflect the semantic meaning of descriptions of novel scenarios that are not likely to happen in the real world, e.g., a stop sign is floating on top of a lake. On the other hand, we also observe that the AttnGAN sometimes generates images which are sharp and detailed, but are not likely realistic. As examples shown in <ref type="figure">Figure 7</ref>, the AttnGAN creates birds with multiple heads, eyes or tails, which only exist in fairy tales. This indicates that our current method is still not perfect in capturing global coherent structures, which leaves room to improve. To sum up, observations shown in <ref type="figure" target="#fig_3">Figure 5</ref>, <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref> further demonstrate the generalization ability of the AttnGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with previous methods</head><p>We compare our AttnGAN with previous state-of-theart GAN models for text-to-image generation on CUB and COCO test sets. As shown in <ref type="table">Table 3</ref>, on the CUB dataset, our AttnGAN achieves 4.36 inception score, which significantly outperforms the previous best inception score of 3.82. More impressively, our AttnGAN boosts the best reported inception score on the COCO dataset from 9.58 to 25.89, a 170.25% improvement relatively. The COCO dataset is known to be much more challenging than the CUB dataset because it consists of images with more complex scenarios. Existing methods struggle in generating realistic high-resolution images on this dataset. Examples in <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 6</ref> illustrate that our AttnGAN succeeds in generating 256×256 images for various scenarios on the COCO dataset, although those generated images of the COCO dataset are not as photo-realistic as that of the CUB dataset. The experimental results show that, compared to previous state-of-the-art approaches, the AttnGAN is more effective for generating complex scenes due to its novel attention mechanism that catches fine-grained word level and sub-region level information in text-to-image generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed AttnGAN. Each attention model automatically retrieves the conditions (i.e., the most relevant word vectors) for generating different sub-regions of the image; the DAMSM provides the fine-grained image-text matching loss for the generative network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Example results of our AttnGAN model trained on CUB while changing some most attended words in the text descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>256×256 images generated from descriptions of novel scenarios using the AttnGAN model trained on COCO. (Intermediate results are given in the supplementary material.) Novel images by our AttnGAN on the CUB test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Text Encoder sentence feature word features Attention models Local image features Deep Attentional Multimodal Similarity Model (DAMSM)</head><label></label><figDesc></figDesc><table><row><cell>Residual</cell><cell>FC with reshape</cell><cell>Upsampling</cell><cell></cell><cell>Joining</cell><cell>Conv3x3</cell></row><row><cell></cell><cell></cell><cell>attn F 1</cell><cell>attn</cell><cell></cell></row><row><cell cols="2">this bird is red with</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">white and has a</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">very short beak</cell><cell></cell><cell>D1</cell><cell>D2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CUB [28] train test</cell><cell cols="2">COCO [14] train test</cell></row><row><cell>#samples</cell><cell cols="2">8,855 2,933</cell><cell>80k</cell><cell>40k</cell></row><row><cell>caption/image</cell><cell>10</cell><cell>10</cell><cell>5</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The best inception score and the corresponding Rprecision rate of each AttnGAN model on CUB (top six rows) and COCO (the last row) test sets. More results inFigure 3.</figDesc><table><row><cell></cell><cell cols="2">inception score R-precision(%)</cell></row><row><cell cols="2">AttnGAN1, no DAMSM 3.98 ± .04</cell><cell>10.37± 5.88</cell></row><row><cell>AttnGAN1, λ = 0.1</cell><cell>4.19 ± .06</cell><cell>16.55± 4.83</cell></row><row><cell>AttnGAN1, λ = 1</cell><cell>4.35 ± .05</cell><cell>34.96± 4.02</cell></row><row><cell>AttnGAN1, λ = 5</cell><cell>4.35 ± .04</cell><cell>58.65± 5.41</cell></row><row><cell>AttnGAN1, λ = 10</cell><cell>4.29 ± .05</cell><cell>63.87± 4.85</cell></row><row><cell>AttnGAN2, λ = 5</cell><cell>4.36 ± .03</cell><cell>67.82 ± 4.43</cell></row><row><cell>AttnGAN2, λ = 50 (COCO)</cell><cell>25.89 ± .47</cell><cell>85.47 ± 3.69</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose an Attentional Generative Adversarial Network, named AttnGAN, for fine-grained textto-image synthesis. First, we build a novel attentional generative network for the AttnGAN to generate high quality image through a multi-stage process. Second, we propose a deep attentional multimodal similarity model to compute the fine-grained image-text matching loss for training the generator of the AttnGAN. Our AttnGAN significantly outperforms previous state-of-the-art GAN models, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. Extensive experimental results clearly demonstrate the effectiveness of the proposed attention mechanism in the AttnGAN, which is especially critical for text-to-image generation for complex scenes.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the bird has a yellow crown and a black eyering that is round this bird has a green crown black primaries and a white belly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Due to the size limit, more examples are available in the appendix, which can be download from this link.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="4" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative learning in sequential pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="14" to="36" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimum classification error rate methods for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="265" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR- 2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10916</idno>
		<title level="m">Realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
