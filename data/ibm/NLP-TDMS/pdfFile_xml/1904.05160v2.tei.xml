<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale Long-Tailed Recognition in an Open World</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>bgong@outlook.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<email>stellayu@berkeley.edu</email>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale Long-Tailed Recognition in an Open World</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes.</p><p>OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes.</p><p>We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic metaembedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale OLTR datasets we curate from object-centric ImageNet, scenecentric Places, and face-centric MS1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future OLTR research and are publicly available at https://liuziwei7.github. io/projects/LongTail.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our visual world is inherently long-tailed and openended: The frequency distribution of visual categories in our daily life is long-tailed <ref type="bibr" target="#b41">[42]</ref>, with a few common classes and many more rare classes, and we constantly encounter new visual concepts as we navigate in an open world. * Equal contribution. † Work done in part at Tencent AI Lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Long-tailed Recognition</head><p>Open World</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imbalanced Classification</head><p>Few-shot Learning While the natural data distribution contains head, tail, and open classes ( <ref type="figure" target="#fig_0">Fig. 1)</ref>, existing classification approaches focus mostly on the head <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>, the tail <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b26">27]</ref>, often in a closed setting <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b33">34]</ref>. Traditional deep learning models are good at capturing the big data of head classes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref>; more recently, few-shot learning methods have been developed for the small data of tail classes <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Classes Tail Classes Open Classes</head><p>We formally study Open Long-Tailed Recognition (OLTR) arising in natural data settings. A practical system shall be able to classify among a few common and many rare categories, to generalize the concept of a single category from only a few known instances, and to acknowledge novelty upon an instance of a never seen category. We define OLTR as learning from long-tail and open-end distributed data and evaluating the classification accuracy over a balanced test set which include head, tail, and open classes in a continuous spectrum ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>OLTR must handle not only imbalanced classification and few-shot learning in the closed world, but also open-set recognition with one integrated algorithm (Tab. 1). Existing classification approaches tend to focus on one aspect and deliver poorly over the entire class spectrum.</p><p>The key challenges for OLTR are tail recognition robustness and open-set sensitivity: As the number of training instances drops from thousands in the head class to the few  <ref type="table">Table 1</ref>: Comparison between our proposed OLTR task and related existing tasks.</p><p>in the tail class, the recognition accuracy should maintain as high as possible; on the other hand, as the number of instances drops to zero in the open set, the recognition accuracy relies on the sensitivity to distinguish unknown open classes from known tail classes. An integrated OLTR algorithm should tackle the two seemingly contradictory aspects of recognition robustness and recognition sensitivity on a continuous category spectrum. To increase the recognition robustness, it must share visual knowledge between head and tail classes; to increase recognition sensitivity, it must reduce the confusion between tail and open classes.</p><p>We develop an OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world.</p><p>Our so-called dynamic meta-embedding handles tail recognition robustness by combining two components: a direct feature computed from the input image, and an induced feature associated with the visual memory. 1) Our direct feature is a standard embedding that gets updated from the training data by stochastic gradient descent over the classification loss. The direct feature lacks sufficient supervision for the rare tail class. 2) Our memory feature is inspired by meta learning methods with memories <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref> to augment the direct feature from the image. A visual memory holds discriminative centroids of the direct feature. We learn to retrieve a summary of memory activations from the direct feature, combined into a meta-embedding that is enriched particularly for the tail class.</p><p>Our dynamic meta-embedding handles open recognition sensitivity by dynamically calibrating the meta-embedding with respect to the visual memory. The embedding is scaled inversely by its distance to the nearest centroid: The farther away from the memory, the closer to the origin, and the more likely an open set instance. We also adopt modulated attention <ref type="bibr" target="#b55">[56]</ref> to encourage the head and tail classes to use different sets of spatial features. As our metaembedding relates head and tail classes, our modulated attention maintains discrimination between them.</p><p>We make the following major contributions. 1) We formally define the OLTR task, which learns from natural long-tail and open-end distributed data and optimizes the overall accuracy over a balanced test set. It provides a comprehensive and unbiased evaluation of visual recogni-tion algorithms in practical settings. 2) We develop an integrated OLTR algorithm with dynamic meta-embedding. It handles tail recognition robustness by relating visual concepts among head and tail embeddings, and it handles open recognition sensitivity by dynamically calibrating the embedding norm with respect to the visual memory.</p><p>3) We curate three large OLTR datasets according to a long-tail distribution from existing representative datasets: object-centric ImageNet, scene-centric MIT Places, and face-centric MS1M datasets. We set up benchmarks for proper OLTR performance evaluation. 4) Our extensive experimentation on these OLTR datasets demonstrates that our method consistently outperforms the state-of-the-art.</p><p>Our code, datasets, and models are publicly available at https://liuziwei7.github.io/projects/ LongTail.html. Our work fills the void in practical benchmarks for imbalanced classification, few-shot learning, and open-set recognition, enabling future research that is directly transferable to real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>While OLTR has not been defined in the literature, there are three closely related tasks which are often studied in isolation: imbalanced classification, few-shot learning, and open-set recognition. Tab. 1 summarizes their differences. Imbalanced Classification. Arising from long-tail distributions of natural data, it has been extensively studied <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b6">7]</ref>. Classical methods include under-sampling head classes, over-sampling tail classes, and data instance re-weighting. We refer the readers to <ref type="bibr" target="#b18">[19]</ref> for a detailed review. Some recent methods include metric learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref>, hard negative mining <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>, and meta learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref>. The lifted structure loss <ref type="bibr" target="#b36">[37]</ref> introduces margins between many training instances. The range loss <ref type="bibr" target="#b63">[64]</ref> enforces data in the same class to be close and those in different classes to be far apart. The focal loss <ref type="bibr" target="#b28">[29]</ref> induces an online version of hard negative mining. MetaModelNet <ref type="bibr" target="#b58">[59]</ref> learns a meta regression net from head classes and uses it to construct the classifier for tail classes.</p><p>Our dynamic meta-embedding combines the strengths of both metric learning and meta learning. On one hand, our direct feature is updated to ensure centroids for different classes are far from each other; On the other hand, our memory feature is generated on-the-fly in a meta learning fashion to effectively transfer knowledge to tail classes. Few-Shot Learning. It is often formulated as meta learning <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b60">61]</ref>. Matching Network <ref type="bibr" target="#b54">[55]</ref> learns a transferable feature matching metric to go beyond given classes. Prototypical Network <ref type="bibr" target="#b51">[52]</ref> maintains a set of separable class templates. Feature hallucination <ref type="bibr" target="#b17">[18]</ref> and augmentation <ref type="bibr" target="#b56">[57]</ref> are also shown effective. Since these methods focus on novel classes, they often suffer a moderate performance drop for head classes. There are a few exceptions. The few-shot learning without forgetting <ref type="bibr" target="#b14">[15]</ref> and incremental few-shot learning <ref type="bibr" target="#b42">[43]</ref> attempt to remedy this issue by leveraging the duality between features and classifiers' weights <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref>. However, the training set used in all of these methods are balanced.</p><p>In comparison, our OLTR learns from a more natural long-tailed training set. Nevertheless, our work is closely related to meta learning with fast weight and associative memory <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36]</ref> to enable rapid adaptation. Compared to these prior arts, our memory feature has two advantages: 1) It transfers knowledge to both head and tail classes adaptively via a learned concept selector; 2) It is fully integrated into the network without episodic training, and is thus especially suitable for large-scale applications.</p><p>Open-Set Recognition. Open-set recognition <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3]</ref>, or out-of-distribution detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>, aims to re-calibrate the sample confidence in the presence of open classes. One of the representative techniques is OpenMax <ref type="bibr" target="#b2">[3]</ref>, which fits a Weibull distribution to the classifier's output logits. However, when there are both open and tail classes, the distribution fitting could confuse the two.</p><p>Instead of calibrating the output logits, our OLTR approach incorporates the confidence estimation into feature learning and dynamically re-scale the meta-embedding w.r.t. to the learned visual memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our OLTR Model</head><p>We propose to map an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our model has two main modules ( <ref type="figure" target="#fig_1">Fig.2)</ref>: dynamic metaembedding and modulated attention. The former relates and transfers knowledge between head and tail classes and the latter maintains discrimination between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic Meta-Embedding</head><p>Our dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes.</p><p>Consider a convolutional neural network (CNN) with a softmax output layer for classification. The second-to-thelast layer can be viewed as the feature and the last layer a linear classifier (cf. φ(·) in <ref type="figure" target="#fig_1">Fig. 2</ref>). The feature and the classifier are jointly trained from big data in an end-to-end fashion. Let v direct denote the direct feature extracted from an input image. The final classification accuracy largely depends on the quality of this direct feature.</p><p>While a feed-forward CNN classifier works well with big training data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>, it lacks sufficient supervised updates from small data in our tail classes. We propose to enrich direct feature v direct with a memory feature v memory that relates visual concepts in a memory module. This mechanism is similar to the memory popular in meta learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b35">36]</ref>. We denote the resulting feature meta embedding v meta , and it is fed to the last layer for classification. Both our memory feature v memory and metaembedding v meta depend on direct feature v direct .</p><p>Unlike the direct feature, the memory feature captures visual concepts from training classes, retrieved from a memory with a much shallower model.</p><p>Learning Visual Memory M . We follow <ref type="bibr" target="#b22">[23]</ref> on class structure analysis and adopt discriminative centroids as the basic building block. Let M denote the visual memory of all the training data,</p><formula xml:id="formula_0">M = {c i } K i=1</formula><p>where K is the number of training classes. Compared to alternatives <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b51">52]</ref>, this memory is appealing for our OLTR task: It is almost effortlessly and jointly learned alongside the direct features {v direct n }, and it considers both intra-class compactness and inter-class discriminativeness.</p><p>We compute centroids in two steps. 1) Neighborhood Sampling: We sample both intra-class and inter-class examples to compose a mini-batch during training. These examples are grouped by their class labels and the centroid c i of each group is updated by the direct feature of this minibatch. 2) Propagation: We alternatively update the direct feature v direct and the centroids to minimize the distance between each direct feature and the centroid of its group and maximize the distance to other centroids. Composing Memory Feature v memory . For an input image, v memory shall enhance its direct feature when there is not enough training data (as in the tail class) to learn it well. The memory feature relates the centroids in the memory, transferring knowledge to the tail class:</p><formula xml:id="formula_1">v memory = o T M := K i=1 o i c i ,<label>(1)</label></formula><p>where o ∈ R K is the coefficients hallucinated from the direct feature. We use a lightweight neural network to obtain the coefficients from the direct feature, o = T hal (v direct ).</p><p>Obtaining Dynamic Meta-Embedding. v meta combines the direct feature and the memory feature, and is fed to the classifier for the final class prediction ( <ref type="figure" target="#fig_2">Fig. 3)</ref>:</p><formula xml:id="formula_2">v meta = (1/γ) · (v direct + e ⊗ v memory ),<label>(2)</label></formula><p>where ⊗ denotes element-wise multiplication. γ &gt; 0 is seemingly a redundant scalar for the closed-world classification tasks. However, in the OLTR setting, it plays an important role in differentiating the examples of the training classes from those of the open-set. γ measures the reachability <ref type="bibr" target="#b46">[47]</ref> of an input's direct feature v direct to the memory M -the minimum distance between the direct feature and the discriminative centroids:</p><formula xml:id="formula_3">γ := reachability(v direct , M ) = min i v direct − c i 2 . (3)</formula><p>When γ is small, the input likely belongs to a training class from which the centroids are derived, and a large reachability weight 1/γ is assigned to the resulting metaembedding v meta . Otherwise, the embedding is scaled down to an almost all-zero vector at the extreme. Such a property is useful for encoding open classes. We now describe the concept selector e in Eq. (2). The direct feature is often good enough for the data-rich head classes, whereas the memory feature is more important for the data-poor tail classes. To adaptively select them in a soft manner, we learn a lightweight network T sel (·) with a tanh(·) activation function:</p><formula xml:id="formula_4">e = tanh(T sel (v direct )).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modulated Attention</head><p>While dynamic meta-embedding facilitates feature sharing between head and tail classes, it is also vital to discriminate between them. The direct feature v direct , e.g., the activation at the second-to-the-last layer in ResNet <ref type="bibr" target="#b19">[20]</ref>, is able to fulfill this requirement to some extent. However, we find it beneficial to further enhance it with spatial attention, since discriminative cues of head and tail classes seem to be distributed at different locations in the image.</p><p>Specifically, we propose modulated attention to encourage samples of different classes to use different contexts. Firstly, we compute a self-attention map SA(f ) from the input feature map by self-correlation <ref type="bibr" target="#b55">[56]</ref>. It is used as contextual information and added back (through skip connections) to the original feature map. The modulated attention M A(f ) is then designed as conditional spatial attention applied to the self-attention map: M A(f ) ⊗ SA(f ), which allows examples to select different spatial contexts <ref type="figure">(Fig. 4</ref>). The final attention feature map becomes:</p><formula xml:id="formula_5">f att = f + M A(f ) ⊗ SA(f ),<label>(5)</label></formula><p>where f is a feature map in CNN, SA(·) is the selfattention operation, and M A(·) is a conditional attention function <ref type="bibr" target="#b53">[54]</ref> with a softmax normalization. Sec. 4.1 shows empirically that our attention design achieves superior performance than the common practice of applying spatial attention to the input feature map. This modulated attention ( <ref type="figure">Fig. 4b</ref>) could be plugged into any feature layer of a CNN.</p><p>Here, we modify the last feature map only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning</head><p>Cosine Classifier. We adopt the cosine classifier <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15]</ref> to produce the final classification results. Specifically, we  <ref type="figure">Figure 4</ref>: Modulated attention is spatial attention applied on self-attention maps ("attention on attention"). It encourages different classes to use different contexts, which helps maintain the discrimination between head and tail classes.</p><p>normalize the meta-embeddings {v meta n }, where n stands for the n-th input as well as the weight vectors</p><formula xml:id="formula_6">{w i } K i=1 of the classifier φ(·) (no bias term): v meta n = v meta n 2 1 + v meta n 2 · v meta n v meta n , w k = w k w k .<label>(6)</label></formula><p>The normalization strategy for the meta-embedding is a non-linear squashing function <ref type="bibr" target="#b43">[44]</ref> which ensures that vectors of small magnitude are shrunk to almost zeros while vectors of big magnitude are normalized to the length slightly below 1. This function helps amplify the effect of the reachability γ (cf. Eq. <ref type="formula" target="#formula_2">(2)</ref>). Loss Function. Since all our modules are differentiable, our model can be trained end-to-end by alternatively updating the centroids {c i } K i=1 and the dynamic meta-embedding v meta n . The final loss function L is a combination of the cross-entropy classification loss L CE and the large-margin loss between the embeddings and the centroids L LM :</p><formula xml:id="formula_7">L = N n=1 L CE (v meta n , y n ) + λ · L LM (v meta n , {c i } K i=1 ),<label>(7)</label></formula><p>where λ is set to 0.1 in our experiments via observing the accuracy curve on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets.</p><p>We curate three open long-tailed benchmarks, ImageNet-LT (object-centric), Places-LT (scene-centric), and MS1M-LT (face-centric), respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MS1M-LT:</head><p>To create a long-tailed version of the MS1M-ArcFace dataset <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>, we sample images for each identity with a probability proportional to the image numbers of each identity. It results in 887.5K images and 74.5K identities, with a long-tailed distribution. To inspect the generalization ability of our approach, the performance is evaluated on the MegaFace benchmark <ref type="bibr" target="#b24">[25]</ref>, which has no identity overlap with MS1M-ArcFace.</p><p>Network Architectures. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b14">15]</ref>, we employ the scratch ResNet-10 <ref type="bibr" target="#b19">[20]</ref> as our backbone network for ImageNet-LT. To make a fair comparison with <ref type="bibr" target="#b58">[59]</ref>, the pre-trained ResNet-152 <ref type="bibr" target="#b19">[20]</ref> is used as the backbone network for Places-LT. For MS1M-LT, the popular pretrained ResNet-50 <ref type="bibr" target="#b19">[20]</ref> is the backbone network.    the most related to our work, we directly contrast our results to the numbers reported in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We firstly investigate the merit of each module in our framework. The performance is reported with open-set top-1 classification accuracy on ImageNet-LT. Effectiveness of the Dynamic Meta-Embedding. Recall that the dynamic meta-embedding consists of three main components: memory feature, concept selector, and confidence calibrator. From <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>, we observe that the combination of the memory feature and concept selector leads to large improvements on all three shots. It is because the obtained memory feature transfers useful visual concepts among classes. Another observation is that the confidence calibrator is the most effective on few-shot classes. The reachability estimation inside the confidence calibrator helps distinguish tail classes from open classes. Effectiveness of the Modulated Attention. We observe from <ref type="figure" target="#fig_4">Fig. 5 (a)</ref> that, compared to medium-shot classes, the modulated attention contributes more to the discrimination between many-shot and few-shot classes. <ref type="figure" target="#fig_4">Fig. 5</ref> (c) further validates that the modulated attention is more effective than directly applying spatial attention on feature maps. It implies that adaptive contexts selection is easier to learn than the conventional feature selection. Effectiveness of the Reachability Calibration. To further demonstrate the merit of reachability calibration for openworld setting, we conduct additional experiments following the standard settings in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref> (CIFAR100 + TinyIma-geNet(resized)). The results are listed in <ref type="table" target="#tab_4">Table 2</ref>, where our approach shows favorable performance over standard open-set methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Result Comparisons</head><p>We extensively evaluate the performance of various representative methods on our benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Images Per Class</head><p>Sorted Class ID w.r.t Occurrences Absolute F1 Score Gains <ref type="figure">Figure 6</ref>: The absolute F1 score of our method over the plain model. Ours has across-the-board performance gains w.r.t. many/medium/few-shot and open classes. <ref type="table" target="#tab_5">Table 3</ref> (a) shows the performance comparison of different methods. We have the following observations. Firstly, both Lifted Loss <ref type="bibr" target="#b36">[37]</ref> and Focal Loss <ref type="bibr" target="#b28">[29]</ref> greatly boost the performance of few-shot classes by enforcing feature regularization. However, they also sacrifice the performance on many-shot classes since there are no built-in mechanism of adaptively handling samples of different shots. Secondly, OpenMax <ref type="bibr" target="#b2">[3]</ref> improves the results under the open-set setting. However, the accuracy degrades when it is evaluated with F-measure, which considers both precision and recall in open-set. When the open classes are compounded with the tail classes, it becomes challenging to perform the distribution fitting that <ref type="bibr" target="#b2">[3]</ref> requires. Lastly, though the few-shot learning without forgetting approach <ref type="bibr" target="#b14">[15]</ref> retains the many-shot class accuracy, it has difficulty dealing with the imbalanced base classes which are lacked in the current few-shot paradigm. As demonstrated in <ref type="figure">Fig. 6</ref>, our approach provides a comprehensive treatment to all the many/medium/few-shot classes as well as the open classes, achieving substantial improvements on all aspects.   <ref type="table">Table 4</ref>: Benchmarking results on MegaFace (left) and SUN-LT (right). Our approach achieves the best performance on natural-world datasets when compared to other state-of-the-art methods. Furthermore, our approach achieves across-board improvements on both 'male' and 'female' sub-groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Places-LT. Similar observations can be made on the Places-LT benchmark as shown in</head><p>der both the closed-set and open-set settings. The advantage is even more profound under the F-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS1M-LT.</head><p>We train on the MS1M-LT dataset and report results on the MegaFace identification track, which is a standard benchmark in the face recognition field. Since the face identities in the training set and the test set are disjoint, we adopt an indirect way to partition the testing set into the subsets of different shots. We approximate the pseudo shots of each test sample by counting the number of training samples that are similar to it by at least a threshold (feature similarity greater than 0.7). Apart from many-shot, fewshot, one-shot subsets, we also obtain a zero-shot subset, for which we cannot find any sufficiently similar samples in the training set. It can be observed that our approach has the most advantage on one-shot identities (3.0% gains) and zero-shot identities (1.8% gains) as shown in <ref type="table">Table 4</ref> (left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUN-LT.</head><p>To directly compare with <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b58">[59]</ref>, we also test on the SUN-LT benchmark they provided. The final results are listed in <ref type="table">Table 4</ref> (right). Instead of learning a series of classifier transformations, our approach transfers visual knowledge among features and achieves a 1.4% improvement over the prior best. Note that our approach also incurs much less computational cost since MetaModel-Net <ref type="bibr" target="#b58">[59]</ref> requires a recursive training procedure.</p><p>Indication for Fairness. Here we report the sensitive attribute performance on MS1M-LT. The last two columns in <ref type="table">Table 4</ref> show that our approach achieves across-board improvements on both 'male' and 'female' sub-groups, which has an implication for effective fairness learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Analysis</head><p>Finally we visualize and analyze some influencing aspects in our framework as well as typical failure cases.</p><p>What memory feature has Infused. Here we inspect the visual concepts that memory feature has infused by visualizing its top activating neurons as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Specifically, for each input image, we identify its top-3 transferred neurons in memory feature. And each neuron is For example, to classify the top left image which belongs to a tail class 'cock', our approach has learned to transfer visual concepts that represents "bird head", "round shape" and "dotted texture" respectively. visualized by a collection of highest activated patches <ref type="bibr" target="#b61">[62]</ref> over the whole training set. For example, to classify the top left image which belongs to a tail class 'cock', our approach has learned to transfer visual concepts that represents "bird head", "round shape" and "dotted texture" respectively. After feature infusion, the dynamic metaembedding becomes more informative and discriminative.</p><p>Influence of Dataset Longtail-ness. The longtail-ness of the dataset (e.g. the degree of imbalance of the class distribution) could have an impact on the model performance.</p><p>For faster investigating, here the weights of the backbone network are freezed during training. From <ref type="figure" target="#fig_6">Fig. 8 (a)</ref>, we observe that as the dataset becomes more imbalanced (i.e. power value α decreases), our approach only undergoes a moderate performance drop. Dynamic meta-embedding enables effective knowledge transfer among data-abundant and data-scarce classes.</p><p>Influence of Open-Set Prob. Threshold. The performance change w.r.t. the open-set probability threshold is demonstrated in <ref type="figure" target="#fig_6">Fig. 8 (b)</ref>. Compared to the plain model <ref type="bibr" target="#b19">[20]</ref> and range loss <ref type="bibr" target="#b63">[64]</ref>, the performance of our approach changes Failure Cases. Since our approach encourages the feature infusion among classes, it slightly sacrifices the fine-grained discrimination for the promotion of under-representative classes. One typical failure case of our approach is the confusion between many-shot and medium-shot classes. For example, the bottom right image in <ref type="figure" target="#fig_5">Fig. 7</ref> is misclassified into 'airplane' because some cross-category traits like "nose shape" and "eye shape" are infused. We plan to explore feature disentanglement <ref type="bibr" target="#b4">[5]</ref> to alleviate this trade-off issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We introduce the OLTR task that learns from natural long-tail open-end distributed data and optimizes the overall accuracy over a balanced test set. We propose an integrated OLTR algorithm, dynamic meta-embedding, in order to share visual knowledge between head and tail classes and to reduce confusion between tail and open classes. We validate our method on three curated large-scale OLTR benchmarks (ImageNet-LT, Places-LT and MS1M-LT). Our publicly available code and data would enable future research that is directly transferable to real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intuitive Explanation of Our Approach</head><p>In this section, we give an intuitive explanation of our approach that tackles the problem open long-tail recognition. From the perspective of knowledge gained from observation (i.e. training set), head classes, tail classes and open classes form a continuous spectrum as illustrated in <ref type="figure" target="#fig_8">Fig. 9</ref>.  Firstly, we obtain a visual memory by aggregating the knowledge from both head and tail classes. Then the visual concepts stored in the memory are infused back as associated memory feature to enhance the original direct feature. It can be understood as using induced knowledge (i.e. memory feature) to assist the direct observation (i.e. direct feature). We further learn a concept selector to control the amount and type of memory feature to be infused. Since head classes already have abundant direct observation, only a small amount of memory feature is infused for them. On the contrary, tail classes suffer from scarce observation, the associated visual concepts in memory feature are extremely beneficial. Finally, we calibrate the confidence of open classes by calculating their reachability to the obtained visual memory. In this way, we provide a comprehensive treatment to the full spectrum of head, tail and open classes, improving the performance on all categories. To summarize, the effects of each component in our approach are listed in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relation to Fairness Analysis</head><p>The open long-tail recognition proposed in our work also has an intrinsic relationship to fairness analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b0">1]</ref>. Their key differences are listed in <ref type="table" target="#tab_9">Table 6</ref>. On the problem setting side, both open long-tail recognition and fairness analysis aim to tackle the imbalance existed in real-world data. Open long-tail recognition focuses on the longtail-ness in both known and unknown categories while fairness analysis deals with the bias in sensitive attributes such as male/female and white/black.</p><p>On the methodology side, both open long-tail recognition and fairness analysis aim to learn transferable representations. Open long-tail recognition optimizes for the overall accuracy of all categories while fairness analysis optimizes for several attribute-wise criteria. The preliminary results in <ref type="table">Table 4</ref> demonstrates that our proposed dynamic metaembedding is also a promising solution to fairness analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Methodology Details</head><p>Notation Summary. We summarize the notations used in the paper in <ref type="table" target="#tab_11">Table 7</ref>.  Open Classes Obtaining Discriminative Centroids. The step-by-step procedure for obtaining discriminative centroids {c i } K i=1 is further illustrated in <ref type="figure" target="#fig_0">Fig. 11</ref>.  <ref type="figure" target="#fig_0">Figure 11</ref>: The discriminative centroids constitute our visual memory, which are obtained with two iterative steps, neighborhood sampling and affinity propagation.</p><p>Detailed Loss Functions. Here we elaborate the two loss functions L CE and L LM described in Eqn. 7 in the main paper. Specifically, L CE is the cross-entropy loss between dynamic meta-embedding v meta n and the ground truth category label y n :</p><formula xml:id="formula_8">L CE (v meta n , y n ) = y n log(φ(v meta n )) + (1 − y n ) log(1 − φ(v meta n )),<label>(8)</label></formula><p>where φ(·) is the cosine classifier described in Eqn. 6 in the main paper. Next we introduce the large margin loss L LM between the embedding v meta n and the centroids {c i } K i=1 :</p><formula xml:id="formula_9">L LM (v meta n , {c i } K i=1 ) = max(0, i=yn v meta n − c i − i =yn v meta n − c i + m),<label>(9)</label></formula><p>where m is the margin and we set it as 5.0 in our experiments. With this formulation, we minimize the distance between each embedding and the centroid of its group and meanwhile maximize the distance between the embedding and the centroids it does not belong to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Open Long-Tail Dataset Preparation</head><p>ImageNet-LT. The training data set was generated using a Pareto distribution <ref type="bibr" target="#b41">[42]</ref> with a power value α=6 and 1,280∼5 images per class from the 1000 classes of Ima-geNet dataset. Images were randomly selected based on the distribution values of each class. The classes were sorted following the benchmark proposed by Bharath &amp; Girshick <ref type="bibr" target="#b17">[18]</ref>, where the 1000 classes were randomly split into 389 base classes and 611 novel classes. The first 389 largest classes in ImageNet-LT are the same as the base classes in the benchmark, and the rest 611 classes are the same as the novel classes. We randomly selected 20 training images per class from the origin training set as validation set. The original validation set of ImageNet was used as testing set in this paper. The dataset specifications are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p><p>Places-LT. The training data set was generated similarly to ImageNet-LT using a Pareto distribution with a power value α=6 and 4,980∼5 images per class from the 365 classes of Places-365-standard data set. We used the distribution order of Places-365-challenge data set (which is imbalanced) to sort the training data classes. We also randomly selected 20 images per class from the original training set as validation set. The original validation set of Places-365 was used as testing set in this paper. The dataset specifications are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>  from the 1M gallery for each sample in the probe set. Then the identification rate is the mean of hit rates. Since the identities in training set and testing set are non-overlapped, we adopt an indirect way to partition the testing set into subsets with different shots. We approximate the pseudo occurrences of each test sample by counting the number of the similar (similarity greater than 0.7) training samples. The similarity is calculated as the feature distance produced by a state-of-the-art face recognition system <ref type="bibr" target="#b8">[9]</ref>. Apart from many-shot, few-shot and one-shot subset, we also define a zero-shot subset, for which we cannot find similar samples in the training set. The dataset specifications are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. SUN-LT. We used the same training and testing data set as provided by <ref type="bibr" target="#b58">[59]</ref>, where there were 1,132∼1 images per class in the training set and 40 images per class in the testing set. We randomly selected 5 images from un-used training data as our validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Data Pre-processing</head><p>All the images were firstly resized to 256 × 256. During training, the images were randomly cropped to 224 × 224, then augmented with random horizontal flip at probability p = 0.5 and random color jitter on brightness, contrast, and saturation with jitter factor of 0.4. During validation and testing, images were center cropped to 224 × 224 without further augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Training Details</head><p>ImageNet-LT. The feature extractor model used in the experiments on ImageNet-LT was a ResNet-10 model initialized from scratch (i.e., random initialization). All different classifiers were also initialized from scratch. Some major hyper-parameters can be found in <ref type="table" target="#tab_15">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Places-LT &amp; SUN-LT.</head><p>We used a two-stage training protocol following <ref type="bibr" target="#b14">[15]</ref> when conducting experiments on both Places-LT and SUN-LT. (1) In the first stage, we used the ImageNet pre-trained ResNet-152 feature model with a dot-product classifier to fine-tune on the training data of Places-LT and SUN-LT. (2) In the second stage, we used the Places-LT/SUN-LT pre-trained model as our feature model and freezed the convolutional weights. Finally we finetuned the classifiers initialized from scratch to produce the experimental results. Some major hyper-parameters can be found in <ref type="table" target="#tab_15">Table 8</ref>.  <ref type="figure" target="#fig_0">Figure 15</ref>: Examples of the infused visual concepts from memory feature in MS1M-LT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS1M-LT.</head><p>We used the ImageNet pre-trained ResNet-50 with a linear classifier and cross-entropy loss to train the face recognition model. Some major hyper-parameters can be found in <ref type="table" target="#tab_15">Table 8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Evaluation Protocols</head><p>Top-1 Classification Accuracy. F-measure. Following <ref type="bibr" target="#b2">[3]</ref>, the F-measure (F ) is calculated as 2 times the product of precision (p) and recall (r) divided by the sum of p and r:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our task of open long-tailed recognition must learn from long-tail distributed training data in an open world and deal with imbalanced classification, few-shot learning, and open-set recognition over the entire spectrum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Method overview. There are two main modules: dynamic meta-embedding and modulated attention. The embedding relates visual concepts between head and tail classes, while the attention discriminates between them. The reachability separates tail and open classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>t-SNE feature visualization of (a) plain ResNet model (b) our dynamic meta-embedding. Ours is more compact for both head and tail classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results of ablation study. Dynamic meta-embedding contributes most on medium-shot and few-shot classes while modulated attention helps maintain the discrimination of many-shot classes. (The performance is reported with open-set top-1 classification accuracy on ImageNet-LT.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Examples of the top-3 infused visual concepts from memory feature. Except for the bottom right failure case (marked in red), all the other three input images are misclassified by the plain model and correctly classified by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The influence of (a) dataset longtail-ness, (b) open-set probability threshold, and (c) the number of open classes. As the dataset becomes more imbalanced, our approach only undergoes a moderate performance drop. Our approach also demonstrates great robustness to the contamination of open classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>steadily as the open-set threshold rises. The reachability estimator in our framework helps calibrate the sample confidence, thus enhancing robustness to open classes. Influence of the Number of Open Classes. Finally we investigate performance change w.r.t. the number of open classes. Fig. 8 (c) indicates that our approach demonstrates great robustness to the contamination of open classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Intuition explanation of our approach. Direct + Memory Feature Modulated Attention Reachability Module Transfer knowledge Maintain discrimination Deal with open classes between head/tail classes between head/tail classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>The dataset statistics of ImageNet-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>The dataset statistics of Places-LT. The dataset statistics of MS1M-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>For ImageNet-LT, Places-LT, and SUN-LT, since the testing sets are balanced, the top-1 classification accuracy are calculated as the mean accuracy over all close-set categories with the contamination of open classes. All open classes are regared as one unknown class. Predictions of data are obtained as the classes with the highest sof tmax probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1904.05160v2 [cs.CV] 16 Apr 2019 Task Setting Imbalanced Train/Base Set #Instances in Tail Class Balanced Test Set Open Class Evaluation: Accuracy Over ?</figDesc><table><row><cell>Imbalanced Classification</cell><cell></cell><cell>20∼50</cell><cell>×</cell><cell>×</cell><cell>all classes</cell></row><row><cell>Few-Shot Learning</cell><cell>×</cell><cell>1∼20</cell><cell></cell><cell>×</cell><cell>novel classes</cell></row><row><cell>Open-Set Recognition</cell><cell>×</cell><cell>N/A</cell><cell></cell><cell></cell><cell>all classes</cell></row><row><cell>Open Long-Tailed Recognition</cell><cell></cell><cell>1∼20</cell><cell></cell><cell></cell><cell>all classes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. ImageNet-LT:We construct a long-tailed version of the original ImageNet-2012<ref type="bibr" target="#b7">[8]</ref> by sampling a subset following the Pareto distribution with the power value α=6.Overall, it has 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set. We make the test set balanced.2. Places-LT:A long-tailed version of Places-2<ref type="bibr" target="#b64">[65]</ref> is constructed in a similar way. It contains 184.5K images from 365 categories, with the maximum of 4980 images per class and the minimum of 5 images per class. The gap between the head and tail classes are even larger than ImageNet-LT. We use the test images from Places-Extra69 as the additional open-set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Open class detection error (%) comparison.</figDesc><table><row><cell>It is performed on the stan-</cell></row><row><cell>dard open-set benchmark, CI-</cell></row><row><cell>FAR100 + TinyImageNet (re-</cell></row><row><cell>sized). " †" denotes the setting</cell></row><row><cell>where open samples are used to</cell></row><row><cell>tune algorithmic parameters.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(b). With a much</cell></row></table><note>(b) Top-1 classification accuracy on Places-LT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Backbone Net</cell><cell></cell><cell></cell><cell cols="2">MegaFace Identification Rate</cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>Acc.</cell></row><row><cell>ResNet-50 Methods</cell><cell>5 Many-shot</cell><cell cols="2">&lt; 5 &amp; 2 &lt; 2 &amp; 1 Few-shot One-shot</cell><cell cols="2">= 0 Zero-shot Full Test</cell><cell cols="2">Sub-Groups Male Female</cell><cell>Plain Model [20] Cost-Sensitive [24]</cell><cell>48.0 52.4</cell></row><row><cell>Plain Model [20]</cell><cell>80.64</cell><cell>71.98</cell><cell>84.60</cell><cell>77.72</cell><cell>73.88</cell><cell>78.30</cell><cell>78.70</cell><cell>Model Reg. [58]</cell><cell>54.7</cell></row><row><cell>Range Loss [64]</cell><cell>78.60</cell><cell>71.36</cell><cell>83.14</cell><cell>77.40</cell><cell>72.17</cell><cell>-</cell><cell>-</cell><cell>MetaModelNet [59]</cell><cell>57.3</cell></row><row><cell>Ours</cell><cell>80.82</cell><cell>72.44</cell><cell>87.60</cell><cell>79.50</cell><cell>74.51</cell><cell>79.04</cell><cell>79.08</cell><cell>Ours</cell><cell>58.7</cell></row></table><note>Benchmarking results on (a) ImageNet-LT and (b) Places-LT. Our approach provides a comprehensive treatment to all the many/medium/few-shot classes as well as the open classes, achieving substantial advantages on all aspects.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The effects of each component in our approach.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Key differences between fairness analysis and open long-tail recognition. "asp." stands for aspects while "obj." stands for objectives.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Summary of notations.</figDesc><table><row><cell>Number of Images per Class</cell><cell>partridge</cell><cell>pitcher</cell><cell>flag pole</cell><cell>swimming cap</cell><cell>water snake</cell><cell>? ? ? ? ?</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>500</cell><cell></cell><cell>1000</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Class ID</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>.</head><label></label><figDesc>MS1M-LT. This dataset was generated from a large-scale face recognition dataset, named MS1M-ArcFace. The original dataset contains about 5.8M images with 85K identities. To create a long-tail version, we sampled images for each identity with a probability proportional to the image numbers of each identity. It results in 887.5K images and 74.5K identities, with a long-tail distribution.For the evaluation set, MegaFace is one of the largest face recognition benchmarks. It contains 3,530 images from FaceScrub dataset as a probe set and 1M images as a gallery set. The identification task is to find top-1 nearest image</figDesc><table><row><cell>Number of Images per Class</cell><cell>amusement park</cell><cell>corridor</cell><cell>canal</cell><cell>history museum</cell><cell>jacuzzi</cell><cell>? ? ? ? ?</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>180</cell><cell></cell><cell>365</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Class ID</cell><cell></cell><cell></cell><cell>Open Classes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Ball Pit Market Outdoor Bus Interior Amusement Arcade Canal Natural Castle Corral Arena Rodeo</head><label></label><figDesc>Transferred Neurons from Memory Feature Plain Model Prediction Figure 14: Examples of the infused visual concepts from memory feature in Places-LT.</figDesc><table><row><cell cols="5">Input Image Top-3 Input Image Top-3 Transferred Neurons from Memory Feature Plain Model Prediction Input Image Input Image Top-1 Transferred Neuron Top-1 Transferred Neuron Input Image from Memory Feature from Memory Feature</cell><cell cols="2">Top-1 Transferred Neuron from Memory Feature</cell></row><row><cell cols="2">High Cheekbones</cell><cell cols="2">Dark Skin Color</cell><cell></cell><cell></cell><cell>Narrow Eyes</cell></row><row><cell>Least Activated</cell><cell>Most Activated</cell><cell>Least Activated</cell><cell>Most Activated</cell><cell cols="2">Least Activated</cell><cell>Most Activated</cell></row><row><cell>Average Image</cell><cell>Average Image</cell><cell>Average Image</cell><cell>Average Image</cell><cell></cell><cell>Average Image</cell><cell>Average Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>The major hyper-parameters used in our experiments. "LR." stands for learning rate.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In this supplementary material, we provide details omitted in the main text including: • Section A: intuitive explanation of our approach (Sec. <ref type="bibr" target="#b0">1</ref> "Introduction" of the main paper.) • Section B: relation to fairness analysis (Sec. 2 "Related Work" of the main paper.) • Section C: more methodology details (Sec. 3 "Approach" of the main paper.) • Section D: detailed experimental setup (Sec. 4 "Experiments" of the main paper.) • Section E: additional visualization of our approach (Sec. 4.3 "Further Analysis" of the main paper.)</p><p>p is calculated as true positive (T p , defined as correct predictions on the closed testing set) over the sum of T p and false positive (F p , defined as incorrect predictions on closed testing set):</p><p>r is calculated as T p over the sum of T p and false negative (F n , defined as number of images from the open set that are predicted as known categories):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Visualization</head><p>Memory Feature in Places-LT. We visualize the memory feature in Places-LT similarly to ImageNet-LT as described in Sec. 4.3 in the main paper. Examples of the infused visual concepts from memory feature in Places-LT are presented in <ref type="figure">Fig. 14.</ref> We observe that memory feature encodes discriminative visual traits for the underlying scene. Memory Feature in MS-1M. Following <ref type="bibr" target="#b31">[32]</ref>, we visualize the memory feature in MS1M-LT by contrasting the least activated average image and the most activated average image of the top firing neuron. From <ref type="figure">Fig. 15</ref>, we observe that memory feature in MS1M-LT infuses several identityrelated attributes (e.g. "high cheekbones", "dark skin color" and "narrow eyes") for precise recognition.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The battle against the long tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Talk on Workshop on Big Data and Statistical Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Class rectification hard mining for imbalanced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rl 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Omer Reingold, and Richard Zemel. Fairness through awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd innovations in theoretical computer science conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the Cognitive Science Society</title>
		<meeting>the ninth annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10125</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06309</idno>
		<title level="m">Learning adversarially fair and transferable representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A comparison of visual features used by humans and machines to classify wildlife. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitlyn</forename><forename type="middle">M</forename><surname>Gaynor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Muellerklein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Norouzzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcinturff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Rauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Bowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Nathon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03993</idno>
		<title level="m">Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00837</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Meta networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The pareto, zipf and other power laws. Economics letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Incremental few-shot learning with attention attractor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07218</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to share visual appearance for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02274</idno>
		<title level="m">Episodic curiosity through reachability</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Toward open set recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archana</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A neural network that embeds its own meta-levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICNN</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05401</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung Yongxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with longtailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Capturing long-tail distributions of object subcategories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Do we need more training data? IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
