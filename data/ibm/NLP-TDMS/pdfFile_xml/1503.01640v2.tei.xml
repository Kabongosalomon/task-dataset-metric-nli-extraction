<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<email>kahe@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>jiansun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with humanannotated, pixel-level segmentation masks. Such pixelaccurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called "BoxSup", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PAS-CAL VOC 2012 and PASCAL-CONTEXT <ref type="bibr" target="#b23">[24]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few months, tremendous progress has been made in the field of semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref>. Deep convolutional neural networks (CNNs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> that play as rich hierarchical feature extractors are a key to these methods. These networks are trained on large-scale datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> as classifiers, and transferred to the semantic segmentation tasks based on the annotated segmentation masks as supervision.</p><p>But pixel-level mask annotations are time-consuming, frustrating, and in the end commercially expensive to obtain. According to the annotation report of the large-scale Microsoft COCO dataset <ref type="bibr" target="#b20">[21]</ref>, the workload of labeling segmentation masks is more than 15 times heavier than that of spotting object locations. Further, the crowdsourcing annotators need to be specially trained for the tedious and diffi-cult task of labeling per-pixel masks. These facts limit the amount of available segmentation mask annotations, and thus hinder the performance of CNNs that in general desire large-scale data for training. On the contrary, bounding box annotations are more economical than masks. There have already existed a large number of available box-level annotations in datasets like PASCAL VOC 2007 <ref type="bibr" target="#b0">1</ref>  <ref type="bibr" target="#b7">[8]</ref> and ImageNet <ref type="bibr" target="#b26">[27]</ref>. Though these box-level annotations are less precise than pixel-level masks, their amount may help improve training deep networks for semantic segmentation.</p><p>In addition, current leading approaches have not fully utilized the detailed pixel-level annotations. For example, in the Convolutional Feature Masking (CFM) method <ref type="bibr" target="#b5">[6]</ref>, the fine-resolution masks are used to generate very lowresolution (e.g., 6 × 6) masks on the feature maps. In the Fully Convolutional Network (FCN) method <ref type="bibr" target="#b21">[22]</ref>, the network predictions are regressed to the ground-truth masks using a large stride (e.g., 8 pixels). These methods yield competitive results without explicitly harnessing the finer masks. If we consider the box-level annotations as very coarse masks, can we still retain comparably good results without using the segmentation masks?</p><p>In this work, we investigate bounding box annotations as an alternative or extra source of supervision to train convolutional networks for semantic segmentation 2 . We resort to unsupervised region proposal methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2]</ref> to generate candidate segmentation masks. The convolutional network is trained under the supervision of these approximate masks. The updated network in turn improves the estimated masks used for training. This process is iterated. Although the masks are coarse at the beginning, they are gradually improved and then provide useful information for network training. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates our training algorithm.</p><p>We extensively evaluate our method, called "BoxSup", on the PASCAL segmentation benchmarks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>. Our <ref type="bibr" target="#b0">1</ref> The PASCAL VOC 2007 dataset only has bounding box annotations. <ref type="bibr" target="#b1">2</ref> The idea of using bounding box annotations for CNN-based semantic segmentation is developed concurrently and independently in <ref type="bibr" target="#b24">[25]</ref>. We also compare with the results of <ref type="bibr" target="#b24">[25]</ref>.  box-supervised (i.e., using bounding box annotations) method shows a graceful degradation compared with its mask-supervised (i.e., using mask annotations) counterpart. As such, our method waives the requirement of pixel-level masks for training. Further, our semi-supervised variant in which 9/10 mask annotations are replaced with bounding box annotations yields comparable accuracy with the fully mask-supervised counterpart. This suggests that we may save expensive labeling effort by using bounding box annotations dominantly. Moreover, our method makes it possible to harness the large number of available box annotations to improve the mask-supervised results. Using the limited provided mask annotations and extra large-scale bounding box annotations, our method achieves state-of-the-art results on both PASCAL VOC 2012 and PASCAL-CONTEXT <ref type="bibr" target="#b23">[24]</ref> benchmarks. Why can a large amount of bounding boxes help improve convolutional networks? Our error analysis reveals that a BoxSup model trained with a large set of boxes effectively increases the object recognition accuracy (the accuracy in the middle of an object), and its improvement on object boundaries is secondary. Though a box is too coarse to contain detailed segmentation information, it provides an instance for learning to distinguish object categories. The large-scale object instances improve the feature quality of the learned convolutional networks, and thus impact the overall performance for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep convolutional networks in general have better accuracy with the growing size of training data, as is evidenced in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>. The ImageNet classification dataset <ref type="bibr" target="#b26">[27]</ref> is one of the largest datasets with quality labels, but the current available datasets for object detection, semantic segmentation, and many other vision tasks mostly have orders of magnitudes fewer labeled samples. The milestone work of R-CNN <ref type="bibr" target="#b8">[9]</ref> proposes to pre-train deep networks as classifiers on the large-scale ImageNet dataset and go on training (fine-tuning) them for other tasks that have limited number of training data. This transfer learning strategy is widely adopted for object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>, semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref>, visual tracking <ref type="bibr" target="#b31">[32]</ref>, and other visual recognition tasks. With the continuously improving deep convolutional models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref>, the accuracy of these vision tasks also improves thanks to the more powerful generic features learned from large-scale datasets.</p><p>Although pre-training partially relieves the problem of limited data, the amount of the task-specific data for finetuning still matters. In <ref type="bibr" target="#b0">[1]</ref>, it has been found that augmenting the object detection training set by combining the VOC 2007 and VOC 2012 sets improves object detection accuracy compared with using VOC 2007 only. In <ref type="bibr" target="#b19">[20]</ref>, the training set for object detection is augmented by visual tracking results obtained from videos and improves detection accuracy. These experiments demonstrate the importance of dataset sizes for task-specific network training.</p><p>For semantic segmentation, there have been existing papers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b9">10]</ref> that investigate exploiting bounding box annotations instead of masks. But the box-level annotations have not been used to supervised deep convolutional networks in those works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Baseline</head><p>Our BoxSup method is in general applicable for many existing CNN-based mask-supervised semantic segmentation methods, such as FCN <ref type="bibr" target="#b21">[22]</ref>, improvements on FCN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>, and others <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. In this paper, we adopt our implementation of the FCN method <ref type="bibr" target="#b21">[22]</ref> refined by CRF <ref type="bibr" target="#b4">[5]</ref> as the mask-supervised baseline, which we briefly introduce  as follows.</p><p>The network training of FCN <ref type="bibr" target="#b21">[22]</ref> is formulated as a perpixel regression problem to the ground-truth segmentation masks. Formally, the objective function can be written as:</p><formula xml:id="formula_0">E(θ) = p e(X θ (p), l(p)),<label>(1)</label></formula><p>where p is a pixel index, l(p) is the ground-truth semantic label at a pixel, and X θ (p) is the per-pixel labeling produced by the fully convolutional network with parameters θ. e(X θ (p), l(p)) is the per-pixel loss function. The network parameters θ are updated by back-propagation and stochastic gradient descent (SGD). A CRF is used to post-process the FCN results <ref type="bibr" target="#b4">[5]</ref>. The objective function in Eqn.(1) demands pixel-level segmentation masks l(p) as supervision. It is not directly applicable if only bounding box annotations are given as supervision. Next we introduce our method for addressing this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unsupervised Segmentation for Supervised Training</head><p>To harness the bounding boxes annotations, it is desired to estimate segmentation masks from them. This is a widely studied supervised image segmentation problem, and can be addressed by, e.g., GrabCut <ref type="bibr" target="#b25">[26]</ref>. But GrabCut can only generate one or a few samples from one box, which may be insufficient for deep network training.</p><p>We propose to generate a set of candidate segments using unsupervised region proposal methods (e.g., Selective Search <ref type="bibr" target="#b30">[31]</ref>) due to their nice properties. First, region proposal methods have high recall rates <ref type="bibr" target="#b1">[2]</ref> of having a good candidate in the proposal pool. Second, region proposal methods generate candidates of greater variance, which provide a kind of data augmentation <ref type="bibr" target="#b17">[18]</ref> for network training. We will show by experiments the improvements of these properties.</p><p>The candidate segments are used to update the deep convolutional network. The semantic features learned by the network are then used to pick better candidates. This procedure is iterated. We formulate this procedure as an objective function as we will describe below.</p><p>It is worth noticing that the region proposal is only used for networking training. For inference, the trained FCN is directly applied on the image and produces pixel-wise predictions. So our usage of region proposals does not impact the test-time efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Formulation</head><p>As a pre-processing, we use a region proposal method to generate segmentation masks. We adopt Multiscale Combinatorial Grouping (MCG) <ref type="bibr" target="#b1">[2]</ref> by default, while other methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17]</ref> are also evaluated. The proposal candidate masks are fixed throughout the training procedure. But during training, each candidate mask will be assigned a label which can be a semantic category or background. The labels assigned to the masks will be updated.</p><p>With a ground-truth bounding box annotation, we expect it to pick out a candidate mask that overlaps the box as much as possible. Formally, we define an overlapping objective function E o as:  Each segmentation mask will be used as the supervision for the next epoch.</p><formula xml:id="formula_1">E o = 1 N S (1 − IoU(B, S))δ(l B , l S ).<label>(2)</label></formula><p>With the candidate masks and their estimated semantic labels, we can supervise the deep convolutional network as in Eqn. <ref type="bibr" target="#b0">(1)</ref>. Formally, we consider the following regression objective function E r :</p><formula xml:id="formula_2">E r = p e(X θ (p), l S (p)).<label>(3)</label></formula><p>Here l S is the estimated semantic label used as supervision for the network training. This objective function is the same as Eqn.</p><p>(1) except that its regression target is the estimated candidate segment. We minimize an objective function that combines the above two terms: </p><formula xml:id="formula_3">min θ,{l S } i (E o + λE r )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Algorithm</head><p>The objective function in Eqn.(4) involves a problem of assigning labels to the candidate segments. Next we propose a greedy iterative solution to find a local optimum.</p><p>With the network parameters θ fixed, we update the semantic labeling {l S } for all candidate segments. In our implementation, we only consider the case in which one ground-truth bounding box can "activate" (i.e., assign a non-background label to) one and only one candidate. As such, we can simply update the semantic labeling by selecting a single candidate segment for each ground-truth bounding box, such that its cost E o + λE r is the smallest among all candidates. The selected segment is assigned the groundtruth semantic label associated with that bounding box. All other pixels are assigned the background label.</p><p>The above winner-takes-all selection tends to repeatedly use the same or very similar candidate segments, and the optimization procedure may be trapped in poor local optima. To increase the sample variance for better stochastic training, we further adopt a random sampling method to select the candidate segment for each ground-truth bounding box. Instead of selecting the single segment with the largest cost E o + λE r , we randomly sample a segment from the first k segments with the largest costs. In this paper we use k = 5. This random sampling strategy improves the accuracy by about 2% on the validation set.</p><p>With the semantic labeling {l S } of all candidate segments fixed, we update the network parameters θ. In this case, the problem becomes the FCN problem <ref type="bibr" target="#b21">[22]</ref> as in Eqn. <ref type="bibr" target="#b0">(1)</ref>. This problem is minimized by SGD.</p><p>We iteratively perform the above two steps, fixing one set of variables and solving for the other set. For each iteration, we update the network parameters using one training epoch (i.e., all training images are visited once), and after that we update the segment labeling of all images. <ref type="figure" target="#fig_3">Fig.3</ref> shows the gradually updated segmentation masks during training. The network is initialized by the model pre-trained in the Ima-geNet classification dataset, and our algorithm starts from the step of updating segment labels.</p><p>Our method is applicable for the semi-supervised case (the ground-truth annotations are mixtures of segmentation masks and bounding boxes). The labeling l(p) is given by candidate proposals as above if a sample only has groundtruth boxes, and is simply assigned as the true label if a sample has ground-truth masks.</p><p>In the SGD training of updating the network, we use a mini-batch size of 20, following <ref type="bibr" target="#b21">[22]</ref>. The learning rate is initialized to be 0.001 and divided by 10 after every 15 epochs. The training is terminated after 45 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In all our experiments, we use the publicly released VGG-16 model <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b28">[29]</ref> that is pre-trained on ImageNet <ref type="bibr" target="#b26">[27]</ref>. The VGG model is also used by all competitors <ref type="bibr">[22, 13, 6,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on PASCAL VOC 2012</head><p>We first evaluate our method on the PASCAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b7">[8]</ref>. This dataset involves 20 semantic categories of objects. We use the "comp6" evaluation protocol. The accuracy is evaluated by mean IoU scores. The original training data has 1,464 images. Following <ref type="bibr" target="#b10">[11]</ref>, the training data with ground-truth segmentation masks are augmented to 10,582 images. The validation and test sets have 1,449 and 1,456 images respectively. When evaluating the validation set or the test set, we only use the training set for training. A held-out 100 random validation images are used for cross-validation to set hyper-parameters. <ref type="table">Table 1</ref> compares the results of using different strategies of supervision on the validation set. When all ground-truth masks are used as supervision, the result is our implementation of the baseline DeepLab-CRF <ref type="bibr" target="#b4">[5]</ref>. Our reproduction has a score of 63.8 ( <ref type="table">Table 1</ref>, "mask only"), which is very close to 63.74 reported in <ref type="bibr" target="#b4">[5]</ref> under the same setting. So we believe that our reproduced baseline is convincing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Supervision Strategies</head><p>When all 10,582 training samples are replaced with bounding box annotations, our method yields a score of 62.0 ( <ref type="table">Table 1</ref>, "box only"). Though the supervision information is substantially weakened, our method shows a graceful degradation (1.8%) compared with the strongly supervised baseline of 63.8. This indicates that in practice we can avoid the expensive mask labeling effort by using only bounding boxes, with small accuracy loss. <ref type="table">Table 1</ref> also shows the semi-supervised result of our method. This result uses the ground-truth masks of the original 1,464 training images and the bounding box annotations of the rest 9k images. The score is 63.5 ( <ref type="table">Table 1</ref>, "semi"), on par with the strongly supervised baseline. Such semi-supervision replaces 9/10 of the segmentation mask annotations with bounding box annotations. This means that we can greatly reduce the labeling effort by dominantly using bounding box annotations.</p><p>As a proof of concept, we further evaluate using a sub- stantially larger set of boxes. We use the Microsoft COCO dataset <ref type="bibr" target="#b20">[21]</ref> that has 123,287 images with available groundtruth segmentation masks. This dataset has 80 semantic categories, and we only use the 20 categories that also present in PASCAL VOC. For our mask-supervised baseline, the result is a score of 68.1 ( <ref type="table">Table 1</ref>). Then we replace the ground-truth segmentation masks in COCO with their tight bounding boxes. Our semi-supervised result is 68.2 (Table 1), on par with the strongly supervised baseline. <ref type="figure" target="#fig_6">Fig. 5</ref> shows some visual results in the validation set. The semi-supervised result (68.2) that uses VOC+COCO is considerably better than the strongly supervised result (63.8) that uses VOC only. The 4.4% gain is contributed by the extra large-scale bounding boxes in the 123k COCO images. This comparison suggests a promising strategywe may make use of the larger amount of existing bounding boxes annotations to improve the overall semantic segmentation results, as further analyzed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis</head><p>Why can a large set of bounding boxes help improve convolutional networks? The error in semantic segmentation can be roughly thought of as two types: (i) recognition error that is due to confusions of recognizing object categories, and (ii) boundary error that is due to misalignments of pixel-level labels on object boundaries. Although the bounding box annotations have no information about the object boundaries, they provide extra object instances for recognizing them. We may expect that the large amount of masks   boxes mainly improve the recognition accuracy.</p><p>To analyze the error, we separately evaluate the performance on the boundary regions and interior regions. Following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5]</ref>, we generate a "trimap" near the ground-truth boundaries <ref type="figure" target="#fig_4">(Fig. 4, top)</ref>. We evaluate mean IoU scores inside/outside the bands, referred to as boundary/interior regions. <ref type="figure" target="#fig_4">Fig. 4 (bottom)</ref> shows the results of using different band widths for the trimaps.</p><p>For the interior region, the accuracy of using the extra COCO boxes (red solid line, <ref type="figure" target="#fig_4">Fig. 4</ref>) is considerably higher than that of using VOC masks only (blue solid line). On the contrary, the improvement on the boundary regions is relatively smaller (red dash line vs. blue dash line). Note that correctly recognizing the interior may also help improve the boundaries (e.g., due to the CRF post-processing). So the improvement of the extra boxes on the boundary regions is secondary.</p><p>Because the accuracy in the interior region is mainly determined by correctly recognizing objects, this analysis suggests that the large amount of boxes improve the feature quality of a learned BoxSup model for better recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Estimated Masks for Supervision</head><p>In <ref type="table" target="#tab_5">Table 2</ref> we evaluate different methods of estimating masks from bounding boxes for supervision. As a naïve baseline, we fill each bounding box with its semantic label, and consider it as a rectangular mask <ref type="figure" target="#fig_2">(Fig. 2(c)</ref>). Using these rectangular masks as the supervision throughout training, the score is 52.3 on the validation set. We also use GrabCut <ref type="bibr" target="#b25">[26]</ref> to generate segmentation masks from boxes ( <ref type="figure" target="#fig_2">Fig. 2(d)</ref>  <ref type="table">Table 4</ref>: Results on PASCAL VOC 2012 test set. In the supervision ("sup") column, "mask" means all training samples are with segmentation mask annotations, "box" means all training samples are with bounding box annotations, and "semi" means mixtures. "V" denotes the VOC data, "C" denotes the COCO data, and "V 07 " denotes the VOC 2007 data which only has bounding boxes available.</p><p>throughout training, the score is 55.2. In both cases, the masks are not updated by the network feedbacks. Our method has a score 62.0 (Table 2) using the same set of bounding box annotations. This is a considerable gain over the baseline using fixed GrabCut masks. This indicates the importance of the mask quality for supervision. <ref type="figure" target="#fig_3">Fig. 3</ref> shows that our method iteratively updates the masks by the network, which in turn improves the network training.</p><p>We also evaluate a variant of our method where each time the updated mask is the candidate with the largest cost, instead of randomly sampled from the first k candidates (see <ref type="bibr">Sec. 4.3)</ref>. This variant has a lower score of 59.7 ( <ref type="table" target="#tab_5">Table 2</ref>). The random sampling strategy, which is data augmentation and increases sample variances, is beneficial for training. <ref type="table" target="#tab_5">Table 2</ref> also shows the result of the concurrent method WSSL <ref type="bibr" target="#b4">[5]</ref> under the same evaluation setting. Its results is 58.5. This result suggests that our method estimates more accurate masks than <ref type="bibr" target="#b4">[5]</ref> for supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Region Proposals</head><p>Our method resorts to unsupervised region proposals for training. In <ref type="table" target="#tab_6">Table 3</ref>, we compare the effects of various region proposals on our method: Selective Search (SS) <ref type="bibr" target="#b30">[31]</ref>, Geodesic Object Proposals (GOP) <ref type="bibr" target="#b16">[17]</ref>, and MCG <ref type="bibr" target="#b1">[2]</ref>. <ref type="table" target="#tab_6">Table 3</ref> shows that MCG <ref type="bibr" target="#b1">[2]</ref> has the best accuracy, which is consistent with its segmentation quality evaluated by other metrics in <ref type="bibr" target="#b1">[2]</ref>. Note that at test-time our method does not need region proposals. So the better accuracy of using MCG implies that our method effectively makes use of the higher quality segmentation masks to train a better network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons on the Test Set</head><p>Next we compare with the state-of-the-art methods on  the PASCAL VOC 2012 test set. In <ref type="table">Table 4</ref>, the methods are based on the same FCN baseline and thus fair comparisons are made to evaluate the impact of mask/box/semisupervision. As shown in <ref type="table">Table 4</ref>, our box-supervised result that only uses VOC bounding boxes is 64.6. This compares favorably with the WSSL <ref type="bibr" target="#b24">[25]</ref> counterpart (60.4) under the same setting. On the other hand, our box-supervised result has a graceful degradation (1.8%) compared with the masksupervised DeepLab-CRF (66.4 <ref type="bibr" target="#b4">[5]</ref>) using the VOC training data. Moreover, our semi-supervised variant which replaces 9/10 segmentation mask annotations with bounding boxes has a score of 66.2. This is on par with the mask-supervised counterpart of DeepLab-CRF, but the supervision information used by our method is much weaker.</p><p>In the WSSL paper <ref type="bibr" target="#b24">[25]</ref>, by using all segmentation mask annotations in VOC and COCO, the strongly masksupervised result is 70.4. Our semi-supervised method shows a higher score of 71.0. Remarkably, our result uses the bounding box annotations from the 123k COCO images. So our method has a more accurate result but uses much weaker annotations than <ref type="bibr" target="#b24">[25]</ref>.</p><p>On the other hand, compared with the DeepLab-CRF result (66.4), our method has a 4.6% gain enjoyed from exploiting the bounding box annotations of the COCO dataset. This comparison demonstrates the power of our method that exploits large-scale bounding box annotations to improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploiting Boxes in PASCAL VOC 2007</head><p>To further demonstrate the effect of BoxSup, we exploit the bounding boxes in the PASCAL VOC 2007 dataset <ref type="bibr" target="#b7">[8]</ref>. This dataset has no mask annotations. It is a de facto dataset which mask-supervised methods are not able to use.</p><p>We exploit all 10k images in the VOC 2007 trainval and test sets. We train a BoxSup model using the union set of VOC 2007 boxes, COCO boxes, and the augmented VOC 2012 training set. The score improves from 71.0 to 73.1 (Table 4) because of the extra box training data. It is reasonable for us to expect further improvement if more bounding box annotations are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Improvement</head><p>Although our focus is mainly on exploiting boxes as supervision, it is worth noticing that our method may also benefit from other improvements on the mask-sup baseline (FCN in our case). Concurrent with our work, there are a series of improvements <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b4">5]</ref> made on FCN, which achieve excellent results using strong mask-supervision from VOC and COCO data.</p><p>To show the potential of our BoxSup method in parallel with improvements on the baseline, we use a simple testtime augmentation to boost our results. Instead of comput-    Our baseline is our implementation of FCN+CRF. "V" denotes the VOC data, and "C" denotes the COCO data.</p><p>ing pixel-wise predictions on a single scale, we compute the score maps from two extra scales (±20% of the original image size) and bilinearly re-scale the score maps to the original size. The scores from three scales are averaged. This simple modification boosts our result from 73.1 to 75.2 (BoxSup+, <ref type="table">Table 4</ref>) in the VOC 2012 test set. This result is on par with the latest results using strong masksupervision from both VOC and COCO, but in our case the COCO dataset only provides bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on PASCAL-CONTEXT</head><p>We further perform experiments on the recently labeled PASCAL-CONTEXT dataset <ref type="bibr" target="#b23">[24]</ref>. This dataset provides ground-truth semantic labels for the whole scene, including object and stuff (e.g., grass, sky, water). Following the protocol in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>, the semantic segmentation is performed on the most frequent 59 categories (identified by <ref type="bibr" target="#b23">[24]</ref>) plus a background category. The accuracy is measured by mean IoU scores. The training and evaluation are performed on the training and validation sets that have 4,998 and 5,105 images respectively.</p><p>To train a BoxSup model for this dataset, we first use the box annotations from all 80 object categories in the COCO dataset to train the FCN (using VGG-16). This network ends with an 81-way (with an extra one for background) layer. Then we remove this last layer and add a new 60way layer for the 59 categories of PASCAL-CONTEXT. We fine-tune this model in the 5k training images of PASCAL-CONTEXT. A CRF for post-processing is also used. We do no use the test-time scale augmentation. <ref type="table" target="#tab_9">Table 5</ref> shows the results in PASCAL-CONTEXT. The methods of CFM <ref type="bibr" target="#b5">[6]</ref> and FCN <ref type="bibr" target="#b21">[22]</ref> are both based on the VGG-16 model. Our baseline method, which is our implementation of FCN+CRF, has a score of 35.7 using masks of the 5k training images. Using our BoxSup model pretrained using the COCO boxes, the result is improved to 40.5. The 4.8% gain is solely because of the bounding box annotations in COCO that improve our network training. <ref type="figure" target="#fig_7">Fig. 6</ref> shows some examples of our results for joint object and stuff segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The proposed BoxSup method can effectively harness bounding box annotations to train deep networks for semantic segmentation. Our BoxSup method that uses 133k bounding boxes and 10k masks achieves state-of-the-art results. Our error analysis suggests that semantic segmentation accuracy is hampered by the failure of recognizing objects, which large-scale data may help with.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our training approach supervised by bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Segmentation masks used as supervision. (a) A training image. (b) Ground-truth. (c) Each box is naïvely considered as a rectangle mask. (d) A segmentation mask is generated by GrabCut [26]. (e) For our method, the supervision is estimated from region proposals (MCG [2]) by considering bounding box annotations and network feedbacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Update of segmentation masks during training. Here we show the masks in epoch #1, epoch #5, and epoch #20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Error analysis on the validation set. Top: (from left to right) image, ground-truth, boundary regions marked as white, interior regions marked as white). Bottom: boundary and interior mean IoU, using VOC masks only (blue) and using extra COCO boxes (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>semi, VOC mask +COCO box</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Example semantic segmentation results on PASCAL VOC 2012 validation using our method. (a) Images. (b) Supervised by masks in VOC. (c) Supervised by boxes in VOC. (d) Supervised by masks in VOC and boxes in COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Example results on PASCAL-CONTEXT validation. (a) Images. (b) Results of our baseline (35.7 mean IoU), trained using VOC masks. (c) Results of BoxSup (40.5 mean IoU), trained using VOC masks and COCO boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Here S represents a candidate segment mask, and B represents a ground-truth bounding box annotation. IoU(B, S) ∈ [0, 1] is the intersection-over-union ratio computed from the ground-truth box B and the tight bounding box of the segment S. The function δ is equal to one if the semantic label l S assigned to segment S is the same as the ground-truth label l B of the bounding box B, and zero otherwise. Minimizing E o favors higher IoU scores when the semantic labels are consistent. This objective function is normalized by the number of candidate segments N .</figDesc><table><row><cell>training image</cell><cell>epoch #1</cell><cell>epoch #5</cell><cell>epoch #20</cell></row><row><cell>person</cell><cell></cell><cell></cell><cell></cell></row><row><cell>chair</cell><cell></cell><cell></cell><cell></cell></row><row><cell>training image</cell><cell>epoch #1</cell><cell>epoch #5</cell><cell>epoch #20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Here the summation i runs over the training images, and λ = 3 is a fixed weighting parameter. The variables to be optimized are the network parameters θ and the labeling {l S } of all candidate segments {S}. If only the term E o exists, the optimization problem in Eqn.(4) trivially finds a candidate segment that has the largest IoU score with the box; if only the term E r exists, the optimization problem in Eqn.(4) is equivalent to FCN. Our formulation simultaneously considers both cases.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of estimated masks for supervision in PASCAL VOC 2012 validation. All methods only use 10,582 bounding boxes as annotations, with no groundtruth segmentation mask used.</figDesc><table><row><cell></cell><cell>SS</cell><cell>GOP</cell><cell>MCG</cell></row><row><cell>mean IoU</cell><cell>59.5</cell><cell>60.4</cell><cell>62.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of the effects of region proposal methods on our method in PASCAL VOC 2012 validation.</figDesc><table /><note>All methods only use 10,582 bounding boxes as annota- tions, with no ground-truth segmentation mask used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>). With the GrabCut masks as the supervision</figDesc><table><row><cell>method</cell><cell>sup.</cell><cell>mask #</cell><cell>box # mIoU</cell></row><row><cell>FCN [22]</cell><cell>mask</cell><cell>V 10k</cell><cell>-62.2</cell></row><row><cell cols="2">DeepLabCRF [5] mask</cell><cell>V 10k</cell><cell>-66.4</cell></row><row><cell>WSSL [25]</cell><cell>box</cell><cell>-</cell><cell>V 10k 60.4</cell></row><row><cell>BoxSup</cell><cell>box</cell><cell>-</cell><cell>V 10k 64.6</cell></row><row><cell>BoxSup</cell><cell>semi</cell><cell>V 1.4k</cell><cell>V 9k 66.2</cell></row><row><cell>WSSL [25]</cell><cell cols="2">mask V+C 133k</cell><cell>-70.4</cell></row><row><cell>BoxSup</cell><cell>semi</cell><cell>V 10k</cell><cell>C 123k 71.0</cell></row><row><cell>BoxSup</cell><cell>semi</cell><cell cols="2">V 10k V07+C 133k 73.1</cell></row><row><cell>BoxSup+</cell><cell>semi</cell><cell cols="2">V 10k V07+C 133k 75.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results on PASCAL-CONTEXT [24] validation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">www.robots.ox.ac.uk/˜vgg/research/very_deep/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Imagenet autoannotation with segmentation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Küttel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust higher order potentials for enforcing label consistency. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="302" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2861</idno>
		<title level="m">Computational baby learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0774</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04587</idno>
		<title level="m">Transferring rich feature hierarchies for robust visual tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic segmentation without annotating segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
