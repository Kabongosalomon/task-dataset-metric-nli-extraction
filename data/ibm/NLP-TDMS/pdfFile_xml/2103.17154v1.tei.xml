<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatio-Temporal Transformer for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatio-Temporal Transformer for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is endto-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on five challenging short-term and long-term benchmarks, while running at real-time speed, being 6× faster than Siam R-CNN <ref type="bibr" target="#b45">[47]</ref>. Code and models are open-sourced at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is a fundamental yet challenging research topic in computer vision. Over the past few years, based on convolutional neural networks, object tracking has achieved remarkable progress <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b45">47]</ref>. However, convolution kernels are not good at modeling long-range dependencies of image contents and features, because they only process a local neighborhood, either in space or time. Current prevailing trackers, including both the offline Siamese trackers and the online learning models, are almost all build upon convolutional operations <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b45">47]</ref>. As a consequence, these methods only perform well on modeling local relationships of image content, but being limited to capturing long-range global interactions. Such deficiency may degrade the model capacities on dealing with the scenarios where the global contextual information is important for localizing target objects, such as the objects undergoing largescale variations or getting in and out of views frequently. * Work performed when Bin is an intern of MSRA. † Corresponding author: houwen.peng@microsoft.com. <ref type="figure">Figure 1</ref>: Comparison with state-of-the-arts on LaSOT <ref type="bibr" target="#b11">[13]</ref>. We visualize the Success performance with respect to the Frames-Per-Seconds (fps) tracking speed. Ours-ST101 and Ours-ST50 indicate the proposed trackers with ResNet-101 and ResNet-50 as backbones, respectively. Better viewed in color.</p><formula xml:id="formula_0">� � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � �</formula><p>The problem of long range interactions has been tackled in sequence modeling through the use of transformer <ref type="bibr" target="#b44">[46]</ref>. Transformer has enjoyed rich success in tasks such as natural language modeling <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b37">39]</ref> and speech recognition <ref type="bibr" target="#b32">[34]</ref>. Recently, transformer has been employed in discriminative computer vision models and drawn great attention <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b33">35]</ref>. Inspired by the recent DEtection TRansformer (DETR) <ref type="bibr" target="#b3">[5]</ref>, we propose a new end-to-end tracking architecture with encoder-decoder transformer to boost the performance of conventional convolution models.</p><p>Both spatial and temporal information are important for object tracking. The former one contains object appearance information for target localization, while the latter one includes the state changes of objects across frames. Previous Siamese trackers <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b4">6]</ref> only exploit the spatial information for tracking, while online methods <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b1">3]</ref> use historical predictions for model updates. Although being successful, these methods do not explicitly model the relationship between space and time. In this work, considering the superior capacity on modeling global dependencies, we resort to transformer to integrate spatial and temporal information for tracking, generating discriminative spatiotemporal features for object localization.</p><p>More specifically, we propose a new spatio-temporal architecture based on the encoder-decoder transformer for visual tracking. The new architecture contains three key components: an encoder, a decoder and a prediction head. The encoder accepts inputs of an initial target object, the current image, and a dynamically updated template. The self-attention modules in the encoder learn the relationship between the inputs through their feature dependencies. Since the template images are updated throughout video sequences, the encoder can capture both spatial and temporal information of the target. The decoder learns a query embedding to predict the spatial positions of the target object. A corner-based prediction head is used to estimate the bounding box of the target object in the current frame. Meanwhile, a score head is learned to control the updates of the dynamic template images.</p><p>Extensive experiments demonstrate that our method establishes new state-of-the-art performance on on both shortterm <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b34">36]</ref> and long-term tracking benchmarks <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref>. For instance, our spatio-temporal transformer tracker surpasses Siam R-CNN <ref type="bibr" target="#b45">[47]</ref> by 3.9% (AO score) and 2.3% (Success) on GOT-10K <ref type="bibr" target="#b16">[18]</ref> and LaSOT <ref type="bibr" target="#b11">[13]</ref>, respectively. It is also worth noting that compared with previous longterm trackers <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b51">53]</ref>, the framework of our method is much simpler. Specifically, previous methods usually consist of multiple components, such as base trackers <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b48">50]</ref>, target verification modules <ref type="bibr" target="#b19">[21]</ref>, and global detectors <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b17">19]</ref>. In contrast, our method only has a single network learned in an end-to-end fashion. Moreover, our tracker can run at real-time speed, being 6× faster than Siam R-CNN (30 v.s. 5 fps) on a Tesla V100 GPU, as shown in <ref type="figure">Fig. 1</ref> In summary, this work has three contributions.</p><p>• We propose a new transformer architecture dedicated to visual tracking. It is capable of capturing global feature dependencies of both spatial and temporal information in video sequences.</p><p>• The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines.</p><p>• The proposed trackers achieve state-of-the-art performance on five challenging short-term and long-term benchmarks, while running at real-time speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformer in Language and Vision. Transformer is originally proposed by Vaswani et al. <ref type="bibr" target="#b44">[46]</ref> for machine translation task, and has became a prevailing architecture in language modeling. Transformer takes a sequence as the input, scans through each element in the sequence and learns their dependencies. This feature makes transformer be intrinsically good at capturing global information in sequential data. Recently, transformer has shown their great potential in vision tasks like image classification <ref type="bibr" target="#b10">[12]</ref>, object detection <ref type="bibr" target="#b3">[5]</ref>, semantic segmentation <ref type="bibr" target="#b47">[49]</ref>, multiple object tracking <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b33">35]</ref>, etc. Our work is inspired by the recent work DETR <ref type="bibr" target="#b3">[5]</ref>, but has following fundamental differences.</p><p>(1) The studied tasks are different. DETR is designed for object detection, while this work is for object tracking. <ref type="bibr" target="#b0">(2)</ref> The network inputs are different. DETR takes the whole image as the input, while our input is a triplet consisting of one search region and two templates. Their features from the backbone are first flatten and concatenated then sent to the encoder. (3) The query design and training strategies are different. DETR uses 100 object queries and uses the Hungarian algorithm to match predictions with groundtruths during training. In contrast, our method only uses one query and always matches it with the ground-truth without using the Hungarian algorithm. (4) The bounding box heads are different. DETR uses a three-layer perceptron to predict boxes. Our network adopts a corner-based box head for higher-quality localization.</p><p>Moreover, TransTrack <ref type="bibr" target="#b42">[44]</ref> and TrackFormer <ref type="bibr" target="#b33">[35]</ref> are two most recently representative works on transformer tracking. TransTrack <ref type="bibr" target="#b42">[44]</ref> has the following features. (1) The encoder takes the image features of both the current and the previous frame as the inputs. (2) It has two decoders, which take the learned object queries and queries from the last frame as the input respectively. With different queries, the output sequence from the encoder are transformed into detection boxes and tracking boxes respectively. (3) The predicted two groups of boxes are matched based on the IoUs using the Hungarian algorithm <ref type="bibr" target="#b22">[24]</ref>. While Trackformer <ref type="bibr" target="#b33">[35]</ref> has the following features. (1) It only takes the current frame features as the encoder inputs. (2) There is only one decoder, where the learned object queries and the track queries from the last frame interact with each other.</p><p>(3) It associates tracks over time solely by attention operations, not relying on any additional matching such as motion or appearance modeling. In contrast, our work has the following fundamental differences with these two methods.</p><p>(1) Network inputs are different. Our input is a triplet consisting of the current search region, the initial template and a dynamic template. (2) Our method captures the appearance changes of the tracked targets by updating the dynamic template, rather than updating object queries as <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b33">35]</ref>.</p><p>Spatio-Temporal Information Exploitation. Exploitation of spatial and temporal information is a core problem in object tracking field. Existing trackers can be divided into two classes: spatial-only ones and spatio-temporal ones. Most of offline Siamese trackers <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b27">29]</ref> belong to the spatial-only ones, which consider the object tracking as a template-matching between the initial template and the current search region. To extract the relationship between the template and the search region along the spatial dimension, most trackers adopt the variants of correlation, including the naive correlation <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b24">26]</ref>, the depth-wise correlation <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b58">60]</ref>, and the point-wise correlation <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b50">52]</ref>. Although achieving remarkable progress in recent years, these methods merely capture local similarity, while ignoring global information. By contrast, the self-attention mechanism in transformer can capture longrange relationship, making it suitable for pair-wise matching tasks. Compared with spatial-only trackers, spatiotemporal ones additionally exploit temporal information to improve trackers' robustness. These methods can also be divided into two classes: gradient-based and gradient-free ones. Gradient-based methods require gradient computation during inference. One of the classical works is MD-Net <ref type="bibr" target="#b35">[37]</ref>, which updates domain-specific layers with gradient descent. To improve the optimization efficiency, later works <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b53">55]</ref> adopt more advanced optimization methods like Gauss-Newton method or meta-learningbased update strategies. However, many real-world devices for deploying deep learning do not support backpropagation, which restricts the application of gradientbased methods. In contrast, gradient-free methods have larger potentials in real-world applications. One class of gradient-free methods <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b55">57]</ref> exploits an extra network to update the template of Siamese trackers <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b59">61]</ref>. Another representative work LTMU <ref type="bibr" target="#b6">[8]</ref> learns a meta-updater to predict whether the current state is reliable enough to be used for the update in long-term tracking. Although being effective, these methods cause the separation between space and time. In contrast, our method integrates the spatial and temporal information as a whole, simultaneously learning them with the transformer.</p><p>Tracking Pipeline and Post-processing. The tracking pipelines of previous trackers <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b45">47]</ref> are complicated. Specifically, they first generate a large number of box proposals with confidence scores, then use various post-processing to choose the best bounding box as the tracking result. The commonly used post-processing includes cosine window, scale or aspect-ratio penalty, bounding box smoothing, tracklet-based dynamic programming, etc. Though it brings better results, post-processing causes the performance being sensitive to hyper-parameters. There are some trackers <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b17">19]</ref> attempting to simplify the tracking pipeline, but their performances still lag far behind that of state-of-the-art trackers. This work attempts to close this gap, achieving top performance by predicting one single bounding box in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we propose the spatio-temporal transformer network for visual tracking, called STARK. For clarity, we first introduce a simple baseline method that directly applies the original encoder-decoder transformer for tracking. The baseline method only considers spatial information and achieves impressive performance. After that, we extend the baseline to learn both spatial and temporal repre- sentations for target localization. We introduce an dynamic template and an update controller to capture the appearance changes of target objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Simple Baseline Based on Transformer</head><p>We present a simple baseline framework based on visual transformer for object tracking. The network architecture is demonstrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. It mainly consists of three components: a convolutional backbone, an encoder-decoder transformer, and a bounding box prediction head.</p><p>Backbone. Our method can use arbitrary convolutional networks as the backbone for feature extraction. Without loss of generality, we adopt the vanilla ResNet <ref type="bibr" target="#b13">[15]</ref> as the backbone. More concretely, except for removing the last stage and fully-connected layers, there is no other change for the original ResNet <ref type="bibr" target="#b13">[15]</ref>. The input of the backbone is a pair of images: a template image of the initial target object z ∈ R 3×Hz×Wz and a search region of the current frame x ∈ R 3×Hx×Wx . After passing through of the backbone, the template z and the search image x are mapped to two feature maps f z ∈ R C× Hz</p><formula xml:id="formula_1">s × Wz s and f x ∈ R C× Hx s × Wx s .</formula><p>Encoder. The feature maps output from the backbone require pre-processing before feeding into the encoder. To be specific, a bottleneck layer is first used to reduce the channel number from C to d. Then the feature maps are flatten and concatenated along the spatial dimension, producing a feature sequence with length of Hz Decoder. The decoder takes a target query and the enhanced feature sequence from the encoder as the input. Different from DETR <ref type="bibr" target="#b3">[5]</ref> adopting 100 object queries, we only input one single query into the decoder to predict one bounding box of the target object. Besides, since there is only one prediction, we remove the Hungarian algorithm <ref type="bibr" target="#b22">[24]</ref> used in DETR for prediction association. Similar to the encoder, the decoder stacks M decoder layers, each of which consists of a self-attention, an encoder-decoder attention, and a feed-forward network. In the encoder-decoder attention module, the target query can attend to all positions on the template and the search region features, thus learning robust representations for the final bounding box prediction.</p><p>Head. DETR <ref type="bibr" target="#b3">[5]</ref> adopts a three-layer perceptron to predict object box coordinates. However, as pointed by GFLoss <ref type="bibr" target="#b26">[28]</ref>, directly regressing the coordinates is equivalent to fitting a Dirac delta distribution, which fails to consider the ambiguity and uncertainty in the datasets. This representation is not flexible and not robust to challenges such as occlusion and cluttered background in object tracking. To improve the box estimation quality, we design a new prediction head through estimating the probability distribution of the box corners. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, we first take the search region features from the encoder's output sequence, then compute the similarity between the search region features and the output embedding from the decoder. Next, the similarity scores are element-wisely multiplied with the search region features to enhance important regions and weaken the less discriminative ones. The new feature sequence is reshaped to a feature map f ∈ R d× Hs s × Ws s , and then fed into a simple fully-convolutional network (FCN). The FCN consists of L stacked Conv-BN-ReLU layers and outputs two probability maps P tl (x, y) and P br (x, y) for the top-left and the bottom-right corners of object bounding boxes, respectively. Finally, the predicted box coordinates ( x tl , y tl ) and ( x br , y br ) are obtained by computing the expectation of corners' probability distribution as shown in Eq. (1). Compared with DETR, our method explicitly models uncertainty in the coordinate estimation, generating more accurate and robust predictions for object tracking. (1) Training and Inference. Our baseline tracker is trained in an end-to-end fashion with the combination of the 1 Loss and the generalized IoU loss <ref type="bibr" target="#b39">[41]</ref> as in DETR. The loss function can be written as  where b i andb i represent the groundtruth and the predicted box respectively and λ iou , λ L1 ∈ R are hyperparameters. But unlike DETR, we do not use the classification loss and the Hungarian algorithm, thus further simplifying the training process. During inference, the template image together with its features from the backbone are initialized by the first frame and fixed in the subsequent frames. During tracking, in each frame, the network takes a search region from the current frame as the input, and returns the predicted box as the final result, without using any post-processing such as cosine window or bounding box smoothing.</p><formula xml:id="formula_2">L = λ iou L iou (b i ,b i ) + λ L1 L 1 (b i ,b i ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-Temporal Transformer Tracking</head><p>Since the appearance of a target object may change significantly as time proceeds, it is important to capture the latest state of the target for tracking. In this section, we demonstrate how to exploit spatial and temporal information simultaneously based on the previously introduced baseline. Three key differences are made, including the network inputs, an extra score head, and the training &amp; inference strategy. We elaborate them one by one as below. The spatio-temporal architecture is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Input. Different from the baseline method which only uses the first and the current frames, the spatio-temporal method introduces an dynamically updated template sampled from intermediate frames as an additional input, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Beyond the spatial information from the initial template, the dynamic template can captures the target appearance changes with time, providing additional temporal information. Similar to the baseline architecture in Sec. 3.1, feature maps of the triplet are flatten and concatenated then sent to the encoder. The encoder extracts discriminative spatio-temporal features by modeling the global relationships among all elements in both spatial and temporal dimensions.</p><p>Head. During tracking, there are some cases where the dynamic template should not be updated. For example, the cropped template is not reliable when the target has been completely occluded or has moved out of view, or when the tracker has drifted. For simplicity, we consider that the dynamic template could be updated as long as the search region contains the target. To automatically determine whether the current state is reliable, we add a simple score prediction head, which is a three-layer perceptron followed by a sigmoid activation. The current state is considered reliable if the score is higher than the threshold τ .</p><p>Training and Inference. As pointed out by recent works <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b41">43]</ref>, jointly learning of localization and classification may cause sub-optimal solutions for both tasks, and it is helpful to decouple localization and classification. Therefore, we divide the training process into two stages, regarding the localization as a primary task and the classification as a secondary task. To be specific, in the first stage, the whole network, except for the score head, is trained end-toend only with the localization-related losses in Eq. 2. In this stage, we ensure all search images to contain the target objects and let the model learn the localization capacity. In the second stage, only the score head is optimized with binary cross-entropy loss defined as</p><formula xml:id="formula_3">L ce = y i log (P i ) + (1 − y i ) log (1 − P i ) ,<label>(3)</label></formula><p>where y i is the groundtruth label and P i is the predicted confidence , and all other parameters are frozen to avoid affecting the localization capacity. In this way, the final model learn both localization and classification capabilities after the two-stage training. During inference, two templates and corresponding features are initialized in the first frame. Then a search region is cropped and fed into the network, generating one bounding box and a confidence score. The dynamic template is updated only when the update interval is reached and the confidence score is higher than the threshold τ . For efficiency, we set the update interval as T u frames. The new template is cropped from the original image and then fed into the backbone for feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section first presents the implementation details and the results of our STARK tracker on multiple benchmarks, with comparisons to state-of-the-art methods. Then, ablation studies are presented to analyze the effects of the key components in the proposed networks. We also report the results of other candidate frameworks and compare them with our method to demonstrate its superiority. Finally, visualization on attention maps of the encoder and the decoder are provided to understand how the transformer works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our trackers are implemented using Python 3.6 and Py-Torch 1.5.1. The experiments are conducted on a server with 8 16GB Tesla V100 GPUs.</p><p>Model. We report the results of three variants of STARK: STARK-S50, STARK-ST50 and STARK-ST101.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STARK-S50 only exploits spatial information and takes</head><p>ResNet-50 <ref type="bibr" target="#b13">[15]</ref> as the backbone, i.e., the baseline tracker presented in Sec. 3.1. STARK-ST50 and STARK-ST101 take ResNet-50 and ResNet-101 as the backbones respectively, exploiting both spatial and temporal information, i.e., the spatio-temporal tracker presented in Sec. 3.2.</p><p>The backbones are initialized with the parameters pretrained on ImageNet. The BatchNorm <ref type="bibr" target="#b18">[20]</ref> layers are frozen during training. Backbone features are pooled from the fourth stage with a stride of 16. The transformer architecture is similar to that in DETR <ref type="bibr" target="#b3">[5]</ref> with 6 encoder layers and 6 decoder layers, which consist of multi-head attention layers (MHA) and feed-forward networks (FFN). The MHA have 8 heads, width 256, while the FFN have hidden units of 2048. Dropout ratio of 0.1 is used. The bounding box prediction head is a lightweight FCN, consisting of 5 stacked Conv-BN-ReLU layers. The classification head is a threelayer perceptron with 256 hidden units in each layer.</p><p>Training. The training data consists of the train-splits of LaSOT <ref type="bibr" target="#b11">[13]</ref>, GOT-10K <ref type="bibr" target="#b16">[18]</ref>, COCO2017 <ref type="bibr" target="#b28">[30]</ref>, and Track-ingNet <ref type="bibr" target="#b34">[36]</ref>. As required by VOT2019 challenge, we remove 1k forbidden sequences from GOT-10K training set. The sizes of search images and templates are 320 × 320 pixels and 128 × 128 pixels respectively, corresponding to 5 2 and 2 2 times of the target box area. Data augmentations, including horizontal flip and brightness jittering, are used. The minimal training data unit for STARK-ST is one triplet, consisting of two templates and one search images. The whole training process of STARK-ST consists of two stages, which take 500 epochs for localization and 50 epochs for classification, respectively. Each epoch uses 6 × 10 4 triplets. The network is optimized using AdamW optimizer <ref type="bibr" target="#b29">[31]</ref> and weight decay 10 −4 . The loss weights λ L1 and λ iou are set to 5 and 2 respectively. Each GPU  <ref type="figure">Figure 5</ref>: Comparisons on LaSOT test set <ref type="bibr" target="#b11">[13]</ref>. hosts 16 triplets, hence the mini-batch size is 128 triplets per iteration. The initial learning rates of the backbone and the rest parts are 10 −5 and 10 −4 respectively. The learning rate drops by a factor of 10 after 400 epochs in the first stage and after 40 epochs in the second stage. The training setting of STARK-S is almost the same as that of STARK-ST, except that (1) the minimal training data unit of STARK-S is a template-search pair; (2) the training process only has the first stage.</p><p>Inference. The dynamic template update interval T u and the confidence threshold τ are respectively set to 200 frames and 0.5 by default. The inference pipeline only contains a forward pass and a coordinate transformation from the search region to the original image, without any extra post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Comparisons</head><p>We compare our STARK with existing state-of-the-art object trackers on three short-term benchmarks (GOT-10K, TrackingNet and VOT2020) and two long-term benchmarks (LaSOT and VOT2020-LT).</p><p>GOT-10K. GOT-10K <ref type="bibr" target="#b16">[18]</ref> is a large-scale benchmark covering a wide range of common challenges in object tracking. GOT-10K requires trackers to only use the training set of GOT-10k for model learning. We follow this policy and retrain our models only with the GOT-10K train set. As reported in Tab. 1, with the same ResNet-50 backbone, STARK-S50 and STARK-ST50 outperform PrDiMP50 <ref type="bibr" target="#b8">[10]</ref> by 3.8% and 4.6% AO scores, respectively. Furthermore, STARK-ST101 obtains a new state-of-the-art AO score of 68.8%, surpassing Siam R-CNN <ref type="bibr" target="#b45">[47]</ref> by 3.9% with the same ResNet-101 backbone.</p><p>TrackingNet. TrackingNet <ref type="bibr" target="#b34">[36]</ref> is a large-scale shortterm tracking benchmark containing 511 video sequences in the test set. Tab. 2 presents that STARK-S50 and STARK-ST50 surpass PrDiMP50 [10] by 4.5% and 5.5% in AUC respectively. With a more powerful ResNet-101 backbone, STARK-ST101 achieves the best AUC of 82.0%, outperforming Siam R-CNN by 0.8%. VOT2020. Different from previous reset-based evaluations <ref type="bibr" target="#b21">[23]</ref>, VOT2020 <ref type="bibr" target="#b20">[22]</ref> proposes a new anchor-based</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cat-20</head><p>Cattle-13 <ref type="figure">Figure 6</ref>: Visualization of the encoder attention and the decoder attention. evaluation protocol and uses binary segmentation masks as the groundtruth. The final metric for ranking is the Expected Average Overlap (EAO). Tab. 3 shows that STARK-S50 achieves a competitive result, which is better than DiMP <ref type="bibr" target="#b1">[3]</ref> and UPDT <ref type="bibr" target="#b2">[4]</ref>. After introducing temporal information, STARK-ST50 obtains an EAO of 0.308, being superior to previous bounding-box trackers. Inspired by AlphaRef <ref type="bibr" target="#b20">[22]</ref>, the winner of VOT2020 real-time challenge, we equip STARK with a refinement module proposed by AlphaRef to generate segmentation masks. The new tracker "STARK-ST50+AR" surpasses previous SOTA trackers, like AlphaRef and OceanPlus <ref type="bibr" target="#b58">[60]</ref>, getting an EAO of 0.505.</p><p>LaSOT. LaSOT <ref type="bibr" target="#b11">[13]</ref> is a large-scale long-term tracking benchmark, which contains 280 videos with average length of 2448 frames in the test set. STARK-S50 and STARK-ST50 achieve a gain of 6.0% and 6.6% over PrDiMP <ref type="bibr" target="#b8">[10]</ref> respectively, using the same ResNet-50 backbone. Furthermore, STARK-ST101 obtains a success of 67.1%, which is 2.3% higher than Siam R-CNN <ref type="bibr" target="#b45">[47]</ref>, as shown in <ref type="figure">Fig. 5</ref>.</p><p>VOT2020-LT. VOT2020-LT consists of 50 long videos, in which target objects disappear and reappear frequently. Besides, trackers are required to report the confidence score of the target being present. Precision (Pr) and Recall (Re) are computed under a series of confidence thresholds. Fscore, defined as F = 2P rRe P r+Re , is used to rank different trackers. Since STARK-S does not predict this score, we do not report its result on VOT2020-LT. Tab. 4 demonstrates that STARK-ST50 and STARK-ST101 outperform all previous methods with an F-score of 70.2% and 70.1%, respectively. It is also worth noting that the framework of STARK is much simpler than that of LTMU B, the winner of VOT2020-LT Challenge. To be specific, LTMU B takes the combination of ATOM <ref type="bibr" target="#b7">[9]</ref> and SiamMask <ref type="bibr" target="#b48">[50]</ref> as the short-term tracker, MDNet <ref type="bibr" target="#b35">[37]</ref> as the verifier, and Global-Track <ref type="bibr" target="#b17">[19]</ref> as the global detector. Whereas there is only one network in STARK and the result is obtained in one forward   <ref type="bibr" target="#b34">[36]</ref>.</p><p>DSiamRPN <ref type="bibr" target="#b59">[61]</ref> ATOM <ref type="bibr" target="#b7">[9]</ref> SiamRPN++ <ref type="bibr" target="#b23">[25]</ref> DiMP50 <ref type="bibr" target="#b1">[3]</ref> SiamAttn <ref type="bibr" target="#b54">[56]</ref> SiamFC++ <ref type="bibr" target="#b49">[51]</ref> MAML-FCOS <ref type="bibr" target="#b46">[48]</ref> PrDiMP50 <ref type="bibr" target="#b8">[10]</ref> SiamRCNN <ref type="bibr" target="#b45">[47]</ref> STARK -S50  <ref type="table">Table 3</ref>: Comparisons on VOT2020 <ref type="bibr" target="#b20">[22]</ref>."+AR" means using Alpha-Refine to predict masks. The upper row summarizes trackers that only predict bounding boxes and the lower row presents trackers that report masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STARK</head><p>IVT <ref type="bibr" target="#b40">[42]</ref> KCF <ref type="bibr" target="#b15">[17]</ref> SiamFC <ref type="bibr" target="#b0">[2]</ref> CSR-DCF <ref type="bibr" target="#b31">[33]</ref> ATOM <ref type="bibr" target="#b7">[9]</ref> DiMP <ref type="bibr" target="#b1">[3]</ref> UPDT <ref type="bibr" target="#b2">[4]</ref> DPMT  <ref type="bibr" target="#b36">[38]</ref> SiamEM SiamMask <ref type="bibr" target="#b48">[50]</ref> SiamMargin <ref type="bibr" target="#b20">[22]</ref> Ocean <ref type="bibr" target="#b58">[60]</ref> D3S <ref type="bibr" target="#b30">[32]</ref> FastOcean AlphaRef <ref type="bibr" target="#b20">[22]</ref> OceanPlus <ref type="bibr" target="#b56">[58]</ref> STARK -S50+AR pass without post-processing. Speed, FLOPs and Params. As demonstrated in Tab. 5, STARK-S50 can run in real-time at more than 40 fps. Besides, the FLOPs and Params of STARK-S50 are 4× and 2× less than those of SiamRPN++. Although STARK-ST50 takes a dynamic template as the extra input and introduces an additional score head, the increases of FLOPs and Params is a little, even negligible. This shows that our method can exploit temporal information in a nearly costfree fashion. When ResNet-101 is used as the backbone, both FLOPs and Params increase significantly but STARK-ST101 can still run at real-time speed, which is 6x faster than Siam R-CNN (5 fps), as shown in <ref type="figure">Fig. 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STARK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Component-wise Analysis</head><p>In this section, we choose STARK-ST50 as the base model and evaluate the effects of different components in it on LaSOT <ref type="bibr" target="#b11">[13]</ref>. For simplicity, encoder, decoder, positional encoding, corner prediction, and score head are abbreviated as enc, dec, pos, corner, and score respectively. As shown in Tab. 6 #1, when the encoder is removed, the success drops significantly by 5.3%. This illustrates that the deep interaction among features from the template and the search region plays a key role. The performance drops by 1.9% when the decoder is removed as shown in #2. This drop is less than that of removing the encoder, showing that the importance of the decoder is less than the encoder. When the positional encoding is removed, the performance only drops by 0.2% as shown in #3. Thus we conclude that the positional encoding is not a key component in our method. We also try to replace the corner head with a three-layer perceptron as in DETR <ref type="bibr" target="#b3">[5]</ref>. #4 shows that the performance of STARK with an MLP as the box head is 2.7% lower than that of the proposed corner head. It demonstrates that the boxes predicted by the corner head are more accurate. As shown in #5, when removing the score head, the performance drops to 64.5%, which is lower than that of STARK-S50 without using temporal information. This demonstrates that improper uses of temporal information may hurt the performance and it is important to filter out unreliable templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Other Frameworks</head><p>In this section, we choose the STARK-ST50 as our base model and compare it with other possible candidate frameworks. These frameworks include generating queries from the template, using the Hungarian algorithm, updating queries as in TrackFormer <ref type="bibr" target="#b33">[35]</ref>, and jointly learning localization and classification. Due to the space limitation, the figures of the detailed architectures are presented in the supplementary material.</p><p>Template images serve as the queries. Queries and templates have similar functions in transformer tracking. For example, both of them are expected to encode information about the target objects. From this view, a natural idea is to use template images to serve as the queries of the decoder. Specifically, first, the template and the search region features are separately fed to a weight-shared encoder then the queries generated from the template features are used to interact with the search region features in the decoder. As shown in Tab. 7, the performance of this framework is 61.2%, which is 5.2% lower than that of our design. We conjecture that the underlying reason is that compared with our method, this design lacks the information flow from the template to the search region, thus weakening the discriminative power of the search region features.</p><p>Using the Hungarian algorithm <ref type="bibr" target="#b3">[5]</ref>. We also try to use    K queries, predicting K boxes with confidence scores. K is equal to 10 in the experiments. The groundtruth is dynamically matched with these queries during the training using the Hungarian algorithm. We observe that this training strategy leads to the "Matthew effect". To be specific, some queries predict slightly more accurate boxes than the others at the beginning of training. Then they are chosen by the Hungarian algorithm to match with the groundtruth, which further enlarges the gaps between the chosen queries and the unselected ones. Finally, there are only one or two queries having the ability to predict high-quality boxes. If they are not selected during inference, the predicted box may become unreliable. As shown in Tab. 7, this strategy performs inferior to our method and the gap is 2.7%. Updating the query embedding. Different from STARK exploiting temporal information by introducing an extra dynamic template, TrackFormer <ref type="bibr" target="#b33">[35]</ref> encodes temporal information by updating the query embedding. Following this idea, we extend the STARK-S50 to a new temporal tracker by updating the target query. Tab. 7 shows that this design achieves a success of 64.8%, which is 1.6% lower than that of STARK-ST50. The underlying reason might be that the extra information brought by an updatable query embedding is much less than that by an extra template.</p><p>Jointly learning of localization and classification. As mentioned in Sec 3.2, localization is regarded as the primary task and is trained in the first stage. While classification is trained in the second stage as the secondary task. We also make an experiment to jointly learn localization and classification in one stage. As shown in Tab. 7, this strategy leads to a sub-optimal result, which is 3.9% lower than that of STARK. Two potential reasons are: (1) Optimization of the score head interferes with the training of the box head, leading to inaccurate box predictions. (2) Training of these two tasks requires different data. To be specific, the localization task expects that all search regions contain tracked targets to provide strong supervision. By contrast, the classification task expects a balanced distribution, half of search regions containing the targets, while the remaining half not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>Encoder Attention. The upper part of the <ref type="figure">Fig. 6</ref> shows a templates-search triplet from the Cat-20, as well as the attention maps from the last encoder layer. The visualized attention is computed by taking the central pixel of the initial template as the query and taking all pixels from the triplet as the key and value It can be seen that the attentions concentrate on the tracked target and have roughly separated it from the background. Besides, the features produced by the encoder also have strong discrimination power between the target and distractors.</p><p>Decoder Attention. The lower part of the <ref type="figure">Fig. 6</ref> demonstrates a templates-search triplet from the Cattle-13, as well as the attention maps from the last decoder layer. It can be seen that decoder attention on templates and search regions are different. Specifically, attention on the templates mainly focuses on the top-left region of the target, while attention on the search region tends to be put on the boundaries of the target. Besides, the learnt attention is robust to distractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a new transformer-based tracking framework, which can capture the long-range dependency in both spatial and temporal dimensions. Besides, the proposed STARK tracker gets rid of hyper-parameters sensitive post-processing, leading to a simple inference pipeline. Extensive experiments show that the STARK trackers perform much better than previous methods on five short-term and long-term benchmarks, while running in real-time. We expect this work can attract more attention on transformer architecture for visual tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Framework for spatial-only tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>dimension of d, which servers as the input for the transformer encoder. The encoder consists of N encoder layers, each of which is made up of a multi-head selfattention module with a feed-forward network. Due to the permutation-invariance of the original transformer<ref type="bibr" target="#b44">[46]</ref>, we add sinusoidal positional embeddings to the input sequence. The encoder captures the feature dependencies among all elements in the sequence and reinforces the original features with global contextual information, thus allowing the model to learn discriminative features for object localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(yy</head><label></label><figDesc>x tl , y tl ) = ( · P tl (x, y)), ( x br , y br ) = ( · P br (x, y)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the box prediction head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Framework for spatio-temporal tracking. The differences with the spatial-only architecture are highlighted in pink.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparisons on GOT-10k test set<ref type="bibr" target="#b16">[18]</ref> </figDesc><table><row><cell></cell><cell>SiamFC</cell><cell>SiamFCv2</cell><cell>ATOM</cell><cell>SiamFC++</cell><cell>D3S</cell><cell>DiMP50</cell><cell>Ocean</cell><cell>PrDiMP50</cell><cell>SiamRCNN</cell><cell>STARK</cell><cell>STARK</cell><cell>STARK</cell></row><row><cell></cell><cell>[2]</cell><cell>[45]</cell><cell>[9]</cell><cell>[51]</cell><cell>[32]</cell><cell>[3]</cell><cell>[60]</cell><cell>[10]</cell><cell>[47]</cell><cell>-S50</cell><cell>-ST50</cell><cell>-ST101</cell></row><row><cell>AO(%)</cell><cell>34.8</cell><cell>37.4</cell><cell>55.6</cell><cell>59.5</cell><cell>59.7</cell><cell>61.1</cell><cell>61.1</cell><cell>63.4</cell><cell>64.9</cell><cell>67.2</cell><cell>68.0</cell><cell>68.8</cell></row><row><cell>SR0.5(%)</cell><cell>35.3</cell><cell>40.4</cell><cell>63.4</cell><cell>69.5</cell><cell>67.6</cell><cell>71.7</cell><cell>72.1</cell><cell>73.8</cell><cell>72.8</cell><cell>76.1</cell><cell>77.7</cell><cell>78.1</cell></row><row><cell>SR0.75(%)</cell><cell>9.8</cell><cell>14.4</cell><cell>40.2</cell><cell>47.9</cell><cell>46.2</cell><cell>49.2</cell><cell>47.3</cell><cell>54.3</cell><cell>59.7</cell><cell>61.2</cell><cell>62.3</cell><cell>64.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on TrackingNet test set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on VOT-LT2020 benchmark<ref type="bibr" target="#b20">[22]</ref> SPLT<ref type="bibr" target="#b51">[53]</ref> ltMDNet SiamDW LT<ref type="bibr" target="#b57">[59]</ref> RLT DiMP CLGS Megtrack LTMU B<ref type="bibr" target="#b6">[8]</ref> LT DSE STARK-ST50 STARK-ST101</figDesc><table><row><cell>F-score(%)</cell><cell>56.5</cell><cell>57.4</cell><cell>65.6</cell><cell>67.0</cell><cell>67.4</cell><cell>68.7</cell><cell>69.1</cell><cell>69.5</cell><cell>70.2</cell><cell>70.1</cell></row><row><cell>Pr(%)</cell><cell>58.7</cell><cell>64.9</cell><cell>67.8</cell><cell>65.7</cell><cell>73.9</cell><cell>70.3</cell><cell>70.1</cell><cell>71.5</cell><cell>71.0</cell><cell>70.2</cell></row><row><cell>Re(%)</cell><cell>54.4</cell><cell>51.4</cell><cell>63.5</cell><cell>68.4</cell><cell>61.9</cell><cell>67.1</cell><cell>68.1</cell><cell>67.7</cell><cell>69.5</cell><cell>70.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison about the speed, FLOPs and Params.</figDesc><table><row><cell>Trackers</cell><cell cols="3">Speed(fps) FLOPs(G) Params(M)</cell></row><row><cell>STARK-S50</cell><cell>42.2</cell><cell>10.5</cell><cell>23.3</cell></row><row><cell>STARK-ST50</cell><cell>41.8</cell><cell>10.9</cell><cell>23.5</cell></row><row><cell>STARK-ST101</cell><cell>31.7</cell><cell>18.5</cell><cell>42.4</cell></row><row><cell>SiamRPN++</cell><cell>35.0</cell><cell>48.9</cell><cell>54.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation for important components. Blank denotes the component is used by default, while represents the component is removed. Performance is evaluated on LaSOT.</figDesc><table><row><cell cols="2"># Enc Dec Pos Corner Score Success</cell></row><row><cell>1</cell><cell>61.1</cell></row><row><cell>2</cell><cell>64.5</cell></row><row><cell>3</cell><cell>66.2</cell></row><row><cell>4</cell><cell>63.7</cell></row><row><cell>5</cell><cell>64.5</cell></row><row><cell>6</cell><cell>66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison between STARK and other candidate frameworks. Performance is evaluated on LaSOT.</figDesc><table><row><cell></cell><cell>Template</cell><cell cols="2">Hungarian Update</cell><cell>Loc-Cls</cell><cell>Ours</cell></row><row><cell></cell><cell>query</cell><cell></cell><cell>query</cell><cell>Joint</cell><cell></cell></row><row><cell>Success</cell><cell>61.2</cell><cell>63.7</cell><cell>64.8</cell><cell>62.5</cell><cell>66.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip H S</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-performance longterm tracking with meta-updater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL-HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SiamCAR: Siamese fully convolutional classification and regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GOT-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globaltrack: A simple and strong baseline for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time MDNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mooyeol</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ondrej Drbohlav, et al. The eighth visual object tracking vot2020 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>Kämäräinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukačehovin</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukežič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ales Leonardis, et al. The seventh visual object tracking VOT2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The hungarian method for the assignment problem. Naval research logistics quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GradNet: Gradient-guided network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">PG-Net: Pixel to global matching network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaonong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">D3S-a discriminative single shot segmentation tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attentionw/o data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">TrackFormer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Incremental learning for robust visual tracking. ijcv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruei-Sung</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">TransTrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Siam R-CNN: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SiamFC++: towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Alpha-refine: Boosting tracking performance by precise bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06815,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Skimming-Perusal&apos; Tracking: A framework for real-time and robust long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ROAM: Recurrently optimizing tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Towards accurate pixel-wise object tracking by attention retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houweng</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02745</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
