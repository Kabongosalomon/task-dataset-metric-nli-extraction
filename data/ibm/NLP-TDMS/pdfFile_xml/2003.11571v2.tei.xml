<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image Synthesis</term>
					<term>Layout-to-Image</term>
					<term>Layout-to-Mask-to-Image</term>
					<term>Deep Generative Learning</term>
					<term>GAN</term>
					<term>ISLA-Norm !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable structured inputs. This paper focuses on a recently emerged task, layout-toimage, whose goal is to learn generative models for synthesizing photo-realistic images from a spatial layout (i.e., object bounding boxes configured in an image lattice) and its style codes (i.e., structural and appearance variations encoded by latent vectors). This paper first proposes an intuitive paradigm for the task, layout-to-mask-to-image, which learns to unfold object masks in a weakly-supervised way based on an input layout and object style codes. The layout-to-mask component deeply interacts with layers in the generator network to bridge the gap between an input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks (GANs) for the proposed layout-to-mask-to-image synthesis with layout and style control at both image and object levels. The controllability is realized by a proposed novel Instance-Sensitive and Layout-Aware Normalization (ISLA-Norm) scheme. A layout semi-supervised version of the proposed method is further developed without sacrificing performance. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation and objective</head><p>R EMARKABLE recent progress has been made on both unconditional and conditional image synthesis <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The former aims to generate highfidelity images from random latent vectors (e.g., sampled from the standard multivariate Gaussian distribution). The latter needs to do so with given conditions satisfied in terms of certain consistency metrics. The conditions may take many forms such as category labels <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, paired or unpaired source images <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, semantic maps <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, text description <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and scene graphs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Conditional image synthesis, especially with coarse yet sophisticated and reconfigurable conditions, remains a long-standing problem. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we shall focus on conditional image synthesis from a spatial layout and its style latent codes, so-called layout-to-image <ref type="bibr" target="#b19">[20]</ref>. Powerful systems, once developed, can pave a way for computers to truly understand visual patterns and their compositions via a comprehensive and systematic "analysisby-synthesis" scheme. Those systems will also enable a wide range of practical applications, e.g., generating high-fidelity data for long-tail scenarios in different vision tasks such as autonomous driving.</p><p>In the layout-to-image synthesis, the layout that a synthesized image needs to satisfy consists of a number of labeled bounding boxes configured in an image lattice (e.g., 256 × 256 pixels). The style of a synthesized image refers to structural and appearance variations at both image and object levels, which is often encoded by corresponding image and object latent codes. Generating images from a spatial layout represents a sweet spot in conditional image synthesis. Spatial layouts are usually used as intermediate representations for other conditional image synthesis tasks such as text-to-image <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref> and scene-graph-to-image <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. And, layouts are more flexible, less constrained and easier to collect than other conditions such as semantic segmentation maps <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For example, existing object detection benchmarks can be exploited in training.</p><p>The generative learning task of layout-to-image synthesis was recently proposed and only a few work have been proposed in the very recent literature <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Although relatively new, it has been well recognized in the computer vision community. For example, the work, Grid2Im by Ashua and Wolf <ref type="bibr" target="#b18">[19]</ref> won the best paper honorable mentions at ICCV 2019. The layout-to-image synthesis task was emerged under the context of remarkable progress made on conditional image synthesis with relatively less complicated conditions such as the class-conditional image synthesis in ImageNet by the BigGAN <ref type="bibr" target="#b4">[5]</ref>, and the amazing style control for specific objects (e.g., faces and cars) by the StyleGAN <ref type="bibr" target="#b7">[8]</ref>Despite the big successes achieved by BigGANs and StyleGANs, learning generative models for layout-to-image synthesis entails more research. In addition to realness, generative models for layout-to-image synthesis need to tackle many spatial and semantic relationships among multiple objects (combinatorial in general). Specifically, learning layout-to-image synthesis requires addressing the problems of learning one-to-many mapping (i.e., one layout covers many plausible realizations in image synthesis to preserve the intrinsic uncertainty), and of handling consistent multi-object generation (e.g., occlusion handling for overlapped bounding boxes and uneven, especially longtail distributions of objects). Because of those, it is difficult to capture underlying probability distributions defined in the solution space of layout-to-image synthesis.  <ref type="bibr" target="#b21">[22]</ref> at the resolution of 256 × 256. On the Left Panel: The proposed method is compared with the prior art, the Grid2Im method <ref type="bibr" target="#b18">[19]</ref>. Each row shows effects of style control, in which three synthesized images are shown using the same input layout on the left by randomly sampling three style latent codes. Each column shows effects of layout control in terms of consecutively adding new objects (the first three) or perturbing an object bounding box (the last one), while retaining the style codes of existing objects unchanged. Advantages of the proposed method: Compared to the Grid2Im method, (i) the proposed method can generate more diverse images with respect to style control (e.g., the appearance of snow, and the pose and appearance of person). (ii) The proposed method also shows stronger controllability in retaining the style between consecutive spatial layouts. For example, in the second row, the snow region is not significantly affected by the newly added mountain and tree regions. Our method can retain the style of snow very similar, while the Grid2Im seems to fail to control. Similarly, between the last two rows, our method can produce more structural variations for the person while retaining similar appearance. Models are trained in the COCO-Stuff dataset <ref type="bibr" target="#b21">[22]</ref> and synthesized images are generated at a resolution of 256 × 256 for both methods. Note that the Grid2Im method <ref type="bibr" target="#b18">[19]</ref> utilizes ground-truth masks in training, while the proposed method is trained without using ground-truth masks, and thus more flexible and applicable in other datasets that do not have mask annotations such as the Visual Genome dataset <ref type="bibr" target="#b22">[23]</ref>. On the Right Panel: Illustration of the fine-grained control at the object instance level. For an input layout and its real image in the first row, four synthesized masks and images are shown. Compared with the 2nd row, the remaining three rows show synthesized masks and images by only changing the latent code for the Person bounding box. This shows that the proposed method is capable of disentangling object instance generation in an synthesized image at both the layout-to-mask level and the mask-to-image level, while maintaining a consistent layout in the reconfiguration. Please see text for details.</p><p>This paper is interested in controllable image synthesis from reconfigurable layouts and style codes. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, by controllable and reconfigurable, it means a generative model is capable of (i) Layout Control -the model is adaptive with respect to changes of layouts (e.g., adding new objects), or perturbations of bounding boxes in a given layout, as well as the style codes associated with the changes of spatial layouts, and (ii) Style Control -the model preserves the intrinsic one-to-many mapping from a given layout to multiple plausible images with sufficiently different structural and appearance styles (i.e., diversity), at both image and object levels (see the right panel of <ref type="figure" target="#fig_0">Fig. 1</ref>). Prior arts on layout-to-image synthesis mainly focus on low resolution (64 × 64) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, except for the very recent Grid2Im method <ref type="bibr" target="#b18">[19]</ref> which can synthesize images at a resolution of 256 × 256. We further study (i) a layout semi-supervised version of the proposed method without sacrificing the synthesis performance, which use half of the annotated bounding boxes in the training dataset and whose results shed light on some interesting and important directions for developing stronger layout-to-image synthesis methods, and (ii) an end-to-end integration of the proposed method with the SPADE in the GauGAN <ref type="bibr" target="#b14">[15]</ref>, which shows the advantage of the proposed ISLA-Norm scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Method overview</head><p>To learn controllable image synthesis from reconfigurable layouts and style codes, we build on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> and present a LayOut-and STylebased architecture and learning paradigm for GANs. We termed the proposed method LostGAN in our previous conference paper presented at ICCV 2019, entitled "Image Synthesis from Reconfigurable Layout and Style" <ref type="bibr" target="#b24">[25]</ref> . We shall call the conference version LostGAN-V1 and the updated model LostGAN-V2 in this paper. We first give an overview of our LostGAN and then summarize the changes of LostGAN-V2.</p><p>The proposed LostGAN addresses the layout-to-image synthesis problem by learning GANs for layout-to-maskto-image synthesis. To account for the gap between bounding boxes in a layout and underlying object shapes, learning layout-to-mask is an intuitive and straightforward intermediate step with advantages in two-fold: It induces finergrained style control of objects in a synthesized image. It also helps decouple learning of object geometry and learning of object appearance. The layout-to-mask generation itself is a relatively easier task than the direct layout-toimage synthesis since object appearance are ignored. In the  <ref type="figure">Fig. 2</ref>. Illustration of the workflow of our proposed LostGANs. Both the generator and discriminator use ResNets <ref type="bibr" target="#b25">[26]</ref> as backbones. In the generator, "ToRGB" is a simple module converting the final feature map to RGB images. Our proposed ISLA-Norm and detailed specifications of the generator are explained in <ref type="figure">Fig. 3</ref>. Best viewed in color. meanwhile, motivated by the impressive recent progress on conditional image synthesis from semantic label maps <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, it also makes sense to integrate a layout-to-mask component. If reasonably good object masks can be inferred for an input layout, the learning of mask-to-image synthesis can then leverage the best practice in conditional image synthesis from semantic label maps. A naïve approach is to develop two-stage generators, which may provide less effective solutions. Instead, we present a single-stage learning paradigm (i.e., using a single generator). <ref type="figure">Fig. 2</ref> illustrates the overall workflow of the proposed LostGAN. <ref type="figure">Fig. 3</ref> illustrates the single-stage learning of layout-to-maskto-image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ISLA-Norm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Backbone</head><p>The generator has three inputs: (i) a spatial layout, L consisting of a number of object bounding boxes in an image lattice, (ii) a latent vector, z img for style control at the image level, and (iii) a concatenation vector between a bucket of object latent vectors, z obji 's and the label embedding vector of object instances in the layout. The object latent vectors are used for style control of object instances respectively. The generator takes (ii) as its direct input for overall style control, while utilizing a novel feature normalization scheme for object-level style control based on (iii).</p><p>The object latent vectors are involved in each stage of the generator for better style control and better diversity, similar in spirit to StyleGANs <ref type="bibr" target="#b7">[8]</ref>  <ref type="figure">(Fig. 3)</ref>.</p><p>The Instance-Sensitive and Layout-Aware Feature Normalization (ISLA-Norm, <ref type="figure">Fig. 3</ref>) scheme is presented to realize the proposed layout-to-mask-to-image synthesis pipeline in our LostGANs. As a feature normalization scheme, it consists of two components: feature standardization and feature recalibration. The former is done as the BatchNorm <ref type="bibr" target="#b26">[27]</ref> in which channel-wise mean and standard deviation are computed in a mini-batch. The latter is different from the BatchNorm.</p><p>Unlike the BatchNorm in which channel-wise affine transformation parameters, β (for re-shifting) and γ (for rescaling) are learned as model parameters and shared across spatial dimensions by all instances, our ISLA-Norm first learns object instance-sensitive channel-wise affine transformations from the concatenation of object label embedding and object style latent vectors, as shown by the arrows in blue in <ref type="figure">Fig. 3</ref>. This is similar in spirit to the Adaptive Instance Normalization (AdaIN) used in StyleGANs <ref type="bibr" target="#b7">[8]</ref> and the projection-based conditional BatchNorm used in cGANs <ref type="bibr" target="#b4">[5]</ref>. Our ISLA-Norm also learns the object masks for objects in an input layout in two pathways: one pathway learns object masks from the concatenation vector between the object label embedding vector and object style latent vectors, which are assembled into a label map, and the other learns a label map from each layer in the generator. A learnable weighted sum of the two label maps are used as the inferred label map at a stage in the generator. Then, to obtain fine-grained spatially-distributed multi-object style control for an input layout, we place the object instance-sensitive channel-wise affine transformations in the learned label map, leading to the instance-sensitive and layout-aware affine transformations for feature recalibration in the generator, as illustrated by the light-grey cubes in <ref type="figure">Fig. 3</ref>.</p><p>The discriminator has two inputs: an input image, either fake or real, and the corresponding spatial layout. It consists of three components: (i) a ResNet <ref type="bibr" target="#b25">[26]</ref> feature backbone, (ii) an image head classifier computing the image realness score based on the extracted features (the higher the score is, the more real an image is), and (iii) an object head classifier computing the realness scores for the object instances. The realness score can also be interpreted as playing the role of the negative energy in energy-based models <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, although we do not apply the likelihood-based learning method in training. The feature representation for an object instance is computed by the RoIAlign operator <ref type="bibr" target="#b30">[31]</ref> using its bounding box in a given layout. Detailed specifications of the discriminator are shown in <ref type="figure">Fig. 4</ref>.</p><p>Motivated by the projection-based conditional GANs <ref type="bibr" target="#b5">[6]</ref> and the practice in BigGANs <ref type="bibr" target="#b4">[5]</ref>, a label projection-based score is added to the realness score of each object instance.</p><p>The loss function consists of both image and object adversarial hinge loss terms <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> (balanced by a trade-off parameter, λ). The hinge loss aims to push the realness score of a synthesized image sufficiently away from that of a real image by a predefined margin. Under the twoplayer minmax game setting of GANs, the hinge loss works better to enforce both the generator and the discriminator more aggressive in synthesizing images of higher fidelity.</p><p>Layout semi-supervised training of LostGANs. To investigate the possibility of learning from less labels (labeled conditions) in conditional image synthesis and to understand the bottleneck of layout-to-image synthesis, a layout semi-supervised version of LostGAN-v2 is proposed, which obtains comparable performance with the fully-supervised 1 LostGAN-v2. It thus sheds light on some interesting and important directions to develop better layout-to-image synthesis systems.</p><p>Summary of Changes. Compared to our previous LostGAN-V1 <ref type="bibr" target="#b24">[25]</ref>, the main changes of our LostGAN-V2 are as follows.</p><p>• The ISLA-Norm is extended by integrating label maps learned from feature maps at different stages in the generator. Comprehensive experiments are conducted to understand both the effectiveness and benefits of the proposed ISLA-Norm. • A layout semi-supervised LostGAN-V2 is studied. <ref type="bibr" target="#b0">1</ref>. By fully-supervised, it means that each training image has its spatial layout annotated. Accordingly, semi-supervised training means that a portion of the training image does not have spatial layouts annotated (e.g., 50% training images).  <ref type="figure">Fig. 4</ref>. Illustration of the discriminator network (left). The shared feature backbone, the image-level feature backbone and the object-level feature pyramid use ResBlocks (right). Each of them consists of a number of ResBlocks depending on the target resolution of layout-to-image synthesis (e.g., 256 × 256). In the ResBlock, "[op]" means an operation is optional subject to the settings. The object-level feature pyramid is used for placing object instances of different sizes at different feature layers (e.g., smaller bounding boxes placed at lower feature layers as done in the FPN <ref type="bibr" target="#b33">[34]</ref>), such that the RoIAlign operation is meaningful. "FC" represents a fully-connected layer (with either a scalar output shown by a grey triangle or a vector output shown by a grey trapezoid). "AvgPool" represents a global channel-wise average pooling over the spatial dimensions.</p><p>• An end-to-end integration of the proposed LostGAN-V2 and the SPADE in the GauGAN <ref type="bibr" target="#b14">[15]</ref> is studied. • The experiments are significantly extended by training models at higher resolutions and by comparing with the prior arts including the Grid2Im <ref type="bibr" target="#b18">[19]</ref> and the Gau-GAN <ref type="bibr" target="#b14">[15]</ref>. • The paper is thoroughly rewritten with much more details on different aspects of the LostGAN and on the experimental settings, together with new figures of the model. • Ablation studies are added to analyze the proposed Lost-GAN and ISLA-Norm. Our source code and pretrained models of both LostGAN-V1 and V2 have been made publicly available at https://github.com/iVMCL/LostGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Related work</head><p>Generative models have been widely studied in recent years such as Autoregressive models, Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). For image generation, Autoregressive models such as Pixel-RNNs <ref type="bibr" target="#b34">[35]</ref> and PixelCNNs <ref type="bibr" target="#b35">[36]</ref> synthesize images pixel by pixel based on conditional distribution over pixels. VAEs <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> jointly train an encoder and decoder where the former maps images into latent distribution and the latter generates images based on the latent distribution. GANs <ref type="bibr" target="#b0">[1]</ref> are able to synthesize realistic and high resolution images under various settings, including both unconditional <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[39]</ref> and conditional tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Typically, a GAN consists of a Generator that produces realistic fake images from input (e.g., random noise) and a Discriminator that distinguishes generated images from real ones. More recently, a unified divergence triangle framework is proposed for joint training of generator model, energy-based model, and inference model for generative tasks <ref type="bibr" target="#b39">[40]</ref>. Our proposed model is built on GANs and aimed at image synthesis conditioned on coarse semantic layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Image Synthesis.</head><p>Conditional image synthesis takes additional information (e.g., class information <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b40">[41]</ref>, source images <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b41">[42]</ref>, a text descriptions <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, scene graphs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref>) as inputs. How to feed conditional information to a GAN model has been studied in various ways. First, in all methods, conditional information are encoded into a vector representation. The encoded condition vector is used differently by different methods. In <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, the encoded condition vector and a sampled latent vector are concatenated as the input to the generator network. In <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b45">[46]</ref>, the encoded condition vector is utilized by the discriminator by simply concatenating with the input or intermediate feature maps. In <ref type="bibr" target="#b5">[6]</ref>, projection-based methods exploit conditional information in the discriminator using the inner product between features in the discriminator and the encoded condition vector, which effectively improve the quality of class conditional image generation. In <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, the encoded condition vector is used to control the re-scaling and re-shifting parameters in the BatchNorm <ref type="bibr" target="#b26">[27]</ref> layers, leading to the conditional BatchNorm. GauGANs <ref type="bibr" target="#b14">[15]</ref> further learn spatially adaptive re-scaling and re-shifting parameters for BatchNorm from an annotated semantic label map. The proposed ISLA-Norm in our previous LostGAN-V1 <ref type="bibr" target="#b24">[25]</ref> is a concurrent work with the feature normalization scheme in GauGANs without resorting to annotated semantic label maps. It learns the layout-to-mask mapping from coarse layout information. The proposed LostGANs also adopt the projection-based methods of exploiting conditional information in the discriminator as done in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Image Synthesis from Layout. Image synthesis from layout has been studied in the very recent literature and proven a difficult task. The layout-to-image task was first studied in <ref type="bibr" target="#b19">[20]</ref> at the resolution of 64×64, which uses a variational autoencoders based network, together with long-short term memory (LSTM), for object feature fusion. In <ref type="bibr" target="#b44">[45]</ref>, an external memory bank is introduced, consisting of objects cropped from real images in training, which are retrieved and pasted in generating images from layouts at the resolution of 64×64. In <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b48">[49]</ref>, layout and object information are utilized in text-to-image synthesis or scene-graph-to-image synthesis. <ref type="bibr" target="#b49">[50]</ref> synthesize scene images from given masks by matching context, shape and parts to a stored library. In <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>, locations of multiple objects are controlled in text-to-image synthesis by adding an extra object pathway in both the generator and discriminator. In <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, a two-step approach is used in image synthesis: generating the semantic layout (class label, bounding boxes and semantic mask) from a text description or a scene graph, and synthesizing images conditioned on the predicted semantic layout and text description (if present). However, in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, pixel-level instance segmentation annotations are needed in training, while the proposed LostGANs do not require pixel-level annotations and can learn semantic masks in a weakly-supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Our contributions</head><p>This paper makes the following main contributions to the field of conditional image synthesis.</p><p>• It presents a layout-and style-based architecture for GANs (termed LostGANs), which addresses the problem of layout-to-image synthesis by learning layout-tomask-to-image synthesis. The proposed LostGANs realize controllable image synthesis from reconfigurable layouts and styles. The proposed LostGANs can be trained in a layout fully-supervised way or a layout semi-supervised way. The outputs of LostGANs include both image and semantic label map synthesis. • It presents an object instance-sensitive and layoutaware feature normalization scheme (termed ISLA-Norm), which explicitly accounts for the joint learning of layout-to-mask generation and spatially-distributed feature recalibration at an object mask level. The ISLA-Norm shows better performance than the SPADE scheme in the GauGAN <ref type="bibr" target="#b14">[15]</ref> in the layout to image synthesis task. • It can synthesize images at a resolution of up to 512 × 512 and shows state-of-the-art performance in terms of the Inception Score <ref type="bibr" target="#b51">[52]</ref>, the Frèchet Inception Distance <ref type="bibr" target="#b52">[53]</ref>, the Diversity Score base on the LPIPS metric <ref type="bibr" target="#b53">[54]</ref>, the classification accuracy score <ref type="bibr" target="#b54">[55]</ref> and Faster-RCNN <ref type="bibr" target="#b55">[56]</ref> based object detection Average Precision (AP) on two widely used datasets, the COCO-Stuff <ref type="bibr" target="#b21">[22]</ref> and the Visual Genome <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Paper organization</head><p>In the remainder of this paper, Section 2 presents the problem formulation of layout-to-image and technical details of our proposed LostGANs and ISLA-Norm. Section 3 shows the experimental settings, quantitative and qualitative results, together with ablation studies. Section 4 concludes this paper with discussions on some directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem formulation</head><p>Denote by Λ an image lattice (e.g., 256 × 256) and by I an image defined on the lattice.</p><formula xml:id="formula_0">Let L = {( i , bbox i ) m i=1</formula><p>} be a layout consisting of m labeled bounding boxes, where a label i ∈ C (e.g., |C| = 171 in the COCO-Stuff dataset <ref type="bibr" target="#b21">[22]</ref>), and a bounding box bbox i ⊆ Λ. Different bounding boxes may overlap and thus have undetermined partial-order of occlusions.</p><p>Let z img be the latent code controlling the image style and z obji the latent code controlling the object instance style for ( i , bbox i ). The latent codes are often randomly sampled from the standard multivariate Gaussian distribution,</p><formula xml:id="formula_1">N (0, 1) under the i.i.d. setting. Denote by Z obj = {z obji } m i=1</formula><p>the set of object instance style latent codes. Image synthesis from layout and style is to learn a mapping from a given input (L, z img , Z obj ) to a synthesized image I syn ,</p><formula xml:id="formula_2">I syn = G(L, z img , Z obj ; Θ G ),<label>(1)</label></formula><p>where Θ G represents the parameters of the generation function. In general, a generator network G(·) is expected to capture the underlying conditional data distribution p(I|L, z img , Z obj ; Θ G ) in a high-dimensional space. While straightforward for synthesizing images (using a single phase of forward computation), the generator G(·) involves a challenging inference step entailed in estimating the model parameters, that is to compute the latent codes for a real image I real by sampling the posterior distribution, p(z img , z obj1 , · · · , z objm |I real , L). To mitigate the difficulty of the posterior inference, GANs propose an adversarial training paradigm which exploits an extra discriminator <ref type="bibr" target="#b0">[1]</ref> under a two-player minmax game setting.</p><p>Reconfigurability of a generator network G(·). In this paper, we are interested in three aspects as follows:</p><p>• Image style reconfiguration: For a fixed layout L, is the generator G(·) capable of synthesizing images with different styles for different (z img , Z obj ) samples, while retaining the layout configuration conditioned on the given L? • Object style reconfiguration: For a fixed input tuple (L, z img , Z obj ) except for one object style latent code z obji ∈ Z obj , is the generator G(·) capable of generating consistent images with different styles for the object ( i , bbox i ) using different z obji samples, while retaining the object configuration conditioned on the given L and the styles of the remaining objects? • Layout reconfiguration:</p><formula xml:id="formula_3">Given an input tuple (L, z img , Z obj ), is the generator G(·) capable of generating consistent images for different (L + , z img , Z + obj )'s,</formula><p>where L + has a newly added object instance or just changes the location and/or the label of an existing bounding box? When a new object is added, a new z obj is added in Z + obj . When only the bounding box location changes, all latent codes are kept unchanged (i.e., Z + obj = Z obj ). It is a challenging problem to address the three aspects by learning a single generator network G(·). Intuitively, it might be difficult for well-trained artistic people to do so at scale (e.g., the 171 categories in the COCO-Stuff dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The LostGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">The generator network</head><p>As illustrated in <ref type="figure">Fig. 3</ref>, the generator G(·) consists of a linear full-connected (FC) layer, followed by a number of residual building blocks (ResBlocks) <ref type="bibr" target="#b25">[26]</ref> depending on the target resolution of image synthesis, and a "ToRGB" module outputting a synthesized image. Detailed network architectures for different image synthesis resolutions are referred to our Github repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">The ISLA-Norm</head><p>There are two ISLA-Norm modules in a ResBlock (the righttop of <ref type="figure">Fig. 3</ref>). Denote by x an input 4D feature map of ISLA-Norm, and x n,c,h,w the feature response at a position (n, c, h, w) (using the convention order of axes for batch, channel, and spatial height and width). We have</p><formula xml:id="formula_4">n ∈ [0, N − 1], c ∈ [0, C − 1], h ∈ [0, H − 1], w ∈ [0, W − 1],</formula><p>where N is the mini-batch size or the accumulated size of synchronized mini-batches, and C, H, W depend on the stage of a ResBlock.</p><p>Feature Standardization. Our ISLA-Norm first computes the channel-wise mean and standard deviation as done in the BatchNorm <ref type="bibr" target="#b26">[27]</ref>. In training, ISLA-Norm first normalizes x n,c,h,w by,</p><formula xml:id="formula_5">x n,c,h,w = x n,c,h,w − µ c σ c ,<label>(2)</label></formula><p>where the channel-wise batch mean µ c = 1 N ·H·W n,h,w x n,c,h,w and standard deviation</p><formula xml:id="formula_6">σ c = 1 N ·H·W n,h,w (x n,c,h,w − µ c ) 2 + ( is a small positive constant for numeric stability).</formula><p>Feature Recalibration. In the BatchNorm <ref type="bibr" target="#b26">[27]</ref>, the recalibration is done by learning channel-wise affine transformations, consisting of the re-scaling paramter, γ c 's and the re-shifting parameters, β c 's. We have,</p><formula xml:id="formula_7">x BN n,c,h,w = γ c ·x n,c,h,w + β c .<label>(3)</label></formula><p>Our ISLA-Norm learns instance-sensitive and layoutaware affine transformation parameters, γ n,c,h,w 's and β n,c,h,w 's, and we have,</p><formula xml:id="formula_8">x n,c,h,w = γ n,c,h,w ·x n,c,h,w + β n,c,h,w ,<label>(4)</label></formula><p>where both γ n,c,h,w 's and β n,c,h,w are functions of (L, z img , Z obj ). Thus, the resulting recalibrated features x n,c,h,w 's are sensitive to both the layout and the image and object style codes, which leads to layout and style reconfigurable image synthesis.</p><p>Computing γ n,c,h,w and β n,c,h,w . Without loss of generality, we show how to compute the gamma and beta parameters for one sample, i.e., γ c,h,w and β c,h,w . As shown in the left of <ref type="figure">Fig. 3</ref>, we have the following six components.</p><p>i) Label Embedding. We use one-hot label vector for the m object instances in a layout L, which results in a onehot label matrix, denoted by Y , of the size m × d , where d is the number of object categories (e.g., d = 171 in the COCO-Stuff dataset). Label embedding is to learn a d × d e embedding matrix, denoted by W , to compute the vectorized representation for labels,</p><formula xml:id="formula_9">Y = Y · W,<label>(5)</label></formula><p>where Y is a m×d e matrix and d e represents the embedding dimension (e.g., d e = 128 in our experiments). ii) Joint Label and Style Encoding. We sample from the standard Gaussian distribution the object style latent codes Z obj which is a m×d obj noise matrix (e.g., d obj = 128 in our experiments). Let S be the joint label and style encoding,</p><formula xml:id="formula_10">S = (Y, Z obj ),<label>(6)</label></formula><p>which is a m × (d e + d obj ) matrix. So, the object instance style depends on both the label embedding (semantics) and i.i.d. latent codes (accounting for style variations). iii) Mask Generation from S. We first generate an mask for each object instance in a layout L at a predefined size, s × s (e.g., s = 32) individually. Then, we resize the generated masks to the sizes of corresponding bounding boxes at a ResBlock stage in the generator.</p><p>• The mask generation process consists of two components:</p><p>one is a simplified generator model (the small trapezoid in purple in <ref type="figure">Fig. 3</ref>  <ref type="figure">Fig. 3</ref>), we use arg max across the m channels of M S to assign the label index for a pixel occupied by more than one objects due to occlusions. iv) Mask updating using the feature maps in a generator. For a ResBlock stage, we learn a mask from its input feature map using a simple "ToMask" operation implemented by Conv3x3+Sigmoid, where the out channel of the Conv3x3 kernel is d (i.e., the number of categories in a dataset). The mask is represented by a tensor of sizes (d , H, W ). We clip the mask based on the layout by keeping values unchanged within the bounding boxes of the object instances in a layout and zeroing out the remainder. Denote by M F (L) the mask tensor of sizes (m, H, W ) after the clipping (omitting the index for a ResBlock in the generator). For the second ISLA-Norm module in a ResBlock (the right-top of <ref type="figure">Fig. 3</ref>), we upsample M F (L) by a factor of 2.</p><p>v) Object instance-sensitive channel-wise affine transformation parameters. They are learned from the joint label and style encoding S. We adopt a linear projection with a learnable (d e + d obj ) × 2C projection matrix A, where C is the number of channels, and we have,</p><formula xml:id="formula_11">T = S · A,<label>(7)</label></formula><p>which is a matrix of sizes (m, 2C). Let T β and T γ be the column-wise first and second half of T . We unsqueeze both T β and T γ to the size of (m, C, H, W ) by replicating values across the spatial dimensions. Learning the affine transformation parameters in this way leads to stronger style control and better diversity of our LostGAN than other layout-to-image methods, since the style latent codes get involved in every stage of the generator, rather than being used as input only to the first stage of the generator in other layout-to-image methods. vi) Computing the ISLA γ c,h,w and β c,h,w . We first unsqueeze the two masks, M S and M F (L), to the sizes (m, C, H, W ) by replicating C channels. Then, we have, Handling Background. To account for the situation in which all object instances do not occupy the entire image lattice (e.g., in the VG dataset <ref type="bibr" target="#b22">[23]</ref>), we introduce a background class 0 with bbox 0 = Λ.</p><formula xml:id="formula_12">γ c,h,w = 1 M c,h,w m i=1 M(i, c, h, w) × T γ (i, c, h, w), (8) β c,h,w = 1 M c,h,w m i=1 M(i, c, h, w) × T β (i, c, h, w), (9) where M(·) = [(1 − α) · M S + α · M F (L)](·) with</formula><p>Why does the ISLA-Norm help image synthesis? In sum, one the one hand, as shown by the blue rounded squares in <ref type="figure">Fig. 3</ref> and Eqn. 6 and Eqn. 7, each layer in the generator network directly learns object instance sensitive channel-wise feature recalibration parameters. So, the object style latent codes Z obj have direct impacts at each layer, unlike the image style latent code z img which may have degenerated impacts on the later layers in the generator in the sense that different z img 's may result in very similar images (i.e., less powerful style control).</p><p>On the other hand, the feature recalibration parameters γ n,c,h,w and β n,c,h,w are further modulated by the learned semantic label map. The semantic label map accounts for two information pathways: one is from the joint embedding of object labels and object style codes (Eqn. 6), and the other from the features in the previous layer (see the 'ToMask' module in <ref type="figure">Fig. 3</ref>). The second pathway is to learn the residual label map w.r.t. the ouput from the first pathway. These enable the feature recalibration parameters γ n,c,h,w and β n,c,h,w to deeply interact with object style codes and the intermediate features (computed from the image latent code) in the generator network.</p><p>By doing those, our ISLA-Norm shares the spirit with the AdaIN in StyleGANs <ref type="bibr" target="#b7">[8]</ref>, but realizes strong style control in a fine-grained spatially-adaptive way. And, our ISLA-Norm further distinguishes itself from the concurrent work of GauGANs <ref type="bibr" target="#b14">[15]</ref> which only use z img in controlling styles and resort to ground-truth masks for fine-grained feature recalibration. For an in-depth comparison with GauGANs, we further investigate the integration between the proposed LostGANs and the GauGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Comparing with the SPADE module of GauGANs</head><p>The main difference between the SPADE module <ref type="bibr" target="#b14">[15]</ref> and the proposed ISLA-Norm module lies in how the affine transformation parameters (Eqn. 8 and 9) are computed. The SPADE module is designed for the label-map-to-image synthesis task and thus directly leverages an input groundtruth label map in learning the affine transformation parameters. To compare the designs of SPADE and ISLA-Norm, we conduct experiments in three aspects:</p><p>• A post-hoc integration which uses a trained LostGAN-V2 to generate the masks as the input label map to the GauGAN trained with ground-truth label maps in testing. This is to verify that (i) the layout-to-mask generation in LostGANs is meaningful in the sense that the generated masks can be "dropped in" the GauGANs trained with ground-truth label maps to obtain good image synthesis results, and (ii) the mask-to-image synthesis in LostGANs is sufficiently strong against the counterpart GauGANs when the input masks to the both are the same. Results are reported in Section 3.4. • An end-to-end integration which uses the mask generation components (i.e., i), ii) and iii) in Section 2.2.2), which corresponds to LostGANs-V1, and the SPADE module in GauGANs in training from scratch. This integration is to investigate whether the SPADE module can be used to replace the ISLA-Norm in this straightforward way. However, the training fails in our many tries in the experiments due to the NaN numeric issue. • Another end-to-end integration which uses both the mask generation components and the mask refinement strategy (i.e., iv) in the Section 2.2.2), which corresponds to LostGANs-V2, and the SPADE module in GauGANs in training from scratch. This resolves the training issues in the straightforward integration stated above. The resulting model, termed LostGAN+SPADE, is slightly worse than the vanilla LostGAN-V2. Results are reported in Section 3.4 with synthesized images shown in <ref type="figure">Fig. 5</ref>. One explanation is that our ISLA-Norm utilizes input bounding boxes to spatially clip the generated and refined masks, while the SPADE module directly uses the generated and refined masks in their entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">The discriminator network</head><p>As shown in <ref type="figure">Fig. 4</ref>, our discriminator consists of three components: a shared ResNet-based feature backbone, an image head classifier and an object head classifier. Detailed network architectures for different image synthesis resolutions are referred to our Github repository. Denote by D(·; Θ D ) the discriminator with parameters Θ D . Given an image I (real or synthesized) and a layout L, the discriminator computes a list of scores,</p><formula xml:id="formula_13">(p img , p obj1 , · · · , p objm ) = D(I, L; Θ D )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">The loss functions</head><p>Under the mini-batch based SGD framework, for the generator, the loss function of Θ G is defined by,</p><formula xml:id="formula_14">L(Θ G |Θ D ) = − (L,I syn ,I gt )∈B P D(I syn ,L;Θ D ) − (11) ||I syn − I gt || 1 − ||F (I syn ) − F (I gt )|| 1 ,</formula><p>where B represents a mini-batch, I syn and I gt represent a synthesized image (Eqn. 1) and the ground-truth image for the spatial layout L, P D(I,L;Θ D ) = λ · p img + 1 m m i=1 p obji with a trade-off parameter λ (0.1 used in our experiments), the second term in the right-hand side is the reconstruction loss, and the last term is the perceptual loss <ref type="bibr" target="#b56">[57]</ref> which measure L1 difference between features, F (·) of generated image and ground truth images by an ImageNet pretrained network such as the VGG network <ref type="bibr" target="#b57">[58]</ref>. Minimizing L(Θ G |Θ D ) is trying to fool the discriminator by generating high fidelity images.</p><p>For the discriminator, we utilize the hinge version <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> of the standard adversarial loss <ref type="bibr" target="#b0">[1]</ref>,</p><formula xml:id="formula_15">l t (I, L) = max(0, 1 − p t ); if I is a real image max(0, 1 + p t ); if I is a fake image<label>(12)</label></formula><p>where t ∈ {img, obj 1 , · · · , obj m }. In the hinge loss, no penalty will occur if the score of a real image (or a real object instance) is greater than or equal to 1, and the score of a fake image (or a fake object instance) is less than or equal to -1. The hinge loss is more aggressive than the real vs fake binary classification in the vanilla GAN. The overall loss is,</p><formula xml:id="formula_16">l(I, L) = λ · l img (I, L) + 1 m m i=1 l obji (I, L).<label>(13)</label></formula><p>The loss function of Θ D is defined by,</p><formula xml:id="formula_17">L(Θ D |Θ G ) = (L,I syn ,I gt )∈B l(I gt , L) + l(I syn , L) ,<label>(14)</label></formula><p>where p(I, L) represents both the real and fake (synthesized by the generator) data. Minimizing L(Θ D |Θ G ) is trying to tell apart the real and fake images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6">Layout semi-supervised training of LostGANs</head><p>Conditional image synthesis is a data and annotation hungry task. For layout-to-image synthesis, it is interesting to investigate the paradigm of learning with less labels (i.e., layouts). We present a straightforward two-stage training procedure. Denote by D s and D u the image dataset with and without object bounding boxes annotated respectively. We first train a Faster-RCNN <ref type="bibr" target="#b55">[56]</ref> object detector using D s . Then, we apply the trained Faster-RCNN detector in D u . In terms of how to leverage the detection results in D u , the most obvious way is to use the bounding boxes of detected objects whose probabilities are greater than a predefined threshold, e.g., 0.5. This gives us reasonably results.</p><p>To account for the uncertainty of Faster-RCNN detection results, we present a detection score re-weighing method which uses the detection probability of a declared object bounding box by the Faster-RCNN detector in the loss of the discriminator network.</p><formula xml:id="formula_18">Let L = ( i , {bbox i , p i } m i=1</formula><p>) the layout for an image I ∈ D u based on the detection results, where p i ≥ τ (e.g., τ = 0.5). Eqn. 13 is rewritten as,</p><formula xml:id="formula_19">l(I, L) = λ · l img (I, L) + 1 m m i=1 p i · l obji (I, L),<label>(15)</label></formula><p>which leads to comparable performance to the fullysupervised LostGAN when only half of the images in the COCO-Stuff dataset use annotated bounding boxes (see results analyses in Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.7">Implementation details</head><p>In training, we follow the practice used in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Synchronized BatchNorm <ref type="bibr" target="#b26">[27]</ref>, where batch statistics for feature standardization are computed over all devices, is adopted in our ISLA-Norm. The Spectral Normalization <ref type="bibr" target="#b3">[4]</ref> of model parameters is also applied in both the Generator and the Discriminator to stabilize training. Parameters of the Generator and the Discriminator are initialized using the Orthogonal Initialization method <ref type="bibr" target="#b58">[59]</ref>. The Adam optimizer <ref type="bibr" target="#b59">[60]</ref> is used with β 1 = 0 and β 2 = 0.999. The learning rate is set constant 10 −4 for both the Generator and the Discriminator. We use a batch size of 128 based on our computing resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We test our LostGANs in the COCO-Stuff dataset <ref type="bibr" target="#b21">[22]</ref> and the Visual Genome (VG) dataset <ref type="bibr" target="#b22">[23]</ref>. We evaluate LostGAN-V1 at two resolutions (64×64 and 128×128) and LostGAN-V2 at three resolutions (128×128, 256×256 and 512 × 512). We evaluate the Semi-LostGAN-V2 at the resolution of 128×128. Our LostGAN-V2 obtains state-of-the-art performance.</p><p>Datasets. The COCO-Stuff 2017 <ref type="bibr" target="#b21">[22]</ref> augments the COCO dataset with pixel-level stuff annotations. The annotation contains 80 thing classes (person, car, etc.) and 91 stuff classes (sky, road, etc.) Following settings of <ref type="bibr" target="#b17">[18]</ref>, objects covering less than 2% of the image area are ignored, and we use images with 3 to 8 objects. For the Visual Genome (VG) dataset <ref type="bibr" target="#b22">[23]</ref>, we follow the settings of <ref type="bibr" target="#b17">[18]</ref> to remove small and infrequent objects, which results in 62,565 images <ref type="bibr">TABLE 1</ref> Quantitative comparisons using the Inception Score (IS, higher is better, illustrated by ↑), FID (lower is better, illustrated by ↓) and Diversity Score (DS, higher is better, illustrated by ↑) evaluation metrics in the COCO-Stuff <ref type="bibr" target="#b21">[22]</ref> and VG <ref type="bibr" target="#b22">[23]</ref> datasets. See text for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IS↑</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods in comparison.</head><p>We compare with four prior arts: i) The pix2pix method <ref type="bibr" target="#b10">[11]</ref> learns to map images between two domains. We reuse the pix2pix results reported in the Layout2Im <ref type="bibr" target="#b19">[20]</ref> in our comparisons, where a pix2pix model is trained to synthesize images from a feature map learned to encode the layout. The number of channels of the feature map is the number of categories (e.g., 171 in COCO-Stuff). ii) The scene graph to image (sg2im) method <ref type="bibr" target="#b17">[18]</ref> synthesizes images from input scene graphs with an intermediate scene-graph-to-layout module. We compare with sg2im using the ground-truth (GT) layouts.</p><p>iii) The Layout2Im method <ref type="bibr" target="#b19">[20]</ref> is the first to synthesize images directly from input layouts. These three methods have only been evaluated at the resolution of 64× 64. iv) The Grid2Im method <ref type="bibr" target="#b18">[19]</ref> extends the sg2im method, which has been tested at two resolutions, 128×128 and 256×256, in the COCO-Stuff dataset only since ground-truth masks are needed in training. We also compare with Grid2Im using the GT layouts.</p><p>Evaluation metrics. It remains a challenging problem to automatically evaluated image synthesis in general. For the layout-to-image synthesis, we adopt four state-of-the-art metrics and test a new one specifically reflecting the layout quality as follows.</p><p>The Inception Score (IS) <ref type="bibr" target="#b51">[52]</ref> uses an Inception V3 network pretrained on the ImageNet-1000 classification benchmark and computes a score (statistics) of the network's outputs with N synthesized images I i 's of a generator model G. The IS aims to capture two desirable qualities of image synthesis: Synthesized image should contain clear and meaningful objects (subject to the ImageNet-1000 training datasets), and diverse images from all the different categories in ImageNet should be observed in synthesized images. So, the larger the IS is, the better a generator model is. Multiple runs are usually used to calculate the mean±std evaluation (e.g., 5 runs are typically used). The IS does not leverage the statistics of real images.</p><p>The Frèchet Inception Distance (FID) <ref type="bibr" target="#b52">[53]</ref> has been proposed to improve IS by incorporating statistics from real images. It also uses an ImageNet-pretrained Inception V3 network and computes the Frèchet distance <ref type="bibr" target="#b60">[61]</ref> between two Gaussian distributions fitted to synthesizes images and real images respectively. The lower the FID is, the better a generator model is. Both the IS and FID do not explicitly measure the quality of one-to-many mapping in layout-toimage synthesis.</p><p>The Diversity Score (DS) aims to compare the perceptual similarity in a DNN feature space between two images, I 1 and I 2 , randomly generated from the same layout. We adopt the LPIPS metric <ref type="bibr" target="#b53">[54]</ref> in computing the DS. The higher the DS is, the better a generator model is.</p><p>The Classification Accuracy Score (CAS) <ref type="bibr" target="#b54">[55]</ref>. One longterm goal of generative learning in practice is to leverage synthesized images in training discriminative models. The CAS aims to verify how well a classification model trained only on synthesized images can perform on real testing images. So, the higher the CAS is, the better a generator model is. In contrast to the CAS, the classification accuracy metric used in the Layout2Im <ref type="bibr" target="#b19">[20]</ref> is based on models trained with real image and tested on synthesized images, which may overlook the diversity of synthesized images.</p><p>The Object Detection Average Precision (DAP). To evaluate the quality of a synthesized image in its entirety and to reflect the quality of the layout of a synthesized image, we first train a Faster-RCNN <ref type="bibr" target="#b55">[56]</ref> using the training data, and then evaluate the detection performance of the Faster-RCNN detector in the synthesized images generated using the layouts in the validation dataset. <ref type="table">Table 1</ref> summarizes the comparisons. <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig 6 show</ref> images synthesized by different models from the same layout in COCO-Stuff and VG respectively. The input layouts are quite complex. Our LostGAN-V2 can generate visually more appealing images with more recognizable objects that are consistent with input layouts at resolution 256×256. We analyze the quantitative results as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Overall synthesis quality based on IS, FID and DS</head><p>At the resolution of 64 × 64. Our LostGAN-V1 obtains the best performance in comparison. It obtains slightly better Inception Score in both datasets and FID in the VG  dataset than the Layout2Im. It obtains significantly better FID in the COCO-Stuff dataset (by more than 5 points reduction) and DS in both datasets. The diversity score of our LostGAN-V1 outperforms the Layout2Im by relative 288.9% and 277.8% in the two datasets respectively. There are a few other methods tested at the resolution of 64 × 64 in the Layout2Im <ref type="bibr" target="#b19">[20]</ref>, including the pix2pixHD <ref type="bibr" target="#b13">[14]</ref>, Bicy-cleGAN <ref type="bibr" target="#b62">[63]</ref> and GauGAN <ref type="bibr" target="#b14">[15]</ref>, which are outperformed by the Layout2Im method and thus not included here for the clarity of the table.</p><p>At the resolution of 128×128. Our LostGAN-V1 obtains better results than the Grid2Im method in the COCO-Stuff dataset, especially by more than 33% reduction in FID and by relative 42.9% increase in DS. Our LostGAN-V2 further improves the results, except for the IS and DS in the VG dataset. The decrease of IS and DS may be caused by the factors as follows.</p><p>Remarks. We observed that the VG dataset includes more diverse object configurations (e.g., bounding boxes may severely overlap in an image such as those for people, cloth and pants). In general, the bounding box annotations in the VG dataset are of lower quality than those in the COCO-Stuff dataset (e.g., they may have significant offsets for certain object instances). Those factors may affect the layout-tomask component, especially the module of predicting masks from feature maps in the generator, which we think is the reason of LostGAN-V1 slightly outperforming LostGAN-V2 in the VG dataset. Similarly, Layout2Im+OWA <ref type="bibr" target="#b19">[20]</ref> suf-  fers a slight drop of performance in the VG dataset after introducing an object-wise attention mechanism to model shape of different objects. Considering those, we only test our LostGAN-V2 at higher resolutions than 128 × 128. At the resolution of 256 × 256. Our LostGAN-V2 also obtains better results than the Grid2Im method by more than 2% increase in IS, 23% reduction in FID and relative 61.8% increase in DS in the COCO-Stuff dataset.</p><p>At the resolution of 512 × 512. There is no results from other baselines. Our LostGAN-V2 obtains better DS than that at the resolution of 256 × 256. However, our LostGAN-V2 obtains slightly worse results than those obtained at the resolution of 256 × 256 in terms of IS and FID. This phenomenon has been also observed in the BigGAN <ref type="bibr" target="#b4">[5]</ref>, which indicates, on the one hand, that more research are entailed to improve the quality of high resolution image synthesis, and on the other hand, that the models (Inception V3 pretrained in ImageNet at the resolution of 300 × 300) used in computing IS and FID may need to change. <ref type="figure" target="#fig_4">Fig. 7</ref> shows some selected examples of synthesized images. We observed that it is more difficult to generate realistic looking images at the resolution of 512 × 512. <ref type="table" target="#tab_6">Table 2</ref> summarizes comparisons of CAS. To compare the CAS, we train the ResNet-101 <ref type="bibr" target="#b25">[26]</ref> on cropped and resized objects at a resolution of 32×32 from generated images (five samples generated for each layout in the testing set) and evaluate the trained model on objects cropped and resized from real testing images. We follow the widely used settings of ResNet-101 on the CIFAR-10/100 (with images at the resolution 32 × 32). We train a 171-category classification ResNet-101 in the COCO-Stuff dataset and a 178-category ResNet-101 in the VG dataset. For synthesized images at the three resolutions, our LostGANs obtain the best accuracy, often by large margin. These results are aligned with the higher DS results consistently obtained by our methods. Hopefully, with more research in the future work, we will be able to generate high-fidelity and high-resolution images from reconfigurable layouts and styles to faciliate more powerful discriminative learning, especially for handling some long-tail or corner situations.   <ref type="table" target="#tab_7">Table 3</ref> summarizes the detection Average Precision (AP) comparisons between our LostGAN-V2 and the prior art, the Grid2Im method. We report the standard COCO metrics of average precision, AP bb 50 using the intersection-over-union (IOU) threshold 0.5, and AP bb 75 using the IOU threshold 0.75, and AP bb with IOU thresholds accumulated from 0.5 to 0.9. Our LostGAN-V2 achieves 2.6% absolute increase of the AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Object synthesis quality based on CAS and DAP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Real image nearest neighbors of generated images</head><p>To further show the fidelity of generated images compared with real images at an exemplar level in addition to the FID at a distribution level, and to check if the model overfits the training data, we compute two types of nearest neighbors in the training dataset for an image synthesized using an input layout in the validation dataset: (i) The AlexNet-Nearest for an synthesized image: It is computed based on the cosine similarities between the synthesized image and images in the training dataset. Images are represented by the pool5 layer features in the ImageNet pretrained AlexNet <ref type="bibr" target="#b61">[62]</ref>, which are 256 × 7 × 7 feature maps for 256 × 256 input images. This will capture both the appearance similarity and the coarse structural similarity between synthesized images and training images. (ii) The GSC-Nearest for the input layout: The Global Scene Consistency (GSC) <ref type="bibr" target="#b49">[50]</ref> metric is used, which measure both the distance of normalized histogram of labels and pixel-to-pixel overlap between query and target layouts, to find the nearest neighbors of the input layout map. Results are shown in <ref type="figure" target="#fig_4">Fig. 6, Fig. 7</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref> (together with other aspects of the proposed method), from which we can see the nearest neighbors are semantically meaningfully aligned with the generated images and the synthesized images are visually different from the nearest neighbor in terms of appearance and structure. This also supports the results that our LostGANs outperform other methods in terms of FID <ref type="table">(Table 1)</ref>.  </p><formula xml:id="formula_20">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Evaluation of the weakly-supervised learning of layout-to-mask generation in LostGANs</head><p>To investigate the quality of learned masks, we resort to the intersection-over-union (IoU) metric used in object semantic segmentation. We measure the IoU performance in the COCO-Stuff training dataset. We first crop masks for each category and then resize all the masks to the same resolution of 32×32. After training the LostGAN-V2 256×256, we run inference on each layout in the training dataset (one run is used for simplicity) and obtain the learned masks. We then crop and resize object masks in the same way as done for the ground-truth object masks. For each learned object mask, we retrieve the top-k nearest neighbors in terms of mask IoU in the set of ground-truth object masks. <ref type="figure" target="#fig_5">Fig. 8</ref> shows four examples with the top-10 nearest neighbors. <ref type="table" target="#tab_8">Table 4</ref> shows the mean IoUs for 13 selected object categories which have reasonably high IoUs. Section 3.2.2 shows qualitative analyses of the layout-to-mask module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Controllability and reconfigurability of style and layout</head><p>We show more examples of layout and style control in our LostGAN-V2 as follows, in addition to <ref type="figure" target="#fig_0">Fig. 1</ref>. Layout controllability is demonstrated by adding object to, or moving a bounding box in a layout. As shown in <ref type="figure" target="#fig_6">Fig. 9</ref>, when adding extra objects or moving the bounding box of one instance, our model can generate reasonable objects at the desired position while keeping existing objects unchanged as we keep the input style vectors of existing objects fixed. When moving the bounding box of an existing object, the style of generated object at the new position also is kept consistent, e.g., in the top-right of <ref type="figure" target="#fig_6">Fig. 9</ref>, the person bounding box is moved, while the style of the synthesized person is retained such as the pose and the color of clothes. Style controllability of our model is shown in <ref type="figure" target="#fig_0">Fig. 10</ref> by synthesizing images with different visual appearance for a given layout, encoded by different (z img , Z obj ) samples, while preserving objects at desired locations. The AlexNet-pool5 nearest neigbhors in the training dataset show that the synthesized images are not observed to suffer from overfitting.</p><p>Fine-grained object-level style controllability of our model is further shown in <ref type="figure" target="#fig_0">Fig. 11</ref> and <ref type="figure" target="#fig_0">Fig. 12. Fig. 11</ref> shows the controllability and the resulting diversity by changing the style latent code of a specific object instance. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the effects of gradually morphing styles of one instance in different synthesized images.</p><p>We have three observations as follows: (i) Our LostGAN-V2 is capable of disentangling the styles in synthesis at the object instance level with sufficient diversity induced for a specific object instance. This is controlled by the object instance specific layout-aware learning of the affine transformation parameters (Section 2.2.2). (ii) Our LostGAN-V2  <ref type="figure" target="#fig_0">Fig. 11</ref>. Fine-grained object-level style control w.r.t. a specific object instance in our LostGAN-V2 256 × 256: The hill bounding box and the tree bounding box are selected as the object instance to vary in the two rows respectively. We can observe sufficient changes of the two selected object instances, while the style of remaining objects are retained. Another example of person is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>is capable of handling the style morphing at the object instance level. (iii) Our LostGAN-V2 is capable of inducing semantically meaningful interpretations for the latent style codes via the proposed ISLA-Norm. For the "stuff" such as grass and sky in the left of <ref type="figure" target="#fig_0">Fig. 12</ref>, the change of an object style code does not affect its own object mask and the styles of remaining objects. For the "things" such as bus and giraffe in the right of <ref type="figure" target="#fig_0">Fig. 12</ref>, our LostGAN-V2 shows some interesting results. When linearly interpolating the latent style codes, for the bus example, the generator mainly changes the appearance according the change of latent codes in the morphing, while for the giraffe example, the generator changes both the appearance (slightly) and the pose. The learned object masks support these. So, it seems that the generator learns to understand the mixed semantic meanings of object latent style codes. image synthesis in a single generator in a weakly-supervised manner, verifying our proposed pipeline of simultaneously learning layout-to-mask-to-image. <ref type="figure" target="#fig_0">Fig. 13</ref> shows examples of mask refinement in the process of generation. The initial mask generation can produce reasonably good results, which are refined in the cascade of integrating masks learned from feature maps, especially for object boundaries (e.g., comparing (b) and (f)). This mask refinement is one of the main technical improvement between our LostGAN-V1 and LostGAN-V2, which also verifies the overall improvement by the LostGAN-V2 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Results of the layout-to-mask module in LostGANs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results of layout semi-supervised LostGANs</head><p>We test the semi-supervised LostGAN-V2 at the resolution of 128 × 128 in the COCO-Stuff dataset <ref type="bibr" target="#b21">[22]</ref>. We randomly and evenly split the training dataset into two subsets, denoted by D 1 and D 2 respectively. We discard the bounding box annotations in D 2 . Using D 1 , we train a Faster-RCNN <ref type="bibr" target="#b55">[56]</ref> detector for the entire 171 categories including both "things" and "stuff". Then, we run the trained Faster-RCNN detector in D 2 to generate detection results with a threshold τ = 0.5. The AP bb 50 in D 2 is 68.9%. <ref type="table" target="#tab_10">Table 5</ref> shows the results. We test four settings: (i) LostGAN-V2 trained with D 1 ; (ii) LostGAN-V2 trained with D 1 and D 2 together with the Faster-RCNN detection results using the threshold 0.5; (iii) LostGAN-V2 trained with D 1 and D 2 together with the Faster-RCNN detection results using the threshold 0.5 and Eqn. 15. First, the diversity score (DS) is not affected since it reflects the variations between different synthesized images and the style control of our LostGAN is not directly associated with the number of data ( <ref type="figure">Fig. 3</ref> and Eqn. 7). Then, in terms of IS and FID, LostGAN-V2 trained with half data (i) has worse performance, but not very significantly. LostGAN-V2 under the setting (iii) obtains results comparable to the fully-supervised counterpart. This may indicate the bottleneck of layout-to-image synthesis is not the amount of annotated bounding boxes in training, but the capabilities of inferring better object masks on-the-fly. This is consistent with observations from existing work: BigGANs <ref type="bibr" target="#b4">[5]</ref> and StyleGANs <ref type="bibr" target="#b7">[8]</ref> have shown great results of image synthesis using simpler conditions than layouts. GauGANs <ref type="bibr" target="#b14">[15]</ref> also have shown great results of image synthesis conditioned on annotated semantic maps.</p><p>Remarks: We train the Faster-RCNN and the LostGAN individually for simplicity. It seems promising that we can train them jointly. In the meanwhile, we also can explore how to leverage the LostGAN to help the Faster-RCNN by generating more data similar in spirit to the CAS evaluation in <ref type="table" target="#tab_6">Table 2</ref>. Thus, it is possible to form a three-player minmax game such that both the Faster-RCNN and the LostGAN can benefit each other under a semi-supervised learning settings. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparisons with the GauGAN</head><p>We conduct experiments for the methods presented in Section 3.4. <ref type="table" target="#tab_11">Table 6</ref> shows the comparisons in terms of IS, FID and DS. First, with the post-hoc integration, the GauGAN obtains slightly better IS and FID than our LostGAN-V2, while our LostGAN-V2 achieves better DS. <ref type="figure" target="#fig_0">Fig. 14</ref> shows some examples, from which we can see the generator in our LostGAN-V2 works reasonably good, comparing to the GauGAN that are trained with ground-truth masks. Second, for the end-to-end integration between the layoutto-mask component and the mask refinement component in our LostGAN-V2 and the SPADE in the GauGAN, our LostGAN-V2 obtains slightly better performance with one possible explanation stated in Section 3.4. Third, the vanilla GauGAN that uses ground-truth semantic masks in both training and testing obtains significantly better FID. The results further verify the importance of object masks in learning layout-to-image generator models, and show the effectiveness of the proposed layout-to-mask module in our LostGANs. Along with our layout semi-supervised Lost-GAN results, developing better layout-to-mask modules will be one of the main directions to be addressed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Effects of different components in LostGANs</head><p>We test the effects of four different components in our LostGAN-V2. Two components in the generator network G: the layout-to-mask component and the mask refinement component. Two components in the discriminator network D: the image head classifier and the object head classifier. Due to the computational budget requirement, we do not perform combinatorial ablation studies between the four components.   <ref type="table" target="#tab_12">Table 7</ref> shows the comparisons between the four components individually. The comparisons are done at the resolution of 128×128. Overall, each of the four components has a significant effect on the synthesis results, which supports the proposed design of our LostGANs-V2. The component that affects the results most is the object head classifier in the discriminator network. This is aligned with the GAN setting: the discriminator network provides learning signals to the generator network, and for layout-to-image synthesis the object level learning signals are of the most importance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">The iterative mask refinement component</head><p>We conduct an ablation study on the iterative mask generation component <ref type="figure" target="#fig_0">(Fig. 13</ref>) to investigate their effects. After training, we compare the performance of different models with some of mask refinement stages removed. As shown in <ref type="table" target="#tab_13">Table 8</ref>, the last row shows the full model with all the mask components, m 0 · · · m 5 . In a backward way, if we remove the mask refinement stage by stage in the generator, the performance (Inception Score and FID) are indeed negatively affected. However, if we remove all the mask refinement stages and only use the initial masks, the performance is better than the model with mask refinement in the first stage, m 0 + m 1 . One potential reason is that the resolution of first stage is very low, from which the learned masks may overlook objects of small sizes and introduce artifacts in the predicted masks. After observing this in the ablation study, we re-trained a model without using m 1 in COCO-Stuff and did not observe performance improvement, so we did not re-train all the models used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS AND DISCUSSIONS</head><p>This paper studies the generative learning problem of layout-to-image with a focus on controllable image synthesis from reconfigurable structured layouts and styles. This paper first presents an intuitive pipeline of learning layout-to-mask-to-image. Then, it presents a layout-and style-based architecture for generative adversarial networks (termed LostGANs). The proposed LostGAN can be trained end-to-end to generate images from reconfigurable layout and style with strong style and layout controllability at both image and object levels. Our proposed LostGAN also can learn fine-grained object masks in a weakly-supervised manner to bridge the gap between layouts and images by a novel object instance-sensitive layout-aware feature normalization (ISLA-Norm) scheme. State-of-the-art performance is obtained in the COCO-Stuff and Visual Genome datasets. Discussions. The generative learning problem of layoutto-image synthesis is still at a early stage of development in terms of synthesizing high-fidelity images, compared to the results of BigGANs <ref type="bibr" target="#b4">[5]</ref> in ImageNet and StyleGANs <ref type="bibr" target="#b7">[8]</ref> for faces. Overall, we can observe the quality of image generation from layout is still not sufficiently good, especially for articulated objects (such as people) and fine-grained object-object interactions at high resolution (e.g., examples in <ref type="figure" target="#fig_0">Fig. 15</ref>). In <ref type="figure" target="#fig_0">Fig. 15</ref>, we observe that our proposed model is not capable of capturing interactions between person and small objects, e.g., person and tennis racket in the middle column. From the learned label maps, we also can see why the model can not synthesize visually good images. We leave this to our future work by investigating methods of learning fine-grained part-level masks.</p><p>In the meanwhile, we also note that the differences between the goals of BigGANs and StyleGANs and those of controllable layout-to-image synthesis are non-trivial. For example, we can use a trained BigGAN to generate cat images, and as long as the generated images look realistic and sharp with one or more than one cats, we shall think it does a great job (without requiring how many cats should appear and where they should be). Similarly, we can train a StyleGAN to generate face images, and we shall be happy if realistic and sharp face images are generated with a natural style (e.g., smiling or sad). Controllable layout-toimage synthesis has more fine-grained requirements, which is relatively more challenging similar in spirit to the classic constraint-satisfaction problems in AI. Those being said, based on the promising results of GauGANs <ref type="bibr" target="#b14">[15]</ref> using annotated semantic maps in image synthesis, we think the proposed layout-to-mask-to-image pipeline and Lost-GANs worth further explorations of seeking more powerful weakly-supervised learning of layout-to-mask. For example, we can develop more sophisticated mask generator networks and the "ToMask" modules in <ref type="figure">Fig. 3</ref>. We also should explore different consistency constraints between the "ToMask" modules along the layers of the generator network, similar to the recently proposed PointRend method for improving Mask-RCNNs <ref type="bibr" target="#b63">[64]</ref>. In addition to improve the layout-to-mask generation, it is an important direction of designing better discriminator networks that provide better fine-grained supervision signals for the generator network. We leave those for the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:2003.11571v2 [cs.CV] 26 Mar 2021 Illustration of controllable image synthesis from reconfigurable spatial layouts and style codes in the COCO-Stuff dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>α being a learnable weight to balance the two masks, and M c,h,w = m i=1 M(i, c, h, w) if the pixel (h, w) is occupied by multiple object bounding boxes, otherwise M c,h,w = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Some selected examples of synthesized images at the resolution of 512 × 512 in COCO-Stuff by our LostGAN-V2. The last two rows show the nearest neighbors of the synthesized images by our LostGAN-V2 in the CoCo-Stuff training dataset using the AlexNet-pool5 feature [62] and the GSC metric [50] (Sec. 3.1.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Examples of learned masks and their nearest neighbors in the ground-truth masks in the COCO-Stuff training dataset: truck, airplane, hydrant and person (from top to bottom). (a) Masks learned by our LostGAN-V2 256 × 256 and (b-k) top-10 nearest neighbors. All masks are cropped and resized to the resolution of 32 × 32. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Layout Control in our LostGAN-V2: image synthesis results by adding new objects, changing the spatial position, the size, the aspect ratio or the category label of a bounding box in a layout. Best viewed in magnification and color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 Fig. 12 . 1 obj i and z 2</head><label>111212</label><figDesc>shows examples of learned masks, in which even for complex scene with multiple overlapping objects, synthesized images and learned masks are consistent and semantically reasonable. Compared to the input bounding boxes, the learned masks help reduce the semantic gap in layout-to-image. Those masks are learned jointly with Fine-grained object-level style control in our LostGAN-V2:We use linear interpolation of object instance style codes, z obj i . The objects-of-interest are grass, sky, bus and giraffe respectively. For each layout, we first generate two images in (b) and (g) using two different z obj i samples, zobj i , while (z img , Z obj \ {z obj i }) are kept the same. Then, we synthesize 4 images in from (c) to (f) using object style codes linearly interpolated from z 1 obj i and z 2 obj i . See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Examples of mask refinement in the generator. (a) Layouts, (b) Initial masks generated form the joint label and style encoding, (c-f) Mask refinement using masks generated from feature maps at different stages in the generator, (h) Synthesized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Comparison of our method and GauGAN [15] at the resolution of 256 × 256. (a) Input Layouts, (b) Masks learned by our model, (c) Synthesized images by GauGAN using the masks in (b), (d) Generated images by our LostGAN-V2, (e) Ground Truth images. Our model achieves comparable visual performance with GauGAN, which is trained with supervision of ground truth masks. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .</head><label>15</label><figDesc>Examples of failure cases of our LostGAN-V2 256 × 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>), and the other is a simple "ToMask" operation with the output tensor of the size m × s × s, representing a s × s mask for each of the m object instances. This canonical size of object masks enable our model to handle aspect ratio changes of bounding boxes in image synthesis. Their detailed specifications are referred our Github repository.</figDesc><table /><note>• After resizing the generated m object instance mask and placing them back into the layout at the spatial resolution (H, W ) of a ResBlock stage, we obtain a mask tensor of dimensions (m, H, W ), denoted by M S , each slice of which has zeros outside the corresponding bounding box, bbox i . For the visualization purpose (e.g., those shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table cup</head><label>cup</label><figDesc>Synthesized images from given layouts in COCO-Stuff by different models. From the top to the bottom: Input layout, Ground-truth image, Layout2Im [20] 64 × 64, our LostGAN-V1 [25] 128 × 128, Grid2Im [19] 256 × 256, our LostGAN-V2 256 × 256, and LostGAN+SPADE<ref type="bibr" target="#b14">[15]</ref> (end-to-end integration, the third method in Section 3.4) 256 × 256.</figDesc><table><row><cell></cell><cell>furniture</cell><cell>bottle</cell></row><row><cell>Input</cell><cell>fork</cell></row><row><cell></cell><cell cols="2">piazza</cell></row><row><cell>Ground-Truth</cell><cell></cell></row><row><cell>Layout2Im</cell><cell></cell></row><row><cell>LostGAN-V1</cell><cell></cell></row><row><cell>Grid2Im</cell><cell></cell></row><row><cell>LostGAN-V2</cell><cell></cell></row><row><cell>LostGAN+SPADE</cell><cell></cell></row><row><cell>Fig. 5.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table wall</head><label>wall</label><figDesc></figDesc><table><row><cell>Input Layout</cell><cell>shirt</cell><cell>water</cell><cell>building building car</cell><cell>sky</cell><cell>person</cell><cell>cloud</cell><cell>tree tree pant</cell><cell>jean picture wall picture shirt</cell><cell>arm tree shirt man</cell><cell>arm head field shirt</cell></row><row><cell>street</cell><cell>sand</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>snow</cell><cell></cell><cell></cell><cell></cell><cell>man</cell></row><row><cell>Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layout2Im</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LostGAN-v1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LostGAN-v2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AlexNet-Nearest</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSC-Nearest</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 6. Synthesized images in VG by different models: Layout2Im [20] 64 × 64, our LostGAN-V1 128 × 128, our LostGAN-V2 256 × 256. The last two rows show the nearest neighbors of the synthesized images by our LostGAN-V2 in the VG training dataset using the AlexNet-pool5 feature [62] and the GSC metric [50] (Sec. 3.1.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2</head><label>2</label><figDesc>Comparisons of the CAS. See text for details.</figDesc><table><row><cell>Methods</cell><cell cols="2">CAS↑ COCO</cell><cell>VG</cell></row><row><cell>Layout2Im [20] 64x64</cell><cell>27.32</cell><cell cols="2">23.25</cell></row><row><cell>Our LostGAN-V1 [25] 64x64</cell><cell>28.81</cell><cell cols="2">27.50</cell></row><row><cell>Grid2Im [19] 128x128</cell><cell>25.89</cell><cell></cell><cell>-</cell></row><row><cell>Our LostGAN-V1 [25] 128x128</cell><cell>30.68</cell><cell cols="2">28.85</cell></row><row><cell>Our LostGAN-V2 128x128</cell><cell>31.98</cell><cell cols="2">29.35</cell></row><row><cell>Grid2Im [19] 256x256</cell><cell>20.54</cell><cell></cell><cell>-</cell></row><row><cell>Our LostGAN-V2 256x256</cell><cell>30.33</cell><cell cols="2">28.81</cell></row><row><cell>Real Images</cell><cell>51.04</cell><cell cols="2">48.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3</head><label>3</label><figDesc>Comparisons of the detection AP (DAP↑). See text for details.</figDesc><table><row><cell>Testing</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell></row><row><cell>Gird2Im [19] 256x256</cell><cell>32.3</cell><cell>58.5</cell><cell>30.9</cell></row><row><cell>Our LostGAN-V2 256x256</cell><cell>34.9</cell><cell>60.8</cell><cell>34.4</cell></row><row><cell>Real Images</cell><cell>44.7</cell><cell>69.5</cell><cell>47.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 mIoU</head><label>4</label><figDesc>between masks and their nearest neighbor in ground truth masks.</figDesc><table><row><cell>person</cell><cell>car</cell><cell>plane</cell><cell>bus</cell><cell>train</cell><cell>truck</cell><cell>boat</cell></row><row><cell>53.8</cell><cell>66.5</cell><cell>58.0</cell><cell>75.0</cell><cell>70.8</cell><cell>66.1</cell><cell>63.1</cell></row><row><cell>zebra</cell><cell>hydrant</cell><cell>pizza</cell><cell>elephant</cell><cell>laptop</cell><cell>bench</cell><cell>mean</cell></row><row><cell>66.9</cell><cell>59.2</cell><cell>77.7</cell><cell>62.3</cell><cell>57.0</cell><cell>62.8</cell><cell>56.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Fig. 10. Style Control w.r.t. (z img , Z obj ) in our LostGAN-V2 256 × 256: multiple images synthesized using the same layout with different styles, (z img , Z obj )'s. We also show the nearest neighbors of the synthesized images by our LostGAN-V2 in the CoCo-Stuff training dataset using the AlexNet-pool5 feature<ref type="bibr" target="#b61">[62]</ref> and the GSC metric<ref type="bibr" target="#b49">[50]</ref> (Sec. 3.1.3). (a) Layout and GT real image, (b) the GSC-Nearest based on the input layout, and (c) Synthesized images by our LostGAN-V2 256 × 256 and their AlexNet-Nearest neighbors.</figDesc><table><row><cell>sky</cell><cell></cell><cell cols="3">mountain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hill</cell><cell></cell><cell></cell><cell></cell><cell>tree</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">grass</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">monitor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">keyboard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">table</cell><cell cols="3">mouse</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tree</cell><cell></cell><cell></cell><cell></cell><cell>sky</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">giraffe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>bush</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">grass</cell><cell>wood</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">bottle</cell><cell cols="2">bottle</cell><cell>table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>food</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">piazza</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fog</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">building</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">car tree</cell><cell>car</cell><cell cols="2">fence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>road</cell><cell></cell><cell cols="2">car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tree</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">person</cell><cell cols="3">person</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">person</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">snow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">building</cell><cell>metal</cell><cell></cell><cell>tree</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sky</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>road</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Layout</cell><cell></cell><cell>Real Image</cell><cell>GSC-Nearest</cell><cell>Synthesized</cell><cell cols="2">AlexNet-Nearest</cell><cell cols="2">Synthesized</cell><cell>AlexNet-Nearest</cell><cell>Synthesized</cell><cell>AlexNet-Nearest</cell><cell>Synthesized</cell><cell>AlexNet-Nearest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sky</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hill</cell><cell cols="2">tree</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">sand</cell><cell></cell><cell></cell><cell>sea</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tree</cell><cell></cell><cell></cell><cell></cell><cell>sky</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">building</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">bus road</cell><cell cols="2">bus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Layout</cell><cell>GT image</cell><cell>Synthesized mask</cell><cell cols="2">Synthesized image</cell><cell cols="2">Synthesized mask</cell><cell cols="2">Synthesized image</cell><cell>Synthesized mask</cell><cell>Synthesized image</cell><cell>Synthesized mask</cell><cell>Synthesized image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5</head><label>5</label><figDesc>Comparisons of semi-supervised LostGAN-V2 128× 128. See text for details.</figDesc><table><row><cell>Setting</cell><cell>IS↑</cell><cell>FID↓</cell><cell>DS↑</cell></row><row><cell>(i) D 1</cell><cell>12.79±0.27</cell><cell>32.77</cell><cell>0.46 ± 0.09</cell></row><row><cell>(ii) D 1 +D 2 : Detection(0.5)</cell><cell>13.41±0.48</cell><cell>28.20</cell><cell>0.46 ± 0.09</cell></row><row><cell>(iii) D 1 +D 2 : Detection(0.5)</cell><cell>13.90±0.38</cell><cell>25.87</cell><cell>0.46 ± 0.09</cell></row><row><cell>+ Eqn. 15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fully-supervised</cell><cell>14.21±0.40</cell><cell>24.76</cell><cell>0.45 ± 0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6</head><label>6</label><figDesc>Quantitative comparisons using the Inception Score (IS, higher is better), the FID (lower is better) and Diversity Score (DS, higher is better) evaluation on COCO-Stuff dataset at the resolution of 256 × 256. See text for details.</figDesc><table><row><cell>Methods</cell><cell>IS↑</cell><cell>FID↓</cell><cell>DS↑</cell></row><row><cell>GauGAN (w/ GT masks) [15]</cell><cell>-</cell><cell>22.6</cell><cell>-</cell></row><row><cell>GauGAN [15] + Our Inferred</cell><cell>19.35 ± 0.73</cell><cell>41.11</cell><cell>0.38 ± 0.12</cell></row><row><cell>Masks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our LostGAN-V2</cell><cell>18.01±0.50</cell><cell>42.55</cell><cell>0.55±0.09</cell></row><row><cell>LostGAN-V2 + SPADE [15]</cell><cell>16.37 ± 0.34</cell><cell>46.08</cell><cell>0.50 ± 0.11</cell></row><row><cell>(end-to-end)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7</head><label>7</label><figDesc>Effects of four components in LostGAN-V2 128× 128. See text for details.</figDesc><table><row><cell>Components</cell><cell>IS↑</cell><cell>FID↓</cell><cell>DS↑</cell></row><row><cell>w/o layout-to-mask in G</cell><cell>12.91±0.23</cell><cell>28.51</cell><cell>0.45±0.09</cell></row><row><cell>w/o mask refinement in G</cell><cell>13.75±0.49</cell><cell>26.11</cell><cell>0.46±0.09</cell></row><row><cell>w/o image head in D</cell><cell>13.85±0.31</cell><cell>25.96</cell><cell>0.43±0.10</cell></row><row><cell>w/o object head in D</cell><cell>9.51±0.22</cell><cell>57.06</cell><cell>0.55±0.11</cell></row><row><cell>Full LostGAN-V2</cell><cell>14.21±0.40</cell><cell>24.76</cell><cell>0.45±0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 8</head><label>8</label><figDesc>Effects of the mask refinement in our LostGAN-V2 256 × 256 in COCO-Stuff. m 0 represents initial masks generated from the joint label and style encoding. m i represents the refined masks at the i-th stage of the generator. See text for details. 16.68 ± 0.42 48.54 m 0 + m 1 14.14 ± 0.33 63.96 m 0 ... m 2 17.10 ± 0.56 48.94 m 0 ... m 3 17.46 ± 0.34 44.38 m 0 ... m 4 17.51 ± 0.41 42.49 m 0 ... m 5 18.01 ± 0.50 42.55</figDesc><table><row><cell>Mask branch</cell><cell>IS↑</cell><cell>FID↓</cell></row><row><cell>m 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by NSF IIS-1909644 and ARO Grant W911NF1810295. The views presented in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tianfu Wu is an assistant professor in the Department of Electrical and Computer Engineering at NC state university (NCSU). He received his Ph.D. in statistics from UCLA under the supervision by Prof. Song-Chun Zhu. His research focuses on computer vision, often motivated by the task of building explainable and improvable visual Turing test and robot autonomy through life-long communicative learning. To accomplish his research goals, he is interested in pursuing a unified framework for machines to ALTER (Ask, Learn, Test, Explain and Refine) recursively in a principled way.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations(ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Specifying object attributes and relations in interactive scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Layout2image: Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image synthesis from reconfigurable layout and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">540</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Predicting structured data</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkxzx0NtDB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5523" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Geometric gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2nd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2352" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Divergence triangle for joint training of generator model, energy-based model, and inferential model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8670" to="8679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">560</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pastegan: A semi-parametric method to generate image from scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yikang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3950" to="3960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations(ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generating multiple objects at spatially distinct locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shapes and context: Inthe-wild image synthesis &amp; manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2317" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Semantic object accuracy for generative text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13321</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations(ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations(ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The fréchet distance between multivariate normal distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multivariate analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="450" to="455" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
