<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TediGAN: Text-Guided Diverse Face Image Generation and Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
							<email>weihaox@outlook.comyang.yujiu@sz.tsinghua.edu.cnjinghao.xue@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistical Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
							<email>wubaoyuan@cuhk.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Chinese University of Hongkong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Secure Computing Lab of Big Data</orgName>
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TediGAN: Text-Guided Diverse Face Image Generation and Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity learns the text-image matching by mapping the image and text into a common embedding space. The instancelevel optimization is for identity preservation in manipulation. Our model can produce diverse and high-quality images with an unprecedented resolution at 1024 2 . Using a control mechanism based on style-mixing, our Tedi-GAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or without instance guidance. To facilitate text-guided multimodal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>How to create or edit an image of the desired content without tedious manual operations is a difficult but mean- He is wearing heavy makeup.</p><p>This young man has red hair.</p><p>This woman is smiling. She has short black hair. He is a young man. He is old. He is wearing eyeglasses. <ref type="figure">Figure 1</ref>. Our TediGAN is the first method that unifies text-guided image generation and manipulation into one same framework, leading to naturally continuous operations from generation to manipulation (a), and inherently supports image synthesis with multimodal inputs (b), such as sketches or semantic labels with or without instance (texts or real images) guidance. ingful task. To make image generation and manipulation more readily and user-friendly, recent studies have been focusing on the image synthesis conditioned on a variety of guidance, such as sketch <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">37]</ref>, semantic label <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">36]</ref>, or textual description <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">39]</ref>. Despite the success of its label and sketch counterparts, most state-of-the-art text-guided image generation and manipulation methods are only able to produce low-quality images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8]</ref>. Those aiming at generating high-quality images from texts typically design a multi-stage architecture and train their models in a progressive manner. To be more specific, there are usually three stages in the main module, and each stage contains a generator and a discriminator. Three stages are trained at the same time, and progressively generate images of three different scales, i.e., 64 2 → 128 2 → 256 2 . The initial image with rough shape and color would be refined to a high-resolution one. However, the multi-stage training process is time-consuming and cumbersome, making the afore-w W latent space of StyleGAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>She wears lipstick. (c) High-Resolution Generation (b) Multi-Modal Synthesis (a) Continuous Operations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN inversion</head><p>He is a young man with short black hair.</p><formula xml:id="formula_0">w v w l Visual-linguistic similarity w v</formula><p>He is a young man. <ref type="figure">Figure 2</ref>. Projecting Multi-Modal Embedding into the W Space of StyleGAN. Taking visual and linguistic embedding for example, the left illustrates visual-linguistic similarity learning, where the visual embedding w v and linguistic embedding w l are expected to be close enough. The right demonstrates text-guided image manipulation. Given a source image and a text guidance, we first get their embedding w v and w l in W space through corresponding encoders. We then perform style mixing for target layers and get the target latent code w t . The final w t * is obtained through instance-level optimization. The edited image can be generated from the StyleGAN generator. mentioned methods unfeasible for higher resolution. Furthermore, the pretrained text-image matching model they used fails to exploit attribute-level cross-modal information and leads to mismatched attributes when generating images from texts <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b4">5]</ref>, or undesired changes of irrelevant attributes when manipulating images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><formula xml:id="formula_1">… w 1 w L w l … w 1 w L mixing … source text guidance edited w t w t w t*</formula><p>Recent progress on generative adversarial networks (GANs) has established an entirely different image generation paradigm that achieves phenomenal quality, fidelity, and realism. StyleGAN <ref type="bibr" target="#b15">[16]</ref>, one of the most notable GAN frameworks, introduces a novel style-based generator architecture and can produce high-resolution images with unmatched photorealism. Some recent work <ref type="bibr" target="#b15">[16]</ref> has demonstrated that the intermediate latent space W of StyleGAN, inducted from a learned piece-wise continuous mapping, yields less entangled representations and offers more feasible manipulation. The superior characteristics of W space appeal to numerous researchers to develop advanced GAN inversion techniques <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> to invert real images back into the StyleGAN's latent space and perform meaningful manipulation. The most popular way <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b28">29]</ref> is to train an additional encoder to map real images into the W space, which leads to not only faithful reconstruction but also semantically meaningful editing. Furthermore, it is easy to introduce the hierarchically semantic property of the W space to any GAN model by simply learning an extra mapping network before a fixed, pretrained StyleGAN generator. We thoroughly investigated the existing GAN inversion methods, and found all is about how to map images into the latent space of a well-trained GAN model. The other modalities like texts, however, have not received any attention.</p><p>In this paper, for the first time, we propose a GAN inversion technique that can map multi-modal information, e.g., texts, sketches, or labels, into a common latent space of a pretrained StyleGAN. Based on that, we propose a very simple yet effective method for Text-guided diverse image generation and manipulation via GAN (abbreviated Tedi-GAN). Our proposed method introduces three novel modules. The first StyleGAN inversion module learns the inversion where an image encoder can map a real image to the W space, while the second visual-linguistic similarity module learns linguistic representations that are consistent with the visual representations by projecting both into a common W space, as shown in <ref type="figure">Figure 2</ref>. The third instance-level optimization module is to preserve the identity after editing, which can precisely manipulate the desired attributes consistent with the texts while faithfully reconstructing the unconcerned ones. Our proposed method can generate diverse and high-quality results with a resolution up to 1024 2 , and inherently support image synthesis with multi-modal inputs, such as sketches or semantic labels with or without instance (texts or real images) guidance. Due to the utilization of a pretrained StyleGAN model, our method can provide the lowest effect guarantee, i.e., our method can always produce pleasing results no matter how uncommon the given text or image is. Furthermore, to fill the gaps in the text-to-image synthesis dataset for faces, we create the Multi-Modal CelebA-HQ dataset to facilitate the research community. Following the format of the two popular textto-image synthesis datasets, i.e., CUB <ref type="bibr" target="#b33">[34]</ref> for birds and COCO <ref type="bibr" target="#b21">[22]</ref> for natural scenes, we create ten unique descriptions for each image in the CelebA-HQ <ref type="bibr" target="#b14">[15]</ref>. Besides real faces and textual descriptions, the introduced dataset also contains the label map and sketch for the text-guided generation with multi-modal inputs.</p><p>In summary, this work has the following contributions:</p><p>• We propose a unified framework that can generate diverse images given the same input text, or text with image for manipulation, allowing the user to edit the appearance of different attributes interactively.</p><p>• We propose a GAN inversion technique that can map multi-modal information into a common latent space of a pretrained StyleGAN where the instance-level image-text alignment can be learned.</p><p>• We introduce the Multi-Modal CelebA-HQ dataset, consisting of multi-modal face images and corresponding textual descriptions, to facilitate the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text-to-Image Generation. There are basically two categories of GAN-based text-to-image generation methods.</p><p>a smiling young woman with short blonde hair he is young and wears beard a young woman with long black hair <ref type="figure">Figure 3</ref>. Diverse High-Resolution Results from Text. Our method can achieve text-guided diverse image generation and manipulation up to an unprecedented resolution at 1024 2 .</p><p>The first category produces images from texts directly by one generator and one discriminator. For example, Reed et al. <ref type="bibr" target="#b27">[28]</ref> propose to use conditional GANs to generate plausible images from given text descriptions. Tao et al. <ref type="bibr" target="#b31">[32]</ref> propose a simplified backbone that generates high-resolution images directly by Wasserstein distance and fuses the text information into visual feature maps to improve the image quality and text-image consistency. Despite the plainness and conciseness, the one-stage models produce dissatisfied results in terms of both photo-realism and text-relevance in some cases. Thus, another thread of research focuses on multi-stage processing. Zhang et al. <ref type="bibr" target="#b42">[42]</ref> stack two GANs to generate high-resolution images from text descriptions through a sketch-refinement process. They further propose a three-stage architecture <ref type="bibr" target="#b43">[43]</ref> that stacks multiple generators and discriminators, where multi-scale images are generated progressively in a course-to-fine manner. Xu et al. <ref type="bibr" target="#b39">[39]</ref> improve the work of <ref type="bibr" target="#b43">[43]</ref> from two aspects. First, they introduce attention mechanisms to explore fine-grained text and image representations. Second, they propose a Deep Attentional Multimodal Similarity Model (DAMSM) to compute the similarity between the generated image and the sentence. The subsequent studies basically follow the framework of <ref type="bibr" target="#b39">[39]</ref> and have proposed several variants by introducing different mechanisms like attention <ref type="bibr" target="#b18">[19]</ref> or memory writing gate <ref type="bibr" target="#b47">[47]</ref>. However, the multi-stage frameworks produce results that look like a simple combination of visual attributes from different image scales.</p><p>Text-Guided Image Manipulation. Similar to text-toimage generation, manipulating given images using texts also produces results that contain desired visual attributes. Differently, the modified results should only change certain parts and preserve text-irrelevant contents of the original images. For example, Dong et al. <ref type="bibr" target="#b7">[8]</ref> propose an encoderdecoder architecture to modify an image according to a given text. Nam et al. <ref type="bibr" target="#b26">[27]</ref> disentangle different visual attributes by introducing a text-adaptive discriminator, which can provide finer training feedback to the generator. Li et al. <ref type="bibr" target="#b19">[20]</ref> introduce a multi-stage network with a novel text-image combination module to produce high-quality results. Similar to text-to-image generation, the text-based image manipulation methods with the best performance are basically based on the multi-stage framework. Different from all existing methods, we propose a novel framework that unifies text-guided image generation and manipulation methods and can generate high-resolution and diverse images directly without multi-stage processing.</p><p>Image-Text Matching. One key of text-guided image generation or manipulation is to match visual attributes with corresponding words. To do this, current methods usually provide explicit word-level training feedback from the elaborately-designed discriminator <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. There is also a rich line of work proposed to address a related direction named image-text matching, or visual-semantic alignment, aiming at exploiting the matching relationships and making the corresponding alignments between text and image. Most of them can be categorized into two-branch deep architecture according to the granularity of representations for both modalities, i.e., global <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> or local <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref> representations. The first category employs deep neural networks to extract the global features of both modalities, based on which their similarities are measured <ref type="bibr" target="#b23">[24]</ref>. Another thread of work performs instance-level image-text matching <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>, learning the correspondences between words and image regions <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The TediGAN Framework</head><p>We first learn the inversion, i.e., training an image encoder to map the real images to the latent space such that all codes produced by the encoder can be recovered at both the pixel-level and the semantic-level. We then use the hierarchical characteristic of W space to learn the text-image matching by mapping the image and text into the same joint embedding space. To preserve identity in manipulation, we propose an instance-level optimization, involving the trained encoder as a regularization to better reconstruct the pixel values without affecting the semantic property of the inverted code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">StyleGAN Inversion Module</head><p>The inversion module aims at training an image encoder that can map a real face image to the latent space of a fixed</p><formula xml:id="formula_2">(a) AttnGAN (b) ControlGAN (c) DM-GAN (d) DF-GAN (e) Ours</formula><p>This man has bags under eyes and big nose. He has no beard.</p><p>This woman is young and has blond hair.</p><p>She has wavy hair, high cheekbones, and oval face. She is wearing lipstick. StyleGAN model pretrained on the FFHQ dataset <ref type="bibr" target="#b15">[16]</ref>. The reason we invert a trained GAN model instead of training one from scratch is that, in this way, we can go beyond the limitations of a paired text-image dataset. The Style-GAN is trained in an unsupervised setting and covers much higher quality and wider diversity, which makes our method able to produce satisfactory edited results with images in the wild. In order to facilitate subsequent alignment with text attributes, our goal for inversion is not only to reconstruct the input image by pixel values but also to acquire the inverted code that is semantically meaningful and interpretable <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">40]</ref>. Before introducing our method, we first briefly establish problem settings and notations. A GAN model typically consists of a generator G(·) : Z → X to synthesize fake images and a discriminator D(·) to distinguish real data from the synthesized. In contrast, GAN inversion studies the reverse mapping, which is to find the best latent code z * by inverting a given image x to the latent space of a well-trained GAN. A popular solution is to train an additional encoder E v (·) : X → Z <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b2">3]</ref> (subscript v means visual). To be specific, a collection of latent codes z s are first randomly sampled from a prior distribution, e.g., normal distribution, and fed into G(·) to get the synthesis x s as the training pairs. The introduced encoder E v (·) takes x s and z s as inputs and supervisions respectively and is trained with</p><formula xml:id="formula_3">min Θ Ev L Ev = ||z s − E v (G(z s ))|| 2 2 ,<label>(1)</label></formula><p>where || · || 2 denotes the l 2 distance and Θ Ev represents the parameters of the encoder E v (·).</p><p>Despite of its fast inference, the aforementioned procedure simply learns a deterministic model with no regard to whether the codes produced by the encoder align with the semantic knowledge learned by G(·). The supervision by only reconstructing z s is not powerful enough to train E v (·), and G(·) is actually not fully used to guide the training of E v (·), leading to the incapability of inverting real images. To solve these problems, we use a totally different strategy to train an encoder for GAN inversion as in <ref type="bibr" target="#b45">[45]</ref>. There are two main differences compared with the conventional framework: (a) the encoder is trained with real images rather than with synthesized images, making it more applicable to real applications; (b) the reconstruction is at the image space instead of latent space, which provides semantic knowledge and accurate supervision and allows integration of powerful image generation losses such as perceptual loss <ref type="bibr" target="#b11">[12]</ref> and LPIPS <ref type="bibr" target="#b44">[44]</ref>. Hence, the training process can be formulated as</p><formula xml:id="formula_4">min Θ Ev LE v = ||x − G(Ev(x))|| 2 2 +λ1||F (x)−F (G(Ev(x)))|| 2 2 −λ2E[Dv(G(Ev(x)))],<label>(2)</label></formula><formula xml:id="formula_5">min Θ Dv LD v = E[Dv(G(Ev(x)))]−E[Dv(x)]+ λ3 2 E[||∇xDv(x)|| 2 2 ],<label>(3)</label></formula><p>where Θ Ev and Θ Dv are learnable parameters, λ 1 , λ 2 , and λ 3 are the hyper-parameters, and F (·) denotes the VGG feature extraction model.</p><p>Through the learned image encoder, we can map a real image into the W space. The obtained code is guaranteed to align with the semantic domain of the StyleGAN generator and can be further utilized to mine cross-modal similarity between the image-text instance pairs. He has brown hair.</p><p>He has no beard.</p><p>This woman wears earrings. She has oval face and high bones. She is smiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>She wears eyeglasses.</head><p>She has oval face and long black hair. She is wearing earrings.</p><p>This old woman has big lips, pale skin, and gray hair. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual-Linguistic Similarity Learning</head><p>Once the inversion module is trained, given a real image, we can map it into the W space of StyleGAN. The next problem is how to train a text encoder that learns the associations and alignments between image and text. Instead of training a text encoder in the same way as the image encoder or the aforementioned DAMSM, we propose a visual-linguistic similarity module to project the image and text into a common embedding space, i.e., the W space, as shown in <ref type="figure">Figure 2</ref>. Given a real image and its descriptions, we encode them into the W space by using the previously trained image encoder and a text encoder. The obtained latent code is the concatenation of L different C-dimensional w vectors, one for each input layer of StyleGAN. The multimodal alignment can be trained with</p><formula xml:id="formula_6">min Θ E l L E l = || L i=1 p i (w v i − w l i )|| 2 2 ,<label>(4)</label></formula><p>where Θ E l represents the parameters of the text encoder E l (·) and subscript l means linguistic; w v , w l ∈ W L×C are the obtained image embedding and text embedding;</p><formula xml:id="formula_7">w v = f (E v (x))</formula><p>is the projected code of the image embedding z in the input latent space Z using a non-linear mapping network f : Z → W; w l shares a similar definition; w v and w l are with the same shape L × C, meaning to have L layers and each with a C-dimensional latent code; and p i is the weight of i-th layer in the latent code. Compared with DAMSM, our proposed module is lightweight and easy to train. More importantly, this module achieves instance-level alignment <ref type="bibr" target="#b35">[35]</ref>, i.e., learning correspondences between visual and linguistic attributes, by leveraging the disentanglability of StyleGAN. The text encoder is trained with the proposed visual-linguistic similarity loss together with the pairwise ranking loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8]</ref>, which is omitted from Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance-Level Optimization</head><p>One of the main challenges of face manipulation is the identity preservation. Due to the limited representation capability, learning a perfect reverse mapping with an encoder alone is not easy. To preserve identity, some recent methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref> incorporate a dedicated face recognition loss <ref type="bibr" target="#b6">[7]</ref> to measure the cosine similarity between the output image and its source. Different from their methods, for text-guided image manipulation, we implement an instancelevel optimization module to precisely manipulate the desired attributes consistent with the descriptions while faithfully reconstructing the unconcerned ones. We use the inverted latent code z as the initialization, and the image encoder is included as a regularization to preserve the latent code within the semantic domain of the generator. To summarize, the objective function for optimization is</p><formula xml:id="formula_8">z * = arg min z ||x − G(z)|| 2 2 + λ 1 ||F (x) − F (G(z))|| 2 2 + λ 2 ||z − E v (G(z))|| 2 2 ,<label>(5)</label></formula><p>where x is the original image to manipulate, λ 1 and λ 2 are the loss weights corresponding to the perceptual loss and the encoder regularization term, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Control Mechanism</head><p>Attribute-Specific Selection. The two different tasks, i.e., text-to-image generation and text-guide image manipu-  lation, are unified into one framework by our proposed control mechanism. Our mechanism is based on the style mixing of StyleGAN. The layer-wise representation of Style-GAN learns disentanglement of semantic fragments (attributes or objects). In general, different layer w i represents different attributes and is fed into the i-th layer of the generator. Changing the value of a certain layer would change the corresponding attributes of the image. As shown in <ref type="figure">Figure 2</ref>, given two codes with the same size w c , w s ∈ W L×C denoting content code and style code, this control mechanism selects attribute-specific layers and mixes those layers of w s by partially replacing corresponding layers of w c . For text-to-image generation, the produced images should be consistent with the textual description, thus w c should be the linguistic code, and randomly sampled latent code with the same size acts as w s to provide diversity (results are shown in <ref type="figure">Figure 7)</ref>. For text-guided image manipulation, w c is the visual embedding while w s is the linguistic embedding, the layers for mixing should be relevant to the text, for the purpose of modifying the relevant attributes only and keeping the unrelated ones unchanged.</p><p>Supported Modality. The style code w s and content code w c could be sketch, label, image, and noise, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>, which makes our TediGAN feasible for multimodal image synthesis. The control mechanism provides high accessibility, diversity, controllability, and accurateness for image generation and manipulation. Due to the control mechanism, as shown in <ref type="figure">Figure 1</ref>, our method inher-ently supports continuous operations and multi-modal synthesis for sketches and semantic labels with descriptions. To produce the diverse results, all we need to do is to keep the layers related to the text unchanged and replace the others with the randomly sampled latent code. If we want to generate images from other modality with text guidance, take the sketch as an example, we can train an additional sketch image encoder E vs in the same way as training the real image encoder and leave the other parts unchanged. This woman has black long hair and wears earrings. She is smiling. <ref type="figure">Figure 7</ref>. Diverse Text-to-image Generation. has 30,000 high-resolution face images, each having a highquality segmentation mask, sketch, and descriptive text. We evaluate our proposed method on text and image partitions, comparing with state-of-the-art approaches AttnGAN <ref type="bibr" target="#b39">[39]</ref>, ControlGAN <ref type="bibr" target="#b18">[19]</ref>, DM-GAN <ref type="bibr" target="#b47">[47]</ref>, and DFGAN <ref type="bibr" target="#b31">[32]</ref> for image generation, and comparing with ManiGAN <ref type="bibr" target="#b19">[20]</ref> for image manipulation using natural language descriptions. All methods are retrained with the default settings on the proposed Multi-Modal CelebA-HQ dataset.</p><p>Evaluation Metric. For evaluation, there are four important aspects: image quality, image diversity, accuracy, and realism <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. The quality of generated or manipulated images is evaluated through Fréchet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref>. The diversity is measured by the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b44">[44]</ref>. For image generation, the accuracy is evaluated by the similarity between the text and the corresponding generated image. For manipulation, the accuracy is evaluated by whether the modified visual attributes of the synthetic image are aligned with the given description and text-irrelevant contents are preserved. The accuracy and realism are evaluated through a user study, where the users are asked to judge which one is more photo-realistic, and more coherent with the given texts. We test accuracy and realism by randomly sampling 50 images with the same conditions and collect more than 20 surveys from different people with various backgrounds. In our experiments, we evaluate the FID and LPIPS on a large number of samples generated from randomly selected text descriptions. To evaluate accuracy and realism, we generate images from 50 randomly sampled texts using different methods. In a user study, users are asked to judge which one is the most photorealistic and most coherent with the given texts. The results are demonstrated in <ref type="table" target="#tab_1">Table 2</ref>. Compared with the state-ofthe-arts, our method achieves better FID, LPIPS, accuracy, and realism values, which proves that our methods can generate images with the highest quality, diversity, photorealism, and text-relevance.</p><p>Qualitative Comparison. Most existing text-to-image generation methods, as shown in <ref type="figure" target="#fig_1">Figure 4</ref>, can generate photo-realistic and text-relevant results. However, some attributes contained in the text do not appear in the generated image, and the generated image looks like featureless paint and lacks details. This "featureless painterly" look <ref type="bibr" target="#b15">[16]</ref> would be significantly obvious and irredeemable when generating higher resolution images using the multi-stage training methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">47]</ref>. Furthermore, most existing solutions have limited diversity of the outputs, even if the provided conditions contain different meanings. For example, "has a beard" might mean a goatee, short or long beard, and could have different colors. Our method can not only generate results with diversity but also realise the expectation to change where you want by using the control mechanism.</p><p>To produce diverse results, with the layers related to the text unchanged, the other layers could be replaced by any values sampled from the prior distribution. For example, as shown in the first row of <ref type="figure">Figure 7</ref>, the key visual attributes (women, black long hair, earrings, and smiling) are preserved, while the other attributes, like haircuts, makeups, face shapes, and head poses, show a great degree of diversity. The images in the second row illustrate more precise control ability. We keep the layers representing face shape and head pose the same and change the others. <ref type="figure">Figure 3</ref> shows high-quality and diverse results with resolution at 1024 × 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Text-Guided Image Manipulation</head><p>Quantitative Comparison. In our experiments, we evaluate the FID and conduct a user study on randomly selected images from both CelebA and Non-CelebA datasets with randomly chosen descriptions. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Compared with ManiGAN <ref type="bibr" target="#b19">[20]</ref>, our method achieves better FID, accuracy, and realism. This indicates that our method can produce high-quality synthetic images, and the modifications are highly aligned with the given descriptions, while preserving other text-irrelevant contents.</p><p>Qualitative Comparison. <ref type="figure" target="#fig_2">Figure 5</ref> shows the visual comparisons between the recent method ManiGAN <ref type="bibr" target="#b19">[20]</ref> and ours. As shown, the second row is to add earrings and change the face shape and hair style of the woman, our method completes this difficult case while ManiGAN fails to produce required attributes. ManiGAN produces less satisfactory modified results: in some cases, the text-relevant regions are not modified and the text-irrelevant ones are changed. Furthermore, since the StyleGAN we used is pretrained on a very large face dataset <ref type="bibr" target="#b15">[16]</ref>, the latent space almost covers the full space of facial attributes, which makes our method robust for real images in the wild. The images in last two columns are results of out-of-distribution (Non-CelebA), i.e., images from other face dataset such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">41]</ref>, which illustrate that our method is prepared to produce pleasing results with images in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study and Discussion</head><p>Instance-Level Optimization. The comparison of with or without instance-level optimization is shown in <ref type="figure" target="#fig_5">Figure 8</ref>. As shown, the inversion results of the image encoder preserve all attributes of the original images, which is sufficient for text-to-image generation since there is no identity to preserve <ref type="figure" target="#fig_5">(Figure 8 (c)</ref>). Manipulating a given image according to a text, however, should not change the unrelated attributes especially one's identity, which is preserved after the instance-level optimization <ref type="figure" target="#fig_5">(Figure 8 (d)</ref>).</p><p>We also compare with a recent inversion-based image synthesis method pSp <ref type="bibr" target="#b28">[29]</ref> that incorporates a dedicated recognition loss <ref type="bibr" target="#b6">[7]</ref> during training. Despite both preserving the identity, the optional instance-level optimization provides a non-deterministic way to refine the final results accordingly.</p><p>Visual-Linguistic Similarity. The text encoder is trained using our visual-linguistic similarity and a very simple pairwise ranking loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8]</ref> to align text and image embedding. Although the learned text embedding can handle near-miss cases, as shown in <ref type="figure">Figure 9</ref>, we found this plain strategy sometimes may lead to insufficient disentanglement of attributes and mismatching of image-text alignment, leaving some room for improvement. He has / has no beard. She has brown / black hair. <ref type="figure">Figure 9</ref>. Illustration of Near-miss Cases.</p><p>Potential Issue with StyleGAN. In our experiments, we found that some unrelated attributes are unwantedly changed when we manipulate a given image according to a text description. We thought it might be the problem of visual-linguistic similarity learning in the first place. However, when performing layer-wise style mixing on the inverted codes of two real images, the interference still occurs. This means some facial attributes remain entangled in the W space, where different attributes should be orthogonal (meaning without affecting other attributes). Another inherent defect of StyleGAN is that some attributes, such as hats, necklaces and earrings, are not well represented in its latent space. This makes our method perform less satisfactorily sometimes when adding or removing jewelry or accessories through natural language descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a novel method for image synthesis using textual descriptions, which unifies two different tasks (text-guided image generation and manipulation) into the same framework and achieves high accessibility, diversity, controllability, and accurateness for facial image generation and manipulation. Through the proposed multi-modal GAN inversion and large-scale multi-modal dataset, our method can effectively synthesize images with unprecedented quality. Extensive experimental results demonstrate the superiority of our method, in terms of the effectiveness of image synthesis, the capability of generating high-quality results, and the extendability for multi-modal inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Yujiu Yang is the corresponding author. This research was partially supported by the Key Program of National Natural Science Foundation of China under Grant No. U1903213 and the Guangdong Basic and Applied Basic Research Foundation (No. 2019A1515011387). † Baoyuan Wu is supported by the Natural Science Foundation of China under Grant No. 62076213, the University Development Fund of the Chinese University of Hong Kong, Shenzhen under Grant No. 01001810, and the Special Project Fund of Shenzhen Research Institute of Big Data under grant No. T00120210003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of Text-to-Image Generation on Our Multi-modal CelebA-HQ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative Comparison of Image Manipulation using Natural Language Descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Control Mechanism of Our TediGAN Framework. Different layer in the StyleGAN generator represents different attributes. Changing the value of a certain layer would change the corresponding attributes of the image. Since the texts and images are mapped into the common latent space, we can synthesize images with certain attributes by selecting attribute-specific layers. The control mechanism mixes layers of the style code w s by partially replacing corresponding layers of the content w c . When w s is a randomly sampled latent code, it is the text-to-image generation and when w s is the image embedding, it performs text-guided image manipulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Inversion Results. (a) original image; (b) inversion result of pSp [29]; (c) inversion result of our image encoder (Section 3.1); (d) inversion results after optimization (Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative Comparison of Text-to-Image Generation. We use FID, LPIPS, accuracy (Acc.), and realism (Real.) to compare the state of the art and our method on the proposed Multimodal CelebA-HQ dataset. ↓ means the lower the better while ↑ means the opposite.</figDesc><table><row><cell>AttnGAN [39]</cell><cell>125.98</cell><cell>0.512</cell><cell>14.2</cell><cell>20.3</cell></row><row><cell cols="2">ControlGAN [19] 116.32</cell><cell>0.522</cell><cell>18.2</cell><cell>22.5</cell></row><row><cell>DFGAN [32]</cell><cell>137.60</cell><cell>0.581</cell><cell>22.8</cell><cell>25.5</cell></row><row><cell>DM-GAN [47]</cell><cell>131.05</cell><cell>0.544</cell><cell>19.5</cell><cell>12.8</cell></row><row><cell>TediGAN</cell><cell>106.37</cell><cell>0.456</cell><cell>25.3</cell><cell>31.7</cell></row></table><note>Method FID ↓ LPIPS ↓ Acc. (%) ↑ Real. (%) ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative Comparison of Text-Guided Image Manipulation. We use FID, accuracy (Acc.), and realism (Real.) to compare with the state of the art ManiGAN<ref type="bibr" target="#b19">[20]</ref>.</figDesc><table><row><cell></cell><cell>CelebA</cell><cell></cell><cell cols="2">Non-CelebA</cell></row><row><cell>Method</cell><cell>ManiGAN [20]</cell><cell>Ours</cell><cell>ManiGAN [20]</cell><cell>Ours</cell></row><row><cell>FID ↓</cell><cell>117.89</cell><cell>107.25</cell><cell>143.39</cell><cell>135.47</cell></row><row><cell>Acc. (%) ↑</cell><cell>40.9</cell><cell>59.1</cell><cell>12.8</cell><cell>87.2</cell></row><row><cell>Real. (%) ↑</cell><cell>36.2</cell><cell>63.8</cell><cell>21.7</cell><cell>78.3</cell></row><row><cell cols="5">4.2. Comparison with State-of-the-Art Methods</cell></row><row><cell cols="3">4.2.1 Text-to-Image Generation</cell><cell></cell><cell></cell></row><row><cell cols="2">Quantitative Comparison.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2StyleGAN: How to embed images into the StyleGAN latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverting layers of a large generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeing what a GAN cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4502" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rewards of beauty: the opioid system mediates social motivation in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Chelnokova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Laeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Eikemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeppe</forename><surname>Riegels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guro</forename><surname>Løseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedda</forename><surname>Maurud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frode</forename><surname>Willoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siri</forename><surname>Leknes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="746" to="747" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RiFeGAN: Rich feature generation for textto-image synthesis from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanling</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10911" to="10920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Caucasian and North African French Faces (CaNAFF): A face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Courset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Palluel-Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annique</forename><surname>Smeding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliette</forename><surname>Manto Jonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Review of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image synthesis via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5706" to="5714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive sketch &amp; fill: Multiclass sketch-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li F Fei-Fei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">TACL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ManiGAN: Text-guided image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightweight generative adversarial networks for text-guided image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Textadaptive generative adversarial networks: Manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Textadaptive generative adversarial networks: manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<title level="m">Encoding in style: a StyleGAN encoder for image-to-image translation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Interpreting the latent space of GANs for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polysemous visualsemantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Slidergan: Synthesizing expressive face images by sliding 3d blendshape parameters. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<title level="m">Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Consensus-aware visual-semantic embedding for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cali-sketch: Stroke calibration and completion for high-quality face image generation from poorly-drawn sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00426</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05278</idno>
		<title level="m">Gan inversion: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">AttnGAN: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Semantic hierarchy emerges in deep generative representations for scene synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09267</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ApdrawingGAN: Generating artistic portrait drawings from face photos with hierarchical gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10743" to="10752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">StackGAN++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Indomain GAN inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
