<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Mihaela</forename><surname>Truşcǎ</surname></persName>
							<email>maria.trusca@csie.ase.ro</email>
							<affiliation key="aff2">
								<orgName type="institution">Bucharest University of Economic Studies</orgName>
								<address>
									<postCode>010374</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wassenberg</surname></persName>
							<email>daan.wassenberg@hotmail.com</email>
							<affiliation key="aff1">
								<address>
									<postCode>0000−0002, 8031−758X]</postCode>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Erasmus University Rotterdam</orgName>
								<address>
									<addrLine>Burgemeester Oudlaan 50</addrLine>
									<postCode>3062 PA</postCode>
									<settlement>Rotterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Flavius Frasincar</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-Hop LCR-ROT · Hierarchical Attention · Contextual Word Embeddings</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Web has become the main platform where people express their opinions about entities of interest and their associated aspects. Aspect-Based Sentiment Analysis (ABSA) aims to automatically compute the sentiment towards these aspects from opinionated text. In this paper we extend the state-of-the-art Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two directions. First we replace the non-contextual word embeddings with deep contextual word embeddings in order to better cope with the word semantics in a given text. Second, we use hierarchical attention by adding an extra attention layer to the HAABSA high-level representations in order to increase the method flexibility in modeling the input data. Using two standard datasets (SemEval 2015 and SemEval 2016) we show that the proposed extensions improve the accuracy of the built model for ABSA.</p><p>the aim of the AD task is to learn aspects that have a broader meaning and refer to the targets' categories. However, in this paper, we focus only on the identification of targets' sentiments (SC task) computed at the sentence level.</p><p>Deep Neural Networks (DNNs) have recently shown a great potential for sentiment classification tasks and gradually replaced rule-based approaches. While the main advantage of DNNs architectures is flexibility, rule-based classifiers imply more manual labour that confers a higher level of domain-control. The two approaches can be easily combined in a two-step method that utilises a backup classifier for all inconclusive predictions of the main classifier. One of the first two-step sentiment classification methods utilises a dictionary-based method and a Support Vector Machine (SVM) algorithm <ref type="bibr" target="#b4">[5]</ref>. Given that this method is a bit naive, we try to tackle the sentiment classification of targets using the more refined Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) that obtains state-of-the-art results for the SC task <ref type="bibr" target="#b23">[24]</ref>. The first step of this hybrid method employs a domain ontology <ref type="bibr" target="#b18">[19]</ref> to determine the sentiments of the given targets. All the sentences for which the ontology is inconclusive input a Left-Center-Right separated neural network with Rotatory attention (LCR-Rot) <ref type="bibr" target="#b27">[28]</ref>, as the backup model.</p><p>In <ref type="bibr" target="#b23">[24]</ref> two extensions of the neural network are proposed, namely Inversed LCR-Rot and Multi-hop LCR-Rot, but since the second one was shown to be the most effective, we choose it as the backup model. In this paper, we propose two extensions for HAABSA to improve the quality of the sentiment predictions. First, we replace the non-contextual GloVe word embeddings with deep contextual word embeddings, i.e., ELMo [15]  and BERT [6]  in order to better consider the semantics of words context. Second, we introduce a hierarchical attention, by supplementing the current attention mechanism with a new attention layer that is able to distinguish the importance of the high-level input sentence representations. We call the new model HAABSA++. The Python source code of our extensions can be found at https://github.com/mtrusca/HAABSA_PLUS_PLUS.</p><p>The rest of the paper is organized as follows. Section 2 briefly introduces the related works. Section 3 presents the details of the utilised datasets. Section 4 discusses the hybrid approach together with the extensions we propose and Sect. 5 presents the experimental settings and the evaluation of our methods. Section 6 gives our conclusions and suggestions for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the evolution of the Social Web, people have benefited from the opportunity to actively interact with others sharing content from both sides. As a result, the amount of opinionated texts has risen and people had to face the problem of filtering the extra data in order to get the desired information <ref type="bibr" target="#b20">[21]</ref>. In this context, sentiment analysis turns out to be an important tool that can find sentiments or opinions at the level of a document, sentence, or aspect <ref type="bibr" target="#b10">[11]</ref>. Among all levels of analysis, the most fine-grained analysis is the one orientated to aspects <ref type="bibr" target="#b17">[18]</ref>. The main tasks of ABSA are target extraction (TE), aspect detection (AD), and target sentiment classification (SC). Whereas, the TE task is concerned with identification of targets, i.e., attributes of the entity of interest,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Initially, ABSA's main tasks were addressed using knowledge-based methods based on part-of-speech tagging models and lexicons <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. Recently, machine learning including deep learning as a subset has turned out to be a more convenient solution with good rates of performance in Natural Language Processing (NLP). Whereas machine learning methods have proven to be more flexible, knowledge-based methods imply more manual labor, which makes them effective especially for in-domains sentiment classification. In <ref type="bibr" target="#b25">[26]</ref> it was shown that these two approaches are in fact complementary. The sentiment polarities of aspects were learnt by applying an approach based on domain knowledge and a bidirectional recurrent neural network with attention mechanism. The research proves that there is not a winning option and while the neural network performs better for the laptop reviews of the SemEval 2015 dataset <ref type="bibr" target="#b16">[17]</ref>, the approach based on domain rules is more effective in the restaurant domain dataset of the same SemEval workshop.</p><p>Recently, hybrid models that take advantage of both approaches in a mixed solution have been investigated in various studies. For instance, in <ref type="bibr" target="#b19">[20]</ref> an SVM model was trained for target sentiment classification on an input created based on the binary presence of features identified using a domain-specific ontology. Another option to enrich the input of a neural network using domain knowledge is presented in <ref type="bibr" target="#b6">[7]</ref>, where a self-defined sentiment lexicon is used to extend the word embeddings. Similar to our work, the neural network described in <ref type="bibr" target="#b7">[8]</ref> aims to learn context-sensitive target embeddings. Next, the attention scores are computed only for relevant words of the context indicated by a dependency parser. While the previous methods focused on integrating rule-based approaches in machine learning, in <ref type="bibr" target="#b3">[4]</ref> it is presented a different method where machine learning is used for building domain knowledge. Namely, a Long Short-Term Memory (LSTM) model with an attention mechanism is employed to create a sentiment dictionary called SenticNet 5.</p><p>Instead of integrating the two approaches in a single model, another option is to apply them sequentially <ref type="bibr" target="#b4">[5]</ref>. This option has been demonstrated to be superior to the individual approaches in <ref type="bibr" target="#b18">[19]</ref>. Namely, in <ref type="bibr" target="#b18">[19]</ref> an ontology developed for restaurant domain reviews is used as the first method for sentiment classification (positive and negative). The backup model, triggered when the ontology is inconclusive, employs a bag-of-words approach trained with a multi-class SVM associated with all three sentiment polarities (positive, neutral, and negative). This work inspired <ref type="bibr" target="#b11">[12]</ref> where the SVM model is replaced with a neural network that assigns polarities to the aspects using multiple attention layers. The first one captures the relation between aspects and their left and right contexts and generates context-dependent word embeddings. The new word vectors together with sentences and aspects embeddings created using the bag-of-words approach feed the last layer of attention.</p><p>The previous line of research is kept in <ref type="bibr" target="#b23">[24]</ref>, where the same ontology is used together with a Multi-Hop LCR-Rot model as backup. Knowing the effectiveness of the two-step approach for the SC task, and considering that the method proposed in <ref type="bibr" target="#b23">[24]</ref> achieves the best results for the SemEval 2015 and the SemEval 2016 <ref type="bibr" target="#b15">[16]</ref> datasets, we choose it as basis for our investigation on the benefits of contextual word embeddings. In addition, inspired by the hierarchical attention approach presented in <ref type="bibr" target="#b26">[27]</ref> we add to the architecture of Multi-hop LCR-Rot a new attention layer for high-level representations of the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets Specification</head><p>The data used in this paper was introduced in the SemEval 2015 and 2016 contests to evaluate the ABSA task and is organised as a collection of reviews in the restaurant domain. Each review has a variable number of sentences and each sentence has one or more aspect categories. Each aspect is linked to one target that has assigned a sentiment polarity (positive, neutral, and negative). <ref type="table" target="#tab_0">Table  1</ref> lists the distribution of sentiment classes in the SemEval 2015 and SemEval 2016 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>HAABSA is a hybrid approach for aspect-based sentiment classification with two steps. First, target polarities are predicted using a domain sentiment ontology. If this rule-based method is inconclusive, a neural network is utilised as backup. Section 4.1 introduces the ontology-based rules for sentiment classification. Section 4.2 gives an overview of HAABSA and presents our extensions based on various word embeddings and hierarchical attention. The new method is called HAABSA++, as a reminiscent of the base method name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ontology-Based Rules</head><p>The employed ontology is a manually designed domain specification for sentiment polarities of aspects that utilises a hierarchical structure of concepts grouped in three classes <ref type="bibr" target="#b18">[19]</ref>. The SentimentValue class groups concepts in the Positive and Negative subclasses, and the AspectMention class identifies aspects related to sentiment expressions. The SentimentMention class represents sentiment expressions. To compute the sentiment of an aspect, we utilise three rules, described below. The first rule always assigns to an aspect the generic sentiment of its connected sentiment expression. The second rule identifies the aspect-specific sentiment expression and the sentiment is assigned only if the aspect and the linked expression belong to the same aspect category. The third rule finds the expression with a varying sentiment with respect to the connected aspect and the overall sentiment is inferred based on the pair aspect-sentiment expression. All these rules are mutually exclusive.</p><p>The rule-based approach can identify only the positive and negative sentiments. By design, the neutral sentiment class is not modeled due to its ambiguous semantics. The ontology is inconclusive in two cases: (1) conflicting sentiment (predicting both positive and negative for a target) or (2) no hits (due to the limited coverage). In these cases a neural network is used as backup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Hop LCR-Rot Neural Network Design</head><p>The LSTM-ATT <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> model enhances the performance of the LSTM model with attention weighting and is a standard structure integrated by numerous sentiment classifiers. The LCR-Rot model <ref type="bibr" target="#b27">[28]</ref> utilises this structure to detect interchangeable information between opinionated expressions and their contexts. In <ref type="bibr" target="#b23">[24]</ref>, the LCR-Rot model is refined with repeated attention and the new classifier is called Multi-Hop LCR-Rot. In this paper, we explore the effect of different word embeddings on the Multi-Hop LCR-Rot model and propose a hierarchical attention structure to increase the model's flexibility.</p><p>The Multi-Hop LCR-Rot neural network splits each sentence into three parts: left context, target, and right context. Each of these three parts feeds three bidirectional LSTMs (bi-LSTMs). Then, a two-step rotatory attention mechanism is applied over the three hidden states associated with the bi-LSTMs (left context:</p><formula xml:id="formula_0">[h l 1 , ..., h l L ], target: [h t 1 , ..., h t T ], and right context: [h r 1 , ..., h r R ],</formula><p>where L, T , and R represent the length of the three input parts). At the first step, the mechanism generates new context representations using target information. Initially, an attention function f is computed taking as input a parameterized product between the hidden states of the context and the target vector r tp extracted using an average pooling operation. Considering for example the left context, the function f is computed by:</p><formula xml:id="formula_1">f ( h l i 1×1 , r tp ) = tanh( h l i 1×2d × W l c 2d×2d × r tp 2d×1 + b l c 1×1 ),<label>(1)</label></formula><p>where W l c is a weight matrix, b l c is a bias term, and d represents the dimension of the i-th hidden state h l i for i = 1, ..., L. Then, the attention normalised scores α l i associated with f are defined using the softmax function as follows:</p><formula xml:id="formula_2">α l i = exp(f (h l i , r rp )) L j=1 exp(f (h l j , r rp ))</formula><p>.</p><p>(</p><p>In the end, context representations are computed using hidden states weighted by attention scores. For example, the left target2context vector is defined as:</p><formula xml:id="formula_4">r l 2d×1 = L i=1 α l i 1×1 × h l i 2d×1 .</formula><p>(</p><p>At the second step of the rotatory attention, target representations are computed similarly, following the previous three equations. The only difference is that instead of the r tp vector that stands for target information, the left and right contexts vectors (r l and r r ) are employed to obtain a better target representation. Taking again the left context as example, the left context2target representation r t l is:</p><formula xml:id="formula_6">r t l 2d×1 = T i=1 α t l i 1×1 × h t i 2d×1 ,<label>(4)</label></formula><p>where α t l represents the target attention scores with respect to the left context computed as above.</p><p>The right vectors, target2context and context2target (r r and r tr ) are computed in a similar way. In a multi-hop rotatory attention mechanism, the two aforementioned steps are applied sequentially for n times. In <ref type="bibr" target="#b23">[24]</ref> the optimal n value is three (the trials were executed for four scenarios: n = 1, 4). One should note that the r tp target vector computed using average pooling is used only for the first iteration of the rotatory attention. At the next iterations the vector r tp is replaced with one of the vectors r t l or r tr , depending on the considered context. At the end of the rotatory attention, all the four vectors are concatenated and feed an MLP layer for the final sentiment prediction.</p><p>The learning process is realised using a backpropagation algorithm by minimising the cross-entropy loss function with L2 regularization. All weight matrices and biases are initialised by a uniform distribution and are updated using stochastic gradient descent with a momentum term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Embeddings</head><p>The first proposed extension examines the effect of deep context-dependent word embeddings on the overall performance of the neural network. Since the Multi-Hop LCR-Rot model already captures shallow context information for each target of a sentence, it is important to analyse how this architecture is possibly improved when we use deep context-sensitive word representations. Hereinafter, we give a short description of some of the most well-known contextual and noncontextual word embeddings.</p><p>Non-contextual Word Embeddings. Non-contextual word embeddings are unique for each word, regardless of its context. As a result, the polysemy of words and the varying local information are ignored. GloVe, word2vec, and fastText context-independent word embeddings are presented below.</p><p>GloVe. The GloVe model generates word embeddings using word occurrences instead of language models (like word2vec), which means that the new word embeddings take into account global count statistics, instead of only the local information <ref type="bibr" target="#b13">[14]</ref>. The idea behind the GloVe model is to determine two word embeddings w i and w k for words i and k, respectively, whose dot product is equal with the logarithmic value of their co-occurrence X ik . The relation is adjusted using two biases (b i and b k ) for both words i and k as follows:</p><formula xml:id="formula_7">w T i w k + b i + b k = log(X ik ).<label>(5)</label></formula><p>The optimal word embeddings are computed using a weighted least-squares method using the cost function defined as:</p><formula xml:id="formula_8">J = V i=1 V k=1 f (X ik )(w T i w k + b i + b k − log(X ik )) 2 ,<label>(6)</label></formula><p>where V is the vocabulary size and f (X ik ) is a weighting function that has to be continuous, non-decreasing, and to generate relatively small values for large input values. The last two conditions for f are necessary to prevent overweighting of either rare or frequent co-occurrences. In this paper, we choose to use 300-dimension GloVe word embeddings trained on the Common Crawl (42 billion words) <ref type="bibr" target="#b13">[14]</ref>.</p><p>Word2vec. The word2vec word embeddings were the first widely used word representations and since their introduction they have shown a significant improvement for many NLP tasks. The word2vec model works like a language model that facilitates generation of the more close word representations in the embedding space for words with similar context <ref type="bibr" target="#b12">[13]</ref>. The word2vec model has two variations: Continuous-Bag-Of-Words (CBOW) and Skip-Gram (SG). CBOW word embeddings represent the weights of a neural network that maximize the likelihood that words are predicted from a given context of words and SG does it the other way around. Both variations exploit the bag-of-words approach and the sequencing of words in the given or predicted context of words is irrelevant. The CBOW and SG models are trained using the following loss functions:</p><formula xml:id="formula_9">CBOW : J = 1 V V t=1 log p(w t |w t−c , . . . , w t−1 , w t+1 , . . . , w t+c ),<label>(7)</label></formula><formula xml:id="formula_10">SG : J = 1 V V t=1 t+c i=t−c,i =t log p(w i |w t ).<label>(8)</label></formula><p>where [-c, c] is the word context of the word w t . CBOW is considered to be faster to train than SG, but SG benefits of a better accuracy for non-frequent words <ref type="bibr" target="#b0">[1]</ref>. Therefore, in the present word, both variations of word2vec are examined. The pre-trained word2vec word embeddings we use are already trained on Google News dataset (100 billion words) and their length is 300 features.</p><p>FastText. The fastText model computes non-contextual word embeddings using a word2vec SG approach where the word context is represented by its n-grams <ref type="bibr" target="#b2">[3]</ref>. As a result, out-of-vocabulary words are better handled as they can benefit from representations closer to the ones of in-vocabulary words with similar meaning in the embedding space. Given that our employed datasets are small, we utilise already computed fastText word embeddings trained on statmt.org news, UMBC webbase corpus, and Wikipedia dumps ((16 billion words). The dimensionality of word embeddings is 300.</p><p>Contextual Word Embeddings. Contextual word embeddings take into account the context of words which means that they handle better the semantics and the polysemy. Below we focus on ELMo and BERT deep contextual word embeddings.</p><p>ELMo. The ELMo word embeddings capture information about the entire input sentence using multiple bidirectional LSTM (bi-LSTM) layers <ref type="bibr" target="#b14">[15]</ref>. The main difference between the ELMo model and other language models developed on LSTM layers is that ELMo word embeddings integrate the hidden states of all L bi-LSTMs layers in a linear combination instead of utilising only the hidden states of the last layer. The ELMo model can be considered a task-specific language model that can be adjusted to different computational linguistic tasks by learning different weights for all LSTM layers. ELMo representation of word i for a given task ELM o task i is computed as follows:</p><formula xml:id="formula_11">ELM o task i = γ task L j=0 s task j h i,j ,<label>(9)</label></formula><p>where h i,j represents the concatenated hidden states of the j bi-LSTM layer</p><formula xml:id="formula_12">(h i,j = [ → h i,j , ← h i,j ]), s task j</formula><p>is its weight, and γ task scales the word embeddings accordingly to the given task.</p><p>The model we use to generate ELMo word embeddings employs two bi-LSTM layers with 512 dimension hidden state which means the size of the final word embeddings is 1024. The model is pre-trained on the 1B Word Benchmark dataset.</p><p>BERT. The BERT model unlike the ELMo language model that utilises LSTM hidden states, creates contextual word representations by averaging token vectors (unique for each vocabulary word), position embeddings (vectors for word locations in the sentence), and segment embeddings (vectors of sentence indices that contains the given word). The new sequence of word embeddings is given as input to a Transformer encoder <ref type="bibr" target="#b21">[22]</ref> based on the (bidirectional) self-attention. The Transformer encoder has L blocks and each one contains a Multi-Head Attention layer followed by a fully connected layer. The output of each block feeds the input of the next one. Each Multi-Head Attention has A parallel attention layers that compute the attention scores for each word with respect to the rest of the words in the sentence. The word representations associated with each Transformer block are computed by concatenating all attention-based representations. Recently, Transformers have become more common than other widely applied neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Network (RNNs) due to their capacity to apply the parallelization (as CNNs) and to control long-term dependencies (as RNNs).</p><p>The BERT model is pre-trained simultaneously on two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP) using BookCorpus (800 million words) and Wikipedia dumps (2,500 million words). The first task employs a bidirectional Transformer to predict some masked words and the second task tries to learn sequence dependencies between sentences. The final loss function is computed as a sum of the task losses. In this paper, BERT word embeddings are generated using the pre-trained BERT Base model (L=12, A=12, H=768), where H stands for hidden states and represents the size of the word embeddings. The final representations of the word i is computed by summing the word embeddings of the last four layers (as it was suggested in <ref type="bibr" target="#b5">[6]</ref>):</p><formula xml:id="formula_13">BERT i = 12 j=9 H i,j .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-Hop LCR-Rot with Hierarchical Attention</head><p>The main disadvantage of Multi-Hop LCR-Rot is that the four target2context and context2target vectors are computed using only local information. Hierarchical attention alleviates this process by providing a high-level representation of the input sentence that updates each target2context and context2target vector with a relevance score computed at the sentence level. The final sentiment prediction considers the newly obtained vectors. First, we have to compute an attention function f defined as: where v i is the representation i of the input sentence (v i ∈ {r r , r l , r tr , r t l }, i = 1, 4), W is a weight matrix, and b is a bias. The attention function f is used to compute new attention scores α i for each input v i :</p><formula xml:id="formula_14">f ( v i 1×1 ) = tanh( v i 1×2d × W 2d×1 + b 1×1 ),<label>(11)</label></formula><formula xml:id="formula_15">α i = exp(f (v i ) 4 j=1 exp(f (v j ))</formula><p>.</p><p>The new scaled context2target or target2context vectors are:</p><formula xml:id="formula_17">v i 2d×1 = α i 1×1 × v i 2d×1 ,<label>(13)</label></formula><p>We consider four methods to introduce hierarchical attention in the architecture of the Multi-Hop LCR-Rot model: -Method 1: attention weighting is applied on the final four vectors of the rotatory attention ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>). -Method 2: attention weighting is applied in each iteration of the rotatory attention, on the intermediate four vectors <ref type="figure" target="#fig_0">(Fig. 1 (b)</ref>). -Method 3: attention weighting is separately applied on the final two context and target vectors pairs of the rotatory attention ( <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>). -Method 4: attention weighting is separately applied in each iteration of the rotatory attention, on the intermediate context and target vectors pairs ( <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>).</p><p>To optimise the performance of the newly proposed methods based on hierarchical attention, we have to tune again some of the model's hyperparameters like the learning rate, the momentum term, the L2 regularization term, and the dropout rate (applied to all hidden layers). The algorithm we employ for tuning is a tree-structured Parzen estimator (TPE) <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We compare our extensions with the baseline Multi-Hop LCR-Rot neural network, a state-of-the-art model in the SC task for both SemEval 2015 and SemEval 2016 datasets. Like <ref type="bibr" target="#b23">[24]</ref>, our main classifier is a domain sentiment ontology. The importance of the hybrid method is pointed out in <ref type="bibr" target="#b18">[19]</ref> where all the inconclusive cases of the domain sentiment ontology are assigned to the majority class of the dataset. The accuracy reported for the reference approach on the SemEval datasets is 63.3% and 76.1%, respectively, much lower than the accuracy of the hybrid approach.</p><p>The evaluation is done in terms of training and testing accuracy. Since our work is an extension of the baseline model, we re-run the Multi-Hop LCR-Rot to assure a fair comparison. First, the embedding layer is optimised by trying different word embeddings; the results thereof are shown in <ref type="table" target="#tab_1">Table 2</ref>. Given that our base model <ref type="bibr" target="#b23">[24]</ref> utilises the GloVe embeddings, we start presenting the results for context-independent word representation models. CBOW and SG models lead to the worst predictions and, as it is already expected, the SG model performs better than CBOW by 1.4%-0.6%. The difference between the performance of the fastText and SG models is equal to three percentage points in the SemEval 2015 test dataset, which means that the fastText model is clearly an improvement of the SG model. Even if fastText outperforms the GloVe model by 0.1% for the SemEval 2016 test dataset, given the overall performance of the GloVe model, we can conclude that it is the best context-independent word representation option.</p><p>As regards deep contextual word embeddings, we notice that a contextsensitive approach not always leads to better results (the ELMo model outperforms the GloVe model only for the SemEval 2016 dataset). However, the BERT model seems to have the best performance, recording the same testing accuracy as the ELMo model for the SemEval 2016 datasets and exceeding the GloVe model by more than one percentage point for SemEval 2015 datasets.</p><p>The second extension we present is an adjustment of the rotatory attention to a hierarchical architecture using BERT word embeddings. <ref type="table" target="#tab_2">Table 3</ref> shows that adding new attention layers leads to a more accurate sentiment prediction than the baseline model with BERT word embeddings listed in <ref type="table" target="#tab_1">Table 2</ref>. Overall it is fair to consider that the best approach to tackle the hierarchical attention is the Method 4, given the small difference between the first rank and the second rank on the SemEval 2016 test dataset.  87.0% PRET+MULT <ref type="bibr" target="#b8">[9]</ref> 81.3% BBLSTM-SL <ref type="bibr" target="#b6">[7]</ref> 85.8% BBLSTM-SL <ref type="bibr" target="#b6">[7]</ref> 81.2% PRET+MULT <ref type="bibr" target="#b8">[9]</ref> 85.6% Sentiue (SW) <ref type="bibr" target="#b16">[17]</ref> 78.7% LSTM+SynATT+TarRep <ref type="bibr" target="#b7">[8]</ref> 84.6%</p><p>Further on, we compare the fourth method with other similar neural networks, state-of-the-art models in SC task. The results are listed in <ref type="table" target="#tab_3">Table 4</ref>. We do not replicate previous works and give the results as reported in papers. The best results reported in the SemEval contests are mentioned as well. While for the SemEval 2015 data, our method achieves the highest accuracy (together with the LSTM+ SynATT+TarRep [8] model) for the SemEval 2016 data, it is ranked on the second position.</p><p>As we already mentioned, the Multi-Hop LCR-Rot model turns the input sentence into four vectors. Knowing that the length of the target expression is small and usually void of sentiment, we can infer that target2context vectors determine the neural network's performance to a greater extent than context2target vectors. Taking as example two sentences from the SemEval 2016 test dataset, we explore how the embedding layer and the hierarchical attention affects the predicted sentiment polarity via target2context vectors.   <ref type="figure" target="#fig_1">Figure 2</ref> graphically presents attention scores associated with target2context vectors for GloVe, ELMo, and BERT word embeddings. The intensity of the blue colour shows the significance of words indicated by the attention scores. The target of the first sentence is the word "place" and the opinionated expression (the word "gem") indicates a positive polarity, and is located in the right context. The left context is too short and irrelevant for the target word. Only ELMo and BERT word embeddings assign the highest attention score to the opinionated word which leads to a good sentiment prediction. On the contrary, the GloVe model finds the word "n't" to be the most relevant for the given example, leading to a negative sentiment prediction. One should note that the BERT model has a slightly different approach to extract tokens of a sentence. This is due to the internal vocabulary used by the BERT model to guarantee the high recall on out-of-sample.</p><p>The second example explores the effect of hierarchical attention (Method 4) using BERT word embeddings. The selected sentence given in <ref type="figure" target="#fig_2">Fig. 3</ref> has two target expressions with different sentiment polarities. Considering the target "atmosphere", the left context is again irrelevant while the right context contains the sentiment expression together with the second target "service" and its opinionated expression. Even if the simple Multi-Hop LCR-Rot model without hierarchical attention assigns the highest attention scores to the words "cozy" and "horrible", it finds the word "service" as relevant. As a result the sentiment prediction of the target "atmosphere" is wrong. Differently, the neural network with hierarchical attention achieves a good prediction, considering the word "cozy" to be the most relevant to the given target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work we extended the backup neural network of the state-of-the-art hybrid approach method for ABSA introduced in [24] using deep contextual word embeddings. Further on, the architecture of the model is integrated with a hierarchical structure that enforces the rotatory attention vectors to take into account high-level representations at the sentence level. Both extensions boost the testing accuracy from 80.3% to 81.7% for SemEval 2015 dataset and from 86.4% to 87.0% for SemEval 2016 dataset.</p><p>As deep learning architectures have the tendency to forget useful information from the lower layers, in future work we would like to investigate the effect of adding word embeddings to the upper layers of the architecture. Also we would like to have a better understanding of the model's inner working by applying diagnostic classification to the various layer representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Multi-Hop LCR-Rot with hierarchical attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Target2Context vectors of the the Multi-Hop LCR-Rot model computed using GloVe, ELMo, and BERT word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Target2Context vectors of the Multi-Hop LCR-Rot model with or without hierarchical attention computed using BERT word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Polarity frequencies of SemEval 2015 and SemEval 2016 datasets (ABSA).</figDesc><table><row><cell></cell><cell></cell><cell>SemEval 2015</cell><cell></cell><cell></cell><cell>SemEval 2016</cell><cell></cell></row><row><cell></cell><cell>Positive</cell><cell>Neutral</cell><cell>Negative</cell><cell>Positive</cell><cell>Neutral</cell><cell>Negative</cell></row><row><cell>Train</cell><cell>72.4%</cell><cell>24.4%</cell><cell>3.2%</cell><cell>70.2%</cell><cell>3.8%</cell><cell>26.0%</cell></row><row><cell>Test</cell><cell>53.7%</cell><cell>41.0%</cell><cell>5.3%</cell><cell>74.3%</cell><cell>4.9%</cell><cell>20.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of word embeddings for the Multi-Hop LCR-Rot model using accuracy. The best results are given in bold font.</figDesc><table><row><cell></cell><cell cols="2">SemEval 2015</cell><cell cols="2">SemEval 2016</cell></row><row><cell></cell><cell cols="4">in-sample out-of-sample in-sample out-of-sample</cell></row><row><cell>Context-independent word embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GloVe (HAABSA)</cell><cell>88.0%</cell><cell>80.3%</cell><cell>89.6%</cell><cell>86.4%</cell></row><row><cell>CBOW</cell><cell>84.8%</cell><cell>74.6%</cell><cell>82.7%</cell><cell>83.5%</cell></row><row><cell>SG</cell><cell>84.7%</cell><cell>76.0%</cell><cell>85.4%</cell><cell>84.1%</cell></row><row><cell>FastText</cell><cell>87.4%</cell><cell>79.0%</cell><cell>87.3%</cell><cell>86.5%</cell></row><row><cell>Context-dependent word embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ELMo</cell><cell>85.1%</cell><cell>80.1%</cell><cell>91.1%</cell><cell>86.7%</cell></row><row><cell>BERT</cell><cell>87.9%</cell><cell>81.1%</cell><cell>89.2%</cell><cell>86.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between the four methods proposed for HAABSA++ using accuracy. The best results are given in bold font.</figDesc><table><row><cell></cell><cell cols="2">SemEval 2015</cell><cell cols="2">SemEval 2016</cell></row><row><cell></cell><cell>in-sample</cell><cell>out-of-sample</cell><cell>in-sample</cell><cell>out-of-sample</cell></row><row><cell>Method 1</cell><cell>87.9%</cell><cell>81.5%</cell><cell>88.0%</cell><cell>87.1%</cell></row><row><cell>Method 2</cell><cell>87.9%</cell><cell>81.7%</cell><cell>88.7%</cell><cell>86.7%</cell></row><row><cell>Method 3</cell><cell>87.8%</cell><cell>81.3%</cell><cell>88.7%</cell><cell>86.7%</cell></row><row><cell>Method 4</cell><cell>88.0%</cell><cell>81.7%</cell><cell>88.9%</cell><cell>87.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison between HAABSA++ (Method 4) with state-of-the-art models in SC task using accuracy. SW stands for the SemEval Winner (the most effective result reported in the SemEval contest). The best results are given in bold font.</figDesc><table><row><cell>SemEval 2015</cell><cell>SemEval 2016</cell><cell></cell></row><row><cell>HAABSA++ (Method 4)</cell><cell>81.7% XRCE (SW) [16]</cell><cell>88.1%</cell></row><row><cell cols="2">LSTM+SynATT+TarRep [8] 81.7% HAABSA++ (Method 4)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating deep learning models for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ay</forename><surname>Karakuş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Talo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hallaç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">İ</forename><forename type="middle">R</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">4783</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Senticnet 5: Discovering conceptual primitives for sentiment analysis by means of context embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1795" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SeNTU: Sentiment analysis of tweets by combining a rule-based classifier with supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chikersal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="647" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis using bitmask bidirectional long short term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Florida Artificial Intelligence Research Society Conference</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="259" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective attention modeling for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Computational Linguistics</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1121" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04346</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Workshop on Semantic Evaluation</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sentiment analysis: Mining opinions, sentiments, and emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aldona: a hybrid solution for sentence-level aspectbased sentiment analysis using a lexicalised domain ontology and a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meškelė</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th ACM Symposium on Applied Computing (SAC 2019)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2489" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27st Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>De Clercq</surname></persName>
		</author>
		<title level="m">Semeval-2016 task 5: Aspect-based sentiment analysis</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>10th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<title level="m">Semeval-2015 task 12: Aspect-based sentiment analysis</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>9th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ontology-driven sentiment analysis of product and service aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Extended Semantic Web Conference (ESWC 2018)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="608" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ontology-enhanced aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Web Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10360</biblScope>
			<biblScope unit="page" from="302" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The paradox of choice: Why more is less</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>HarperCollins</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dcu: Aspect-based polarity classification for SemEval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hybrid approach for aspect-based sentiment analysis using a lexicalized domain ontology and attentional neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wallaart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Extended Semantic Web Conference (ESWC 2019)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11503</biblScope>
			<biblScope unit="page" from="363" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">bunji at SemEval-2016 task 5: Neural and syntactic models of entity-attribute relationship for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Semantic Evaluation</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Left-center-right separated neural network for aspect-based sentiment analysis with rotatory attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00892</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
