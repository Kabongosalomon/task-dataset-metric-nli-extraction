<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-25">September 25, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
							<email>g.french@uea.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
							<email>mark.fisher@uea.ac.uk</email>
						</author>
						<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-25">September 25, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant [29] of temporal ensembling <ref type="bibr" target="#b13">[14]</ref>, a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The strong performance of deep learning in computer vision tasks comes at the cost of requiring large datasets with corresponding ground truth labels for training. Such datasets are often expensive to produce, owing to the cost of the human labour required to produce the ground truth labels.</p><p>Semi-supervised learning is an active area of research that aims to reduce the quantity of ground truth labels required for training. It is aimed at common practical scenarios in which only a small subset of a large dataset has corresponding ground truth labels. Unsupervised domain adaptation is a closely related problem in which one attempts to transfer knowledge gained from a labeled source dataset to a distinct unlabeled target dataset, within the constraint that the objective (e.g.digit classification) must remain the same. Domain adaptation offers the potential to train a model using labeled synthetic data -that is often abundantly available -and unlabeled real data. The scale of the problem can be seen in the VisDA-17 domain adaptation challenge images shown in <ref type="figure" target="#fig_1">Figure 1</ref>. We will present our winning solution in Section 4.2.   <ref type="bibr" target="#b28">[29]</ref> has demonstrated the effectiveness of self-ensembling with random image augmentations to achieve state of the art performance in semisupervised learning benchmarks.</p><p>We have developed the approach proposed by Tarvainen et al. <ref type="bibr" target="#b28">[29]</ref> to work in a domain adaptation scenario. We will show that this can achieve excellent results in specific small image domain adaptation benchmarks. More challenging scenarios, notably MNIST → SVHN and the VisDA-17 domain adaptation challenge required further modifications. To this end, we developed confidence thresholding and class balancing that allowed us to achieve state of the art results in a variety of benchmarks, with some of our results coming close to those achieved by traditional supervised learning. Our approach is sufficiently flexble to be applicable to a variety of network architectures, both randomly initialized and pre-trained.</p><p>Our paper is organised as follows; in Section 2 we will discuss related work that provides context and forms the basis of our technique; our approach is described in Section 3 with our experiments and results in Section 4; and finally we present our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section we will cover self-ensembling based semi-supervised methods that form the basis of our approach and domain adaptation techniques to which our work can be compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-ensembling for semi-supervised learning</head><p>Recent work based on methods related to self-ensembling have achieved excellent results in semi-supervised learning scenarious. A neural network is trained to make consistent predictions for unsupervised samples under different augmentation <ref type="bibr" target="#b23">[24]</ref>, dropout and noise conditions or through the use of adversarial training <ref type="bibr" target="#b17">[18]</ref>. We will focus in particular on the self-ensembling based approaches of Laine et al. <ref type="bibr" target="#b13">[14]</ref> and Tarvainen et al. <ref type="bibr" target="#b28">[29]</ref> as they form the basis of our approach.</p><p>Laine et al. <ref type="bibr" target="#b13">[14]</ref> present two models; their Π-model and their temporal model. The Π-model passes each unlabeled sample through a classifier twice, each time with different dropout, noise and image translation parameters. Their unsupervised loss is the mean of the squared difference in class probability predictions resulting from the two presentations of each sample. Their temporal model maintains a per-sample moving average of the historical network predictions and encourages subsequent predictions to be consistent with the average. Their approach achieved state of the art results in the SVHN and CIFAR-10 semisupervised classification benchmarks.</p><p>Tarvainen et al. <ref type="bibr" target="#b28">[29]</ref> further improved on the temporal model <ref type="bibr" target="#b13">[14]</ref> by using an exponential moving average of the network weights rather than of the class predictions. Their approach uses two networks; a student network and a teacher network, where the student is trained using gradient descent and the weigthts of the teacher are the exponential moving average of those of the student. The unsupervised loss used to train the student is the mean square difference between the predictions of the student and the teacher, under different dropout, noise and image translation parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain adaptation</head><p>There is a rich body of literature tackling the problem of domain adaptation. We focus on deep learning based methods as these are most relevant to our work.</p><p>Auto-encoders are unsupervised neural network models that reconstruct their input samples by first encoding them into a latent space and then decoding and reconstructing them. Ghifary et al. <ref type="bibr" target="#b5">[6]</ref> describe an auto-encoder model that is trained to reconstruct samples from both the source and target domains, while a classifier is trained to predict labels from domain invariant features present in the latent representation using source domain labels. Bousmalis et al. <ref type="bibr" target="#b1">[2]</ref> reckognised that samples from disparate domains have distinct domain specific characteristics that must be represented in the latent representation to support effective reconstruction. They developed a split model that separates the latent representation into shared domain invariant features and private features specific to the source and target domains. Their classifier operates on the domain invariant features only.</p><p>Ganin et al. <ref type="bibr" target="#b4">[5]</ref> propose a bifurcated classifier that splits into label classification and domain classification branches after common feature extraction layers. A gradient reversal layer is placed between the common feature extraction layers and the domain classification branch; while the domain classification layers attempt to determine which domain a sample came from the gradient reversal operation encourages the feature extraction layers to confuse the domain classifier by extracting domain invariant features. An alternative and simpler implementation described in their appendix minimises the label cross-entropy loss in the feature and label classification layers, minimises the domain crossentropy in the domain classification layers but maximises it in the feature layers. The model of Tzeng et al. <ref type="bibr" target="#b29">[30]</ref> runs along similar lines but uses separate feature extraction sub-networks for source and domain samples and train the model in two distinct stages.</p><p>Saito et al. <ref type="bibr" target="#b21">[22]</ref> use tri-training <ref type="bibr" target="#b31">[32]</ref>; feature extraction layers are used to drive three classifier sub-networks. The first two are trained on samples from the source domain, while a weight similarity penalty encourages them to learn different weights. Pseudo-labels generated for target domain samples by these source domain classifiers are used to train the final classifier to operate on the target domain.</p><p>Generative Adversarial Networks <ref type="bibr" target="#b6">[7]</ref> (GANs) are unsupervised models that consist of a generator network that is trained to generate samples that match the distribution of a dataset by fooling a discriminator network that is simultaneously trained to distinguish real samples from generates samples. Some GAN based models -such as that of Sankaranarayanan et al. <ref type="bibr" target="#b24">[25]</ref> -use a GAN to help learn a domain invariant embedding for samples. Many GAN based domain adaptation approaches use a generator that transforms samples from one domain to another.</p><p>Bousmalis et al. <ref type="bibr" target="#b0">[1]</ref> propose a GAN that adapts synthetic images to better match the characteristics of real images. Their generator takes a synthetic image and noise vector as input and produces an adapted image. They train a classifier to predict annotations for source and adapted samples alonside the GAN, while encouraing the generator to preserve aspects of the image important for annotation. The model of <ref type="bibr" target="#b25">[26]</ref> consists of a refiner network (in the place of a generator) and discriminator that have a limited receptive field, limiting their model to making local changes while preserving ground truth annotations. The use of refined simulated images with corresponding ground truths resulted in improved performance in gaze and hand pose estimation.</p><p>Russo et al. <ref type="bibr" target="#b20">[21]</ref> present a bi-directional GAN composed of two generators that transform samples from the source to the target domain and vice versa. They transform labelled source samples to the target domain using one generator and back to the source domain with the other and encourage the network to learn label class consistency. This work bears similarities to CycleGAN <ref type="bibr" target="#b32">[33]</ref>.</p><p>A number of domain adaptation models maximise domain confusion by minimising the difference between the distributions of features extracted from source and target domains. Deep CORAL <ref type="bibr" target="#b27">[28]</ref> minimises the difference between the feature covariance matrices for a mini-batch of samples from the source and target domains. Tzeng et al. <ref type="bibr" target="#b30">[31]</ref> and Long et al. <ref type="bibr" target="#b16">[17]</ref> minimise the Maximum Mean Discrepancy metric <ref type="bibr" target="#b7">[8]</ref>. Li et al. <ref type="bibr" target="#b14">[15]</ref> described adaptive batch normalization, a variant of batch normalization <ref type="bibr" target="#b11">[12]</ref> that learns separate batch normalization statistics for the source and target domains in a two-pass process, establishing new state-of-the-art results. In the first pass standard supervised learning is used to train a classifier for samples from the source domain. In the second pass, normalization statistics for target domain samples are computed for each batch normalization layer in the network, leaving the network weights as they are. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our model builds upon the mean teacher semi-supervised learning model <ref type="bibr" target="#b28">[29]</ref>, which we will describe. Subsequently we will present our modifications that enable domain adaptation.</p><p>The structure of the mean teacher model <ref type="bibr" target="#b28">[29]</ref> -also discussed in section 2.1 -is shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. The student network is trained using gradient descent, while the weights of the teacher network are an exponential moving average of those of the student. During training each input sample x i is passed through both the student and teacher networks, generating predicted class probability vectors z i (student) andz i (teacher). Different dropout, noise and image translation parameters are used for the student and teacher pathways.</p><p>During each training iteration a mini-batch of samples is drawn from the dataset, consisting of both labeled and unlabeled samples. The training loss is the sum of a supervised and an unsupervised component. The supervised loss is cross-entropy loss computed using z i (student prediction). It is masked to 0 for unlabeled samples for which no ground truth is available. The unsupervised component is the self-ensembling loss. It penalises the difference in class predictions between student (z i ) and teacher (z i ) networks for the same input sample. It is computed using the mean squared difference between the class probability predictions z i andz i .</p><p>Laine et al. <ref type="bibr" target="#b13">[14]</ref> and Tarvainen et al. <ref type="bibr" target="#b28">[29]</ref> found that it was necessary to apply a time-dependent weighting to the unsupervised loss during training in order to prevent the network from getting stuck in a degenerate solution that gives poor classification performance. They used a function that follows a Gaussian curve from 0 to 1 during the first 80 epochs.</p><p>In the following subsections we will describe our contributions in detail along with the motivations for introducing them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adapting to domain adaptation</head><p>We minimise the same loss as in <ref type="bibr" target="#b28">[29]</ref>; we apply cross-entropy loss to labeled source samples and unsupervised self-ensembling loss to target samples. As in <ref type="bibr" target="#b28">[29]</ref>, self-ensembling loss is computed as the mean-squared difference between predictions produced by the student (z T i ) and teacher (z T i ) networks with different augmentation, dropout and noise parameters.</p><p>The models of <ref type="bibr" target="#b28">[29]</ref> and of <ref type="bibr" target="#b13">[14]</ref> were designed for semi-supervised learning problems in which a subset of the samples in a single dataset have ground truth labels. During training both models mix labeled and unlabeled samples together in a mini-batch. In contrast, unsupervised domain adaptation problems use two distinct datasets with different underlying distributions; labeled source and unlabeled target. Our variant of the mean teacher model -shown in <ref type="figure" target="#fig_2">Figure 2b</ref> has separate source (X Si ) and target (X T i ) paths. Inspired by <ref type="bibr" target="#b14">[15]</ref>, we process mini-batches from the source and target datasets separately (per iteration) so that batch normalization uses different normalization statistics for each domain during training. 1 . We do not use the approach of <ref type="bibr" target="#b14">[15]</ref> as-is, as they handle the source and target datasets separtely in two distinct training phases, where our approach must train using both simultaneously. We also do not maintain separate exponential moving averages of the means and variances for each dataset for use at test time.</p><p>As seen in the 'MT+TF' row of <ref type="table" target="#tab_1">Table 1</ref>, the model described thus far achieves state of the art results in 5 out of 8 small image benchmarks. The MNIST → SVHN, STL → CIFAR-10 and Syn-digits → SVHN benchmarks however require additional modifications to achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Confidence thresholding</head><p>We found that replacing the Gaussian ramp-up factor that scales the unsupervised loss with confidence thresholding stabilized training in more challenging domain adaptation scenarios. For each unlabeled sample x T i the teacher network produces the predicted class probabilty vectorz T ij -where j is the class index drawn from the set of classes C -from which we compute the confidencẽ f T i = max j∈C (z T ij ); the predicted probability of the predicted class of the sample. Iff T i is below the confidence threshold (a parameter search found 0.968 to be an effective value for small image benchmarks), the self-ensembling loss for the sample x i is masked to 0.</p><p>Our working hypothesis is that confidence thresholding acts as a filter, shifting the balance in favour of the student learning correct labels from the teacher. While high network prediction confidence does not guarantee correctness there is a positive correlation. Given the tolerance to incorrect labels reported in <ref type="bibr" target="#b13">[14]</ref>, we believe that the higher signal-to-noise ratio underlies the success of this component of our approach.</p><p>The use of confidence thresholding achieves a state of the art results in the STL → CIFAR-10 and Syn-digits → SVHN benchmarks, as seen in the 'MT+CT+TF' row of <ref type="table" target="#tab_1">Table 1</ref>. While confidence thresholding can result in very slight reductions in performance (see the MNIST ↔ USPS and SVHN → MNIST results), its ability to stabilise training in challenging scenarios leads us to recommend it as a replacement for the time-dependent Gaussian ramp-up used in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data augmentation</head><p>We explored the effect of three data augmentation schemes in our small image benchmarks (section 4.1). Our minimal scheme (that should be applicable in non-visual domains) consists of Gaussian noise (with σ = 0.1) added to the pixel values. The standard scheme (indicated by 'TF' in <ref type="table" target="#tab_1">Table 1</ref>) was used in <ref type="bibr" target="#b13">[14]</ref> and adds translations in the interval [−2, 2] and horizontal flips for the CIFAR-10 ↔ STL experiments. The affine scheme (indicated by 'TFA') adds random affine transformations defined by the matrix in <ref type="bibr" target="#b0">(1)</ref>, where N (0, 0.1) denotes a real value drawn from a normal distribution with mean 0 and standard deviation 0.1.</p><formula xml:id="formula_0">1 + N (0, 0.1) N (0, 0.1) N (0, 0.1) 1 + N (0, 0.1)<label>(1)</label></formula><p>The use of translations and horizontal flips has a significant impact in a number of our benchmarks. It is necessary in order to outpace prior art in the MNIST ↔ USPS and SVHN → MNIST benchmarks and improves performance in the CIFAR-10 ↔ STL benchmarks. The use of affine augmentation can improve performance in experiments involving digit and traffic sign recognition datasets, as seen in the 'MT+CT+TFA' row of <ref type="table" target="#tab_1">Table 1</ref>. In contrast it can impair performance when used with photographic datasets, as seen in the the STL → CIFAR-10 experiment. It also impaired performance in the VisDA-17 experiment (section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Class balance loss</head><p>With the adaptations made so far the challenging MNIST → SVHN benchmark remains undefeated due to training instabilities. During training we noticed that the error rate on the SVHN test set decreases at first, then rises and reaches high values before training completes. We diagnosed the problem by recording the predictions for the SVHN target domain samples after each epoch. The rise in error rate correlated with the predictions evolving toward a condition in which most samples are predicted as belonging to the '1' class; the most populous class in the SVHN dataset. We hypothesize that the class imbalance in the SVHN dataset caused the unsupervised loss to reinforce the '1' class more often than the others, resulting in the network settling in a degenerate local minimum. Rather than distinguish between digit classes as intended it seperated MNIST from SVHN samples and assigned the latter to the '1' class.</p><p>We addressed this problem by introducing a class balance loss term that penalises the network for making predictions that exhibit large class imbalance. For each target domain mini-batch we compute the mean of the predicted sample class probabilities over the sample dimension, resulting in the mini-batch mean per-class probability. The loss is computed as the binary cross entropy between the mean class probability vector and a uniform probability vector. We balance the strength of the class balance loss with that of the self-ensembling loss by multiplying the class balance loss by the average of the confidence threshold mask (e.g. if 75% of samples in a mini-batch pass the confidence threshold, then the class balance loss is multiplied by 0.75). <ref type="bibr" target="#b1">2</ref> We would like to note the similarity between our class balance loss and the entropy maximisation loss in the IMSAT clustering model of Hu et al. <ref type="bibr" target="#b10">[11]</ref>; IMSAT employs entropy maximisation to encourage uniform cluster sizes and entropy minimisation to encourage unambiguous cluster assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our implementation was developed using PyTorch ( <ref type="bibr" target="#b2">[3]</ref>) and is publically available at http://github.com/Britefury/self-ensemble-visual-domain-adapt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Small image datasets</head><p>Our results can be seen in <ref type="table" target="#tab_1">Table 1</ref>. The 'train on source' and 'train on target' results report the target domain performance of supervised training on the source and target domains. They represent the exepected baseline and best achievable result. The 'Specific aug.' experiments used data augmentation specific to the MNIST → SVHN adaptation path that is discussed further down.</p><p>The small datasets and data preparation procedures are described in Appendix A. Our training procedure is described in Appendix B and our network architectures are described in Appendix D. The same network architectures and augmentation parameters were used for domain adaptation experiments and the supervised baselines discussed above. It is worth noting that only the training sets of the small image datasets were used during training; the test sets used for reporting scores only.</p><p>MNIST ↔ USPS (see <ref type="figure" target="#fig_3">Figure 3a</ref>). MNIST and USPS are both greyscale hand-written digit datasets. In both adaptation directions our approach not only demonstrates a significant improvement over prior art but nearly achieves the performance of supervised learning using the target domain ground truths. The strong performance of the base mean teacher model can be attributed to the similarity of the datasets to one another. It is worth noting that data augmentation allows our 'train on source' baseline to outpace prior domain adaptation methods.  <ref type="figure" target="#fig_3">Figure 3b</ref>). CIFAR-10 and STL are both 10-class image datasets, although we removed one class from each (see Appendix A.2). We obtained strong performance in the STL → CIFAR-10 path, but only by using confidence thresholding. The CIFAR-10 → STL results are more interesting; the 'train on source' baseline performance outperforms that of a network trained on the STL target domain, most likely due to the small size of the STL training set. Our self-ensembling results outpace both the baseline performance and the 'theoretical maximum' of a network trained on the target domain, lending further evidence to the view of <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b13">[14]</ref> that self-ensembling acts as an effective regulariser.</p><p>Syn-Digits → SVHN (see <ref type="figure" target="#fig_3">Figure 3c</ref>). The Syn-Digits dataset is a synthetic dataset designed by <ref type="bibr" target="#b4">[5]</ref> to be used as a source dataset in domain adaptation experiments with SVHN as the target dataset. Other approaches have achieved good scores on this benchmark, beating the baseline by a significant margin. Our result improves on them, reducing the error rate from 6.9% to 2.9%; even slightly outpacing the 'train on target' 3.4% error rate achieved using supervised learning.</p><p>Syn-Signs → GTSRB (see <ref type="figure" target="#fig_3">Figure 3d</ref>). Syn-Signs is another synthetic dataset designed by <ref type="bibr" target="#b4">[5]</ref> to target the 43-class GTSRB <ref type="bibr" target="#b26">[27]</ref> (German Traffic Signs Recognition Benchmark) dataset. Our approach halved the best error rate of competing approaches. Once again, our approaches slightly outpaces the 'train on target' supervised learning upper bound.</p><p>SVHN → MNIST (see <ref type="figure" target="#fig_3">Figure 3e</ref>). Google's SVHN (Street View House Numbers) is a colour digits dataset of house number plates. Our approach significantly outpaces other techniques and achieves an accuracy close to that of supervised learning.</p><p>MNIST → SVHN (see <ref type="figure" target="#fig_3">Figure 3f</ref>). This adaptation path is somewhat more challenging as MNIST digits are greyscale and uniform in terms of size, aspect ratio and intensity range, in contrast to the variably sized colour digits present in SVHN. As a consequence, adapting from MNIST to SVHN required additional work. Class balancing loss was necessary to ensure training stability and additional experiment specific data augmentation was required to achieve good accuracy. The use of translations and affine augmentation (see section 3.3) results in an accuracy score of 37%. Significant improvements resulted from ad-ditional augmentation in the form of random intensity flips (negative image), and random intensity scales and offsets drawn from the intervals [0.25, 1.5] and [−0.5, 0.5] respectively. These hyper-parameters were selected in order to augment MNIST samples to match the intensity variations present in SVHN, as illustrated in <ref type="figure" target="#fig_3">Figure 3f</ref>. With these additional modifications, we achieve a result that significantly outperforms prior art and nearly achieves the accuracy of a supervised classifier trained on the target dataset. We found that applying these additional augmentations to the source MNIST dataset only yielded good results; applying them to the target SVHN dataset as well yielded a small improvement but was not essential. It should also be noted that this augmentation scheme raises the performance of the 'train on source' baseline to just above that of much of the prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VisDA-2017 visual domain adaptation challenge</head><p>The VisDA-2017 image classification challenge is a 12-class domain adaptation problem consisting of three datasets: a training set consisting of 3D renderings of sketchup models, and validation and test sets consisting of real images (see <ref type="figure" target="#fig_1">Figure 1</ref>) drawn from the COCO <ref type="bibr" target="#b15">[16]</ref> and YouTube BoundingBoxes <ref type="bibr" target="#b19">[20]</ref> datasets respectively. The objective is to learn from labeled computer generated images and correctly predict the class of real images. Ground truth labels were made available for the training and validation sets only; test set scores were computed by a server operated by the competition organisers.</p><p>While the algorithm is that presented above, we base our network on the pretrained ResNet-152 <ref type="bibr" target="#b9">[10]</ref> network provided by PyTorch ( <ref type="bibr" target="#b2">[3]</ref>), rather than using a randomly initialised network as before. The final 1000-class classification layer is removed and replaced with two fully-connected layers; the first has 512 units with a ReLU non-linearity while the final layer has 12 units with a softmax non-linearity. Results from our original competition submissions and newer results using two data augmentation schemes are presented in <ref type="table" target="#tab_2">Table 2</ref>. Our reduced augmentation scheme consists of random crops, random horizontal flips and random uniform scaling. It is very similar to scheme used for ImageNet image classification in <ref type="bibr" target="#b9">[10]</ref>. Our competition configuration includes additional augmentation that was specifically designed for the VisDA dataset, although we subsequently found that it makes little difference. Our hyper-parameters and competition data augmentation scheme are described in Appendix C.1. It is worth noting that we applied test time augmentation (we averaged predictions form 16 differently augmented images) to achieve our competition results. We present resuts with and without test time augmentation in <ref type="table" target="#tab_2">Table 2</ref>. Our VisDA competition test set score is also the result of ensembling the predictions of 5 different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented an effective domain adaptation algorithm that has achieved state of the art results in a number of benchmarks and has achieved accuracies that are almost on par with traditional supervised learning on digit recognition benchmarks targeting the MNIST and SVHN datasets. The resulting networks will exhibit strong performance on samples from both the source and target domains. Our approach is sufficiently flexible to be usable for a variety of network architectures, including those based on randomly initialised and pretrained networks.</p><p>Miyato et al. <ref type="bibr" target="#b17">[18]</ref> stated that the self-ensembling methods of <ref type="bibr" target="#b13">[14]</ref> -on which our algorithm is based -operate by label propagation. This view is supported by our results, in particular our MNIST → SVHN experiment. The latter requires additional intensity augmentation in order to sufficiently align the dataset distributions, after which good quality label predictions are propagated throughout the target dataset. In cases where data augmentation is insufficient to align the dataset distributions, a pre-trained network may be used to bridge the gap, as in our solution to the VisDA-17 challenge. This leads us to conclude that effective domain adaptation can be achieved by first aligning the distributions of the source and target datasets -the focus of much prior art in the field -and then refining their correspondance; a task to which self-ensembling is well suited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and Data Preparation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Small image datasets</head><p>The datasets used in this paper are described in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data preparation</head><p>Some of the experiments that involved datasets described in <ref type="table" target="#tab_3">Table 3</ref> required additional data preparation in order to match the resolution and format of the input samples and match the classification target. These additional steps will now be described.</p><p>MNIST ↔ USPS The USPS images were up-scaled using bilinear interpolation from 16 × 16 to 28 × 28 resolution to match that of MNIST.</p><p>CIFAR-10 ↔ STL CIFAR-10 and STL are both 10-class image datasets. The STL images were down-scaled to 32×32 resolution to match that of CIFAR-10. The 'frog' class in CIFAR-10 and the 'monkey' class in STL were removed as they have no equivalent in the other dataset, resulting in a 9-class problem with 10% less samples in each dataset.</p><p>Syn-Signs → GTSRB GTSRB is composed of images that vary in size and come with annotations that provide region of interest (bounding box around the sign) and ground truth classification. We extracted the region of interest from each image and scaled them to a resolution of 40 × 40 to match those of Syn-Signs.</p><p>MNIST ↔ SVHN The MNIST images were padded to 32 × 32 resolution and converted to RGB by replicating the greyscale channel into the three RGB channels to match the format of SVHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Small image experiment training B.1 Training procedure</head><p>Our networks were trained for 300 epochs. We used the Adam <ref type="bibr" target="#b12">[13]</ref> gradient descent algorithm with a learning rate of 0.001. We trained using mini-batches composed of 256 samples, except in the Syn-digits → SVHN and Syn-signs → GTSRB experiments where we used 128 in order to reduce memory usage. The self-ensembling loss was weighted by a factor of 3 and the class balancing loss was weighted by 0.005. Our teacher network weights t i were updated so as to be an exponential moving average of those of the student s i using the formula t i = αt i−1 + (1 − α)s i , with a value of 0.99 for α. A complete pass over the target dataset was considered to be one epoch in all experiments except the MNIST → USPS and CIFAR-10 → STL experiments due to the small size of the target datasets, in which case one epoch was considered to be a pass over the larger soure dataset.</p><p>We found that using the proportion of samples that passed the confidence threshold can be used to drive early stopping ( <ref type="bibr" target="#b18">[19]</ref>). The final score was the tar-get test set performance at the epoch at which the highest confidence threshold pass rate was obtained.</p><p>C VisDA-17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Hyper-parameters</head><p>Our training procedure was the same as that used in the small image experiments, except that we used 160 × 160 images, a batch size of 56 (reduced from 64 to fit within the memory of an nVidia 1080-Ti), a self-ensembling weight of 10 (instead of 3), a confidence threshold of 0.9 (instead of 0.968) and a class balancing weight of 0.01. We used the Adam <ref type="bibr" target="#b12">[13]</ref> gradient descent algorithm with a learning rate of 10 −5 for the final two randomly initialized layers and 10 −6 for the pre-trained layers. The first convolutional layer and the first group of convolutional layers (with 64 feature channels) of the pre-trained ResNet were left unmodified during training.</p><p>Reduced data augmentation:</p><p>• scale image so that its smallest dimension is 176 pixels, then randomly crop a 160 × 160 section from the scaled image</p><p>• No random affine transformations as they increase confusion between the car and truck classes in the validation set</p><p>• random uniform scaling in the range [0.75, 1.333]</p><p>• horizontal flipping Competition data augmentation adds the following in addition to the above:</p><p>• random intensity/brightness scaling in the range [0.75, 1.333]</p><p>• random rotations, normally distributed with a standard deviation of 0.2π</p><p>• random desaturation in which the colours in an image are randomly desaturated to greyscale by a factor between 0% and 100%</p><p>• rotations in colour space, around a randomly chosen axes with a standard deviation of 0.05π</p><p>• random offset in colour space, after standardisation using parameters specified by         </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) VisDa-17 training set images; the labeled source domain (b) VisDa-17 validation set images; the unlabeled target domain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Images from the VisDA-17 domain adaptation challenge Recent work</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The network structures of the original mean teacher model and our model. Dashed lines in the mean teacher model indicate that ground truth labels -and therefore cross-entropy classification loss -are only available for labeled samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) MNIST ↔ USPS (b) CIFAR-10 ↔ STL (c) Syn-digits → SVHN (d) Syn-signs → GTSRB (e) SVHN → MNIST (f) MNIST (specific augmentation) → SVHN Small image domain adaptation example images CIFAR-10 ↔ STL (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>RevGrad results were available in both<ref type="bibr" target="#b4">[5]</ref> and<ref type="bibr" target="#b5">[6]</ref>; we drew results from both papers to obtain results for all of the experiments shown. MNIST → SVHN specific intensity augmentation as described in Section 4.1. MNIST → SVHN experiments used class balance loss.</figDesc><table><row><cell></cell><cell>USPS</cell><cell cols="2">MNIST SVHN</cell><cell cols="3">MNIST CIFAR STL</cell><cell>Syn</cell><cell>Syn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Digits</cell><cell>Signs</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">MNIST USPS</cell><cell cols="2">MNIST SVHN</cell><cell>STL</cell><cell cols="3">CIFAR SVHN GTSRB</cell></row><row><cell cols="2">TRAIN ON SOURCE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SupSrc *</cell><cell>77.55</cell><cell>82.03</cell><cell>66.5</cell><cell>25.44</cell><cell>72.84</cell><cell>51.88</cell><cell>86.86</cell><cell>96.95</cell></row><row><cell></cell><cell>±0.8</cell><cell>±1.16</cell><cell>±1.93</cell><cell>±2.8</cell><cell>±0.61</cell><cell>±1.44</cell><cell>±0.86</cell><cell>±0.36</cell></row><row><cell>SupSrc+TF</cell><cell>77.53</cell><cell>95.39</cell><cell>68.65</cell><cell>24.86</cell><cell>75.2</cell><cell>59.06</cell><cell>87.45</cell><cell>97.3</cell></row><row><cell></cell><cell>±4.63</cell><cell>±0.93</cell><cell>±1.5</cell><cell>±3.29</cell><cell>±0.28</cell><cell>±1.02</cell><cell>±0.65</cell><cell>±0.16</cell></row><row><cell>SupSrc+TFA</cell><cell>91.97</cell><cell>96.25</cell><cell>71.73</cell><cell>28.69</cell><cell>75.18</cell><cell>59.38</cell><cell>87.16</cell><cell>98.02</cell></row><row><cell></cell><cell>±2.15</cell><cell>±0.54</cell><cell>±5.73</cell><cell>±1.59</cell><cell>±0.76</cell><cell>±0.58</cell><cell>±0.85</cell><cell>±0.20</cell></row><row><cell>Specific aug. b</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.99</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>±3.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevGrad a [1]</cell><cell>74.01</cell><cell>91.11</cell><cell>73.91</cell><cell>35.67</cell><cell>66.12</cell><cell>56.91</cell><cell>91.09</cell><cell>88.65</cell></row><row><cell>DCRN [2]</cell><cell>73.67</cell><cell>91.8</cell><cell>81.97</cell><cell>40.05</cell><cell>66.37</cell><cell>58.65</cell><cell>-</cell><cell>-</cell></row><row><cell>G2A [3]</cell><cell>90.8</cell><cell>92.5</cell><cell>84.70</cell><cell>36.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ADDA [4]</cell><cell>90.1</cell><cell>89.4</cell><cell>76.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ATT [5]</cell><cell>-</cell><cell>-</cell><cell>86.20</cell><cell>52.8</cell><cell>-</cell><cell>-</cell><cell>93.1</cell><cell>96.2</cell></row><row><cell cols="2">SBADA-GAN [6] 97.60</cell><cell>95.04</cell><cell>76.14</cell><cell>61.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ADA [7]</cell><cell>-</cell><cell>-</cell><cell>97.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.86</cell><cell>97.66</cell></row><row><cell>OUR RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MT+TF</cell><cell>98.07</cell><cell>98.26</cell><cell>99.18</cell><cell>13.96 c</cell><cell>80.08</cell><cell>18.3</cell><cell>15.94</cell><cell>98.63</cell></row><row><cell></cell><cell>±2.82</cell><cell>±0.11</cell><cell>±0.12</cell><cell>±4.41</cell><cell>±0.25</cell><cell>±9.03</cell><cell>±0.0</cell><cell>±0.09</cell></row><row><cell>MT+CT *</cell><cell>92.35</cell><cell>88.14</cell><cell>93.33</cell><cell>33.87 c</cell><cell>77.53</cell><cell>71.65</cell><cell>96.01</cell><cell>98.53</cell></row><row><cell></cell><cell>±8.61</cell><cell>±0.34</cell><cell>±5.88</cell><cell>±4.02</cell><cell>±0.11</cell><cell>±0.67</cell><cell>±0.08</cell><cell>±0.15</cell></row><row><cell>MT+CT+TF</cell><cell>97.28</cell><cell>98.13</cell><cell>98.64</cell><cell>34.15 c</cell><cell>79.73</cell><cell>74.24</cell><cell>96.51</cell><cell>98.66</cell></row><row><cell></cell><cell>±2.74</cell><cell>±0.17</cell><cell>±0.42</cell><cell>±3.56</cell><cell>±0.45</cell><cell>±0.46</cell><cell>±0.08</cell><cell>±0.12</cell></row><row><cell cols="2">MT+CT+TFA 99.54</cell><cell>98.23</cell><cell>99.26</cell><cell>37.49 c</cell><cell>80.09</cell><cell>69.86</cell><cell>97.11</cell><cell>99.37</cell></row><row><cell></cell><cell>±0.04</cell><cell>±0.13</cell><cell>±0.05</cell><cell>±2.44</cell><cell>±0.31</cell><cell>±1.97</cell><cell>±0.04</cell><cell>±0.09</cell></row><row><cell>Specific aug. b</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.0 c</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>±0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TRAIN ON TARGET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SupTgt *</cell><cell>99.53</cell><cell>97.29</cell><cell>99.59</cell><cell>95.7</cell><cell>67.75</cell><cell>88.86</cell><cell>95.62</cell><cell>98.49</cell></row><row><cell></cell><cell>±0.02</cell><cell>±0.2</cell><cell>±0.08</cell><cell>±0.13</cell><cell>±2.23</cell><cell>±0.38</cell><cell>±0.2</cell><cell>±0.32</cell></row><row><cell>SupTgt+TF</cell><cell>99.62</cell><cell>97.65</cell><cell>99.61</cell><cell>96.19</cell><cell>70.98</cell><cell>89.83</cell><cell>96.18</cell><cell>98.64</cell></row><row><cell></cell><cell>±0.04</cell><cell>±0.17</cell><cell>±0.04</cell><cell>±0.1</cell><cell>±0.79</cell><cell>±0.39</cell><cell>±0.09</cell><cell>±0.09</cell></row><row><cell>SupTgt+TFA</cell><cell>99.62</cell><cell>97.83</cell><cell>99.59</cell><cell>96.65</cell><cell>70.03</cell><cell>90.44</cell><cell>96.59</cell><cell>99.22</cell></row><row><cell></cell><cell>±0.03</cell><cell>±0.17</cell><cell>±0.06</cell><cell>±0.11</cell><cell>±1.13</cell><cell>±0.38</cell><cell>±0.09</cell><cell>±0.22</cell></row><row><cell>Specific aug. b</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>±0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">[1] [5], [2] [6], [3] [25], [4] [30], [5] [22], [6] [21], [7] [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">PyTorch implementation of ResNet-152</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">D Network architectures</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Our network architectures are shown in Tables 6 -8.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>abc</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Small image benchmark classification accuracy; each result is presented as mean ± standard deviation, computed from 5 independent runs. The abbreviations for components of our models are as follows: MT = mean teacher, CT = confidence thresholding, TF = translation and horizontal flip augmentation, TFA = translation, horizontal flip and affine augmentation, * indicates minimal augmentation. Used test-time augmentation; averaged predictions of 16 differently augmentations versions of each image b Our competition submission ensembled predictions from 5 independently trained networks</figDesc><table><row><cell>VALIDATION PHASE</cell><cell></cell><cell>TEST PHASE</cell><cell></cell></row><row><cell>Team / model</cell><cell>Mean class acc.</cell><cell>Team / model</cell><cell>Mean class acc.</cell></row><row><cell>OTHER TEAMS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bchidlovski [1]</cell><cell>83.1</cell><cell>NLE-DA [1]</cell><cell>87.7</cell></row><row><cell>BUPT OVERFIT</cell><cell>77.8</cell><cell>BUPT OVERFIT</cell><cell>85.4</cell></row><row><cell>Uni. Tokyo MIL [2]</cell><cell>75.4</cell><cell>Uni. Tokyo MIL [2]</cell><cell>82.4</cell></row><row><cell cols="2">OUR COMPETITION RESULTS</cell><cell></cell><cell></cell></row><row><cell>ResNet-50 model</cell><cell>82.8 a</cell><cell>ResNet-152 model</cell><cell>92.8 ab</cell></row><row><cell cols="3">OUR NEWER RESULTS (all using ResNet-152)</cell><cell></cell></row><row><cell>Minimal aug. *</cell><cell>74.2 ±0.86</cell><cell>Minimal aug. *</cell><cell>77.52 ±0.78</cell></row><row><cell>Reduced aug.</cell><cell>85.4 ±0.2</cell><cell>Reduced aug.</cell><cell>91.17 ±0.17</cell></row><row><cell>+ test time aug.</cell><cell>86.6 ±0.18 a</cell><cell>+ test time aug.</cell><cell>92.25 ±0.21 a</cell></row><row><cell>Competition config.</cell><cell>84.29 ±0.24</cell><cell>Competition config.</cell><cell>91.14 ±0.14</cell></row><row><cell>+ test time aug.</cell><cell>85.52 ±0.29 a</cell><cell>+ test time aug.</cell><cell>92.41 ±0.15 a</cell></row><row><cell>[1] [4], [2] [23]</cell><cell></cell><cell></cell><cell></cell></row></table><note>a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>VisDA-17 performance, presented as mean ± std-dev of 5 independent runs. Full results are presented in Tables 4 and 5 in Appendix C. Available from http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip. train.gz and http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip. test.gz b Available from http://ai.stanford.edu/~acoates/stl10/ c Available from Ganin's website at http://yaroslav.ganin.net/</figDesc><table><row><cell></cell><cell cols="4"># train # test # classes Target</cell><cell cols="2">Resolution Channels</cell></row><row><cell>USPS a</cell><cell>7,291</cell><cell>2,007</cell><cell>10</cell><cell>Digits</cell><cell>16 × 16</cell><cell>Mono</cell></row><row><cell>MNIST</cell><cell>60,000</cell><cell>10,000</cell><cell>10</cell><cell>Digits</cell><cell>28 × 28</cell><cell>Mono</cell></row><row><cell>SVHN</cell><cell>73,257</cell><cell>26,032</cell><cell>10</cell><cell>Digits</cell><cell>32 × 32</cell><cell>RGB</cell></row><row><cell>CIFAR-10</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell><cell>Object ID</cell><cell>32 × 32</cell><cell>RGB</cell></row><row><cell>STL b</cell><cell>5,000</cell><cell>8,000</cell><cell>10</cell><cell>Object ID</cell><cell>96 × 96</cell><cell>RGB</cell></row><row><cell cols="2">Syn-Digits c 479,400</cell><cell>9,553</cell><cell>10</cell><cell>Digits</cell><cell>32 × 32</cell><cell>RGB</cell></row><row><cell>Syn-Signs</cell><cell cols="2">100,000 -</cell><cell>43</cell><cell cols="2">Traffic signs 40 × 40</cell><cell>RGB</cell></row><row><cell>GTSRB</cell><cell>32,209</cell><cell>12,630</cell><cell>43</cell><cell cols="2">Traffic signs varies</cell><cell>RGB</cell></row><row><cell>a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : datasets</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Plane</cell><cell cols="2">Bicycle Bus</cell><cell>Car</cell><cell>Horse</cell><cell>Knife</cell><cell></cell></row><row><cell cols="2">COMPETITION RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50</cell><cell>96.3</cell><cell>87.9</cell><cell>84.7</cell><cell>55.7</cell><cell>95.9</cell><cell>95.2</cell><cell></cell></row><row><cell cols="3">NEWER RESULTS (ResNet-152)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Minimal aug</cell><cell>92.94</cell><cell>84.88</cell><cell>71.56</cell><cell>41.24</cell><cell>88.85</cell><cell>92.40</cell><cell></cell></row><row><cell></cell><cell>±0.52</cell><cell>±0.73</cell><cell cols="2">±3.08 ±1.01</cell><cell cols="2">±1.31 ±1.14</cell><cell></cell></row><row><cell>Reduced aug</cell><cell>96.19</cell><cell>87.83</cell><cell>84.38</cell><cell>66.47</cell><cell>96.07</cell><cell>96.06</cell><cell></cell></row><row><cell></cell><cell>±0.17</cell><cell>±1.62</cell><cell cols="2">±0.92 ±4.53</cell><cell cols="2">±0.28 ±0.62</cell><cell></cell></row><row><cell>+ test time aug</cell><cell>97.13</cell><cell>89.28</cell><cell>84.93</cell><cell>67.67</cell><cell>96.54</cell><cell>97.48</cell><cell></cell></row><row><cell></cell><cell>±0.18</cell><cell>±1.45</cell><cell cols="2">±1.09 ±4.66</cell><cell cols="2">±0.36 ±0.43</cell><cell></cell></row><row><cell cols="2">Competition config. 95.93</cell><cell>87.36</cell><cell>85.22</cell><cell>58.56</cell><cell>96.23</cell><cell>95.65</cell><cell></cell></row><row><cell></cell><cell>±0.29</cell><cell>±1.19</cell><cell cols="2">±0.86 ±1.81</cell><cell cols="2">±0.18 ±0.60</cell><cell></cell></row><row><cell>+ test time aug</cell><cell>96.89</cell><cell>89.06</cell><cell>85.51</cell><cell>59.73</cell><cell>96.59</cell><cell>97.55</cell><cell></cell></row><row><cell></cell><cell>±0.32</cell><cell>±1.24</cell><cell cols="2">±0.83 ±1.96</cell><cell cols="2">±0.13 ±0.48</cell><cell></cell></row><row><cell></cell><cell cols="2">M.cycle Person</cell><cell>Plant</cell><cell cols="2">Sk.brd Train</cell><cell>Truck</cell><cell>Mean Class Acc.</cell></row><row><cell cols="2">COMPETITION RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50</cell><cell>88.6</cell><cell>77.4</cell><cell>93.3</cell><cell>92.8</cell><cell>87.5</cell><cell>38.2</cell><cell>82.8</cell></row><row><cell cols="3">NEWER RESULTS (ResNet-152)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Minimal aug</cell><cell>67.51</cell><cell>63.46</cell><cell>84.47</cell><cell>71.84</cell><cell>83.22</cell><cell>48.09</cell><cell>74.20</cell></row><row><cell></cell><cell>±1.79</cell><cell>±1.72</cell><cell cols="2">±1.22 ±5.40</cell><cell cols="3">±0.73 ±1.41 ±0.86</cell></row><row><cell>Reduced aug</cell><cell>90.49</cell><cell>81.45</cell><cell>95.27</cell><cell>91.48</cell><cell>87.54</cell><cell>51.60</cell><cell>85.40</cell></row><row><cell></cell><cell>±0.27</cell><cell>±0.90</cell><cell cols="2">±0.36 ±0.76</cell><cell cols="3">±1.16 ±2.35 ±0.20</cell></row><row><cell>+ test time aug</cell><cell>90.99</cell><cell>83.33</cell><cell>96.12</cell><cell>94.69</cell><cell>88.53</cell><cell>52.54</cell><cell>86.60</cell></row><row><cell></cell><cell>±0.37</cell><cell>±0.91</cell><cell cols="2">±0.32 ±0.71</cell><cell cols="3">±1.20 ±2.82 ±0.24</cell></row><row><cell cols="2">Competition config. 90.60</cell><cell>80.03</cell><cell>94.79</cell><cell>90.77</cell><cell>88.42</cell><cell>47.90</cell><cell>84.29</cell></row><row><cell></cell><cell>±1.08</cell><cell>±1.23</cell><cell cols="2">±0.35 ±0.65</cell><cell cols="3">±0.87 ±2.16 ±0.24</cell></row><row><cell>+ test time aug</cell><cell>91.00</cell><cell>81.59</cell><cell>95.58</cell><cell>94.29</cell><cell>89.28</cell><cell>49.21</cell><cell>85.52</cell></row><row><cell></cell><cell>±1.17</cell><cell>±1.20</cell><cell cols="2">±0.38 ±0.63</cell><cell cols="3">±0.85 ±2.26 ±0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Full VisDA-17 validation set results</figDesc><table><row><cell></cell><cell>Plane</cell><cell cols="2">Bicycle Bus</cell><cell>Car</cell><cell>Horse</cell><cell>Knife</cell><cell></cell></row><row><cell cols="4">COMPETITION RESULTS (ensemble of 5 models)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-152</cell><cell>96.9</cell><cell>92.4</cell><cell>92.0</cell><cell>97.2</cell><cell>95.2</cell><cell>98.8</cell><cell></cell></row><row><cell cols="3">NEWER RESULTS (ResNet-152)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Minimal aug</cell><cell>88.44</cell><cell>84.80</cell><cell>75.08</cell><cell>84.08</cell><cell>79.95</cell><cell>72.62</cell><cell></cell></row><row><cell></cell><cell>±1.37</cell><cell>±1.81</cell><cell cols="2">±1.63 ±2.28</cell><cell cols="2">±1.93 ±7.98</cell><cell></cell></row><row><cell>Reduced aug</cell><cell>95.63</cell><cell>89.90</cell><cell>91.44</cell><cell>96.18</cell><cell>94.17</cell><cell>96.51</cell><cell></cell></row><row><cell></cell><cell>±0.61</cell><cell>±0.64</cell><cell cols="2">±0.34 ±0.63</cell><cell cols="2">±0.25 ±0.41</cell><cell></cell></row><row><cell>+ test time aug</cell><cell>96.72</cell><cell>91.67</cell><cell>92.21</cell><cell>96.41</cell><cell>94.72</cell><cell>98.03</cell><cell></cell></row><row><cell></cell><cell>±0.59</cell><cell>±0.73</cell><cell cols="2">±0.45 ±0.65</cell><cell cols="2">±0.21 ±0.40</cell><cell></cell></row><row><cell cols="2">Competition config. 95.13</cell><cell>90.09</cell><cell>91.21</cell><cell>96.94</cell><cell>94.39</cell><cell>96.87</cell><cell></cell></row><row><cell></cell><cell>±0.39</cell><cell>±0.37</cell><cell cols="2">±0.82 ±0.34</cell><cell cols="2">±0.48 ±0.33</cell><cell></cell></row><row><cell>+ test time aug</cell><cell>96.48</cell><cell>91.96</cell><cell>91.92</cell><cell>97.22</cell><cell>95.12</cell><cell>98.44</cell><cell></cell></row><row><cell></cell><cell>±0.31</cell><cell>±0.38</cell><cell cols="2">±0.65 ±0.36</cell><cell cols="2">±0.52 ±0.13</cell><cell></cell></row><row><cell></cell><cell cols="2">M.cycle Person</cell><cell>Plant</cell><cell cols="2">Sk.brd Train</cell><cell>Truck</cell><cell>Mean Class Acc.</cell></row><row><cell cols="4">COMPETITION RESULTS (ensemble of 5 models)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-152</cell><cell>86.3</cell><cell>75.3</cell><cell>97.7</cell><cell>93.3</cell><cell>94.5</cell><cell>93.3</cell><cell>92.8</cell></row><row><cell cols="3">NEWER RESULTS (ResNet-152)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Minimal aug</cell><cell>63.60</cell><cell>56.59</cell><cell>95.40</cell><cell>73.79</cell><cell>77.57</cell><cell>78.33</cell><cell>77.52</cell></row><row><cell></cell><cell>±1.55</cell><cell>±1.73</cell><cell cols="2">±0.52 ±5.43</cell><cell cols="3">±1.76 ±3.12 ±0.78</cell></row><row><cell>Reduced aug</cell><cell>85.02</cell><cell>71.31</cell><cell>97.35</cell><cell>91.11</cell><cell>92.42</cell><cell>93.03</cell><cell>91.17</cell></row><row><cell></cell><cell>±0.83</cell><cell>±0.97</cell><cell cols="2">±0.49 ±1.05</cell><cell cols="3">±0.46 ±0.36 ±0.17</cell></row><row><cell>+ test time aug</cell><cell>85.40</cell><cell>73.19</cell><cell>97.84</cell><cell>93.53</cell><cell>93.31</cell><cell>93.91</cell><cell>92.25</cell></row><row><cell></cell><cell>±1.08</cell><cell>±0.86</cell><cell cols="2">±0.45 ±0.71</cell><cell cols="3">±0.35 ±0.39 ±0.21</cell></row><row><cell cols="2">Competition config. 85.12</cell><cell>70.78</cell><cell>97.22</cell><cell>90.39</cell><cell>93.18</cell><cell>92.38</cell><cell>91.14</cell></row><row><cell></cell><cell>±1.30</cell><cell>±1.53</cell><cell cols="2">±0.19 ±0.64</cell><cell cols="3">±0.49 ±0.52 ±0.14</cell></row><row><cell>+ test time aug</cell><cell>85.75</cell><cell>74.06</cell><cell>97.77</cell><cell>92.91</cell><cell>94.21</cell><cell>93.09</cell><cell>92.41</cell></row><row><cell></cell><cell>±1.20</cell><cell>±1.69</cell><cell cols="2">±0.16 ±0.45</cell><cell cols="3">±0.52 ±0.44 ±0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Full VisDA-17 test set results</figDesc><table><row><cell>Description</cell><cell>Shape</cell></row><row><cell>28 × 28 Mono image</cell><cell>28 × 28 × 1</cell></row><row><cell>Conv 5 × 5 × 32, batch norm</cell><cell>24 × 24 × 32</cell></row><row><cell>Max-pool, 2x2</cell><cell>12 × 12 × 32</cell></row><row><cell>Conv 3 × 3 × 64, batch norm</cell><cell>10 × 10 × 64</cell></row><row><cell>Conv 3 × 3 × 64, batch norm</cell><cell>8 × 8 × 64</cell></row><row><cell>Max-pool, 2x2</cell><cell>4 × 4 × 64</cell></row><row><cell>Dropout, 50%</cell><cell>4 × 4 × 64</cell></row><row><cell>Fully connected, 256 units</cell><cell>256</cell></row><row><cell cols="2">Fully connected, 10 units, softmax 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>MNIST ↔ USPS architecture × 3 × 128, pad 1, batch norm 32 × 32 × 128 Conv 3 × 3 × 128, pad 1, batch norm 32 × 32 × 128 Conv 3 × 3 × 128, pad 1, batch norm 32 × 32 × 128 Max-pool, 2x2 16 × 16 × 128 Dropout, 50% 16 × 16 × 128 Conv 3 × 3 × 256, pad 1, batch norm 16 × 16 × 256 Conv 3 × 3 × 256, pad 1, batch norm 16 × 16 × 256 Conv 3 × 3 × 256, pad 1, batch norm 16 × 16 × 256</figDesc><table><row><cell>Description</cell><cell>Shape</cell></row><row><cell>32 × 32 RGB image</cell><cell>32 × 32 × 3</cell></row><row><cell>Conv 3 Max-pool, 2x2</cell><cell>8 × 8 × 256</cell></row><row><cell>Dropout, 50%</cell><cell>8 × 8 × 256</cell></row><row><cell cols="2">Conv 3 × 3 × 512, pad 0, batch norm 6 × 6 × 512</cell></row><row><cell>Conv 1 × 1 × 256, batch norm</cell><cell>6 × 6 × 256</cell></row><row><cell>Conv 1 × 1 × 128, batch norm</cell><cell>6 × 6 × 128</cell></row><row><cell>Global pooling layer</cell><cell>1 × 1 × 128</cell></row><row><cell>Fully connected, 10 units, softmax</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>MNIST ↔ SVHN, CIFAR-10 ↔ STL and Syn-Digits → SVHN architecture × 3 × 192, pad 1, batch norm 20 × 20 × 192 Conv 3 × 3 × 192, pad 1, batch norm 20 × 20 × 192 Conv 3 × 3 × 192, pad 1, batch norm 20 × 20 × 192 Max-pool, 2x2 10 × 10 × 192 Dropout, 50% 10 × 10 × 192 Conv 3 × 3 × 384, pad 1, batch norm 10 × 10 × 384 Conv 3 × 3 × 384, pad 1, batch norm 10 × 10 × 384 Conv 3 × 3 × 384, pad 1, batch norm 10 × 10 × 384</figDesc><table><row><cell>Description</cell><cell>Shape</cell></row><row><cell>40 × 40 RGB image</cell><cell>40 × 40 × 3</cell></row><row><cell>Conv 3 × 3 × 96, pad 1, batch norm</cell><cell>40 × 40 × 96</cell></row><row><cell>Conv 3 × 3 × 96, pad 1, batch norm</cell><cell>40 × 40 × 96</cell></row><row><cell>Conv 3 × 3 × 96, pad 1, batch norm</cell><cell>40 × 40 × 96</cell></row><row><cell>Max-pool, 2x2</cell><cell>20 × 20 × 96</cell></row><row><cell>Dropout, 50%</cell><cell>20 × 20 × 96</cell></row><row><cell>Conv 3 Max-pool, 2x2</cell><cell>5 × 5 × 384</cell></row><row><cell>Dropout, 50%</cell><cell>5 × 5 × 384</cell></row><row><cell>Global pooling layer</cell><cell>1 × 1 × 384</cell></row><row><cell>Fully connected, 43 units, softmax</cell><cell>43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Syn-signs → GTSRB architecture</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is simple to implement using most neural network toolkits; evaluate the network once for source samples and a second time for target samples, compute the supervised and unsupervised losses respectively and combine.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We expect that class balance loss is likely to adversely affect performance on target datasets with large class imbalance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded by a grant from Marine Scotland.</p><p>We would also like to thank nVidia corporation for their generous donation of a Titan X GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dilip Krishnan, and Dumitru Erhan. Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Boris Chidlovskii, and Stephane Clinchant. Discrepancy-based networks for unsupervised domain adaptation: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Schi-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03976</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Xin Pan, and Vincent Vanhoucke. Youtube-boundingboxes: A large high-precision humanannotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<idno>abs/1702.00824</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08824</idno>
		<title level="m">From source to target and back: symmetric bi-directional adaptive gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01575</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial dropout regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01705</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The German Traffic Sign Recognition Benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05464</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial discriminative domain adaptation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
