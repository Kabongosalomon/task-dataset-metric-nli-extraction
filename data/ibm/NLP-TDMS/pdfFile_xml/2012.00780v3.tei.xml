<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 REFINING DEEP GENERATIVE MODELS VIA DISCRIMINATOR GRADIENT FLOW</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Fatir</forename><surname>Ansari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
							<email>angmingliang@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Soh</surname></persName>
							<email>harold@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 REFINING DEEP GENERATIVE MODELS VIA DISCRIMINATOR GRADIENT FLOW</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient f low (DGf low), a new technique that improves generated samples via the gradient flow of entropy-regularized f -divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS &amp; MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DGf low leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep generative models (DGMs) have excelled at numerous tasks, from generating realistic images <ref type="bibr" target="#b10">(Brock et al., 2019)</ref> to learning policies in reinforcement learning <ref type="bibr" target="#b23">(Ho &amp; Ermon, 2016)</ref>. Among the variety of proposed DGMs, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref> have received widespread popularity for their ability to generate high quality samples that resemble real data. Unlike Variational Autoencoders (VAEs) <ref type="bibr" target="#b26">(Kingma &amp; Welling, 2014)</ref> and <ref type="bibr">Normalizing Flows (Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b27">Kingma &amp; Dhariwal, 2018)</ref>, GANs are likelihood-free methods; training is formulated as a minimax optimization problem involving a generator and a discriminator. The generator seeks to generate samples that are similar to the real data by minimizing a measure of discrepancy (between the generated samples and real samples) furnished by the discriminator. The discriminator is trained to distinguish the generated samples from the real samples. Once trained, the generator is used to simulate samples and the discriminator has traditionally been discarded.</p><p>However, recent work has shown that discarding the discriminator is wasteful -it actually contains useful information about the underlying data distribution. This insight has led to sample improvement techniques that use this information to improve the quality of generated samples <ref type="bibr" target="#b5">(Azadi et al., 2019;</ref><ref type="bibr" target="#b46">Turner et al., 2019;</ref><ref type="bibr" target="#b45">Tanaka, 2019;</ref><ref type="bibr" target="#b11">Che et al., 2020)</ref>. Unfortunately, current methods either rely on wasteful rejection operations in the data space <ref type="bibr" target="#b5">(Azadi et al., 2019;</ref><ref type="bibr" target="#b46">Turner et al., 2019)</ref>, or require a sensitive diffusion term to ensure sample diversity <ref type="bibr" target="#b11">(Che et al., 2020)</ref>. Prior work has also focused on improving GANs with scalar-valued discriminators, which excludes a large family of GANs with vector-valued critics, e.g., MMDGAN <ref type="bibr" target="#b29">(Li et al., 2017;</ref><ref type="bibr" target="#b8">Bińkowski et al., 2018)</ref> and <ref type="bibr">OCFGAN (Ansari et al., 2020)</ref>, and likelihood-based generative models.</p><p>Published as a conference paper at ICLR 2021     In this work, we propose Discriminator Gradient f low (DGf low) which formulates sample improvement as refining inferior samples using the gradient flow of fdivergences between the generator and the real data distributions ( <ref type="figure">Fig. 1)</ref>. DGf low avoids wasteful rejection operations and can be used in a deterministic setting without a diffusion term. Existing state-of-the-art methods -specifically, Discriminator Optimal Transport (DOT) <ref type="bibr" target="#b45">(Tanaka, 2019)</ref> and Discriminator Driven Latent Sampling (DDLS) (Che et al., 2020) -can be viewed as special cases of DGf low. Similar to DDLS, DGf low recovers the real data distribution when the gradient flow is simulated exactly.</p><formula xml:id="formula_0">Z o F F K f Y q S 0 1 O m L M R 9 c D o o l q 2 x l M P 8 S e 0 F K Z x + Q o T U o v v W H H M c B C R V m S M q e b U X K T Z B Q F D M y K / R j S S K E J 2 h E e p q G K C D S T b J r Z + a R V o a m z 4 W u U J m Z + n 0 i Q Y G U 0 8 D T n Q F S Y / n b S 8 X / v F 6 s / L q b 0 D C K F Q n x f J E f M 1 N x M 3 3 d H F J B s G J T T R A W V N 9 q 4 j E S C C s d U C E L o V G 1 H c f R v z d O n c p J I y V 1 x 6 r W v k L o V M q 2 U 7 a u q q V m a 5 4 G 5 O E A D u E Y b K h B E y 6 g B W 3 A c A v 3 8 A h P B j c e j G f j Z d 6 a M x Y z + / A D x u s n O 7 i Q p g = = &lt; / l a t e x i t &gt; ⇢ N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 W Q 0 o v C F 7 X Q M T m x O c a I q J k p w B 3 U = " &gt; A A A B 6 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q 2 t l 1 Z c O O y g n 1 A G 8 p k O m 2 H z k z C z E Q o o b / g x o U i b v 0 h d / 6 F n 2 C S F v F 1 4 M L h n H u 5 9 x 4 / 5 E w b 2 3 6 3 V l b X 1 j c 2 c 1 v 5 7 Z 3 d v f 3 C w W F b B 5 E i t E U C H q i u j z X l T N K W Y Y b T b q g o F j 6 n H X 9 6 l f q d O 6 o 0 C + S t m Y X U E 3 g s 2 Y g R b F K p r 5 k Y F I p 2 y c 6 A / h J n S Y q X H 5 C h O S i 8 9 Y c B i Q S V h n C s d c + x Q + P F W B l G O J 3 n + 5 G m I S Z T P K a 9 h E o s q P b i 7 N Y 5 O k 2 U I R o F K i l p U K Z + n 4 i x 0 H o m / K R T Y D P R v 7 1 U / M / r R W Z U 8 2 I m w 8 h Q S R a L R h F H J k D p 4 2 j I F C W G z x K C i W L J r Y h M s M L E J P H k s x D q F c d 1 3 e T 3 + o V b P q + n p O b a l e p X C O 1 y y X F L 9 k 2 l 2 G g u 0 o A c H M M J n I E D V W j A N T S h B Q Q m c A + P 8 G Q J 6 8 F 6 t l 4 W r S v W c u Y I f s B 6 / Q T n 6 o / l &lt; / l a t e x i t &gt; ⇠ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 W Q 0 o v C F 7 X Q M T m x O c a I q J k p w B 3 U = " &gt; A A A B 6 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q 2 t l 1 Z c O O y g n 1 A G 8 p k O m 2 H z k z C z E Q o o b / g x o U i b v 0 h d / 6 F n 2 C S F v F 1 4 M L h n H u 5 9 x 4 / 5 E w b 2 3 6 3 V l b X 1 j c 2 c 1 v 5 7 Z 3 d v f 3 C w W F b B 5 E i t E U C H q i u j z X l T N K W Y Y b T b q g o F j 6 n H X 9 6 l f q d O 6 o 0 C + S t m Y X U E 3 g s 2 Y g R b F K p r 5 k Y F I p 2 y c 6 A / h J n S Y q X H 5 C h O S i 8 9 Y c B i Q S V h n C s d c + x Q + P F W B l G O J 3 n + 5 G m I S Z T P K a 9 h E o s q P b i 7 N Y 5 O k 2 U I R o F K i l p U K Z + n 4 i x 0 H o m / K R T Y D P R v 7 1 U / M / r R W Z U 8 2 I m w 8 h Q S R a L R h F H J k D p 4 2 j I F C W G z x K C i W L J r Y h M s M L E J P H k s x D q F c d 1 3 e T 3 + o V b P q + n p O b a l e p X C O 1 y y X F L 9 k 2 l 2 G g u 0 o A c H M M J n I E D V W j A N T S h B Q Q m c A + P 8 G Q J 6 8 F 6 t l 4 W r S v W c u Y I f s B 6 / Q T n 6 o / l &lt; / l a t e x i t &gt; ⇠ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 W Q 0 o v C F 7 X Q M T m x O c a I q J k p w B 3 U = " &gt; A A A B 6 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q 2 t l 1 Z c O O y g n 1 A G 8 p k O m 2 H z k z C z E Q o o b / g x o U i b v 0 h d / 6 F n 2 C S F v F 1 4 M L h n H u 5 9 x 4 / 5 E w b 2 3 6 3 V l b X 1 j c 2 c 1 v 5 7 Z 3 d v f 3 C w W F b B 5 E i t E U C H q i u j z X l T N K W Y Y b T b q g o F j 6 n H X 9 6 l f q d O 6 o 0 C + S t m Y X U E 3 g s 2 Y g R b F K p r 5 k Y F I p 2 y c 6 A / h J n S Y q X H 5 C h O S i 8 9 Y c B i Q S V h n C s d c + x Q + P F W B l G O J 3 n + 5 G m I S Z T P K a 9 h E o s q P b i 7 N Y 5 O k 2 U I R o F K i l p U K Z + n 4 i x 0 H o m / K R T Y D P R v 7 1 U / M / r R W Z U 8 2 I m w 8 h Q S R a L R h F H J k D p 4 2 j I F C W G z x K C i W L J r Y h M s M L E J P H k s x D q F c d 1 3 e T 3 + o V b P q + n p O b a l e p X C O 1 y y X F L 9 k 2 l 2 G g u 0 o A c H M M J n I E D V W j A N T S h B Q Q m c A + P 8 G Q J 6 8 F 6 t l 4 W r S v W c u Y I f s B 6 / Q T n 6 o / l &lt; / l a t e x i t &gt; ⇠ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 W Q 0 o v C F 7 X Q M T m x O c a I q J k p w B 3 U = " &gt; A A A B 6 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q 2 t l 1 Z c O O y g n 1 A G 8 p k O m 2 H z k z C z E Q o o b / g x o U i b v 0 h d / 6 F n 2 C S F v F 1 4 M L h n H u 5 9 x 4 / 5 E w b 2 3 6 3 V l b X 1 j c 2 c 1 v 5 7 Z 3 d v f 3 C w W F b B 5 E i t E U C H q i u j z X l T N K W Y Y b T b q g o F j 6 n H X 9 6 l f q d O 6 o 0 C + S t m Y X U E 3 g s 2 Y g R b F K p r 5 k Y F I p 2 y c 6 A / h J n S Y q X H 5 C h O S i 8 9 Y c B i Q S V h n C s d c + x Q + P F W B l G O J 3 n + 5 G m I S Z T P K a 9 h E o s q P b i 7 N Y 5 O k 2 U I R o F K i l p U K Z + n 4 i x 0 H o m / K R T Y D P R v 7 1 U / M / r R W Z U 8 2 I m w 8 h Q S R a L R h F H J k D p 4 2 j I F C W G z x K C i W L J r Y h M s M L E J P H k s x D q F c d 1 3 e T 3 + o V b P q + n p O b a l e p X C O 1 y y X F L 9 k 2 l 2 G g u 0 o A c H M M J n I E D V W j A N T S h B Q Q m c A + P 8 G Q J 6 8 F 6 t l 4 W r S v W c u Y I f s B 6 / Q T n 6 o / l &lt; / l a t e x i t &gt; ⇠ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 W Q 0 o v C F 7 X Q M T m x O c a I q J k p w B 3 U = " &gt; A A A B 6 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q 2 t l 1 Z c O O y g n 1 A G 8 p k O m 2 H z k z C z E Q o o b / g x o U i b v 0 h d / 6 F n 2 C S F v F 1 4 M L h n H u 5 9 x 4 / 5 E w b 2 3 6 3 V l b X 1 j c 2 c 1 v 5 7 Z 3 d v f 3 C w W F b B 5 E i t E U C H q i u j z X l T N K W Y Y b T b q g o F j 6 n H X 9 6 l f q d O 6 o 0 C + S t m Y X U E 3 g s 2 Y g R b F K p r 5 k Y F I p 2 y c 6 A / h J n S Y q X H 5 C h O S i 8 9 Y c B i Q S V h n C s d c + x Q + P F W B l G O J 3 n + 5 G m I S Z T P K a 9 h E o s q P b i 7 N Y 5 O k 2 U I R o F K i l p U K Z + n 4 i x 0 H o m / K R T Y D P R v 7 1 U / M / r R W Z U 8 2 I m w 8 h Q S R a L R h F H J k D p 4 2 j I F C W G z x K C i W L J r Y h</formula><formula xml:id="formula_1">F B P D j A b 3 M n f p C K v T g 5 s = " &gt; A A A B 7 H i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S l I b 2 6 4 s u H E l F U x b a E O Z T K f t 0 M k k z E y E G v o N b l w o 4 t Y P c u d f + A l O 0 i K + D l w 4 n H M v 9 9 7 j R 4 x K Z V n v x t L y y u r a e m 4 j v 7 m 1 v b N b 2 N t v y T A W m L g 4 Z K H o + E g S R j l x F V W M d C J B U O A z 0 v Y n F 6 n f v i V C 0 p D f q G l E v A C N O B 1 S j J S W 3 L t + c j X r F 4 p W y c p g / i X 2 g h T P P y B D s 1 9 4 6 w 1 C H A e E K 8 y Q l F 3 b i p S X I K E o Z m S W 7 8 W S R A h P 0 I h 0 N e U o I N J L s m N n 5 r F W B u Y w F L q 4 M j P 1 + 0 S C A i m n g a 8 7 A 6 T G 8 r e X i v 9 5 3 V g N a 1 5 C e R Q r w v F 8 0 T B m p g r N 9 H N z Q A X B i k 0 1 Q V h Q f a u J x 0 g g r H Q + + S y E e s V 2 H E f / X j 9 z y q f 1 l N Q c q 1 L 9 C q F V L t l O y b q u F B v N e R q Q g 0 M 4 g h O w o Q o N u I Q m u I C B w j 0 8 w p P B j Q f j 2 X i Z t y 4 Z i 5 k D + A H j 9 R P H s Z B p &lt; / l a t e x i t &gt; z N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m F F G x i 1 f T Y 2 G X k w g c O N w 6 Q g B + f I = " &gt; A A A B 8 n i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S l I b 2 6 4 s u H F Z w T 4 0 D W U y n b Z D J 5 k w M x F K 6 G e 4 c a G I W 7 / G n X / h J z h J i / g 6 M H A 4 5 1 7 m 3 O N H j E p l W e / G 0 v L K 6 t p 6 b i O / u b W 9 s 1 v Y 2 2 9 L H g t M W p g z L r o + k o T R k L Q U V Y x 0 I 0 F Q 4 D P S 8 S c X q d + 5 I 0 J S H l 6 r a U S 8 A I 1 C O q Q Y K S 2 5 v Q C p M U Y s u Z 3 1 C 0 W r Z G U w / x J 7 Q Y r n H 5 C h 2 S + 8 9 Q Y c x w E J F W Z I S t e 2 I u U l S C i K G Z n l e 7 E k E c I T N C K u p i E K i P S S L P L M P N b K w B x y o V + o z E z 9 v p G g Q M p p 4 O v J N K L 8 7 a X i f 5 4 b q 2 H N S 2 g Y x Y q E</formula><formula xml:id="formula_2">M = " &gt; A A A C c n i c b V H d b t M w G H X C 3 y h / 3 R A 3 I I G h Q r Q a q 5 L S s P U C M c E N l 0 O i 2 6 S 6 R F 8 c p 7 V m J 8 F 2 E M X K A / B 6 3 P E U c M E D 4 K Q T K j + f Z P n o n P P Z n 4 + T U n B t g u C b 5 1 + 4 e O n y l a 2 r n W v X b 9 y 8 1 d 3 e O d Z F p S i b 0 k I U 6 j Q B z Q T P 2 d R w I 9 h p q R j I R L C T 5 O x 1 o 5 9 8 Z E r z I n 9 n V i W b S 1 j k P O M U j K P i 7 h c i w S y T z H 6 u Y 8 t 3 w x q / w B s U x 3 u Y M A M k h 0 R A b D e l O n v S Z + / t X h q T c s n 7 i 5 i Y p b P 2 N z 2 D Q T 3 A u 5 j o D 8 r Y U X v S A q S E m i S F S P V K u s 2 S T 9 x Z 4 2 4 v G A Z t 4 X 9 B e</formula><formula xml:id="formula_3">D M v p + q K s E t g U u M k f p 1 w x a s T K A a C K u 1 k x X Y I C a t w v d d o Q J u M w i i L 3 9 s n z a P R s 0 o C D K B j v / w 7 h e D Q M o 2 H w d t w 7 f L V O A 2 2 h e + g R 6 q M Q 7 a N D 9 A Y d o S m i 6 L t 3 x 7 v v P f B + + n f 9 h 3 5 v b f W 9 8 5 7 b 6 I / y n / 4 C v k X A g Q = = &lt; / l a t e x i t &gt; z i+1 = z i ⌘r zi f 0 (e d (g✓(zi)) ) + p 2⌘ ⇠ i</formula><p>We further present a generalized framework that employs existing pre-trained discriminators to refine samples from a variety of deep generative models: we demonstrate our method can be applied to GANs with vector-valued critics, and even likelihood-based models such as VAEs and Normalizing Flows. Empirical results on synthetic datasets, and benchmark image (CIFAR10, STL10) and text (Billion Words) datasets demonstrate that our gradient flow-based approach outperforms DOT and DDLS on multiple quantitative evaluation metrics.</p><p>In summary, this paper's key contributions are:</p><p>• DGf low, a method to refine deep generative models using the gradient flow of f -divergences;</p><p>• a framework that extends DGf low to GANs with vector-valued critics, VAEs, and Normalizing Flows; • experiments on a variety of generative models trained on synthetic, image (CIFAR10 &amp; STL10), and text (Billion Words) datasets demonstrating that DGf low is effective in improving samples from generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: GRADIENT FLOWS</head><p>The following gives a brief introduction to gradient flows; we refer readers to the excellent overview by Santambrogio (2017) for a more thorough introduction.</p><p>Let (X , · 2 ) be a Euclidean space and F : X → R be a smooth energy function. The gradient flow of F is the smooth curve {x t } t∈R+ that follows the direction of steepest descent, i.e.,</p><formula xml:id="formula_4">x (t) = −∇F (x(t)).</formula><p>(1) The value of the energy F is minimized along this curve. This idea of steepest descent curves can be characterized in arbitrary metric spaces via the minimizing movement scheme <ref type="bibr" target="#b24">(Jordan et al., 1998)</ref>. Of particular interest is the metric space of probability measures that is endowed with the Wasserstein distance (W p ); the Wasserstein distance is a metric and the W p topology satisfies weak convergence of probability measures <ref type="bibr">(Villani, 2008, Theorem 6.9)</ref>. Gradient flows in the 2-Wasserstein space (P 2 (Ω), W 2 ) -i.e., the space of probability measures with finite second moments and the 2-Wasserstein metric -have been studied extensively. Let {ρ t } t∈R+ be the gradient flow of a functional F in the 2-Wasserstein space, where ρ t is absolutely continuous with respect to the Lebesgue measure. The curve {ρ t } t∈R+ satisfies the continuity equation <ref type="bibr">(Ambrosio et al., 2008, Theorem 8.3</ref></p><formula xml:id="formula_5">.1), ∂ t ρ t + ∇ · (ρ t v t ) = 0.<label>(2)</label></formula><p>The velocity field v t in Eq.</p><p>(2) is given by</p><formula xml:id="formula_6">v t (x) = −∇ x δF δρ (ρ),<label>(3)</label></formula><p>where δF δρ denotes the first variation of the functional F. Since the seminal work of <ref type="bibr" target="#b24">Jordan et al. (1998)</ref> that showed that the Fokker-Plank equation is the gradient flow of a particular functional in the Wasserstein space, gradient flows in the Wasserstein metric have been a popular tool in the analysis of partial differential equations (PDEs). For example, they have been applied to the study of the porous-medium equation <ref type="bibr" target="#b36">(Otto, 2001)</ref>, crowd modeling <ref type="bibr" target="#b32">(Maury et al., 2010;</ref>, and mean-field games <ref type="bibr" target="#b0">(Almulla et al., 2017)</ref>. More recently, gradient flows of various distances used in deep generative modeling literature have been proposed, notably that of the sliced Wasserstein distance <ref type="bibr" target="#b31">(Liutkus et al., 2019)</ref>, the maximum mean discrepancy <ref type="bibr" target="#b3">(Arbel et al., 2019)</ref>, the Stein discrepancy <ref type="bibr" target="#b30">(Liu, 2017)</ref>, and the Sobolev discrepancy <ref type="bibr" target="#b35">(Mroueh et al., 2019)</ref>. Gradient flows have also been used for learning non-parametric and parametric implicit generative models <ref type="bibr" target="#b31">(Liutkus et al., 2019;</ref><ref type="bibr" target="#b15">Gao et al., 2019;</ref>. As an example of the latter, Variational Gradient Flow <ref type="bibr" target="#b15">(Gao et al., 2019</ref>) learns a mapping between latent vectors and samples evolved using the gradient flow of f -divergences. In this work, we present a method using gradient flows of entropy-regularized f -divergences for refining samples from deep generative models employing existing discriminators as density-ratio estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERATOR REFINEMENT VIA DISCRIMINATOR GRADIENT FLOW</head><p>This section describes our main contribution: Discriminator Gradient f low (DGf low). As an overview, we begin with the construction of the gradient flow of entropy-regularized f -divergences and describe its application to sample refinement. We then discuss how to simulate the gradient flow in the latent space of the generator -a procedure more suitable for high-dimensional datasets. Finally, we present a simple technique that extends our method to generative models that have not yet been studied in the context of refinement. Due to space constraints, we focus on conveying the key concepts and relegate details (e.g., proofs) to the appendix.</p><p>The entropy-regularized f -divergence functional is defined as</p><formula xml:id="formula_7">F f µ (ρ) D f (µ ρ) − γH(ρ),<label>(4)</label></formula><p>where the f -divergence term D f (µ ρ) ensures that the "distance" between the probability density ρ and the target density µ decreases along the gradient flow. The differential entropy term H(ρ) improves diversity and expressiveness when the gradient flow is simulated for finite time-steps. We now construct the gradient flow of F f µ . Lemma 3.1. Define the functional F f µ : P 2 (Ω) → R as</p><formula xml:id="formula_8">F f µ (ρ) f (ρ(x)/µ(x)) µ(x)dx f-divergence + γ ρ(x) log ρ(x)dx negative entropy ,<label>(5)</label></formula><p>where f is a twice-differentiable convex function with f (1) = 0. The gradient flow of the functional F f µ (ρ) in the Wasserstein space (P 2 (Ω), W 2 ) is given by the following PDE,</p><formula xml:id="formula_9">∂ t ρ t (x) − ∇ x · (ρ t (x)∇ x f (ρ t (x)/µ(x))) − γ∆ xx ρ t (x) = 0,<label>(6)</label></formula><p>where ∇ x · and ∆ xx denote the divergence and the Laplace operators respectively.</p><p>The proof is given in Appendix A.1. The PDE in Eq. (6) is a type of Fokker-Plank equation (FPE). FPEs have been studied extensively in the literature of stochastic processes and have a Stochastic Differential Equation (SDE) counterpart <ref type="bibr" target="#b39">(Risken, 1996)</ref>. In the case of Eq. (6), the equivalent SDE is given by</p><formula xml:id="formula_10">dx t = −∇ x f (ρ t /µ) (x t )dt drift + 2γdw t diffusion ,<label>(7)</label></formula><p>where dw t denotes the standard Wiener process. Eq. <ref type="formula" target="#formula_10">(7)</ref> defines the evolution of a particle x t under the influence of drift and diffusion. Specifically, it is a McKean-Vlasov process <ref type="bibr" target="#b9">(Braun &amp; Hepp, 1977)</ref> which is a type of non-linear stochastic process as the drift term at any time t depends on the distribution ρ t of the particle x t . Eqs. <ref type="formula" target="#formula_9">(6)</ref> and <ref type="formula" target="#formula_10">(7)</ref> are equivalent in the sense that the distribution of the particle x t in Eq. <ref type="formula" target="#formula_10">(7)</ref> solves the PDE in Eq. (6). Consequently, samples from the density ρ t along the gradient flow can be obtained by first drawing samples x 0 ∼ ρ 0 and then simulating the SDE in Eq. <ref type="formula" target="#formula_10">(7)</ref>. The SDE can be approximately simulated via the stochastic Euler scheme (also known as the Euler-Maruyama method) <ref type="bibr" target="#b7">(Beyn &amp; Kruse, 2011)</ref> given by</p><formula xml:id="formula_11">x τn+1 = x τn − η∇ x f (ρ τn /µ) (x τn ) + 2γηξ τn ,<label>(8)</label></formula><p>where ξ τn ∼ N (0, I), the time interval [0, T ] is partitioned into equal intervals of size η and τ 0 &lt; τ 1 &lt; · · · &lt; τ N denote the discretized time-steps.</p><p>Eq. (8) provides a non-parametric procedure to refine samples from a generator g θ where we let µ be the density of real samples and ρ τ0 the density of samples generated from g θ obtained by first sampling from the prior latent distribution z ∼ p Z (z) and then feeding z into g θ . We first generate particles x 0 ∼ ρ τ0 and then update the particles using Eq. (8) for N time steps.</p><p>Given a binary classifier (discriminator) D that has been trained to distinguish between samples from µ and ρ τ0 , the density-ratio ρ τ0 (x)/µ(x) can be estimated via the well-known density-ratio trick <ref type="bibr" target="#b44">(Sugiyama et al., 2012)</ref>,</p><formula xml:id="formula_12">ρ τ0 (x)/µ(x) = 1 − D(y = 1|x) D(y = 1|x) = exp(−d(x)),<label>(9)</label></formula><p>where D(y = 1|x) denotes the conditional probability of the sample x being from µ and d(x) denotes the logit output of the classifier D. We term this procedure where samples are refined via gradient flow of f -divergences as Discriminator Gradient f low (DGf low).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">REFINEMENT IN THE LATENT SPACE</head><p>Eq. (8) requires a running estimate of the density-ratio ρ τn (x)/µ(x), which can be approximated using the stale estimate ρ τn (x)/µ(x) ≈ ρ τ0 (x)/µ(x) for η → 0 and small N , where the density ρ τn will be close to ρ τ0 . However, our initial image experiments showed that refining directly in high-dimensional data-spaces with the stale estimate is problematic; error is accumulated at each time-step leading to a visible degradation in the quality of data samples (e.g., appearance of artifacts in images).</p><p>To tackle this problem, we propose refining the latent vectors before mapping them to samples in data-space using g θ . We describe a procedure analogous to Eq. (8) but in the latent space for generators g θ that take a latent vector z ∈ Z as input and generate a sample x ∈ X . We first show in Lemma 3.2 that the density-ratio in the latent space between two distributions can be estimated via the density-ratio of corresponding distributions in the data space. Lemma 3.2. Let g : Z → X be a sufficiently well-behaved injective function where Z ⊆ R n and X ⊂ R m with m &gt; n. Let p Z (z), pẐ(ẑ) be probability densities on Z and q X (x), qX (x) be the densities of the pushforward measures g Z, g Ẑ respectively. Assume that p Z (z) and pẐ(ẑ) have same support, and the Jacobian matrix J g has full column rank. Then, the density-ratio pẐ(u)/p Z (u) at the point u ∈ Z is given by</p><formula xml:id="formula_13">pẐ(u) p Z (u) = qX (g(u)) q X (g(u)) .<label>(10)</label></formula><p>Algorithm 1 Refinement in the Latent Space using DGf low.</p><p>Require: First derivative of f (f ), generator (g θ ), discriminator (d φ ), number of update steps (N ), stepsize (η), noise factor (γ).</p><formula xml:id="formula_14">1: z 0 ∼ p Z (z)</formula><p>Sample from the prior. 2: for i ← 0, N do 3:</p><formula xml:id="formula_15">ξ i ∼ N (0, I) 4: z i+1 = z i − η∇ zi f (e −d φ (g θ (zi)) ) + √ 2ηγξ i 5: end for 6: return g θ (z n )</formula><p>The refined sample.</p><p>The proof is in Appendix A.2. We let pẐ(ẑ) be the density of the "correct" latent space distribution induced by a generator g θ , i.e., pẐ(ẑ) is the density of a probability measure whose pushforward under g θ approximately equals the target data density µ. The density-ratio of the prior latent distribution p Z (z) and pẐ(ẑ) can now be computed by combining Lemma 3.2 with Eq. <ref type="formula" target="#formula_12">(9)</ref>,</p><formula xml:id="formula_16">p Z (u) pẐ(u) = ρ τ0 (g θ (u)) µ(g θ (u)) = exp(−d(g θ (u))).<label>(11)</label></formula><p>Although a generator g θ parameterized by a neural network may not satisfy the conditions of injectivity and full column rank Jacobian matrix J g θ , Eq. (11) provides an approximation that works well in practice as shown by our experiments. Combining Eq. (11) with Eq. (8) provides us with an update rule for refining samples in the latent space,</p><formula xml:id="formula_17">u τn+1 = u τn − η∇ u f p uτ n /pẐ (u τn ) + 2γηξ τn ,<label>(12)</label></formula><p>where u τ0 ∼ p Z (z) and the density-ratio p uτ n /pẐ is approximated using the stale estimate p uτ 0 /pẐ = exp(−d(g θ (u))). We summarize the complete algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">REFINEMENT FOR ALL</head><p>Thus far, prior work <ref type="bibr" target="#b5">(Azadi et al., 2019;</ref><ref type="bibr" target="#b46">Turner et al., 2019;</ref><ref type="bibr" target="#b45">Tanaka, 2019;</ref><ref type="bibr" target="#b11">Che et al., 2020)</ref> has focused on improving samples for GANs with scalar-valued discriminators, which comprises the canonical GAN as well as recent variants, e.g., WGAN <ref type="bibr" target="#b21">(Gulrajani et al., 2017)</ref>, and SNGAN <ref type="bibr" target="#b34">(Miyato et al., 2018)</ref>. Here, we propose a technique that extends our approach to refine samples from a larger class of DGMs including GANs with vector-valued critics, VAEs, and Normalizing Flows.</p><p>Let p θ be the density of the samples generated by a generator g θ and µ be the density of the real data distribution. We are interested in refining samples from g θ ; however, a corresponding density-ratio estimator for p θ /µ is unavailable, as is the case with the aforementioned generative models.</p><p>Let D φ be a discriminator that has been trained on the same dataset but for a different generative model g φ (e.g., let g φ and D φ be the generator and discriminator of SNGAN respectively). D φ can be used to compute the density ratio p φ /µ. A straightforward technique would be to use the crude approximation p θ /µ ≈ p φ /µ, which could work provided p θ and p φ are not too far from each other. Our experiments show that this simple approximation works to a limited extent (see appendix E).</p><p>To improve upon the crude approximation above, we propose to correct the density-ratio estimate. Specifically, a discriminator D λ is initialized with the weights from D φ and is fine-tuned on samples from g φ and g θ . D φ and D λ are then used to approximate the density-ratio p θ /µ,</p><formula xml:id="formula_18">p θ (x) µ(x) = p φ (x) µ(x) p θ (x) p φ (x) = exp(−d φ (x)) · exp(−d λ (x)),<label>(13)</label></formula><p>where d φ and d λ are logits output from D φ and D λ , respectively. We term the network D λ the density ratio corrector, which experiments show produces higher quality samples than using p θ /µ ≈ p φ /µ. The estimate in Eq. (13) is similar to telescoping density-ratio estimation (TRE), a technique proposed in very recent independent work <ref type="bibr" target="#b38">(Rhodes et al., 2020)</ref>. In brief, <ref type="bibr" target="#b38">Rhodes et al. (2020)</ref> show that classifier-based density ratio estimators perform poorly when distributions are "too far apart"; the classifier can easily distinguish between the distributions, even with a poor estimate of the density ratio. TRE expands the standard density ratio into a telescoping product of more difficultto-distinguish intermediate density ratios. Likewise, in Eq. (13), we treat p φ as an intermediate distribution and estimate the final density-ratio as a product of two density-ratios. <ref type="bibr" target="#b5">Azadi et al. (2019)</ref> first proposed the idea of improving samples from a GAN's generator by discriminator rejection sampling (DRS), making use of the density-ratio provided by the discriminator to estimate the acceptance probability. Metropolis-Hastings GAN (MH-GAN) <ref type="bibr" target="#b46">(Turner et al., 2019)</ref> improved upon the costly rejection sampling procedure via the Metropolis-Hastings algorithm. Unlike DGf low, both of these methods reject inferior samples instead of refining them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Recent work has also sought to improve generative models via sample evolution in the training/generation process. In energy-based generative models <ref type="bibr">(Arbel et al., 2021;</ref><ref type="bibr" target="#b14">Deng et al., 2020)</ref>, the energy functions can be viewed as a component that improves some base generator. For example, the Generalized Energy-Based Model (GEBM) <ref type="bibr">(Arbel et al., 2021)</ref> jointly trains an implicit generative model with an energy function and uses Langevin dynamics to sample from the combination of the two. The Noise Conditional Score Network (NCSN) ; 2020)a score-based generative model -can be seen as a gradient flow that refines a sample right from noise to data. Latent Optimization GAN (LOGAN) <ref type="bibr" target="#b48">(Wu et al., 2020</ref>) optimizes a latent vector via natural gradient descent as part of the GAN training process. While related, these techniques re-sult in "self-contained" generative models; unlike DGf low, they are not general sample refinement techniques that can be applied across generative models 1 .</p><p>Our method is closely related to recent state-of-the-art techniques, specifically Discriminator-Driven Latent Sampling (DDLS) (Che et al., 2020) and Discriminator Optimal Transport (DOT) <ref type="bibr" target="#b45">(Tanaka, 2019)</ref>. In fact, both these methods can be seen as special cases of DGf low.</p><p>DDLS treats a GAN as an energy-based model and uses Langevin dynamics to sample from the energy-based latent distribution p t (z) ∝ p Z (z) exp(d(g θ (z))) induced by performing rejection sampling in the latent space. This distribution is the same as pẐ(ẑ), which can be seen by rearranging terms in Eq. (11). If we use the KL-divergence by setting f = r log r , DGf low is equivalent to DDLS. However, there are practical differences that make DGf low more appealing. DDLS requires estimation of the score function ∇ z {log p Z (z) + d(g θ (z))} to perform the update which becomes undefined if z escapes the support of p Z (z), e.g., in the case of the uniform prior distribution commonly used in GANs; handling such cases would require techniques such as projected gradient descent. This problem does not arise in the case of DGf low since it only uses the density-ratio that is implicitly defined by the discriminator. Moreover, DDLS uses Langevin dynamics which requires the sensitive diffusion term to ensure diversity and to prevent points from collapsing to the maximum-likelihood point. In DGf low, the sample diversity is ensured by the density-ratio term and the diffusion term serves as an enhancement. Note that DGf low performs well even without the diffusion term (i.e., with γ = 0, see <ref type="table" target="#tab_0">Tables 13 &amp; 14</ref> in the appendix). This deterministic variant of DGf low is a practical alternative with one less hyperparameter to tune.</p><p>DOT refines samples by constructing an Optimal Transport (OT) map induced by the WGAN discriminator. The OT map is realized by means of a deterministic optimization problem in the vicinity of the generated samples. If we further analyze the case of DGf low with γ = 0 and solve the resulting ordinary differential equation (ODE) using the backward Euler method,</p><formula xml:id="formula_19">u τn+1 = arg min u∈R n f p uτ n /pẐ (u) + 1 2λ u − u τn 2 ,<label>(14)</label></formula><p>DOT emerges as a special case when we consider a single update step of Eq. (14) using gradient descent and set f (t) = log(t) 2 with λ = 1 2 . This connection of DGf low to DOT, an optimal transport technique, is perhaps unsurprising given the relationship between gradient flows and the dynamical Benamou-Brenier formulation of optimal transport (Santambrogio, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we present empirical results on various deep generative models trained on multiple synthetic and real world datasets. Our primary goals were to determine if (a) DGf low is effective in improving the quality of samples from generative models, (b) the proposed extension to other generative models improves their sample quality, and (c) DGf low is generalizable to different types of data and metrics. Note that we did not seek to achieve state-of-the-art results for the datasets studied but to demonstrate that DGf low is able to significantly improve samples from the bare generators for different models. the samples generated from the WGAN-GP generator (blue) and the refined samples using different techniques (red) against the real samples from the training dataset (brown). Although the WGAN-GP generator learned the overall structure of the dataset, it also learned a number of spurious modes. DOT is able to refine the spurious samples but to a limited degree. In contrast, DDLS and DGf low are able to correct almost all spurious samples and are able to recover the correct structure of the data. Visualizations for DGf low with different f -divergences can be found in the appendix <ref type="figure" target="#fig_9">(Fig.  4</ref>).  <ref type="bibr">(log D)</ref> 84.5 ± .3 -3036 ± 14</p><p>We also compared the different methods quantitatively on two metrics: % high quality samples and kernel density estimate (KDE) score. A sample is classified as a high quality sample if it lies within 4 standard deviations of its nearest Gaussian. The KDE score is computed by first estimating the KDE using generated samples and then computing the log-likelihood of the training samples under the KDE estimate. We computed both the metrics 10 times using 5000 samples and report the mean in <ref type="table" target="#tab_0">Table 1</ref>. The quantitative metrics reinforce the qualitative analysis and show that DDLS and DGf low significantly improve the samples from the generator, with DGf low performing slightly better than DDLS in terms of the KDE score. We conducted experiments on the CIFAR10 and STL10 datasets to demonstrate the efficacy of DGf low in the real-world setting. We followed the setup of <ref type="bibr" target="#b45">Tanaka (2019)</ref> for our image experiments. We used the Fréchet Inception Distance (FID) <ref type="bibr" target="#b22">(Heusel et al., 2017)</ref> and Inception Score (IS) <ref type="bibr" target="#b40">(Salimans et al., 2016)</ref> metrics to evaluate the quality of generated samples before and after refinement. A high value of IS and a low value of FID corresponds to high quality samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">IMAGE EXPERIMENTS</head><p>We first applied DGf low to GANs with scalar-valued discriminators (e.g., WGAN-GP, SNGAN) trained on the CIFAR10 and the STL10 datasets. <ref type="table" target="#tab_1">Table 2</ref> shows that DGf low significantly improves the quality of   <ref type="bibr" target="#b25">(Karras et al., 2017)</ref> 8.80 (.05) SN-ResNet-GAN <ref type="bibr" target="#b34">(Miyato et al., 2018)</ref> 8.22 (.05) NCSN  8.87 (.12) DCGAN 2.88 DCGAN + DRS (cal) <ref type="bibr" target="#b5">(Azadi et al., 2019)</ref> 3.07 DCGAN + MH (cal) <ref type="bibr" target="#b46">(Turner et al., 2019)</ref> 3 the samples in terms of the FID score and outperforms DOT on multiple models. The corresponding values of the Inception score can be found in the Appendix <ref type="table" target="#tab_0">(Table 11)</ref>, which shows DGf low outperforms DOT on all models. In <ref type="table" target="#tab_2">Table 3</ref>, we reproduce previously reported IS results for generative models and other sample improvement methods (DRS, MH-GAN, and DDLS) for completeness. DGf low performs the best in terms of relative improvement from the base score and even outperforms the state-of-the-art BigGAN <ref type="bibr" target="#b10">(Brock et al., 2019)</ref>, a conditional generative model, without the need for additional labels. Qualitatively, DGf low improves the vibrance of the samples and corrects deformations in the foreground object. <ref type="figure" target="#fig_6">Fig. 3</ref> shows the change in the quality of samples when using DGf low where the leftmost columns show the image generated form the base models and the successive columns show the refined sample using DGf low over increments of 5 update steps.</p><p>We then evaluated the ability of DGf low to refine samples from generative models without corresponding discriminators, namely MMDGAN, OCFGAN-GP, VAEs, and Normalizing Flows (Glow). We used the SN-DCGAN (ns) as the surrogate discriminator D φ for these models and fine-tuned density ratio correctors D λ for each model as described in section 3.2. <ref type="table" target="#tab_4">Table 4</ref> shows the FID scores achieved by these models without and with refinement using DGf low. We obtain a clear improvement in quality of samples when these generative models are combined with DGf low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CHARACTER-LEVEL LANGUAGE MODELING</head><p>Finally, we conducted an experiment on the character-level language modeling task proposed by <ref type="bibr" target="#b21">Gulrajani et al. (2017)</ref> to show that DGf low works on different types of data. We trained a character-level GAN language model on the Billion Words Dataset <ref type="bibr" target="#b12">(Chelba et al., 2013)</ref>, which was pre-processed into 32-character long strings. We evaluated the generated samples using the JS-4 and JS-6 scores which compute the Jensen-Shannon divergence between the 4-gram and 6-gram probabilities of the data generated by the model and the real data.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated by WGAN-GP Refined by DGf low</head><p>In Ruoduce that fhance would pol In product that chance could rol I said thowe toot lind talker . I said this stood line talked 10 Now their rarning injurer hows Now their warning injurer shows Police report in B0sbu does off Police report inturner will befe We gine jaid 121 , one bub like</p><p>We gave wall said left out like In years in 19mbisuch said he h</p><p>In years in 1900b such said he h an improvement in the JS-4 and JS-6 scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed a technique to improve samples from deep generative models by refining them using gradient flow of f -divergences between the real and the generator data distributions. We also presented a simple framework that extends the proposed technique to commonly used deep generative models: GANs, VAEs, and Normalizing Flows. Experimental results indicate that gradient flows provide an excellent alternative methodology to refine generative models. </p><formula xml:id="formula_20">∂ t ρ t + ∇ · (ρ t v) = 0.<label>(15)</label></formula><p>The velocity field v in Eq. (15) is given by</p><formula xml:id="formula_21">v(x) = −∇ x δF δρ (ρ),<label>(16)</label></formula><p>where δF δρ (ρ) denotes the first variation of the functional F. The first variation is defined as</p><formula xml:id="formula_22">d dε F(ρ + εχ) ε=0 = δF δρ (ρ)χ,<label>(17)</label></formula><p>where χ = ν − ρ for some ν ∈ P 2 (Ω).</p><p>Let's derive an expression for the the first variation of F. In the following, we drop the notation for dependence on x for clarity,</p><formula xml:id="formula_23">d dε F(ρ + εχ) ε=0 = d dε f ρ + εχ µ µ + γ (ρ + εχ) log(ρ + εχ) ε=0 (18) = f ρ + εχ µ χ + γ (log(ρ + εχ) + 1)χ ε=0 (19) = f ρ µ + γ log(ρ) + γ χ.<label>(20)</label></formula><p>Substituting δF δρ (ρ) in Eq. (16) we get,</p><formula xml:id="formula_24">v(x) = −∇ x f ρ µ + γ log(ρ) + γ (21) = −∇ x f ρ µ − γ ρ ∇ x ρ.<label>(22)</label></formula><p>Substituting v in Eq. (15) we get the gradient flow,</p><formula xml:id="formula_25">∂ t ρ t − ∇ x · ρ t ∇ x f ρ µ + ρ t γ ρ ∇ x ρ = 0 (23) ∂ t ρ t (x) − ∇ x · ρ t ∇ x f ρ(x) µ(x) − γ∆ xx ρ t (x) = 0,<label>(24)</label></formula><p>where ∆ xx and ∇ x · denote the Laplace and the divergence operators respectively.</p><p>A.2 LEMMA 3.2</p><p>Proof. Let f be an integrable function on X . If J g has full column rank and g is an injective function, then we have the following change-of-variables equation <ref type="bibr" target="#b6">(Ben-Israel, 1999;</ref><ref type="bibr" target="#b17">Gemici et al., 2016)</ref>,</p><formula xml:id="formula_26">X f (x)dx = Z (f • g)(z) det J g J g (z)dz.<label>(25)</label></formula><p>This implies that the infinitesimal volumes dx and dz are related as dx = det J g J g (z)dz and the densities p Z (z) and q X (x) are related as p Z (z) = q X (g(z)) det J g J g (z). Similarly, pẐ(ẑ) = qX (g(ẑ)) det J g J g (ẑ). Finally, the density-ratio pẐ(u)/p Z (u) at the point u ∈ Z is given by</p><formula xml:id="formula_27">pẐ(u) p Z (u) = qX (g(u)) det J g J g (u) q X (g(u)) det J g J g (u) = qX (g(u)) q X (g(u)) .<label>(26)</label></formula><p>B A DISCUSSION ON DGf LOW FOR WGAN We apply DGf low to WGAN models by treating the output from their critics as the logit for the estimation of density-ratio. However, it is well-known that WGAN critics are not density-ratio estimators as they are trained to maximize the 1-Wasserstein distance with an unconstrained output. In this section, we provide theoretical justification for the good performance of DGf low on WGAN models. We show that DGf low is related to the gradient flow of the entropy-regularized 1-Wasserstein functional F W µ : P 2 (Ω) → R,</p><formula xml:id="formula_28">F W µ (ρ) sup d Lip≤1 d (x) µ(x)dx − d (x) ρ(x)dx 1-Wasserstein distance + γ ρ(x) log ρ(x)dx negative entropy ,<label>(27)</label></formula><p>where µ denotes the target density, d Lip denotes the Lipschitz constant of the function d.</p><p>Let d * be the function that achieves the supremum in Eq. <ref type="formula" target="#formula_5">(27)</ref>. This results in the functional,</p><formula xml:id="formula_29">F W µ (ρ) = d * (x) µ(x)dx − d * (x) ρ(x)dx + γ ρ(x) log ρ(x)dx.<label>(28)</label></formula><p>Following a similar derivation as in Appendix A.1, the gradient flow of F W µ (ρ) is given by the following PDE,</p><formula xml:id="formula_30">∂ t ρ t (x) + ∇ x · (ρ t ∇ x d * (x)) − γ∆ xx ρ t (x) = 0.<label>(29)</label></formula><p>If d * is approximated using the critic (d φ ) of WGAN, we get the following gradient flow,</p><formula xml:id="formula_31">∂ t ρ t (x) + ∇ x · (ρ t ∇ x d φ (x)) − γ∆ xx ρ t (x) = 0,<label>(30)</label></formula><p>which is same as the gradient flow of entropy-regularized f -divergence with f = r log r (i.e., the KL divergence) when d φ is treated as a density-ratio estimator. The gradient flow of entropy-regularized f -divergence with f = r log r is simplified below,</p><formula xml:id="formula_32">∂ t ρ t (x) − ∇ x · (ρ t ∇ x f (exp(−d φ (x)))) − γ∆ xx ρ t (x) = 0 (31) ∂ t ρ t (x) − ∇ x · (ρ t ∇ x (log(exp(−d φ (x))) + 1)) − γ∆ xx ρ t (x) = 0 (32) ∂ t ρ t (x) + ∇ x · (ρ t ∇ x d φ (x)) − γ∆ xx ρ t (x) = 0.<label>(33)</label></formula><p>The equality of Eq. (30) and Eq. (33) implies that DGf low approximates the gradient flow of the 1-Wasserstein distance when the critic of WGAN is used for density-ratio estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C FURTHER DISCUSSION ON RELATED WORK</head><p>Energy-based &amp; Score-based Generative Models DGf low is related to recently proposed energy-based generative models <ref type="bibr">(Arbel et al., 2021;</ref><ref type="bibr" target="#b14">Deng et al., 2020)</ref> -one can view the energy functions used in these methods as a component that improves some base model. For example, the Generalized Energy-Based Model (GEBM) <ref type="bibr">(Arbel et al., 2021)</ref> jointly trains an implicit generative model with an energy function and uses Langevin dynamics to sample from the combination of the two. Similarly, in <ref type="bibr" target="#b14">Deng et al. (2020)</ref>, a discriminator that estimates the energy function is combined with a language model to train an energy-based text-generation model.</p><p>Score-based generative modeling (SBGM)  is another active area of research closely-related to energy-based models. Noise Conditional Score Network (NCSN) ), a SBGM, trains a neural network to estimate the score function of a probability density at various noise levels. Once trained, this score network is used to evolve samples from noise to the data distribution using Langevin dynamics. NCSN can be viewed as a gradient flow that refines a sample right from noise to data; however, unlike DGf low, NCSN is a complete generative models in itself and not a sample refinement technique that can be applied to other generative models.</p><p>Other Related Work Monte Carlo techniques have been used for improving various components in generative models, e.g., <ref type="bibr" target="#b19">Grover et al. (2018)</ref> proposed Variational Rejection Sampling which performs rejection sampling in the latent space of VAEs to improve the variational posterior and <ref type="bibr" target="#b20">Grover et al. (2019)</ref> used likelihood-free importance sampling for bias correction in generative models . <ref type="bibr" target="#b48">Wu et al. (2020)</ref> proposed Latent Optimization GAN (LOGAN) which optimizes the latent vector as part of the training process unlike DGf low that refines the latent vector post training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D IMPLEMENTATION DETAILS D.1 2D DATASETS</head><p>Datasets The 25 Gaussians dataset was constructed by generating 100000 samples from a mixture of 25 equally likely 2D isotropic Gaussians with means {−4, −2, 0, 2, 4} × {−4, −2, 0, 2, 4} ⊂ R 2 and standard deviation 0.05. Once generated, the data-points were normalized by 2 √ 2 following Tanaka (2019). The 2DSwissroll dataset was constructed by first generating 100000 samples of the 3D swissroll dataset using make swiss roll from scikit-learn with noise=0.25 and then only keeping dimensions {0, 2}. The generated samples were normalized by 7.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Models</head><p>We trained a WGAN-GP model for both the datasets. The generator was a fullyconnected network with ReLU non-linearities that mapped z ∼ N (0, I 2×2 ) to x ∈ R 2 . Similarly, the discriminator was a fully-connected network with ReLU non-linearities that mapped x ∈ R 2 to R. We refer the reader to <ref type="bibr" target="#b21">Gulrajani et al. (2017)</ref> for the exact network structures. The gradient penalty factor was set to 10. The models were trained for 10K generator iterations with a batch size of 256 using the Adam optimizer with a learning rate of 10 −4 , β 1 = 0.5, and β 1 = 0.9. We updated the discriminator 5 times for each generator iteration.</p><p>Hyperparameters We ran DOT for 100 steps and performed gradient descent using the Adam optimizer with a learning rate of 0.01 and β = (0., 0.9) as suggested by <ref type="bibr" target="#b45">Tanaka (2019)</ref>. DDLS was run for 50 iterations with a step-size of 0.01 and the Gaussian noise was scaled by a factor of 0.1 as suggested by <ref type="bibr" target="#b11">Che et al. (2020)</ref>. For DGf low, we set the step-size η = 0.01, the number of steps n = 100, and the noise regularizer γ = 0.01. We used the output from the WGAN-GP discriminator directly as a logit for estimating the density ratio for DDLS and DGf low.</p><p>Metrics We compared the different methods quantitatively on two metrics: % high quality samples and kernel density estimate (KDE) score. A sample is classified as a high quality sample if it lies within 4 standard deviations of its nearest Gaussian. The KDE score is computed by first estimating the KDE using generated samples and then computing the log-likelihood of the training samples under the KDE estimate. KDE was performed using sklearn.neighbors.KernelDensity with a Gaussian kernel and a kernel bandwidth of 0.1. The quantitative metrics were averaged over 10 runs with 5000 samples from each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 IMAGE EXPERIMENTS</head><p>Datasets CIFAR10 <ref type="bibr" target="#b28">(Krizhevsky et al., 2009</ref>) is a dataset of 60K natural RGB images of size 32 × 32 from 10 classes. STL10 is a dataset of 100K natural RGB images of size 96 × 96 from 10  <ref type="figure">: (b, m, 1, 1)</ref> classes. We resized the STL10 <ref type="bibr" target="#b13">(Coates et al., 2011)</ref> dataset to 48 × 48 for SNGAN and WGAN-GP, and to 32 × 32 for MMDGAN, OCFGAN-GP, and VAE since the respective base models were trained on these sizes.</p><p>Base Models for CIFAR10 We used the publicly available pre-trained models for WGAN-GP, SN-DCGAN (hi), and SN-DCGAN (ns). We refer the reader to <ref type="bibr" target="#b45">Tanaka (2019)</ref> for exact details about these models. For SN-ResNet-GAN and OCFGAN-GP we used the pre-trained models from <ref type="bibr" target="#b34">Miyato et al. (2018)</ref> and <ref type="bibr" target="#b2">Ansari et al. (2020)</ref> respectively. We used the respective discriminators of SN-DCGAN (ns), SN-DCGAN (hi), and WGAN-GP for density-ratio estimation when refining their generators. For the SN-ResNet-GAN (hi) generator, we used SN-DCGAN (ns) discriminator as the non-saturating loss provides a better density-ratio estimation than a discriminator trained using the hinge loss. where we used 10K samples. Following <ref type="bibr" target="#b45">Tanaka (2019)</ref>, we used the entire training and test set (60K images) for CIFAR10 and the entire unlabeled set (100K images) for STL10 as the set of real images used to compute FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 CHARACTER LEVEL LANGUAGE MODELING</head><p>Dataset We used the Billion Words dataset <ref type="bibr" target="#b12">(Chelba et al., 2013)</ref> which was pre-processed into 32-character long strings.</p><p>Base Model Our generator was a 1D CNN which followed the architecture used by <ref type="bibr" target="#b21">Gulrajani et al. (2017)</ref>.</p><p>Hyperparameters We performed 50 updates of DGf low with a step size of 0.1 and noise factor γ = 0.</p><p>Metrics The JS-4 and JS-6 scores were computed using the code provided by <ref type="bibr" target="#b21">Gulrajani et al. (2017)</ref> at https://github.com/igul222/improved wgan training. We used 10000 samples from the models to compute the JS-4 score.  <ref type="figure" target="#fig_9">Fig. 4</ref> shows the samples generated by WGAN-GP (leftmost, blue) and refined samples generated using DGf low with different f -divergences (red). <ref type="figure" target="#fig_10">Fig. 5</ref> shows the deterministic component, −∇ x f (ρ 0 /µ)(x 0 ), of the velocity for different f -divergences on the 2D datasets. <ref type="figure" target="#fig_11">Fig. 6</ref> (right) shows the latent space distribution recovered by DGf low when applied in the latent space for the 2D datasets. This latent space is same as the one derived by Che et al. (2020), i.e., p t (z) ∝ p Z (z) exp(d(g θ (z))) which is shown in <ref type="figure" target="#fig_11">Fig. 6 (left)</ref> for both datasets. <ref type="table" target="#tab_0">Table 11</ref> shows the comparison of DGf low with DOT in terms of the inception score for the CI-FAR10 and STL10 datasets. DGf low outperforms DOT significantly for all the base GAN models on both the datasets. <ref type="table" target="#tab_0">Table 12 compares</ref>    <ref type="table" target="#tab_0">Tables 13 and 14</ref> compare the deterministic variant of DGf low (γ = 0) against DOT and DDLS. These results show that the diffusion term only serves as an enhancement for DGf low, not a necessity, and it outperforms competing methods even without added noise. <ref type="table" target="#tab_0">Table 15</ref> shows the results of DGf low on MMDGAN, OCFGAN-GP, and VAE models when the SN-DCGAN (ns) discriminator is directly used as a density-ratio estimator without an additional density-ratio corrector. <ref type="bibr">Figures 7,</ref><ref type="bibr">8,</ref><ref type="bibr">9,</ref><ref type="bibr">and 10</ref> show the samples generated by the base model (left) and the refined samples (right) using DGf low for the CIFAR10 and STL10 datasets. Runtime DGf low performs a backward pass through d φ • g θ to compute the gradient of the density-ratio with respect to the latent vector. This results in the same runtime complexity as that of DOT and DDLS. <ref type="table" target="#tab_10">Table 8</ref> shows a comparison of the runtimes of DOT, DDLS, and, DGf low on the 25Gaussians dataset under same conditions. As expected, these refinement methods have similar runtimes in practice. The wall-clock time required for DGf low (KL) to refine 100 samples from different base models on the CIFAR10 and STL10 datasets is reported in tables 9 and 10.         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 1 l V E J v n 2 F / + U J G y Z k z L Q X L Z b E U = " &gt; A A A B 7 X i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V Z L a 2 H Z l w Y 3 L C v Y B b S i T 6 a Q d O 8 m E m Y l Q Q v / B j Q t F 3 P o / 7 v w L P 8 F J W s T X g Q u H c + 7 l 3 n u 8 i F G p L O v d y K 2 s r q 1 v 5 D c L W 9 s 7 u 3 v F / Y O O 5 L H A p I 0 5 4 6 L n I U k Y D U l b U c V I L x I E B R 4 j X W 9 6 m f r d O y I k 5 e G N m k X E D d A 4 p D 7 F S G m p M x A T P r S G x Z J V t j K Y f 4 m 9 J K W L D 8 j Q G h b f B i O O 4 4 C E C j M k Z d + 2 I u U m S C i K G Z k X B r E k E c J T N C Z 9 T U M U E O k m 2 b V z 8 0 Q r I 9 P n Q l e o z E z 9 P p G g Q M p Z 4 O n O A K m J / O 2 l 4 n 9 e P 1 Z + 3 U 1 o G M W K h H i x y I + Z qb i Z v m 6 O q C B Y s Z k m C A u q b z X x B A m E l Q 6 o k I X Q q N q O 4 + j f G + d O 5 a y R k r p j V W t f I X Q q Z d s p W 9 f V U r O 1 S A P y c A T H c A o 2 1 K A J V 9 C C N m C 4 h X t 4 h C e D G w / G s / G y a M 0 Z y 5 l D + A H j 9 R M O Q J C I &lt; / l a t e x i t &gt; ⇢ 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f h k M Z 6 C z 3 k + / G f K Z R l j b e i s 8 D P E = " &gt; A A A B 7 X i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V Z L a 2 H Z l w Y 3 L C v Y B b S i T 6 a Q d O 8 m E m Y l Q Q v / B j Q t F 3 P o / 7 v w L P 8 F J W s T X g Q u H c + 7 l 3 n u 8 i F G p L O v dy K 2 s r q 1 v 5 D c L W 9 s 7 u 3 v F / Y O O 5 L H A p I 0 5 4 6 L n I U k Y D U l b U c V I L x I E B R 4 j X W 9 6 m f r d O y I k 5 e G N m k X E D d A 4 p D 7 F S G m p M x A T P r S H x Z J V t j K Y f 4 m 9 J K W L D 8 j Q G h b f B i O O 4 4 C E C j M k Z d + 2 I u U m S C i K G Z k X B r E k E c J T N C Z 9 T U M U E O k m 2 b V z 8 0 Q r I 9 P n Q l e o z E z 9 P p G g Q M p Z 4 O n O A K m J / O 2 l 4 n 9 e P 1 Z + 3 U 1 o G M W K h H i x y I + Z q b i Z v m 6 O q C B Y s Z k m C A u q b z X x B A m E l Q 6 o k I X Q q N q O 4 + j f G + d O 5 a y R k r p j V W t f I X Q q Z d s p W 9 f V U r O 1 S A P y c A T H c A o 2 1 K A J V 9 C C N m C 4 h X t 4 h C e D G w / G s / G y a M 0 Z y 5 l D + A H j 9 R M P x J C J &lt; / l a t e x i t &gt; ⇢ 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / z 5 Q x v a d + w 0 m r o R A G z 5 8 x L X 9 P X o = " &gt; A A A B 7 X i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V Z L a 2 H Z l w Y 3 L C v Y B b S i T 6 a Q d O 8 m E m Y l Q Q v / B j Q t F 3 P o / 7 v w L P 8 F J W s T X g Q u H c + 7 l 3 n u 8 i F G p L O v d y K 2 s r q 1 v 5 D c L W 9 s 7 u 3 v F / Y O O 5 L H A p I 0 5 4 6 L n I U k Y D U l b U c V I L x I E B R 4 j X W 9 6 m f r d O y I k 5 e G N m k X E D d A 4 p D 7 F S G m p M x A T P q w M i y W r b G U w / x J 7 S U o X H 5 C h N S y + D U Y c x w E J F W Z I y r 5 t R c p N k F A U M z I v D G J J I o S n a E z 6 m o Y o I N J N s m v n 5 o l W R q b P h a 5 Q m Z n 6 f S J B g Z S z w N O d A V I T + d t L x f + 8 f q z 8 u p v Q M I o V C f F i k R 8 z U 3 E z f d 0 c U U G w Y j N N E B Z U 3 2 r i C R I I K x 1 Q I Q u h U b U d x 9 G / N 8 6 d y l k j J X X H q t a + Q u h U y r Z T t q 6 r p W Z r k Q b k 4 Q i O 4 R R s q E E T r q A F b c B w C / f w C E 8 G N x 6 M Z + N l 0 Z o z l j O H 8 A P G 6 y c R S J C K &lt; / l a t e x i t &gt; ⇢ 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L M 0 2 l Y j 2 P a x / x m l O v D p 8 k 9 y n i l U = " &gt; A A A B 7 X i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V Z L a 2 H Z l w Y 0 r q W B b o Q 1 l M p 2 0 Y y e Z M D M R S u g / u H G h i F v / x 5 1 / 4 S c 4 S Y v 4 O n D h c M 6 9 3 H u P F z E q l W W 9 G 7 m l 5 Z X V t f x 6 Y W N z a 3 u n u L v X k T w W m L Q x Z 1 z c e E g S R k P S V l Q x c h M J g g K P k a 4 3 O U / 9 7 h 0 R k v L w W k 0 j 4 g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>M s M L E J P H k s x D q F c d 1 3 e T 3 + o V b P q + n p O b a l e p X C O 1 y y X F L 9 k 2 l 2 G g u 0 o A c H M M J n I E D V W j A N T S h B Q Q m c A + P 8 G Q J 6 8 F 6 t l 4 W r S v W c u Y I f s B 6 / Q T n 6 o / l &lt; / l a t e x i t &gt; ⇠ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T x O L w t r q x Y a 9 6 2 C D Z i y F p S a 5 S D o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w Y 0 l q Y 9 u V B T e u p I J 9 Y B v K Z D p t h 0 4 m Y W Y i l N C / c O N C E b f + j T v / w k 9 w k h b x d e D C 4 Z x 7 u f c e L 2 R U K s t 6 N z J L y y u r a 9 n 1 3 M b m 1 v Z O f n e v J Y N I Y N L E A Q t E x 0 O S M M p J U 1 H F S C c U B P k e I 2 1 v c p H 4 7 T s i J A 3 4 j Z q G x P X R i N M h x U h p 6 b Y n x k E / v j q x Z / 1 8 w S p a K c y / x F 6 Q w v k H p G j 0 8 2 + 9 Q Y A j n 3 C F G Z K y a 1 u h c m M k F M W M z H K 9 S J I Q 4 Q k a k a 6 m H P l E u n F 6 8 c w 8 0 s r A H A Z C F 1 d m q n 6 f i J E v 5 d T 3 d K e P 1 F j + 9 h L x P 6 8 b q W H V j S k P I 0 U 4 n i 8 a R s x U g Z m 8 b w 6 o I F i x q S Y I C 6 p v N f E Y C Y S V D i m X h l A r 2 4 7 j 6 N 9 r Z 0 7 p t J a Q q m O V K 1 8 h t E p F 2 y l a 1 + V C v T F P A 7 J w A I d w D D Z U o A 6 X 0 I A m Y O B w D 4 / w Z E j j w X g 2 X u a t G W M x s w 8 / Y L x + A u F j k i Q = &lt; / l a t e x i t &gt; ⇢ N 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x y 2 s b a o 8 B B Z L Y l 4 t 2 P 7 Y 1 i A m 3 g c = " &gt; A A A B 6 n i c b V D J S g N B E K 1 x j X G L e v Q y G A R P o S d m T H I y 4 M V j R L N A M o S e T k / S p G e h u 0 e I Q z 7 B i w d F v P p F 3 v w L P 8 G e S R C 3 B w W P 9 6 q o q u d G n E m F 0 L u x t L y y u r a e 2 8 h v b m 3 v 7 B b 2 9 t s y j AW h L R L y U H R d L C l n A W 0 p p j j t R o J i 3 + W 0 4 0 4 u U r 9 z S 4 V k Y X C j p h F 1 f D w K m M c I V l q 6 v h u g Q a G I S i i D + Z d Y C 1 I 8 / 4 A M z U H h r T 8 M S e z T Q B G O p e x Z K F J O g o V i h N N Z v h 9 L G m E y w S P a 0 z T A P p V O k p 0 6 M 4 + 1 M j S 9 U O g K l J m p 3 y c S 7 E s 5 9 V 3 d 6 W M 1 l r + 9 V P z P 6 8 X K q z k J C 6 J Y 0 Y D M F 3 k x N 1 V o p n + b Q y Y o U X y q C S a C 6 V t N M s Y C E 6 X T y W c h 1 C u W b d v 6 9 / q Z X T 6 t p 6 R m o 0 r 1 K 4 R 2 u W T Z J X R V K T a a 8 z Q g B 4 d w B C d g Q R U a c A l N a A G B E d z D I z w Z 3H g w n o 2 X e e u S s Z g 5 g B 8 w X j 8 B 1 X O P P w = = &lt; / l a t e x i t &gt; z 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u V D / z N v 0 B V x S P w W p T 8 f E o / / e J 0 Q = " &gt; A A A B 6 n i c b V D J S g N B E K 1 x j X G L e v T S G A R P Y S Z m T H I y 4 M V j R L N A M o S e T k / S p G e h u 0 e I Q z 7 B i w d F v P p F 3 v w L P 8 G e S R C 3 B w W P 9 6 q o q u d G n E l l m u / G 0 v L K 6 t p 6 b i O / u b W 9 s 1 v Y 2 2 / L M B a E t k j I Q 9 F 1 s a S c B b S l m O K 0 G w m K f Z f T j j u 5 S P 3 O L R W S h c G N m k b U 8 f E o Y B 4 j W G n p + m 5 g D Q p F s 2 R m Q H + J t S D F 8 w / I 0 B w U 3 v r D k M Q + D R T h W M q e Z U b K S b B Q j H A 6 y / d j S S N M J n h E e 5 o G 2 K f S S b J T Z + h Y K 0 P k h U J X o F C m f p 9 I s C / l 1 H d 1 p 4 / V W P 7 2 U v E / r x c r r + Y k L I h i R Q M y X + T F H K k Q p X + j I R O U K D 7 V B B P B 9 K 2 I j L H A R O l 0 8 l k I 9 Y p l 2 7 b + v X 5 m l 0 / r K a n Z Z q X 6 F U K 7 X L L s k n l V K T a a 8 z Q g B 4 d w B C d g Q R U a c A l N a A G B E d z D I z w Z 3 H g w n o 2 X e e u S s Z g 5 g B 8 w X j 8 B 1 v e P Q A = = &lt; / l a t e x i t &gt; z 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L I 6 F y 3 d R 2 X w S 5 h u O b H x t s A f z F 2 c = " &gt; A A A B 6 n i c b V D J S g N B E K 1 x j X G L e v T S G A R P Y S Z m T H I y 4 M V j R L N A M o S e T k / S p G e h u 0 e I Q z 7 B i w d F v P p F 3 v w L P 8 G e S R C 3 B w W P 9 6 q o q u d G n E l l m u / G 0 v L K 6 t p 6 b i O / u b W 9 s 1 v Y 2 2 / L M B a E t k j I Q 9 F 1 s a S c B b S l m O K 0 G w m K f Z f T j j u 5 S P 3 O L R W S h c G N m k b U 8 f E o Y B 4 j W G n p + m 5 Q H h S K Z s n M g P 4 S a 0 G K 5 x + Q o T k o v P W H I Y l 9 G i j C s Z Q 9 y 4 y U k 2 C h G O F 0 l u / H k k a Y T P C I 9 j Q N s E + l k 2 S n z t C x V o b I C 4 W u Q K F M / T 6 R Y F / K q e / q T h + r s f z t p e J / X i 9 W X s 1 J W B D F i g Z k v s i L O V I h S v 9 G Q y Y o U X y q C S a C 6 V s R G W O B i d L p 5 L M Q 6 h X L t m 3 9 e / 3 M L p / W U 1 K z z U r 1 K 4 R 2 u W T Z J f O q U m w 0 5 2 l A D g 7 h C E 7 A g i o 0 4 B K a 0 A I C I 7 i H R 3 g y u P F g P B s v 8 9 Y l Y z F z A D 9 g v H 4 C 2 H u P Q Q = = &lt; / l a t e x i t &gt; z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / K f K s l h V 4 0 K 5 p m a I v J N q c Y D g h I Y = " &gt; A A A B 7 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w Y 0 l q Y 9 u V B T e u p I J 9 Q B v K Z D p t h 0 4 m Y W Y i 1 N C P c O N C E b d + j z v / w k 9 w k h b x d e D C 4 Z x 7 u f c e L 2 R U K s t 6 N z J L y y u r a 9 n 1 3 M b m 1 v Z O f n e v J Y N I Y N L E A Q t E x 0 O S M M p J U 1 H F S C c U B P k e I 2 1 v c p H 4 7 V s i J A 3 4 j Z q G x P X R i N M h x U h p q X 3 X j 6 9 O 7 F k / X 7 C K V g r z L 7 E X p H D + A S k a / f x b b x D g y C d c Y Y a k 7 N p W q N w Y C U U x I 7 N c L 5 I k R H i C R q S r K U c + k W 6 c n j s z j 7 Q y M I e B 0 M W V m a r f J 2 L k S z n 1 P d 3 p I z W W v 7 1 E / M / r R m p Y d W P K w 0 g R j u e L h h E z V W A m v 5 s D K g h W b K o J w o L q W 0 0 8 R g J h p R P K p S H U y r b j O P r 3 2 p l T O q 0 l p O p Y 5 c p X C K 1 S 0 X a K 1 n W 5 U G / M 0 4 A s H M A h H I M N F a j D J T S g C R g m c A + P 8 G S E x o P x b L z M W z P G Y m Y f f s B 4 / Q S j 4 Z D b &lt; / l a t e x i t &gt; z N 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L b P h c H T 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e P 7 R M G a m 4 m Z 6 v z m g g m D F p p o g L K j O a u I x E g g r 3 V I + K 6 F e s R 3 H 0 b f X z 5 z y a T 0 l N c e q V L 9 K a J d L t l O y r i r F x s 2 8 D c j B I R z B C d h Q h Q Z c Q h N a g I H D P T z C k 6 G M B + P Z e J m P L h m L n Q P 4 A e P 1 E 2 F K k x c = &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R S 0 k 4 p r 8 / S K b e w o E c p 6 u s 1 7 a H P 0 = " &gt; A A A B 9 H i c b V D L T g I x F L 2 D L 8 Q X 6 t J N I z F x R W a Q E V h J 4 s Y l J v I w M C G d U q C h 8 7 D t k J A J 3 + H G h c a 4 9 W P c + R d + g p 2 B G F 8 n a X J y z r 2 5 p 8 c N O Z P K N N + N z M r q 2 v p G d j O 3 t b 2 z u 5 f f P 2 j J I B K E N k n A A 9 F x s a S c + b S p m O K 0 E w q K P Z f T t j u 5 T P z 2 l A r J A v 9 G z U L q e H j k s y E j W G n J 6 X l Y j Q n m c W P e L / X z B b N o p k B / i b U k h Y s P S N H o 5 9 9 6 g 4 B E H v U V 4 V j K r m W G y o m x U I x w O s / 1 I k l D T C Z 4 R L u a + t i j 0 o n T 0 H N 0 o p U B G g Z C P 1 + h V P 2 + E W N P y p n n 6 s k k p P z t J e J / X j d S w 6 o T M z + M F P X J 4 t A w 4 k g F K G k A D Z i g R P G Z J p g I p r M i M s Y C E 6 V 7 y q U l 1 M q W b d v 6 7 7 V z u 3 R W S 0 j V N s u V r x J a p a J l F 8 3 r c q F + u 2 g D s n A E x 3 A K F l S g D l f Q g C Y Q u I N 7 e I Q n Y 2 o 8 G M / G y 2 I 0 Y y x 3 D u E H j N d P g T i T s g = = &lt; / l a t e x i t &gt; P 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i O q 8 W f n 0 e / o O R O k 2 0 P O x H T x 4 2 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A 5 6 L 3 + g t o 7 i 7 l e S F r S S L D d U g N a z M C j N 3 I I y n A p W d 0 i l W Q n 0 D B Z s 5 m A O k u m 5 b S O r 8 W P H p D g r l F u 5 w S 2 7 2 W F B 6 m Y 8 5 2 y e o / / W G v J / 2 q w y 2 c H c 8 r y s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " G d W o c F U g r Z 8 Z K K u 0 3 v p M c F r q h + A = " &gt; A A A C A H i c b V B N i x N B E K 1 Z d c 1 G d z e 6 B w 9 e m g 1 C 9 j L 0 T D L J z M m A I B 4 j m A / I Z E N P p y d p 0 v N B d 4 8 Q h l z 8 K 1 4 8 u I h X f 8 b e / B f + B H s m I r v i g 4 L H e 1 V U 1 Y t y w Z X G + K d 1 9 O D h o + P H j Z P m k 6 e n Z + e t Z 8 8 n K i s k Z W O a i U z O I q K Y 4 C k b a 6 4 F m + W S k S Q S b B p t 3 1 T + 9 C O T i m f p B 7 3 L 2 S I h 6 5 T H n B J t p G X r R Z g Q v a F E l G / 3 y z A p r u N O K D f Z 1 b L V x j Y e 9 B 3 s I m y 7 3 a 7 f 8 w 3 x + o P A 7 S P H x j X a r 3 9 B j d G y d R u u M l o k L N V U E K X m D s 7 1 o i R S c y r Y v h k W i u W E b s m a z Q 1 N S c L U o q w f 2 K N X R l m h O J O m U o 1 q 9 e 5 E S R K l d k l k O q t z 1 b 9 e J f 7 P m x c 6 9 h c l T / N C s 5 Q e F s W F Q D p D V R p o x S W j W u w M I V R y c y u i G y I J 1 S a z Z h 1 C 0 H M 8 z z O / B 3 3 P 7 Q Y V 8 T 3 c G / w N Y e L a j m f j 9 7 3 2 c H R I A x r w E i 6 h A w 4 M Y A j v Y A R j o L C H z / A V b q x P 1 h f r m / X 9 0 H p k / Z m 5 g H u w f v w G y W S Y S Q = = &lt; / l a t e x i t &gt; F f µ (⇢) Figure 1: An illustration of refinement using DGf low, with the gradient flow in the 2-Wasserstein space P2 (top) and the corresponding discretized SDE in the latent space Z (bottom). The image samples from the densities along the gradient flow are shown in the middle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison of DGf low(KL) with DOT and DDLS on synthetic 2D datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Improvement in the quality of samples generated from the base model (leftmost columns) over the steps of DGf low for SN-ResNet-GAN and SN-DCGAN on the CIFAR10 and STL10 datasets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) JS-4 and JS-6 scores. Lower scores are better. (.0009) 0.574 (.0015) DGf low (KL) 0.212 (.0008) 0.512 (.0012) DGf low (JS) 0.186 (.0007) 0.508 (.0011) DGf low (log D) 0.209 (.0005) 0.506 (.0008) (b) Examples of text samples refined by DGf low.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison of DGf low with different f -divergences on the 25Gaussians and 2DSwissroll datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>different variants of DGf low applied to MMDGAN, OCFGAN-GP, VAE, and Glow generators in terms of the inception score. DGf low leads to a A vector plot showing the deterministic component of the velocity, i.e., the drift −∇xf (ρ0/µ)(x0), for different f -divergences on the 25Gaussians and 2DSwissroll dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Latent space recovered by DGf low (right) for the 2D datasets is same as the one derived by Che et al. (2020) (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(h) Glow + DGf lowFigure 8: Samples from different models for the CIFAR10 dataset before (left) and after (right) refinement using DGf low.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Samples from different models for the STL10 dataset before (left) and after (right) refinement using DGf low.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison on the 25Gaussians dataset. Higher scores are better.</figDesc><table><row><cell></cell><cell>% High Quality</cell><cell>KDE Score</cell></row><row><cell>GAN DOT DDLS</cell><cell>26.5 ± .8 69.8 ± .7 89.3 ± .6</cell><cell>-7037 ± 64 -4149 ± 39 -2997 ± 17</cell></row><row><cell>DGf low(KL) DGf low(JS) DGf low</cell><cell>89.5 ± .4 82.6 ± .4</cell><cell>-2893 ± 07 -3118 ± 19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different variants of DGf low with DOT on the CIFAR10 and STL10 datasets. For SN-DCGAN, (hi) denotes the hinge loss and (ns) denotes the non-saturating loss. Lower scores are better. DGf low's results have been averaged over 5 random runs with the standard deviation in parentheses.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Fréchet Inception Distance</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell cols="4">Base Model DOT DGf low(KL) DGf low(JS) DGf low (log D)</cell></row><row><cell>CIFAR10</cell><cell>WGAN-GP SN-DCGAN (hi) SN-DCGAN (ns) SN-ResNet-GAN</cell><cell>28.37 (.08) 24.14 20.70 (.05) 17.12 20.90 (.11) 15.78 14.10 (.06) -</cell><cell>24.68 (.09) 15.68 (.07) 15.30 (.08) 9.62 (.03)</cell><cell>23.15 (.07) 16.45 (.06) 15.90 (.11) 9.79 (.02)</cell><cell>24.53 (.11) 17.36 (.05) 16.42 (.05) 9.73 (.05)</cell></row><row><cell>STL10</cell><cell>WGAN-GP SN-DCGAN (hi) SN-DCGAN (ns)</cell><cell>51.50 (.15) 44.45 40.54 (.17) 34.85 41.86 (.12) 34.84</cell><cell>39.07 (.07) 34.95 (.06) 34.60 (.11)</cell><cell>50.83 (.06) 36.37 (.12) 35.37 (.12)</cell><cell>39.71 (.29) 36.56 (.08) 37.07 (.14)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Inception scores of different generative models, DRS, MH-GAN, DDLS, and DGf low on the CI-FAR10 dataset. Higher scores are better.</figDesc><table><row><cell>Model</cell><cell>Inception Score</cell></row><row><cell>WGAN-GP (Gulrajani et al., 2017)</cell><cell>7.86 (.07)</cell></row><row><cell>ProgressiveGAN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>(a) shows that DGf low leads to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different variants of DGf low applied to MMDGAN, OCFGAN-GP, VAE, and Glow models. Lower scores are better. Results have been averaged over 5 random runs with the standard deviation in parentheses.</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell cols="2">Fréchet Inception Distance</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Base Model DGf low(KL) DGf low(JS) DGf low (log D)</cell></row><row><cell>CIFAR10</cell><cell>MMDGAN OCFGAN-GP VAE Glow</cell><cell>41.98 (.12) 31.98 (.12) 129.5 (.13) 100.5 (.52)</cell><cell>36.75 (.09) 26.89 (.06) 116.0 (.21) 79.02 (.23)</cell><cell>38.06 (.14) 28.20 (.06) 128.9 (.13) 94.61 (.34)</cell><cell>37.75 (.10) 27.82 (.09) 115.2 (.06) 81.12 (.35)</cell></row><row><cell>STL10</cell><cell>MMDGAN OCFGAN-GP VAE</cell><cell>47.20 (.07) 36.55 (.08) 150.5 (.09)</cell><cell>43.21 (.06) 31.12 (.13) 130.1 (.18)</cell><cell>46.74 (.05) 36.05 (.11) 149.9 (.08)</cell><cell>43.06 (.05) 30.61 (.14) 132.5 (.28)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of DGf low on a character-level GAN language model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>(b) shows example sentences where DGf low</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Moving forward, we are considering several technical enhancements to improve DGf low's performance. At present, DGf low uses a stale estimate of the density-ratio, which could adversely affect sample evolution when the gradient flow is simulated for larger number of steps; how we can efficiently update this estimate is an open question. Another related question is when the evolution of the samples should be stopped; running chains for too long may modify characteristics of the original sample (e.g., orientation and color) which may be undesirable. This issue does not just affect DGf low; a method for automatically stopping sample evolution could improve results across refinement techniques.Proof. Gradient flows in the Wasserstein space are of the form of the continuity equation (see<ref type="bibr" target="#b1">Ambrosio et al. (2008)</ref>, page 281), i.e,</figDesc><table><row><cell>A PROOFS</cell></row><row><cell>A.1 LEMMA 3.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Network architectures used for MMDGAN and VAE models.</figDesc><table><row><cell>(a) Generator or Decoder</cell><cell>(b) Discriminator or Encoder</cell></row><row><cell>Input Shape: (b, d, 1, 1)</cell><cell>Input Shape: (b, 3, 32, 32)</cell></row><row><cell>Upconv(256)</cell><cell>Conv(64)</cell></row><row><cell>BatchNorm</cell><cell>LeakyReLU(0.2)</cell></row><row><cell>ReLU</cell><cell>Conv(128)</cell></row><row><cell>Upconv(128)</cell><cell>BatchNorm</cell></row><row><cell>BatchNorm</cell><cell>LeakyReLU(0.2)</cell></row><row><cell>ReLU</cell><cell>Conv(256)</cell></row><row><cell>Upconv(64)</cell><cell>BatchNorm</cell></row><row><cell>BatchNorm</cell><cell>LeakyReLU(0.2)</cell></row><row><cell>ReLU</cell><cell>Conv(m)</cell></row><row><cell>Upconv(3) Tanh</cell><cell>Output Shape</cell></row><row><cell>Output Shape: (b, 3, 32, 32)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>f -divergences and their derivatives.</figDesc><table><row><cell>f -divergence</cell><cell>f</cell><cell>f</cell><cell>f</cell></row><row><cell>KL JS log D</cell><cell cols="2">r log r r log r − (r + 1) log r+1 2 (r + 1) log(r + 1) − 2 log 2 log(r + 1) + 1 log r + 1 log 2r r+1</cell><cell>1 r r 2 +r 1 1 r+1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Runtime comparison of DOT, DDLS, and DGf low(KL) on the 25Gaussians dataset. The runtime is averaged over 100 runs with standard deviation reported in parentheses. the quality of samples for all the models.</figDesc><table><row><cell>Method</cell><cell>Runtime (s) per 5K samples</cell></row><row><cell>DOT</cell><cell>2.24 (0.18)</cell></row><row><cell>DDLS</cell><cell>2.23 (0.14)</cell></row><row><cell>DGf low</cell><cell>2.22 (0.15)</cell></row><row><cell>significant improvement in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Runtime of DGf low(KL) for models that do not require density-ratio correction on a single GeForce RTX 2080 Ti GPU. The runtime is averaged over 100 runs with standard deviation reported in parentheses.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Runtime (s) per 100 samples</cell></row><row><cell>CIFAR10</cell><cell>WGAN-GP SN-DCGAN (hi) SN-DCGAN (ns) SN-ResNet-GAN</cell><cell>0.897 (0.017) 0.952 (0.008) 0.952 (0.007) 1.982 (0.013)</cell></row><row><cell>STL10</cell><cell>WGAN-GP SN-DCGAN (hi) SN-DCGAN (ns)</cell><cell>1.376 (0.025) 1.413 (0.015) 1.415 (0.013)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Runtime of DGf low(KL) for models that require density-ratio correction on a single GeForce RTX 2080 Ti GPU. The runtime is averaged over 100 runs with standard deviation reported in parentheses.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Runtime (s) per 100 samples</cell></row><row><cell>CIFAR10</cell><cell>MMDGAN OCFGAN-GP VAE</cell><cell>1.192 (0.007) 1.186 (0.011) 1.186 (0.012)</cell></row><row><cell>STL10</cell><cell>MMDGAN OCFGAN-GP VAE</cell><cell>1.036 (0.004) 1.029 (0.010) 1.028 (0.011)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Comparison of different variants of DGf low with DOT on the CIFAR10 and STL10 datasets. Higher scores are better.</figDesc><table><row><cell></cell><cell>Inception Score</cell></row><row><cell>Model</cell><cell>Base Model DOT DGf low(KL) DGf low(JS) DGf low (log D)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Comparison of different variants of DGf low without diffusion (i.e., γ = 0) on the CIFAR10 and STL10 datasets. Lower scores are better.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Fréchet Inception Distance</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell cols="4">Base Model DOT DGf low(KL) DGf low(JS) DGf low (log D)</cell></row><row><cell>CIFAR10</cell><cell>WGAN-GP SN-DCGAN (hi) SN-DCGAN (ns)</cell><cell>28.34 (.11) 24.14 20.67 (.09) 17.12 20.94 (.12) 15.78</cell><cell>24.64 (.13) 15.79 (.07) 15.47 (.11)</cell><cell>23.30 (.11) 16.79 (.09) 16.32 (.11)</cell><cell>24.42 (.19) 17.79 (.05) 16.97 (.08)</cell></row><row><cell>STL10</cell><cell>WGAN-GP SN-DCGAN (hi) SN-DCGAN (ns)</cell><cell>51.34 (.21) 44.45 40.82 (.16) 34.85 41.83 (.20) 34.84</cell><cell>38.96 (.08) 35.18 (.09) 34.81 (.08)</cell><cell>50.44 (.09) 36.53 (.13) 35.75 (.10)</cell><cell>39.35 (.12) 36.75 (.13) 37.68 (.08)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Comparison of DDLS with DGf low (with and without diffusion) on the CIFAR10 dataset. Higher scores are better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>Comparison of different variants of DGf low applied to MMDGAN, OCFGAN-GP, and VAE models without density-ratio correction. Lower scores are better. VAE 150.49 (.07) 151.76 (.01) 152.03 (.05) 151.88 (.11)</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="3">Fréchet Inception Distance</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Base Model</cell><cell>KL</cell><cell>JS</cell><cell>log D</cell></row><row><cell>CIFAR10</cell><cell cols="4">MMDGAN OCFGAN-GP VAE 129.49 (.19) 127.50 (.15) 128.24 (.11) 42.03 (.06) 39.06 (.08) 39.68 (.06) 31.95 (.07) 27.92 (.08) 29.25 (.06)</cell><cell>39.47 (.07) 28.82 (.10) 128.3 (.14)</cell></row><row><cell></cell><cell>Glow</cell><cell>100.7 (.14)</cell><cell>93.47 (.09)</cell><cell>97.50 (.11)</cell><cell>97.78 (.14)</cell></row><row><cell>STL10</cell><cell>MMDGAN OCFGAN-GP</cell><cell>47.22 (.04) 36.60 (.15)</cell><cell>45.75 (.10) 34.17 (.18)</cell><cell>45.96 (.07) 34.42 (.04)</cell><cell>46.26 (.13) 34.99 (.07)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For further discussion about these techniques, please refer to appendix C.2  This implies that f (t) = t log t − t + 1, which is a twice-differentiable convex function with f (0) = 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We experimented with three f -divergences, namely the Kullback-Leibler (KL) divergence, the Jensen-Shannon (JS) divergence, and the log D divergence <ref type="bibr" target="#b15">(Gao et al., 2019)</ref>. The specific forms of the functions f and corresponding derivatives are tabulated in <ref type="table">Table 7</ref> (appendix). We compare DGf low with two state-of-the-art competing methods: DOT and DDLS. In this section we discuss the main results and relegate details to the appendix. Our code is available online at https://github.com/clear-nus/DGflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">2D DATASETS</head><p>We first tested DGf low on two synthetic datasets, 25Gaussians and 2DSwissroll, to visually inspect the improvement in the quality of generated samples. We generated 5000 samples from a trained WGAN-GP generator and refined them using DOT, DDLS, and DGf low. We performed refinement in the latent space for DDLS and directly in the data-space for DOT and DGf low. We trained our own models for MMDGAN, VAE, and Glow. We used the generator and discriminator architectures shown in <ref type="table">Table 6</ref> for MMDGAN with d = 32. VAE used the same architecture with d = 64. Our Glow model was trained using the code available at https: //github.com/y0ast/Glow-PyTorch with a batch size of 56 for 150 epochs. The density ratio correctors, D λ (see section 3.2), were initialized with the weights from the SN-DCGAN (ns) released by <ref type="bibr" target="#b45">Tanaka (2019)</ref>. D λ was then fine-tuned on images from SN-DCGAN (ns)'s generator and the generator being improved (e.g., MMDGAN and OCFGAN-GP) using SGD with a learning rate of 10 −4 and momentum of 0.9. We fine-tuned D λ for 10000 iterations with a batch size of 64.</p><p>Base Models for STL10 We used the publicly available pre-trained models <ref type="bibr" target="#b45">(Tanaka, 2019;</ref><ref type="bibr" target="#b2">Ansari et al., 2020)</ref> for WGAN-GP, SN-DCGAN (hi), SN-DCGAN (ns), and OCFGAN-GP. We trained our own models for MMDGAN and VAE with the same architecture and training details as CIFAR10. We fine-tuned the density ratio correctors for STL10 for 5000 iterations with other details being the same as CIFAR10.</p><p>Hyperparameters We performed 25 updates of DGf low for CIFAR10 and STL10 with a step size of 0.1 for models that do not require density ratio corrections. For STL10 models that require a density ratio correction, we performed 15 updates with a step size of 0.05. The noise regularizer (γ), whenever used, was set to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>We used the Fréchet Inception Distance (FID) <ref type="bibr" target="#b22">(Heusel et al., 2017)</ref> and Inception Score (IS) <ref type="bibr" target="#b40">(Salimans et al., 2016)</ref> metrics to evaluate the quality of generated samples before and after refinement. The IS denotes the confidence in classification of the generated samples using a pretrained InceptionV3 network whereas the FID is the Fréchet distance between multivariate Gaussians fitted to the 2048 dimensional feature vectors extracted from the InceptionV3 network for real and generated data. Both the metrics were computed using 50K samples for all the models, except Glow</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two numerical approaches to stationary mean-field games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noha</forename><surname>Almulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamic Games and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gradient flows: in metric spaces and in the space of probability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Gigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Savaré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A characteristic function approach to deep implicit generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Abdul Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Scarlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum mean discrepancy gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adil</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalized energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminator rejection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The change-of-variables formula using matrix volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Ben-Israel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Numerical methods for stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Wolf-Jurgen Beyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Book</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Vlasov dynamics and its fluctuations in the 1/n limit of interacting classical particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in mathematical physics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Residual energy-based models for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep generative learning via variational gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuling</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunkang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning implicit generative models with theoretical guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuling</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02862</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mevlana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02304</idno>
		<title level="m">Normalizing flows on Riemannian manifolds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Miguel Lazaro-Gredilla, Dale Schuurmans, and Stefano Ermon. Variational rejection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramki</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bias correction of learned generative models using likelihood-free importance weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improved training of Wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The variational formulation of the Fokker-Planck equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on mathematical analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MMDGAN: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stein variational gradient descent as gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Simsekli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Majewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A macroscopic crowd motion model of gradient flow type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Maury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Roudneff-Chupin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Santambrogio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Models and Methods in Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Handling congestion in crowd motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Maury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Roudneff-Chupin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Santambrogio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliette</forename><surname>Venel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.4102</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sobolev descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anant</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The geometry of dissipative evolution equations: the porous medium equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Otto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Telescoping density-ratio estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fokker-Planck equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Risken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview</title>
	</analytic>
	<monogr>
		<title level="j">Bulletin of Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Density ratio estimation in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takafumi</forename><surname>Kanamori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discriminator optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Metropolis-Hastings generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Lillicrap</forename><surname>Logan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00953</idno>
		<title level="m">Latent optimisation for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Comparison of different variants of DGf low applied to MMDGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vae</forename><surname>Ocfgan-Gp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glow</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">WGAN-GP + DGf low (c) SN-DCGAN (hi) (d) SN-DCGAN (hi) + DGf low</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wgan-Gp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<title level="m">SN-DCGAN (ns) (f) SN-DCGAN (ns) + DGf low (g) SN-ResNet-GAN (h) SN-ResNet-GAN + DGf low Figure 7: Samples from different models for the CIFAR10 dataset before (left) and after (right) refinement using DGf low. (a) WGAN-GP (b) WGAN-GP + DGf low (c) SN-DCGAN (hi) (d) SN-DCGAN (hi) + DGf low</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<title level="m">SN-DCGAN (ns) (f) SN-DCGAN (ns) + DGf low Figure 9: Samples from different models for the STL10 dataset before (left) and after (right) refinement using DGf low</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
