<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TABFACT: A LARGE-SCALE DATASET FOR TABLE- BASED FACT VERIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<email>wenhuchen@ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
							<email>hongminwang@ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
							<email>yunkaizhang@ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
							<email>hongwang600@ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@ucsb.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TABFACT: A LARGE-SCALE DATASET FOR TABLE- BASED FACT VERIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: <ref type="table">Table-</ref>BERT and Latent Program Algorithm (LPA). <ref type="table">Table-</ref>BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in https://github.com/wenhuchen/Table-Fact-Checking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Verifying whether a textual hypothesis is entailed or refuted by the given evidence is a fundamental problem in natural language understanding <ref type="bibr" target="#b18">(Katz &amp; Fodor, 1963;</ref><ref type="bibr" target="#b35">Van Benthem et al., 2008)</ref>. It can benefit many downstream applications like misinformation detection, fake news detection, etc.</p><p>Recently, the first-ever end-to-end fact-checking system has been designed and proposed in <ref type="bibr" target="#b14">Hassan et al. (2017)</ref>. The verification problem has been extensively studied under different natural language tasks such as recognizing textual entailment (RTE) <ref type="bibr" target="#b8">(Dagan et al., 2005)</ref>, natural language inference (NLI) <ref type="bibr" target="#b6">(Bowman et al., 2015)</ref>, claim verification <ref type="bibr" target="#b29">(Popat et al., 2017;</ref><ref type="bibr" target="#b13">Hanselowski et al., 2018;</ref><ref type="bibr" target="#b34">Thorne et al., 2018)</ref> and multimodal language reasoning (NLVR/NLVR2) <ref type="bibr" target="#b32">(Suhr et al., 2017;</ref>. RTE and NLI view a premise sentence as the evidence, claim verification views passage collection like Wikipedia 1 as the evidence, NLVR/NLVR2 views images as the evidence. These problems have been previously addressed using a variety of techniques including logic rules, knowledge bases, and neural networks. Recently large-scale pre-trained language models <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref><ref type="bibr" target="#b28">Peters et al., 2018;</ref><ref type="bibr" target="#b42">Yang et al., 2019;</ref> have surged to dominate the other algorithms to approach human performance on several textual entailment tasks .</p><p>However, existing studies are restricted to dealing with unstructured text as the evidence, which would not generalize to the cases where the evidence has a highly structured format. Since such structured evidence <ref type="bibr">(graphs, tables, or databases)</ref> are also ubiquitous in real-world applications like database systems, dialog systems, commercial management systems, social networks, etc, we argue that the fact verification under structured evidence forms is an equivalently important yet underexplored problem. Therefore, in this paper, we are specifically interested in studying fact verification with semi-structured Wikipedia tables <ref type="bibr" target="#b5">(Bhagavatula et al., 2013)</ref> 2 as evidence owing to its structured and ubiquitous nature <ref type="bibr" target="#b16">(Jauhar et al., 2016;</ref><ref type="bibr">Zhong et al., 2017;</ref><ref type="bibr" target="#b27">Pasupat &amp; Liang, 2015)</ref>. To this end, we introduce a large-scale dataset called TABFACT, which consists of 118K manually annotated statements with regard to 16K Wikipedia tables, their relations are classified as ENTAILED and REFUTED 3 . The entailed and refuted statements are both annotated by human workers. With some examples in <ref type="figure" target="#fig_0">Figure 1</ref>, we can clearly observe that unlike the previous verification related problems, TABFACT combines two different forms of reasoning in the statements, (i) Linguistic Reasoning: the verification requires semantic-level understanding. For example, "John J. Mcfall failed to be re-elected though being unopposed." requires understanding over the phrase "lost renomination ..." in the table to correctly classify the entailment relation. Unlike the existing QA datasets <ref type="bibr">(Zhong et al., 2017;</ref><ref type="bibr" target="#b27">Pasupat &amp; Liang, 2015)</ref>, where the linguistic reasoning is dominated by paraphrasing, TABFACT requires more linguistic inference or common sense. (ii) Symbolic Reasoning: the verification requires symbolic execution on the table structure. For example, the phrase "There are three Democrats incumbents" requires both condition operation (where condition) and arithmetic operation (count). Unlike question answering, a statement could contain compound facts, all of these facts need to be verified to predict the verdict. For example, the "There are ..." in <ref type="figure" target="#fig_0">Figure 1</ref> requires verifying three QA pairs (total count=5, democratic count=2, republic count=3). The two forms of reasoning are interleaved across the statements making it challenging for existing models.</p><p>In this paper, we particularly propose two approaches to deal with such mixed-reasoning challenge: (i) Table-BERT, this model views the verification task completely as an NLI problem by linearizing a table as a premise sentence p, and applies state-of-the-art language understanding pre-trained model to encode both the table and statements h into distributed representation for classification. This model excels at linguistic reasoning like paraphrasing and inference but lacks symbolic reasoning skills. (ii) Latent Program Algorithm, this model applies lexical matching to find linked entities and triggers to filter pre-defined APIs (e.g. argmax, argmin, count, etc). We adopt bread-first-search with memorization to construct the potential program candidates, a discriminator is further utilized to select the most "consistent" latent programs. This model excels at the symbolic reasoning aspects by executing database queries, which also provides better interpretability by laying out the decision rationale. We perform extensive experiments to investigate their performances: the best-achieved accuracy of both models are reasonable, but far below human performance. Thus, we believe that the proposed table-based fact verification task can serve as an important new benchmark towards the goal of building powerful AI that can reason over both soft linguistic form and hard symbolic forms. To facilitate future research, we released all the data, code with the intermediate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TABLE FACT VERIFICATION DATASET</head><p>First, we follow the previous <ref type="table" target="#tab_6">Table-</ref>based Q&amp;A datasets <ref type="bibr" target="#b27">(Pasupat &amp; Liang, 2015;</ref><ref type="bibr">Zhong et al., 2017)</ref> to extract web tables <ref type="bibr" target="#b5">(Bhagavatula et al., 2013)</ref> with captions from WikiTables 4 . Here we filter out overly complicated and huge tables (e.g. multirows, multicolumns, latex symbol) and obtain 18K relatively clean tables with less than 50 rows and 10 columns.</p><p>For crowd-sourcing jobs, we follow the human subject research protocols 5 to pay Amazon Mechanical Turk 6 workers from the native English-speaking countries "US, GB, NZ, CA, AU" with approval rates higher than 95% and more than 500 accepted HITs. Following WikiTableQuestion <ref type="bibr" target="#b27">(Pasupat &amp; Liang, 2015)</ref>, we provide the annotators with the corresponding table captions to help them better understand the background. To ensure the annotation quality, we develop a pipeline of "positive two-channel annotation" → "negative statement rewriting" → "verification", as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">POSITIVE TWO-CHANNEL COLLECTION &amp; NEGATIVE REWRITING STRATEGY</head><p>To harvest statements of different difficulty levels, we design a two-channel collection process: Low-Reward Simple Channel: the workers are paid 0.45 USD for annotating one Human Intelligent Task (HIT) that requires writing five statements. The workers are encouraged to produce plain statements meeting the requirements: (i) corresponding to a single row/record in the table with unary fact without involving compound logical inference. (ii) mention the cell values without dramatic modification or paraphrasing. The average annotation time of a HIT is 4.2 min. High-Reward Complex Channel: the workers are paid 0.75 USD for annotating a HIT (five statements). They are guided to produce more sophisticated statements to meet the requirements: (i) involving multiple rows in the tables with higher-order semantics like argmax, argmin, count, difference, average, summarize, etc. (ii) rephrase the table records to involve more semantic understanding. The average annotation time of a HIT is 6.8 min. The data obtained from the complex channel are harder in terms of both linguistic and symbolic reasoning, the goal of the two-channel split is to help us understand the proposed models can reach under different levels of difficulty.</p><p>As suggested in <ref type="bibr">(Zellers et al., 2018)</ref>, there might be annotation artifacts and conditional stylistic patterns such as length and word-preference biases, which can allow shallow models (e.g. bag-ofwords) to obtain artificially high performance. Therefore, we design a negative rewriting strategy to minimize such linguistic cues or patterns. Instead of letting the annotators write negative statements from scratch, we let them rewrite the collected entailed statements. During the annotation, the workers are explicitly guided to modify the words, phrases or sentence structures but retain the sentence style/length to prevent artificial cues. We disallow naive negations by adding "not, never, etc" to revert the statement polarity in case of obvious linguistic patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">QUALITY CONTROL</head><p>To control the quality of the annotation process, we review a randomly sampled statement from each HIT to decide whether the whole annotation job should be rejected during the annotation process. Specifically, a HIT must satisfy the following criteria to be accepted: (i) the statements should contain neither typos nor grammatical errors. (ii) the statements do not contain vague claims like might, few, etc. (iii) the claims should be explicitly supported or contradicted by the table without requiring the additional knowledge, no middle ground is permitted. After the data collection, we re-distribute all the annotated samples to further filter erroneous statements, the workers are paid 0.05 USD per statement to decide whether the statement should be rejected. The criteria we apply are similar: no ambiguity, no typos, explicitly supported or contradictory. Through the post-filtering process, roughly 18% entailed and 27% refuted instances are further abandoned due to poor quality.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DATASET STATISTICS</head><p>Inter-Annotator Agreement: After the data collection pipeline, we merged the instances from two different channels to obtain a diverse yet clean dataset for table-based fact verification. We sample 1000 annotated (table, statement) pairs and re-distribute each to 5 individual workers to re-label them as either ENTAILED or REFUTED. We follow the previous works <ref type="bibr" target="#b34">(Thorne et al., 2018;</ref><ref type="bibr" target="#b6">Bowman et al., 2015)</ref> to adopt the Fleiss Kappa <ref type="bibr" target="#b11">(Fleiss, 1971)</ref> as an indicator, where Fleiss κ =p c −pe 1−pe is computed from from the observed agreementp c and the agreement by chancep e . We obtain a Fleiss κ = 0.75, which indicates strong inter-annotator agreement and good-quality.</p><p>Dataset Statistics: As shown in <ref type="table" target="#tab_1">Table 1</ref>, the amount of data harvested via the complex channel slightly outnumbers the simple channel, the averaged length of both the positive and negative samples are indistinguishable. More specifically, to analyze to which extent the higher-order operations are included in two channels, we group the common higher-order operations into 8 different categories. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we sample 200 sentences from two different channels to visualize their distribution. We can see that the complex channel overwhelms the simple channel in terms of the higher-order logic, among which, count and superlatives are the most frequent. We split the whole data roughly with 8:1:1 into train, validation 7 , and test splits and shows their statistics in <ref type="table" target="#tab_1">Table 1</ref>. Each table with an average of 14 rows and 5-6 columns corresponds to 2-20 different statements, while each cell has an average of 2.1 words. In the training split, the positive instances slightly outnumber the negative instances, while the validation and test split both have rather balanced distributions over positive and negative instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODELS</head><p>With the collected dataset, we now formally define the table-based fact verification task: the dataset is comprised of triple instances (T, S, L) consisting of a table T, a natural language statement S = s 1 , · · · , s n and a verification label L ∈ {0, 1}. The table T = {T i,j |i ≤ R T , j ≤ C T } has R T rows and C T columns with the T ij being the content in the (i, j)-th cell. T ij could be a word, a number, a phrase, or even a natural language sentence. The statement S describes a fact to be verified against the content in the table T. If it is entailed by T, then L = 1, otherwise the label L = 0. <ref type="figure" target="#fig_0">Figure 1</ref> shows some entailed and refuted examples. During training, the model and the learning algorithm are presented with K instances like (T, S, L) K k=1 from the training split. In the testing stage, the model is presented with (T, S) K k=1 and supposed to predict the label asL. We measure the performance by the prediction accuracy Acc = 1 K K 1 I(L k = L k ) on the test set. Before building the model, we first perform entity linking to detect all the entities in the statements. Briefly, we first lemmatize the words and search for the longest sub-string matching pairs between statements and table cells/captions, where the matched phrases are denoted as the linked entities. To focus on statement verification against the table, we do not feed the caption to the model and simply mask the phrases in the statements which link to the caption with placeholders. The details of the entity linker are listed in the Appendix. We describe our two proposed models as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LATENT PROGRAM ALGORITHM (LPA)</head><p>In this approach, we formulate the table fact verification as a program synthesis problem, where the latent program algorithm is not given in TABFACT. Thus, it can be seen as a weakly supervised learning problem as discussed in <ref type="bibr" target="#b20">Liang et al. (2017)</ref>; <ref type="bibr" target="#b19">Lao et al. (2011)</ref>. Under such a setting, we propose to break down the verification into two stages: (i) latent program search, (ii) discriminator ranking. In the first program synthesis step, we aim to parse the statement into programs to represent its semantics. We define the plausible API set to include roughly 50 different functions like min, max, count, average, filter, and and realize their interpreter with Python-Pandas. Each API is defined to take arguments of specific types (number, string, bool, and view (e.g sub-table)) to output specifictype variables. During the program execution, we store the generated intermediate variables to different-typed caches N , R, B, V (Num, Str, Bool, View). At each execution step, the program can fetch the intermediate variable from the caches to achieve semantic compositionality. In order to shrink the search space, we follow NSM <ref type="bibr" target="#b20">(Liang et al., 2017)</ref> to use trigger words to prune the API set and accelerate the search speed. The definitions of all API, trigger words can be found in the Appendix. The comprehensive the latent program search procedure is summarized in Algorithm 1,</p><formula xml:id="formula_0">Algorithm 1 Latent Program Search with Comments 1: Initialize Number Cache N , String Cache R, Bool Cache B, View Cache V → ∅ 2:</formula><p>Push linked numbers, strings from the given statement S into N , R, and push T into V 3: Initialize the result collector P → ∅ and an empty program trace P = ∅ 4: Initialize the Queue Q = [(P, N , R, B, V)], we use Q to store the intermediate states 5: Use trigger words to find plausible function set F, for example, more will trigger Greater function. 6: while loop over time t = 1 → MAXSTEP do:</p><formula xml:id="formula_1">7: while (P, N , R, B, V) = Q.pop() do: 8: while loop over function set f ∈ F do: 9:</formula><p>if arguments of f are in the caches then 10:</p><p>Pop out the required arguments arg1, arg2, · · · , argn for different cachess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Execute A = f (arg1, · · · , argn) and concatenate the program trace P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>if Type(A)=Bool then 13:</p><p>if N = S = B = ∅ then 14:</p><p>P.push((P, A)) # The program P is valid since it consumes all the variables. 15: P = ∅ # Collect the valid program P into set P and reset P 16: else 17:</p><p>B.push(A) # The intermediate boolean value is added to the bool cache 18:</p><p>Q.push((P, N , R, B, V)) # Add the refreshed state to the queue again 19:</p><p>if Type(A) ∈ {Num, Str, View} then 20:</p><p>if N = S = B = ∅ then 21: P = ∅;break # The program ends without consuming the cache, throw it. 22: else 23:</p><p>push A into N or S or V # Add the refreshed state to the queue for further search 24:</p><p>Q.push((P, N , R, B, V)) 25: Return the triple (T, S, P) # Return <ref type="table">(Table, Statement</ref> <ref type="bibr">, Program Set)</ref> and the searching procedure is illustrated in <ref type="figure">Figure 3</ref>.</p><p>After we collected all the potential program candidates P = {(P 1 , A 1 ), · · · , (P n , A n )} for a given statement S (where (P i , A i ) refers to i-th candidate), we need to learn a discriminator to identify the "appropriate" traces from the set from many erroneous and spurious traces. Since we do not have the ground truth label about such discriminator, we use a weakly supervised training algorithm by viewing all the label-consistent programs as positive instances {P i |(P i , A i ); A i = L} and the label-inconsistent program as negative instances {P i |(P i , A i ); A i = L} to minimize the cross-entropy of discriminator p θ (S, P ) with the weakly supervised label. Specifically, we build our discriminator with a Transformer-based two-way encoder <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, where the statement encoder encodes the input statement S as a vector Enc S (S) ∈ R n×D with dimension D, while the program encoder encodes the program P = p 1 , · · · , p m as another vector Enc P (P ) ∈ R m×D , we concatenate these two vectors and feed it into a linear projection layer There are more democrats than republicans in the election.  <ref type="table">Table   LISP Engine</ref> Entailed S e a r c h <ref type="figure">Figure 3</ref>: The program synthesis procedure for the table in <ref type="figure" target="#fig_0">Figure 1</ref>. We link the entity (e.g. democratic, republican), and then composite functions on the fly to return the values from the  <ref type="figure">Figure 4</ref>: The diagram of <ref type="table" target="#tab_6">Table-</ref>BERT with horizontal scan, two different linearizations are depicted.</p><p>to compute p θ (S, P ) = σ(v T p [Enc S (S); Enc P (P )]) as the relevance between S and P with weight v p ∈ R D . At test time, we use the discriminator p θ to assign confidence p θ (S, P ) to each candidate P ∈ P, and then either aggregate the prediction from all hypothesis with the confidence weights or rank the highest-confident hypothesis and use their outputs as the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TABLE-BERT</head><p>In this approach, we view the table verification problem as a two-sequence binary classification problem like NLI or MPRC  by linearizing a table T into a sequence and treating the statement as another sequence. Since the linearized table can be extremely long surpassing the limit of sequence models like LSTM, Transformers, etc. We propose to shrink the sequence by only retaining the columns containing entities linked to the statement to alleviate such a memory issue. In order to encode such sub-table as a sequence, we propose two different linearization methods, as is depicted in <ref type="figure">Figure 4</ref>. (i) Concatenation: we simply concatenate the table cells with [SEP] tokens in between and restart position counter at the cell boundaries; the column name is fed as another type embedding to the input layer. Such design retains the table information in its machine format. (ii) Template: we adopt simple natural language templates to transform a table into a "somewhat natural" sentence. Taking the horizontal scan as an example, we linearize a table as "row one's game is 51; the date is February; ..., the score is 3.4 (ot). row 2 is ...". The isolated cells are connected with punctuations and copula verbs in a language-like format.</p><p>After obtaining the linearized sub-tableT, we concatenate it with the natural language statement S and prefix a [CLS] token to the sentence to obtain the sequence-level representation H = f BERT ([T, S]), with H ∈ R 768 from pre-trained BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. The representation is further fed into multi-layer perceptron f M LP to obtain the entailment probability p θ (T, S) = σ(f M LP (H)), where σ is the sigmoid function. We finetune the model θ (including the parameters of BERT and MLP) to minimize the binary cross entropy L(p θ (T, S), L) on the training set. At test time, we use the trained BERT model to compute the matching probability between the (table, statement) pair, and classify it as ENTAILED statement when p θ (T, S) is greater than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we aim to evaluate the proposed methods on TABFACT. Besides the standard validation and test sets, we also split the test set into a simple and a complex partition based on the channel from which they were collected. This facilitates analyzing how well the model performs under different levels of difficulty. Additionally, we also hold out a small test set with 2K samples for human evaluation, where we distribute each (table, statement) pair to 5 different workers to approximate human judgments based on their majority voting, the results are reported in <ref type="table" target="#tab_5">Table 2</ref>  NSM We follow <ref type="bibr" target="#b20">Liang et al. (2017)</ref> to modify their approach to fit the setting of TABFACT. Specifically, we adopt an LSTM as an encoder and another LSTM with copy mechanism as a decoder to synthesize the program. However, without any ground truth annotation for the intermediate programs, directly training with reinforcement learning is difficult as the binary reward is underspecified, which is listed in <ref type="table" target="#tab_5">Table 2</ref> as "NSM w/ RL". Further, we use LPA as a teacher to search the top programs for the NSM to bootstrap and then use reinforcement learning to finetune the model, which achieves reasonable performance on our dataset listed as "NSM w/ ML + RL". <ref type="table" target="#tab_6">Table-</ref>BERT We build <ref type="table" target="#tab_6">Table-</ref>BERT based on the open-source implementation of BERT 8 using the pre-trained model with 12-layer, 768-hidden, 12-heads, and 110M parameters trained in 104 languages. We use the standard BERT tokenizer to break the words in both statements and tables into subwords and join the two sequences with a [SEP] token in between. The representation corresponding to [CLS] is fed into an MLP layer to predict the verification label. We finetune the model on a single TITAN X GPU with a mini-batch size of 6. The best performance is reached after about 3 hours of training (around 10K steps). We implement and compare the following variants of the LPA We run the latent program search in a distributed fashion on three 64-core machines to generate the latent programs. The search terminates once the buffer has more than 50 traces or the path length is larger than 7. The average search time for each statement is about 2.5s. For the discriminator model, we design two transformer-based encoders (3 layers, 128-dimension hidden embedding, and 4 heads at each layer) to encode the programs and statements, respectively. The variants of LPA models considered include (i) Voting: assign each program with equal weight and vote without the learned discriminator. (ii) Weighted-Voting: compute a weighted-sum to aggregate the predictions of all latent programs with the discriminator confidence as the weights. (iii) Ranking: rank all the hypotheses by the discriminator confidence and use the top-rated hypothesis as the output. (Caption) means feeding the caption as a sequence of words to the discriminator during ranking. Preliminary Evaluation In order to test whether our negative rewriting strategy eliminates the artifacts or shallow cues, we also fine-tune a pre-trained BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> to classify the statement S without feeding in table information. The result is reported as "BERT classifier w/o <ref type="table">Table"</ref> in <ref type="table" target="#tab_5">Table 2</ref>, which is approximately the majority guess and reflects the effectiveness of the rewriting strategy. Before presenting the experiment results, we first perform a preliminary study to evaluate how well the entity linking system, program search, and the statement-program discriminator perform. Since we do not have the ground truth labels for these models, we randomly sample 100 samples from the dev set to perform the human study. For the entity linking, we evaluate its accuracy as the number of correctly linked sentences / total sentences. For the latent program search, we evaluate whether the "true" programs are included in the candidate set P as recall score.</p><p>Results We report the performance of different methods as well as human performance in <ref type="table" target="#tab_5">Table 2</ref>. First of all, we observe that the naive serialized model fails to learn anything effective (same as the Majority Guess). It reveals the importance of template when using the pre-trained BERT <ref type="bibr" target="#b9">(Devlin et al., 2019</ref>) model: the "natural" connection words between individual cells is able to unleash the power of the large pre-trained language model and enable it to perform reasoning on the structured table form. Such behavior is understandable given the fact that BERT is pre-trained on purely natural language corpora. In addition, we also observe that the horizontal scan excels in the vertical scan because it better captures the convention of human expression. Among different LPA methods, we found that LPA-Ranking performs the best since it can better suppress the spurious programs than the voting-based algorithm. Overall, the LPA model is on par with <ref type="table" target="#tab_6">Table-</ref>BERT on both simple and test split without any pre-training on external corpus, which reflects the effectiveness of LPA to leverage symbolic operations in the verification process.</p><p>Through our human evaluation, we found that only 58% of sentences have been correctly linked without missing-link or over-link, while the systematic search has a recall of 51% under the cases where the sentence is correctly linked. With that being said, the chance for LPA method to cover the correct program (rationale) is roughly under 30%. After the discriminator's re-ranking step, the probability of selecting these particular oracle program is even much lower. However, we still observe a final overall accuracy of 65%, which indicates that the spurious problem is quite severe in LPA, where the correct label is predicted based on the wrong reason.</p><p>Through our human evaluation, we also observe that <ref type="table" target="#tab_6">Table-</ref>BERT exhibits poor consistency as it can misclassify simple cases but correctly-classify hard cases. These two major weaknesses are yet to be solved in future studies. In contrast, LPA behaves much more consistently and provides a clear latent rationale for its decision. But, such a pipeline system requires laborious handcrafting of API operations and is also very sensitive to the entity linking accuracy. Both methods have pros and cons; how to combine them still remains an open question.</p><p>Program Annotation To further promote the development of different models in our dataset, we collect roughly 1400 human-annotated programs paired with the original statements. These statements include the most popular logical operations like superlative, counting, comparison, unique, etc. We provide these annotations in Github 9 , which can either be used to bootstrap the semantic parsers or provide the rationale for NLI models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Natural Language Inference &amp; Reasoning: Modeling reasoning and inference in human language is a fundamental and challenging problem towards true natural language understanding. There has been extensive research on RTE in the early years <ref type="bibr" target="#b8">(Dagan et al., 2005)</ref> and more recently shifted to <ref type="bibr">NLI (Bowman et al., 2015;</ref><ref type="bibr" target="#b41">Williams et al., 2017)</ref>. NLI seeks to determine whether a natural language hypothesis h can be inferred from a natural language premise p. With the surge of deep learning, there have been many powerful algorithms like the Decomposed Model <ref type="bibr" target="#b26">(Parikh et al., 2016)</ref>, Enhanced-LSTM  and BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. Besides the textual evidence, NLVR <ref type="bibr" target="#b32">(Suhr et al., 2017)</ref> and NLVR2 <ref type="bibr" target="#b33">(Suhr et al., 2019)</ref> have been proposed to use images as the evidence for statement verification on multi-modal setting. Our proposed fact verification task is closely related to these inference tasks, where our semi-structured table can be seen as a collection of "premises" exhibited in a semi-structured format. Our proposed problem hence could be viewed as the generalization of NLI under the semi-structured domain. Jordi Arrese achieves better score in 1986 than in 1985.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jordi Arrese</head><p>Jordi Arrese won both of the final games in 1986.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1986: Winner 1986: Runner-up</head><p>Linguistic Inference 7 -5 , 6 -4</p><p>Mathematic Inference <ref type="formula">(2)</ref> (1) <ref type="figure">Figure 5</ref>: The two uniqueness of <ref type="table" target="#tab_3">Table-based fact verification against standard QA problems.</ref> question answering, such as MCQ (Jauhar et al., 2016), WikiTableQuestion <ref type="bibr" target="#b27">(Pasupat &amp; Liang, 2015)</ref>, Spider <ref type="bibr" target="#b44">(Yu et al., 2018)</ref>, Sequential Q&amp;A <ref type="bibr" target="#b15">(Iyyer et al., 2017), and</ref><ref type="bibr">WikiSQL (Zhong et al., 2017)</ref>, for which approaches have been extended to handle large-scale tables from Wikipedia <ref type="bibr" target="#b5">(Bhagavatula et al., 2013)</ref>. However, in these Q&amp;A tasks, the question types typically provide strong signals needed for identifying the type of answers, while TABFACT does not provide such specificity. The uniqueness of TABFACT lies in two folds: 1) a given fact is regarded as a false claim as long as any part of the statement contains misinformation. Due to the conjunctive nature of verification, a fact needs to be broken down into several sub-clauses or (Q, A) pairs to separate evaluate their correctness. Such a compositional nature of the verification problem makes it more challenging than a standard QA setting. On one hand, the model needs to recognize the multiple QA pairs and their relationship. On the other hand, the multiple sub-clauses make the semantic form longer and logic inference harder than the standard QA setting. 2) some facts cannot even be handled using semantic forms, as they are driven by linguistic inference or common sense. In order to verify these statements, more inference techniques have to be leveraged to enable robust verification. We visualize the above two characteristics of TABFACT in <ref type="figure">Figure 5</ref>.</p><p>Program Synthesis &amp; Semantic Parsing: There have also been great interests in using program synthesis or logic forms to solve different natural language processing problems like question answering <ref type="bibr" target="#b4">Berant et al., 2013;</ref><ref type="bibr" target="#b3">Berant &amp; Liang, 2014)</ref>, visual navigation <ref type="bibr" target="#b2">(Artzi et al., 2014;</ref><ref type="bibr" target="#b1">Artzi &amp; Zettlemoyer, 2013)</ref>, code generation <ref type="bibr" target="#b43">(Yin &amp; Neubig, 2017;</ref><ref type="bibr" target="#b10">Dong &amp; Lapata, 2016)</ref>, SQL synthesis <ref type="bibr" target="#b44">(Yu et al., 2018)</ref>, etc. The traditional semantic parsing papers <ref type="bibr" target="#b2">(Artzi et al., 2014;</ref><ref type="bibr" target="#b1">Artzi &amp; Zettlemoyer, 2013;</ref><ref type="bibr">Zettlemoyer &amp; Collins, 2005;</ref><ref type="bibr" target="#b4">Berant et al., 2013)</ref> greatly rely on rules, lexicon to parse natural language sentences into different forms like lambda calculus, DCS, etc. More recently, researchers strive to propose neural models to directly perform end-to-end formal reasoning like Theory Prover , Neural Turing Machine <ref type="bibr" target="#b12">(Graves et al., 2014)</ref>, Neural Programmer <ref type="bibr" target="#b24">(Neelakantan et al., 2016;</ref> and Neural-Symbolic Machines <ref type="bibr" target="#b20">(Liang et al., 2017;</ref><ref type="bibr" target="#b0">Agarwal et al., 2019)</ref>. The proposed TABFACT serves as a great benchmark to evaluate the reasoning ability of different neural reasoning models. Specifically, TABFACT poses the following challenges: 1) spurious programs (i.e., wrong programs with the true returned answers): since the program output is only a binary label, which can cause serious spurious problems and misguide the reinforcement learning with the under-specified binary rewards. 2) decomposition: the model needs to decompose the statement into sub-clauses and verify the sub-clauses one by one, which normally requires the longer logic inference chains to infer the statement verdict. 3) linguistic reasoning like inference and paraphrasing.</p><p>Fact Checking The problem of verifying claims and hypotheses on the web has drawn significant attention recently due to its high social influence. Different fact-checking pioneering studies have been performed including LIAR <ref type="bibr" target="#b40">(Wang, 2017)</ref>, PolitiFact <ref type="bibr" target="#b37">(Vlachos &amp; Riedel, 2014)</ref>, FEVER <ref type="bibr" target="#b34">(Thorne et al., 2018)</ref> and AggChecker <ref type="bibr" target="#b17">(Jo et al., 2019)</ref>, etc. The former three studies are mainly based on textual evidence on social media or Wikipedia, while AggChecker is closest to ours in using relational databases as the evidence. Compared to AggChecker, our paper proposes a much larger dataset to benchmark the progress in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper investigates a very important yet previously under-explored research problem: semistructured fact verification. We construct a large-scale dataset and proposed two methods, <ref type="table" target="#tab_6">Table-</ref>BERT and LPA, based on the state-of-the-art pre-trained natural language inference model and program synthesis. In the future, we plan to push forward this research direction by inspiring more sophisticated architectures that can perform both linguistic and symbolic reasoning. Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 FUNCTION DESCRIPTION</head><p>We list the detailed function description in <ref type="figure">Figure 6</ref>. We also visualize the functionality of the most  We list all the trigger words for different functions in <ref type="figure" target="#fig_3">Figure 8</ref> Trigger Function 'average' average 'difference <ref type="bibr">', 'gap', 'than', 'separate' diff 'sum', 'summation', 'combine', 'combined', 'total', 'add', 'all', '</ref>there are' ddd, sum 'not', 'no', 'never', "didn't", "won't", "wasn't", "isn't,"haven't", "weren't", "won't", 'neither', 'none', <ref type="bibr">'unable, 'fail', 'different', 'outside', 'unable', 'fail' not_eq, not_within, Filter_not_eq, none 'not', 'no', 'none' none 'first', 'top', 'latest', 'most' first 'last', 'bottom', 'latest', 'most' last 'RBR', 'JJR', 'more', 'than', 'above', 'after' filter_greater, greater 'RBR', 'JJR', 'less', 'than', 'below', 'under' filter_less, less 'all', 'every', 'each' all_eq, all_less, all_greater, ['all', 'every', '</ref>each'], ['not', 'no', 'never', "didn't", "won't", "wasn't"] all_not_eq 'at most', 'than' all_less_eq, all_greater_eq 'RBR', 'RBS', 'JJR', 'JJS' max, min 'JJR', 'JJS', 'RBR', 'RBS', 'top', 'first' argmax, argmin 'within', 'one', 'of', 'among' within 'follow', 'following', 'followed', 'after', 'before', 'above', 'precede' before 'follow', 'following', 'followed', 'after', 'before', 'above', 'precede' after 'most' most_freq ordinal First, second, third, fourth 2. Negation: the negation operation refers to sentences like "xxx did not get the best score", "xxx has never obtained a score higher than 5".</p><p>3. Superlative: the superlative operation refers to sentences like "xxx achieves the highest score in", "xxx is the lowest player in the team". 4. Comparative: the comparative operation refers to sentences like "xxx has a higher score than yyy".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Ordinal: the ordinal operation refers to sentences like "the first country to achieve xxx is xxx", "xxx is the second oldest person in the country". 6. Unique: the unique operation refers to sentences like "there are 5 different nations in the tournament, ", "there are no two different players from U.S" 7. All: the for all operation refers to sentences like "all of the trains are departing in the morning", "none of the people are older than 25." 8. None: the sentences which do not involve higher-order operations like "xxx achieves 2 points in xxx game", "xxx player is from xxx country".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ERROR ANALYSIS</head><p>Before we quantitatively demonstrate the error analysis of the two methods, we first theoretically analyze the bottlenecks of the two methods as follows:</p><p>Symbolic We first provide a case in which the symbolic execution can not deal with theoretically in <ref type="figure" target="#fig_4">Figure 9</ref>. The failure cases of symbolic are either due to the entity link problem or function coverage problem. For example, in the given statement below, there is no explicit mention of "7-5, 6-4" cell. Therefore, the entity linking model fails to link to this cell content. Furthermore, even though we can successfully link to this string, there is no defined function to parse "7-5, 6-5" as "won two games" because it requires linguistic/mathematical inference to understand the implication from the string. Such cases are the weakness of symbolic reasoning models. Jordi Arrese achieves better score in 1986 than in 1985.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jordi Arrese</head><p>Jordi Arrese won both of the final games in 1986.  <ref type="table" target="#tab_6">Table-</ref>BERT model seems to have no coverage problem as long as it can feed the whole table content. However, due to the template linearization, the table is unfolded into a long sequence as depicted in <ref type="figure" target="#fig_0">Figure 10</ref>. The useful information, "clay" are separated in a very long span of unrelated words. How to grasp such a long dependency and memorize the history information poses a great challenge to the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1986: Winner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jordi Arrese</head><p>Jordi Arrese played all of his games on clay surface.</p><p>Given the table titled "Jordi Arrese", in row one, the outcome is runner-up, the date is 1985, … , the surface is clay …. …… , In row two, the outcome is … , the surface is clay. In row three, the outcome is …, … the surface is clay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Dependency</head><p>The three "Clay" are separated by more over 20 words <ref type="figure" target="#fig_0">Figure 10</ref>: The error case of BERT NLI model</p><p>Statistics Here we pick 200 samples from the validation set which only involve single semantic and divide them into different categories. We denote the above-mentioned cases as "linguistic inference", and the sentences which only describe information from one row as "Trivial", the rest are based on their logic operation like Aggregation, Superlative, Count, etc. We visualize the accuracy of LPA and Table-BERT in <ref type="figure" target="#fig_0">Figure 11</ref>. From which we can observe that the statements with linguistic inference are much better handled with the BERT model, while LPA achieves an accuracy barely higher than a random guess. The BERT model can deal with trivial cases well as it uses a horizontal scan order. In contrast, the LPA model outperforms BERT on higher-order logic cases, especially when the statement involves operations like Count and Superlative. Error Analysis of LPA/ <ref type="table" target="#tab_6">Table-BERT   Table-</ref>BERT LPA <ref type="figure" target="#fig_0">Figure 11</ref>: The error analysis of two different models D REASONING DEPTH Given that our LPA has the breadth to cover a large semantic space. Here we also show the reasoning depth in terms of how many logic inference steps are required to tackle verify the given claims. We visualize the histogram in <ref type="figure" target="#fig_0">Figure 12</ref> and observe that the reasoning steps are concentrated between 4 to 7. Such statistics indicate the difficulty of fact verification in our TABFACT dataset. Before crowd-sourcing the annotation for the tables, we observed that the previous WikiTableQuestion <ref type="bibr" target="#b27">Pasupat &amp; Liang (2015)</ref> provides context (Wikipedia title) during annotation while the Wik-iSQL Zhong et al. <ref type="formula">(2017)</ref> does not. Therefore, we particularly design ablation annotation tasks to compare the annotation quality between w/ and w/o Wikipedia title as context. We demonstrate a typical example in <ref type="figure" target="#fig_0">Figure 13</ref>, where a Wiki table 10 aims to describe the achievements of a tennis player named Dennis, but itself does not provide any explicit hint about "Tennis Player Dennis". Unsurprisingly, the sentence fluency and coherence significantly drop without such information. Actually, a great portion of these Wikipedia tables requires background knowledge (like sports, celebrity, music, etc) to understand. We perform a small user study to measure the fluency of annotated statements. Specifically, we collected 50 sentences from both annotation w/ and w/o title context and randomly shuffle them as pairs, which are distributed to the 8 experts without telling them their source to compare the language fluency. It turns out that the experts ubiquitously agree that the statements with Wikipedia titles are more human-readable. Therefore, we argue that such a context is necessary for annotators to understand the background knowledge to write more fluent sentences. On the other end, we also hope to minimize the influence of the textual context in the table-based verification task, therefore, we design an annotation criterion: the Wikipedia title is provided to the workers during the annotation, but they are explicitly banned from bringing any unrelated background information other than the title into the annotation. As illustrated in <ref type="figure" target="#fig_0">Figure 13</ref>, the title only acts as a placeholder in the statements to make it sound more natural.  <ref type="figure" target="#fig_0">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Survey Instructions (Click to expand)</head><p>You are given a table with its wikipedia source, your job is to compose non-trivial statements supported by the table.</p><p>-"Trivial": the sentence can be easily generated by looking only a certain row without understanding the table.</p><p>-"Non-trivial": the sentence requires reading multiple rows of the 1. In the TV series "The Island", Derrick Kosinski is a male character. (Easy! You can simply look into first row to produce this sentence.) 2. Derrick Kosinski has the placing of winner in the TV series. 3. Kenny Santucci is from original season of "Fresh Meat". We set minimum length to 9, and sentences with more complicated grammar structures are preferred. Tips2: Do not limited to only one type of description like superlative or relative. Tips3: Copying the records from the table is encouraged, which can help avoid typos and mis-spelling as much as possible, . Tips4: Do not vague words like "maybe", "perhaps", "good", "excellent", "most", etc.</p><p>4. Jenn Grijalva is Runner-Up of the challenge. First Read the following table, then write five diverse non-trivial facts for this given table: Please write a non-trivial statement, minimum 9 words Please write a non-trivial statement, minimum 9 words Please write a non-trivial statement, minimum 9 words Please write a non-trivial statement, minimum 9 words Please write a non-trivial statement, minimum 9 words</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples from the TABFACT dataset. The top table contains the semi-structured knowledge facts with caption "United...". The left and right boxes below provide several entailed and refuted statements. The error parts are highlighted with red font.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Proportion of different higher-order operations from the simple/complex channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>The function definition used in TabFact. typical functions and their input/output examples in Figure 7. The visualization of different functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>The trigger words used to shrink the search space. B HIGHER-ORDER OPERATIONS 1. Aggregation: the aggregation operation refers to sentences like "the averaged age of all ....", "the total amount of scores obtained in ...", etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>The error case of symbolic reasoning model BERT In contrast,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>The histogram of reasoning steps required to verify the claims E WHETHER TO KEEP WIKIPEDIA CONTEXT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the TV series "The Island", Evelyn Smith is the highest ranked female. (Comparitive): In the TV series "The Island", Jenn Grijalva appears later than Colie Edison in the series. (Relational): Ashli Robson appears one episode later than Rachel Robinson in the TV series. (Summarization): there are three male winners in the challenge. (Rephrase): Evelyn Smith never eliminated in any episode in the TV series. (Combination): Derrick Kosinski is the winner and Jenn Grijalva is Runner-Up of the challenge. (Negation): jenn grijalva is not the female winning the challenge. (Inclusion): Evelyn smith is one of the four winner for the challenge.rank member association points group stage play -off afc cup s3.amazonaws.com/mturk_bulk/hits/370501562/uNGk1Dz1zM48BZI6mALfxA.html 3/4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Basic statistics of the data collected from the simple/complex channel and the division of</cell></row><row><cell>Train/Val/Test Split in the dataset, where "Len" denotes the averaged sentence length.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Label</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Game</cell><cell cols="2">Date</cell><cell cols="2">Opponent Score</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">12-Layer BERT-Base Model</cell><cell></cell><cell></cell><cell>51 52</cell><cell cols="3">February 3 , 2009 February 4 , 2009</cell><cell>Florida Buffalo</cell><cell>3-4 0-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53</cell><cell cols="4">February 7 , 2010 Montreal</cell><cell>5-2</cell></row><row><cell>Concat</cell><cell>Type Position Word</cell><cell>[CLS] 0 TOK</cell><cell>51 1 game</cell><cell>[SEP] 0 TOK</cell><cell>February 1 date</cell><cell cols="2">3 2 date date , 3</cell><cell>2009 4 date</cell><cell></cell><cell>[SEP] 0 TOK</cell><cell>Florida 1 S</cell><cell cols="2">is playing 2 3 S S</cell><cell>TOK 0 [SEP]</cell></row><row><cell>Template</cell><cell>Position Word</cell><cell>[CLS] 0</cell><cell>1 row</cell><cell>one 2</cell><cell>game 3</cell><cell>is 4</cell><cell cols="2">51 ; date 5 6 7</cell><cell>is 8</cell><cell>February 3 9 10</cell><cell>2019 11</cell><cell>; 12</cell><cell>Florida 13</cell><cell>[SEP] 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Model</cell><cell>Val</cell><cell cols="4">Test Test (simple) Test (complex) Small Test</cell></row><row><cell>BERT classifier w/o Table</cell><cell cols="2">50.9 50.5</cell><cell>51.0</cell><cell>50.1</cell><cell>50.4</cell></row><row><cell cols="3">Table-BERT-Horizontal-F+T-Concatenate 50.7 50.4</cell><cell>50.8</cell><cell>50.0</cell><cell>50.3</cell></row><row><cell>Table-BERT-Vertical-F+T-Template</cell><cell cols="2">56.7 56.2</cell><cell>59.8</cell><cell>55.0</cell><cell>56.2</cell></row><row><cell>Table-BERT-Vertical-T+F-Template</cell><cell cols="2">56.7 57.0</cell><cell>60.6</cell><cell>54.3</cell><cell>55.5</cell></row><row><cell>Table-BERT-Horizontal-F+T-Template</cell><cell cols="2">66.0 65.1</cell><cell>79.0</cell><cell>58.1</cell><cell>67.9</cell></row><row><cell>Table-BERT-Horizontal-T+F-Template</cell><cell cols="2">66.1 65.1</cell><cell>79.1</cell><cell>58.2</cell><cell>68.1</cell></row><row><cell>NSM w/ RL (Binary Reward)</cell><cell cols="2">54.1 54.1</cell><cell>55.4</cell><cell>53.1</cell><cell>55.8</cell></row><row><cell>NSM w/ LPA-guided ML + RL</cell><cell cols="2">63.2 63.5</cell><cell>77.4</cell><cell>56.1</cell><cell>66.9</cell></row><row><cell>LPA-Voting w/o Discriminator</cell><cell cols="2">57.7 58.2</cell><cell>68.5</cell><cell>53.2</cell><cell>61.5</cell></row><row><cell>LPA-Weighted-Voting</cell><cell cols="2">62.5 63.1</cell><cell>74.6</cell><cell>57.3</cell><cell>66.8</cell></row><row><cell>LPA-Ranking w/ Discriminator</cell><cell cols="2">65.2 65.0</cell><cell>78.4</cell><cell>58.5</cell><cell>68.6</cell></row><row><cell cols="3">LPA-Ranking w/ Discriminator (Caption) 65.1 65.3</cell><cell>78.7</cell><cell>58.5</cell><cell>68.9</cell></row><row><cell>Human Performance</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>The results of different models, the numbers are in percentage. T+F means table followed by fact, while F+T means fact followed by table. NSM is modified from<ref type="bibr" target="#b20">Liang et al. (2017)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table - BERT</head><label>-</label><figDesc></figDesc><table /><note>model including (i) Concatenation vs. Template: whether to use natural language tem- plates during linearization. (ii) Horizontal vs. Vertical: scan direction in linearization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">United States House of Representatives Elections, 1972</cell></row><row><cell>District</cell><cell>Incumbent</cell><cell></cell><cell>Party</cell><cell></cell><cell>Result</cell><cell></cell><cell>Candidates</cell></row><row><cell cols="2">California 3 John E. Moss</cell><cell></cell><cell cols="2">democratic</cell><cell>re-elected</cell><cell></cell><cell>John E. Moss (d) 69.9% John Rakus (r) 30.1%</cell></row><row><cell cols="2">California 5 Phillip Burton</cell><cell></cell><cell cols="2">democratic</cell><cell>re-elected</cell><cell></cell><cell>Phillip Burton (d) 81.8% Edlo E. Powell (r) 18.2%</cell></row><row><cell cols="3">California 8 George Paul Miller</cell><cell cols="2">democratic</cell><cell cols="3">lost renomination democratic hold</cell><cell>Pete Stark (d) 52.9% Lew M. Warden , Jr. (r) 47.1%</cell></row><row><cell cols="3">California 14 Jerome R. Waldie</cell><cell cols="2">republican</cell><cell>re-elected</cell><cell></cell><cell>Jerome R. Waldie (d) 77.6% Floyd E. Sims (r) 22.4%</cell></row><row><cell cols="2">California 15 John J. Mcfall</cell><cell></cell><cell cols="2">republican</cell><cell>re-elected</cell><cell></cell><cell>John J. Mcfall (d) unopposed</cell></row><row><cell cols="8">There are five candidates in total, two of them are democrats and three of them are republicans.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conjunctive</cell></row><row><cell cols="3">Question: How many of candidates in total? Answer: 5</cell><cell>∧</cell><cell cols="4">Question: How many democrats are there? Answer: 2</cell><cell>∧</cell><cell>Question: How many republicans are there? Answer: 3</cell></row><row><cell>Eq(Count(T), 5)=T</cell><cell></cell><cell></cell><cell>∧</cell><cell cols="4">Eq(Count(Filter(T, party='dem..')), 2)=F</cell><cell>∧</cell><cell>Eq(Count(Filter(T, party='rep..')), 3)=F</cell></row><row><cell>outcome</cell><cell>date</cell><cell cols="2">tournament</cell><cell></cell><cell>surface</cell><cell>partner</cell><cell>opponents in the final</cell><cell>score in the final</cell></row><row><cell cols="2">runner -up 1985</cell><cell cols="2">bologna , italy</cell><cell></cell><cell>clay</cell><cell>alberto tous</cell><cell>paolo canè simone colombo</cell><cell>5 -7 , 4 -6</cell></row><row><cell>winner</cell><cell>1986</cell><cell cols="3">bordeaux , france</cell><cell>clay</cell><cell>david de miguel</cell><cell>ronald agénor mansour bahrami 7 -5 , 6 -4</cell></row><row><cell>winner</cell><cell>1989</cell><cell cols="4">prague , czechoslovakia clay</cell><cell>horst skoff</cell><cell>petr korda tomáš šmíd</cell><cell>6 -4 , 6 -4</cell></row></table><note>Question Answering: Another line of research closely related to our task is the table-based9 https://github.com/wenhuchen/Table-Fact-Checking/tree/master/bootstrap</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 93-104, 2018. Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: structured classification with probabilistic categorial grammars. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, pp. 658-666. AUAI Press, 2005.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table -</head><label>-</label><figDesc>BERT model.</figDesc><table><row><cell>outcome</cell><cell>date</cell><cell>tournament</cell><cell>surface</cell><cell>partner</cell><cell>opponents in the final</cell><cell>score in the final</cell></row><row><cell cols="2">runner -up 1985</cell><cell>Bologna , Italy</cell><cell>clay</cell><cell>Alberto Tous</cell><cell>Paolo Canè Simone Colombo</cell><cell>5 -7 , 4 -6</cell></row><row><cell>winner</cell><cell>1986</cell><cell>Bordeaux , France</cell><cell>clay</cell><cell>David De Miguel</cell><cell cols="2">Ronald Agénor Mansour Bahrami 7 -5 , 6 -4</cell></row><row><cell>winner</cell><cell>1989</cell><cell cols="2">Prague , Czechoslovakia clay</cell><cell>Horst Skoff</cell><cell>Petr Korda Tomáš šmíd</cell><cell>6 -4 , 6 -4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Here we propose to use the longest string match to find all the candidate entities in the table, when multiple candidates coexist, we select the one with the minimum edit distances. The visualization is demonstrated in</figDesc><table><row><cell>outcome</cell><cell>year</cell><cell>championship</cell><cell>surface</cell><cell>partner</cell></row><row><cell>winner</cell><cell>1960</cell><cell>Wimbledon championships</cell><cell>grass</cell><cell>Rafael Osuna</cell></row><row><cell>winner</cell><cell>1961</cell><cell>US Championships</cell><cell>grass</cell><cell>Chuck Mckinley</cell></row><row><cell>runner -up</cell><cell>1962</cell><cell>US Championships</cell><cell>grass</cell><cell>Chuck Mckinley</cell></row><row><cell>winner</cell><cell>1963</cell><cell>US Championships (2)</cell><cell>grass</cell><cell>Chuck Mckinley</cell></row><row><cell>Context</cell><cell cols="2">Richard Dennis Ralston (born July 27, 1942,</cell><cell cols="2">No Information is provided</cell></row><row><cell>(Title)</cell><cell cols="2">an American former tennis player</cell><cell></cell><cell></cell></row><row><cell>Annotate</cell><cell cols="2">From 1960 to 1969, Ralston won five major</cell><cell cols="2">Winner is on the grass surface.</cell></row><row><cell></cell><cell cols="2">double championships.</cell><cell cols="2">Rafael Osuna is partner in the Wimbeldon</cell></row><row><cell cols="5">Figure 13: Comparison of worker annotation w/ and w/o Wikipedia title as context</cell></row><row><cell cols="2">F ENTITY LINKING</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>comparative, negation, relational, inclusion, superlative, aggregational, rephrase or combinations of them</head><label></label><figDesc>table and understanding of the table content. For example, the sentences which include summarization, are non-trivial. But non-trvial is not limited to these types, any statement involving understanding and reasoning is accepted. We list two examples below to help you understand, you are encouraged to open the table wikipedia link to understand the context of the table. (Everything in the table is lower-cased, you are free to use lower or upper case in your sentence):</figDesc><table><row><cell cols="4">Table Wikipedia Link: Road_Rules_Challenge:_The_Island</cell><cell></cell></row><row><cell cols="5">(https://en.wikipedia.org/wiki/Real_World/Road_Rules_Challenge:_The_Island)</cell></row><row><cell>player</cell><cell>original season</cell><cell cols="2">gender eliminated</cell><cell>placing</cell></row><row><cell>derrick kosinski</cell><cell>rr : x -treme</cell><cell>male</cell><cell>winner</cell><cell>winner</cell></row><row><cell>evelyn smith</cell><cell>fresh meat</cell><cell>female</cell><cell>winner</cell><cell>winner</cell></row><row><cell cols="2">johnny devenanzio rw : key west</cell><cell>male</cell><cell>winner</cell><cell>winner</cell></row><row><cell>kenny santucci</cell><cell>fresh meat</cell><cell>male</cell><cell>winner</cell><cell>winner</cell></row><row><cell>jenn grijalva</cell><cell>rw : denver</cell><cell>female</cell><cell>episode 8</cell><cell>runner -up</cell></row><row><cell>paula meronek</cell><cell>rw : key west</cell><cell>female</cell><cell>episode 8</cell><cell>runner -up</cell></row><row><cell>robin hibbard</cell><cell>rw : san diego</cell><cell>female</cell><cell>episode 8</cell><cell>runner -up</cell></row><row><cell>ryan kehoe</cell><cell>fresh meat</cell><cell>male</cell><cell>episode 8</cell><cell>runner -up</cell></row><row><cell>dunbar merrill</cell><cell>rw : sydney</cell><cell>male</cell><cell>episode 8</cell><cell>9th place</cell></row><row><cell>johanna botta</cell><cell>rw : austin</cell><cell>female</cell><cell>episode 8</cell><cell>10th place</cell></row><row><cell>kellyanne judd</cell><cell>rw : sydney</cell><cell>female</cell><cell>episode 8</cell><cell>11th place</cell></row><row><cell>dan walsh</cell><cell cols="2">rr : viewers' revenge male</cell><cell>episode 8</cell><cell>12th place</cell></row><row><cell>colie edison</cell><cell>rw : denver</cell><cell>female</cell><cell>episode 7</cell><cell>13th place</cell></row><row><cell>cohutta grindstaff</cell><cell>rw : sydney</cell><cell>male</cell><cell>episode 6</cell><cell>14th place</cell></row><row><cell>tyrie ballard</cell><cell>rw : denver</cell><cell>male</cell><cell>episode 5</cell><cell>15th place</cell></row><row><cell>ashli robson</cell><cell>rw : sydney</cell><cell>female</cell><cell>episode 4</cell><cell>16th place</cell></row><row><cell>rachel robinson</cell><cell>rr : campus crawl</cell><cell>female</cell><cell>episode 3</cell><cell>17th place</cell></row><row><cell>abram boise</cell><cell>rr : south pacific</cell><cell>male</cell><cell>episode 2</cell><cell>18th place</cell></row><row><cell>dave malinosky</cell><cell>rw : hollywood</cell><cell>male</cell><cell cols="2">episode 2 (quit) 19th place</cell></row><row><cell cols="2">Rejected ("Trivial") examples:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table Wikipedia</head><label>Wikipedia</label><figDesc></figDesc><table><row><cell>Link: AFC_Champions_League (https://en.wikipedia.org/wiki/AFC_Champions_League)</cell></row><row><cell>Tips1:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table Source :</head><label>Source</label><figDesc>athletics at the 1952 summer olympics -men 's pole vault (https://en.wikipedia.org/wiki/Athletics_at_the_1952_Summer_Olympics_%E2%80%93_Men%27s_pole_vault)</figDesc><table><row><cell>athlete</cell><cell>nationality</cell><cell cols="4">3.60 3.80 3.95 result</cell></row><row><cell>bob richards</cell><cell cols="2">united states -</cell><cell>-</cell><cell>o</cell><cell>4.55 or</cell></row><row><cell>don laz</cell><cell cols="2">united states -</cell><cell>-</cell><cell>o</cell><cell>4.50</cell></row><row><cell>ragnar lundberg</cell><cell>sweden</cell><cell>-</cell><cell>-</cell><cell>o</cell><cell>4.40</cell></row><row><cell>petro denysenko</cell><cell>soviet union</cell><cell>-</cell><cell>-</cell><cell>o</cell><cell>4.40</cell></row><row><cell>valto olenius</cell><cell>finland</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.30</cell></row><row><cell>bunkichi sawada</cell><cell>japan</cell><cell>-</cell><cell>o</cell><cell>xxo</cell><cell>4.20</cell></row><row><cell cols="2">volodymyr brazhnyk soviet union</cell><cell>-</cell><cell>o</cell><cell>o</cell><cell>4.20</cell></row><row><cell>viktor knyazev</cell><cell>soviet union</cell><cell>-</cell><cell>o</cell><cell>o</cell><cell>4.20</cell></row><row><cell>george mattos</cell><cell cols="2">united states -</cell><cell>-</cell><cell>o</cell><cell>4.20</cell></row><row><cell>erkki kataja</cell><cell>finland</cell><cell>-</cell><cell>-</cell><cell>o</cell><cell>4.10</cell></row><row><cell>tamás homonnay</cell><cell>sweden</cell><cell>-</cell><cell>o</cell><cell>o</cell><cell>4.10</cell></row><row><cell>lennart lind</cell><cell>hungary</cell><cell>-</cell><cell>o</cell><cell>o</cell><cell>4.10</cell></row><row><cell>milan milakov</cell><cell>yugoslavia</cell><cell>-</cell><cell>o</cell><cell>xo</cell><cell>4.10</cell></row><row><cell>rigas efstathiadis</cell><cell>greece</cell><cell>-</cell><cell>o</cell><cell>o</cell><cell>3.95</cell></row><row><cell>torfy bryngeirsson</cell><cell>iceland</cell><cell>-</cell><cell>o</cell><cell>o</cell><cell>3.95</cell></row><row><cell>erling kaas</cell><cell>norway</cell><cell>-</cell><cell>o</cell><cell>xxx</cell><cell>3.80</cell></row><row><cell>theodosios balafas</cell><cell>greece</cell><cell>o</cell><cell>o</cell><cell>xxx</cell><cell>3.80</cell></row><row><cell>jukka piironen</cell><cell>finland</cell><cell>-</cell><cell>xo</cell><cell>xx</cell><cell>3.80</cell></row><row><cell>zeno dragomir</cell><cell>romania</cell><cell>-</cell><cell>xo</cell><cell>xx</cell><cell>3.80</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In contrast to the database tables, where each column has strong type constraint, the cell records in our semi-structured tables can be string/data/integer/floating/phrase/sentences.3  we leave out NEUTRAL due to its low inter-worker agreement, which is easily confused with REFUTED.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://websail-fe.cs.northwestern.edu/wikiTables/about/ 5 https://en.wikipedia.org/wiki/Minimum_wage_in_the_United_States 6 https://www.mturk.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We filter roughly 400 sentences from abnormal tables including hyperlinks, math symbols, etc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/huggingface/pytorch-pretrained-BERT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://en.wikipedia.org/wiki/Dennis_Ralston</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G THE PROGRAM CANDIDATES</head><p>Here we demonstrate some program candidates in <ref type="figure">Figure 15</ref>, and show how our proposed discriminator is designed to compute the matching probability between the statement and program. Specifically, we employ two transformer-based encoder <ref type="bibr" target="#b36">Vaswani et al. (2017)</ref>, the left one is aimed to encode the program sequence and the right one is aimed to encode the statement sequence. Their output from [CLS] position is concatenated and fed into an MLP to classify the verification label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H HIT INTERFACE</head><p>We provide the human intelligent task interface on AMT in the following. Very detailed instructions on what are trivial statements and what are non-trivial statements. Comprehensive examples have been given to guide the Turkers to write well-formed while logically plausible statements. In order to harvest fake statements without statistical cues, we also provide detailed instructions on how to re-write the "fake" statements. During the annotation, we hire 8 experts to perform sanity checks on each of the HIT to make sure that the annotated dataset is clean and meets our requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Survey Instructions (Click to expand)</head><p>Please first read a table to understand its content, an example is shown below, which contains the leaderboard of a competition. You are given a sentence to describe a fact in the table, please follow the following two cases to finish the job:</p><p>* If the given sentence is fluent and consistent with the table, then please re-write it to make it "fake" based on the following criteria: 1. Contradictory: it should still be a fluent and coherent, but it needs be explicitly contrdictory to the facts in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submit</head><p>First Read the given tables, then rewrite the statements to make them fake: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to generalize from sparse and underspecified rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning compact lexicons for ccg semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Methods for exploring and mining tables on wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Sekhar Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanapon</forename><surname>Noraset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics</title>
		<meeting>the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ukp-athene: Multi-sentence textual entailment for claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01479</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Claimbuster: the first-ever end-to-end fact-checking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naeemul</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Gawsane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohedul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minumol</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kumar Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tables as semi-structured knowledge for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aggchecker: A fact-checking system for text summaries of relational data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The structure of a semantic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jerrold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fodor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">language</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="210" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory augmented policy optimization for program synthesis and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9994" to="10006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Where the truth lies: Explaining the credibility of emerging claims on the web and social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Programming with a differentiable forth interpreter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end differentiable proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3788" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fever: a largescale dataset for fact extraction and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A brief history of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Benthem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>LondonCollege Publica-tions9781904987444</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fact checking: Task definition and dataset construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science</title>
		<meeting>the ACL 2014 Workshop on Language Technologies and Computational Social Science</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">liar, liar pants on fire: A new benchmark dataset for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
	<note>Statement: There are more democratic than republican in the election</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Less(Count(Filter(incumbent==democratic)), Count(Filter(incumbent==republican)))=False</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Less(Count(Filter(incumbent==republican)), Count(Filter(incumbent==democratic)))=True</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Greater(Count(Filter(incumbent==republican)), Count(Filter(incumbent==democratic)))=False</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Count(Filter(incumbent==republican)))=True Within((Filter(incumbent==democratic), incumbent, republican)=False Within((Filter(incumbent== republican), incumbent, democratic)=False</title>
	</analytic>
	<monogr>
		<title level="m">Greater(Count(Filter(incumbent==democratic))</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">And(Same(all_rows, incumbent, democratic), Same(all_rows, incumbent, republican))=True</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Same(all_rows, incumbent, democratic), Same(all_rows, incumbent, republican))=True</title>
	</analytic>
	<monogr>
		<title level="j">Or</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Eq(Count(Filter(incumbent==republican)), Count(Filter(incumbent==democratic))</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Trivial</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Non-Trivial&quot;) examples: (Negation): iran is one of the two countries getting into the 4th stage. (Average): uae and qatar have an average of 1 play -off during the champion league. (Algorithmic): saudi arabia achieves 22.3 more points than qatar. (Comparison): india got lower points than jordan in the league. (Summarization): there are two team which have won the afc cup twice. (Superlative): In the Champions League, saudi arabia achieves the highest points. (Combination): saudi arabia</title>
		<imprint/>
	</monogr>
	<note>When member association is india, the points is 106.4. Accepted. is the group stage 4 while iran is in group stage 3</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">Bad Faking (Ambiguous, who is Ashli?): Ashli was not eliminated on episode 4. Bad Faking (Irrelevant): Ashli was born in</title>
		<meeting><address><addrLine>Mexico</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">Bad Faking (Too subjective, what do you mean by &quot;early&quot;): AshlDerrick Kosinski lost the game very early. Bad Faking (Not verifiable): AshlDerrick Kosinski was the most popular player</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bad Faking: (There is nothing larger than 20th) Tonya Cooley is after the 20th place. Bad Faking: (Half Wrong/half Right) When the gneder is female, the player is Tonya Colley. Bad Faking (Introduce values outside the table): Tonya Cooley is in the 43th place</title>
	</analytic>
	<monogr>
		<title level="m">Given statement: Tonya Cooley is in the 20th place</title>
		<meeting><address><addrLine>Typo</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Good Faking: Tonya Cooley is eliminated in episode 1 but not the last in placing. Tonya Cooler is in the 20th palace</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
