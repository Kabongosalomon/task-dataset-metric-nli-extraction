<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusion and pose variations, which can change facial appearance significantly, are two major obstacles for automatic Facial Expression Recognition (FER). Though automatic FER has made substantial progresses in the past few decades, occlusion-robust and pose-invariant issues of FER have received relatively less attention, especially in real-world scenarios. This paper addresses the real-world pose and occlusion robust FER problem in the following aspects. First, to stimulate the research of FER under real-world occlusions and variant poses, we annotate several in-the-wild FER datasets with pose and occlusion attributes for the community. Second, we propose a novel Region Attention Network (RAN), to adaptively capture the importance of facial regions for occlusion and pose variant FER. The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed-length representation. Last, inspired by the fact that facial expressions are mainly defined by facial action units, we propose a region biased loss to encourage high attention weights for the most important regions. We validate our RAN and region biased loss on both our built test datasets and four popular datasets: FERPlus, AffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose. Our method also achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW. Code and the collected test data will be publicly available.</p><p>Index Terms-Facial expression recognition, occlusion-robust and pose-invariant, region attention network, deep convolutional neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F Acial expressions play important roles in daily humanhuman communication. Automatic facial expression analysis is an important area of artificial intelligence. Due to its potential applications in various fields, such as intelligent tutoring systems, service robots, driver fatigue monitoring, Facial Expression Recognition (FER) has attracted increasing attention in the computer vision community recently <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The main challenges of FER come from illumination variation, occlusions, variant poses, identity bias, insufficient qualitative data, etc.</p><p>Occlusions and variant poses are two major problems in the field of face analysis since they lead to significant change of facial appearance. These issues have received wide interest in face identity recognition <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b37">[38]</ref>, however, less attention has been paid in real-world FER partly due to the lack of a facial expression dataset with occlusion and pose annotations.</p><p>Earlier works mainly investigate the effects of occlusion for FER systems with partial artificially-masked faces collected in a controlled laboratory environment. Boucher and Ekman <ref type="bibr" target="#b6">[7]</ref> investigate facial parts to understand which are most important regions for human perception by occluding key parts. Bourel et al. <ref type="bibr" target="#b7">[8]</ref> present the first FER system under the presence of occlusion by recovering geometric facial points. Kotsia et al. <ref type="bibr" target="#b31">[32]</ref> present a comprehensive analysis on occluded FER based on Gabor features and human observers, and find that an occluded mouth degrades FER more than occluded eyes on JAFFE <ref type="bibr" target="#b43">[44]</ref> and CK <ref type="bibr" target="#b29">[30]</ref>. Sparse representation classifier (SRC) is widely used for artificially-occluded FER in 2010s <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Subsequently, a number of works handle FER with sub-region based features and fusion schemes <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, which detect the occlusion regions first and then remove their local features. With the popularity of data-driven deep learning techniques, several recent efforts on FER have been made on the collection of large-scale datasets <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and many works <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b34">[35]</ref> exploit deep convolutional neural networks (CNN) to improve the performance of FER.</p><p>We argue that explicitly removing occlusion regions is not practical since real-world occlusion is difficult to detect in itself. Directly using CNN on whole face images ignores the characteristics of occlusion and variant pose. In practices, occlusion and pose variations can lead to unseen regions of input faces, which bring difficulties for face alignment and harm the feature extraction process. Contrasted with these difficulties, human have the remarkable ability to understand facial expressions under challenging conditions. Psychological studies indicated that human can effectively exploit both local regions and holistic faces to perceive the semantics delivered through incomplete faces <ref type="bibr" target="#b63">[64]</ref>. Inspired by these facts, this paper proposes a region based deep attention architecture for pose and occlusion robust FER, which adaptively integrates visual clues from regions and whole faces. Specifically, we addresses the real-world pose and occlusion robust FER problem in the following aspects.</p><p>First, to investigate the occlusion and pose variant FER problem, we build six real-world test datasets from FERPlus and AffectNet, namely Occlusion-FERPlus, Pose-FERPlus, Occlusion-AffectNet, and Pose-AffectNet, Occlusion-RAF-DB, and Pose-RAF-DB. The occlusion test datasets are manually annotated with occlusion types of wearing mask/glasses, objects in left/right, objects in upper face, objects in bottom face. The pose-variant test datasets are automatically labeled by a recent head pose estimation toolbox <ref type="bibr" target="#b2">[3]</ref>. We observe that arXiv:1905.04075v2 [cs.CV] 5 Sep 2019 the performance of existing CNN methods degrade significantly in occlusion and pose-variant environments.</p><p>Second, we propose the Region Attention Network (RAN), to capture the importance of facial regions for occlusion and pose robust FER. The RAN is comprised of a feature extraction module, a self-attention module, and a relation attention module. The later two modules aim to learn coarse attention weights and refine them with global context, respectively. Given a number of facial regions, our RAN learns attention weights for each regions in an end-to-end manner, and aggregates their CNN-based features into a compact fixed-length representation. Besides, the RAN model has two auxiliary effects on the face images. On one hand, cropping regions can enlarge the training data which is important for those insufficient challenging samples. On the other hand, rescaling the regions to the size of original images highlights fine-grain facial features. Extensive experiments indicate that our RAN significantly improves the performance of FER in occlusion and pose variant conditions. Third, since facial expressions are mainly defined by multiple facial action units <ref type="bibr" target="#b6">[7]</ref>, we propose a Region Biased Loss (RB-Loss) to encourage a high attention weight for the most important region. Our RB-Loss resorts a simple constraint on the RAN that the maximum attention weight of facial regions should be larger than the one of the original face image. Experiments show that the RB-Loss further improves FER slightly without additional computation cost. Our FER solution achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW with accuracies of 89.16%, 59.5%, 86.9%, and 56.4%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we mainly present related works on normal FER problem, the occlusion and pose variant FER problem, and attention mechanism.</p><p>Facial Expression Recognition. Generally, a FER system mainly consists of three stages, namely face detection, feature extraction, and expression recognition. In face detection, several face detectors like MTCNN <ref type="bibr" target="#b66">[67]</ref> and Dlib <ref type="bibr" target="#b2">[3]</ref>) are used to locate faces in complex scenes. The detected faces can be further aligned alternatively. For feature extraction, various methods are designed to capture facial geometry and appearance features caused by facial expressions. According to the feature type, they can be grouped into engineered features and learning-based features. For the engineered features, they can be further divided into texture-based local features, geometrybased global features, and hybrid features. The texture-based features mainly include SIFT <ref type="bibr" target="#b48">[49]</ref>, HOG <ref type="bibr" target="#b13">[14]</ref>, Histograms of LBP <ref type="bibr" target="#b51">[52]</ref>, Gabor wavelet coefficients <ref type="bibr" target="#b38">[39]</ref>, etc. The geometrybased features are mainly based on the landmark points around noses, eyes, and mouths. Combining two or more of the engineered features refers to the hybrid feature extraction, which can further enrich the representation. For the learned features, Fasel <ref type="bibr" target="#b21">[22]</ref> finds that a shallow CNN is robust to face poses and scales. Tang <ref type="bibr" target="#b56">[57]</ref> and Kahou et al. <ref type="bibr" target="#b28">[29]</ref> utilize deep CNNs for feature extraction, and win the FER2013 and Emotiw2013 challenge, respectively. Liu et al. <ref type="bibr" target="#b39">[40]</ref> propose a Facial Action Units based CNN architecture for expression recognition. After feature extraction, the next stage is to feed the features into a supervised classifier such as Support Vector Machines (SVMs), softmax layer, and logistic regression to assign expression categories.</p><p>To avoid overfitting on small facial expression datasets, many recent studies <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b19">[20]</ref> utilize face recognition datasets to pre-train a network, and then finetune it on target expression datasets. Levi and Hassner <ref type="bibr" target="#b32">[33]</ref> leverage the CASIA-WebFace <ref type="bibr" target="#b62">[63]</ref> face recognition dataset to pretrain four different VGGNet <ref type="bibr" target="#b52">[53]</ref> and GoogleNet <ref type="bibr" target="#b54">[55]</ref>. Zhao et al. <ref type="bibr" target="#b71">[72]</ref> propose a Peak Gradient Suppression (PGS) scheme for training and also pretrain their models on CASIA-WebFace. Ding et al. <ref type="bibr" target="#b19">[20]</ref> propose a FaceNet2ExpNet framework which jointly trains FER task and face recognition task. FER in Occlusion and Pose Variant Condition. Occlusion and variant pose usually occur in real-world scenarios as facial regions can be easily occluded by sunglasses, a hat, a scarf, etc. Partial occlusion can be divided into two types according to whether the real object causes occlusion: one is artificial occlusion, and the other is real-life occlusion. Few attempts have been made on the real-world occlusion FER problem. Kotsia et al. <ref type="bibr" target="#b31">[32]</ref> demonstrate how artificial partial occlusion affects the FER, and discuss how to deal with it. Liu et al. <ref type="bibr" target="#b40">[41]</ref> propose a novel FER method to address partial occlusion problem based on Gabor multi-orientation features fusion and local Gabor binary pattern histogram sequence (LGBPHS). Cotter et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> propose to use sparse representation classifier for partial occlusion FER. The latest related work <ref type="bibr" target="#b36">[37]</ref> designs a patch-based attention network for occlusion aware FER. The patches are cropped from the area of eyes, nose, mouth and so on. The selected 24 patches are fed into an attention network which is near to the self-attention module in our work. Our work differs from <ref type="bibr" target="#b36">[37]</ref> in that i) we crop relative large regions instead of small fixed parts by considering that the facial expression is connected to multiple AUs, and ii) we refine the attention weights with a relationattention module and region bias loss function. As for the pose variant FER problem, Rudovic et al. <ref type="bibr" target="#b50">[51]</ref> propose the Coupled Scaled Gaussian Process Regression (CSGPR) model for head-pose normalization. Different from existing methods, we address both occlusion and pose variant FER problems in an end-to-end manner with an elaborately-designed region attention network architecture and collected test datasets.</p><p>Attention Networks. Attention mechanisms are firstly developed on the basis of reinforcement algorithm. Mnih et al. <ref type="bibr" target="#b46">[47]</ref> use the attention on the RNN model for image classification, and then it is successfully utilized for machine translation tasks. Bahdanau et al. <ref type="bibr" target="#b3">[4]</ref> use an attention-like mechanism to simultaneously translate and align the source languages, and their work is the first attempt to apply attention mechanism to machine translation. Afterwards, many self-attention models are proposed for different tasks, such as LSTM for machine reading <ref type="bibr" target="#b10">[11]</ref>, multi-head attention for machine translation <ref type="bibr" target="#b57">[58]</ref> and attention clusters for video classification <ref type="bibr" target="#b41">[42]</ref>. Yu et al. <ref type="bibr" target="#b58">[59]</ref> propose an attention network for face detection, which highlights the face regions in anchor generation step. Perhaps the most similar work to ours is the Neural Aggregation Network (NAN) proposed by Yang et al. <ref type="bibr" target="#b61">[62]</ref>. NAN uses a cascade attention mechanism to aggregate face features of a video or set into a compact video representation. Our work differs from NAN by that self-attention and relation-attention module is used in RAN to aggregate facial region features for FER in static images, and a region biased loss is introduced to enhance region weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we first give an overview of our proposed region attention networks (RAN), and then detail each module and the region biased loss in RAN. We then present the region generation strategies and finally describe the collected occlusion and pose variant FER dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>As mentioned in Sec. I, several early works try to detect the occlusion regions and then remove the region features to address the facial expression recognition with regional occlusion. Along with this idea, we aim to automatically reduce or eliminate the effect of occlusion and irrelated regions with an end-to-end deep architecture.</p><p>Considering both large pose and occlusion issues in facial expression recognition, we propose a Region Attention Network (RAN) to alleviate the degradation of naive face based CNN models. The proposed RAN can adaptively capture the importance of facial region information, and make a reasonable trade-off between region and global features. The pipeline of our RAN is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. It mainly consists of three modules, namely region cropping and feature extraction module, self-attention module, and relation-attention module. Given a face image (after face detection), we first crop it into a number of regions with fixed position cropping or random cropping. We will compare these strategies in experiments. These regions along with the original face region are then fed into a backbone CNN model for region feature extraction. Subsequently, the self-attention module assigns an attention weight for each region using a fully-connected (FC) layer and the sigmoid function. An alternative region biased loss (RB-Loss) is further introduced to regularize the attention weights and enhance the most valuable region in self-attention module. We aggregate these region features to a global representation (F m in <ref type="figure" target="#fig_1">Figure 1</ref>). Then the relation-attention module uses a similar attention mechanism on the concatenation of individual region feature and global representation to further capture content-aware attention weights. Finally, we leverage the weighted region feature and the global representation to predict the expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Region Attention Networks</head><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the proposed RAN mainly consists of two stages. The first stage is to coarsely calculate the importance of each region by a FC layer conducted on its own feature, which is called self-attention module. The second stage seeks to find more accurate attention weights by modeling the relation between the region features and the aggregated content representation from the first stage, which is called relation-attention module.</p><p>Formally, we denote a face image as I, its duplicate as I 0 , and its crops as I 1 , I 2 , · · · , I k , and the backbone CNN as r(·; θ). The feature set X of I is defined by:</p><formula xml:id="formula_0">X = [F 0 , F 1 , · · · , F k ] = [r(I 0 ; θ), r(I 1 ; θ), · · · , r(I k ; θ)],<label>(1)</label></formula><p>where θ is the parameter of backbone CNN. Self-attention module. With these region features, the selfattention module applies a FC layer and a sigmoid function to estimate coarse attention weights. Mathematically, the attention weight of the i-th region is defined by:</p><formula xml:id="formula_1">µ i = f (F i q 0 ),<label>(2)</label></formula><p>where q 0 is the parameter of FC, f denotes the sigmoid function. In this stage, we summarize all the region features with their attention weights into a global representation F m as follows,</p><formula xml:id="formula_2">F m = 1 n i=0 µ i n i=0 µ i F i .<label>(3)</label></formula><p>F m is a compact representation and can be used as the final input of classifier. We compare the self-attention aggregation to the straightforward average pooling and concatenation (with fixed number of crops) in Sec. IV.</p><p>Relation-attention module. The self-attention module learns weights with individual features and non-linear mapping, which is rather coarse. Since the aggregated representation F m inherently represents the contents of all the facial regions, the attention weights can be further refined by modeling the relation between region features and this global representation F m .</p><p>Inspired by the global attention in neural machine translation <ref type="bibr" target="#b42">[43]</ref> and the relation-Net in low-shot learning <ref type="bibr" target="#b60">[61]</ref>, we use the sample concatenation and another FC layer to estimate new attention weights for region features. The new attention weight of the i-th region in relation-attention module is formulated as,</p><formula xml:id="formula_3">ν i = f ([F i : F m ] q 1 ),<label>(4)</label></formula><p>where q 1 is the parameter of FC, and f denotes the sigmoid function. In this stage, we aggregate all the region information along with the coarse global representation from self-attention into a new compact feature as,</p><formula xml:id="formula_4">P RAN = 1 n i=0 µ i ν i n i=0 µ i ν i [F i : F m ].<label>(5)</label></formula><p>P RAN is used as the final representation of the proposed RAN method.</p><p>Region Biased Loss. Inspired by the observation that different facial expressions are mainly defined by different facial regions <ref type="bibr" target="#b6">[7]</ref>, we make a straightforward constraint on the attention weights of self-attention, i.e. region biased loss (RB-Loss). This constraint enforces that one of the attention weights from facial crops should be larger than the original face image with a margin. For example, the Crop-2 in <ref type="figure" target="#fig_1">Figure  1</ref> can be more discriminative than the original one. Formally, the RB-Loss is defined as,</p><formula xml:id="formula_5">L RB = max{0, α − (µ max − µ 0 )},<label>(6)</label></formula><p>where α is a hyper-parameter served as a margin, µ 0 is the attention weight of the copy face image, µ max denotes the maximum weight of all facial crops.</p><p>In training, the classification loss is jointly optimized with the region biased loss. The proposed RB-Loss enhances the effect of region attention and encourages RAN to obtain superior weights of region and global representations. In fact, the RB-Loss can be also added to the relation-attention module. However, since the features of the relation-attention module already include holistic information, we experimentally find there is no gain by adding RB-Loss on the relation attention module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Region Generation</head><p>Cropping multiple regions is a fundamental step of our RAN. Too large regions lead to the reduced diversity of features and degrade to the case of many duplicates of the original face. Too small regions lead to insufficient discrimination ability of region features. In this paper, we evaluate three kinds of region generation schemes for our region attention networks, namely fixed position cropping, random cropping, and landmark-based cropping which are depicted in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Fixed position cropping. Since the face image can be well aligned by the recent advanced face alignment methods, a simple region generation scheme is to crop regions in fixed positions with fixed scales. Specifically, we crop five regions. Three of them are the top-left, top-right and center-down face regions, which have fixed size of 0.75 scale ratio of the original face. The other two regions are similar to those used in the smile-classification task of <ref type="bibr" target="#b67">[68]</ref>. Here we crop the center regions with sizes of 0.9, and 0.85 scale ratio of the original face. All the crops are resized to have the same input size of the backbone CNN.</p><p>Random cropping. In deep face recognition, the DeepID method uses 200 random crops for each face image to enhance its performance <ref type="bibr" target="#b53">[54]</ref>. For random cropping in our approach, we randomly crop N regions with random sizes ranged from 0.7 to 0.95 scale ratio of the original face.</p><p>Landmark-based cropping. Given facial landmarks, a straightforward method is to crop regions surrounding them, which is also used in <ref type="bibr" target="#b36">[37]</ref>. Here we use MTCNN to detect five facial landmarks (i.e. left eye, right eye, nose, left mouth corner, and right mouth corner), and use them to crop five regions. Specifically, according to each facial landmark, we use a radius r to crop regions and remove the regions which are out of the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Occlusion and Pose Variant Dataset</head><p>Though our proposed RAN can be used for FER in any conditions, we focus on the real-world occlusion and pose variantion problems. To the best of our knowledge, there is only a small real-world occlusion test dataset released in <ref type="bibr" target="#b36">[37]</ref> very recently, and there is no publicly available facial expression dataset that addresses both occlusion and pose annotations. To examine our method under realworld scenario, we build six test datasets from the existing large-scale FER datasets. From the test set of FER-Plus <ref type="bibr" target="#b4">[5]</ref>, the validation set of AffectNet <ref type="bibr" target="#b47">[48]</ref>, and the test set of RAF-DB <ref type="bibr" target="#b33">[34]</ref>, we collect the Occlusion-FERPlus, Pose-FERPlus, Occlusion-AffectNet, Pose-AffectNet, Occlusion-RAF-DB, and Pose-RAF-DB for testing. The test set will be available at https://github.com/kaiwang960112/Challengecondition-FER-dataset. These real-world test sets are annotated with different occlusion types and different pose degrees. Some examples are illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>For the pose variant test sets, we use the popular OpenFace toolbox <ref type="bibr" target="#b2">[3]</ref> to estimate the Euler Angle in pitch, yaw, roll directions. Since the roll angle is in-plane which can be eliminated by face alignment, we only consider the pose in pitch and yaw directions. Those faces with pitch or yaw angle larger than 30 o are collected to Pose-FERPlus, Pose-AffectNet, and Pose-RAF-DB.</p><p>For the occlusion test sets, we first define several occlusion types, namely wearing mask, wearing glasses, objects in left/right, objects in upper face and objects in bottom face, non-occlusion. Then we manually assign these categories to the test sets of FERPlus, AffectNet, and RAF-DB. Images with at least one type of occlusion are selected as the occlusion test sets.</p><p>We present the statistics of our collected test sets in <ref type="table" target="#tab_0">Table  I</ref>. Among all the occlusion types on FERPluse, AffectNet, and RAF-DB, the upper occlusion has the smallest samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first describe the used datasets and our implementation details. We then present our collected occlusion and pose variant test datasets and evaluate our proposed RAN on them. We further explore each components of RAN on FERPlus <ref type="bibr" target="#b4">[5]</ref>, AffectNet <ref type="bibr" target="#b47">[48]</ref> , and SFEW <ref type="bibr" target="#b14">[15]</ref>. Finally, we compare our method to the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To evaluate our method, we use four popular in-the-wild facial expression datasets, namely FERPlus <ref type="bibr" target="#b4">[5]</ref>, AffectNet <ref type="bibr" target="#b47">[48]</ref>, RAF-DB <ref type="bibr" target="#b33">[34]</ref>, and SFEW <ref type="bibr" target="#b14">[15]</ref>. These datasets cover different scales of face images and the challenging conditions. Besides, we also build occlusion and pose variant test datasets from FERPlus, AffectNet, and RAF-DB.</p><p>FERPlus <ref type="bibr" target="#b4">[5]</ref>. The FERPlus is extent from FER2013 <ref type="bibr" target="#b22">[23]</ref> introduced during the ICML 2013 Challenges in Representation Learning. It is a large-scale and real-world datasets collected by the Google search engine, and consists of 28,709 training images, 3,589 validation images and 3,589 test images. All face images in the dataset are aligned and resized to 48×48. The main difference between FER2013 and FERPlus is the annotation. FER2013 is annotated with seven expression labels (neutral, happiness, surprise, sadness, anger, disgust, fear) by one tagger, while FERPlus adds a contempt label and is annotated by 10 labels. In <ref type="bibr" target="#b4">[5]</ref>, the authors evaluate several training schemes, such as one-hot label (majority voting) and label distribution with cross-entropy loss. We mainly report the overall accuracy on the test set with supervision of majority voting and label distribution.</p><p>AffectNet <ref type="bibr" target="#b47">[48]</ref>. The AffectNet is by far the largest dataset that provides both categorical and Valence-Arousal annotations. The dataset contains more than one million images from Internet by querying expression-related keywords in three search engines, of which 450,000 images are manually annotated with eight basic expression labels as FERPlus. AffectNet has an imbalanced test set, a balanced validation set, and an imbalanced training set. We mainly report accuracy on the validation set where each category contains 500 samples.</p><p>SFEW <ref type="bibr" target="#b14">[15]</ref>. The Static Facial Expressions in the Wild (SFEW) dataset is built by selecting frames from AFEW <ref type="bibr" target="#b15">[16]</ref>, which covers unconstrained facial expressions, varied head poses, large age range, occlusions, varied focus, different resolution of the face and real-world illumination. We use the newest version of SFEW in <ref type="bibr" target="#b17">[18]</ref> where it has been divided into three sets: train (958 images), validation (436 images), and test (372 images). Each image is labeled with one of the seven expressions including angry, disgust, fear, happy, sad, surprise, and neutral by two independent labelers. We mainly report our performance on the validation set. RAF-DB. RAF-DB <ref type="bibr" target="#b33">[34]</ref> contains 30,000 facial images annotated with basic or compound expressions by 40 trained human coders. In our experiment, only images with basic emotions were used, including 12,271 images as training data and 3,068 images as test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In all the following experiments, we use the CNN detector and the ERT <ref type="bibr" target="#b30">[31]</ref> based face alignment method in Dlib toolbox 1 to crop and align faces, and then resize them to the size of 224×224. We implement our methods with Pytorch toolbox 2 . For the backbone CNN, we mainly use the ResNet-18 <ref type="bibr" target="#b24">[25]</ref> and VGG16 <ref type="bibr" target="#b49">[50]</ref>. The ResNet-18 is pre-trained on MS-Celeb-1M face recognition dataset and VGG16 is downloaded from website 3 . The last pooling layer of ResNet-18, and the first FC feature of VGG16 is used for facial representation. In training phase of fixed cropping, we use all the five regions along with original face for each face image (i.e. k = 5 in <ref type="figure" target="#fig_1">Figure 1</ref>). For training with random cropping, we replace the fixed five regions with randomly cropped ones. When jointly training with RB-Loss and Cross-Entropy loss, the default loss weight ratio is 1:1. On all datasets, the learning rate is initialized as 0.01, and divided by 10 after 15 epochs and 30 epochs. We stop training in 40 epochs. The margin in RB-Loss is default as 0.02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FER with occlusion and variant pose in the wild</head><p>To address the occlusion and pose variant issues, we construct several test subsets with occlusion and pose annotations, i.e. Occlusion-FERPlus, Pose-FERPlus, Occlusion-AffectNet, Pose-AffectNet, Occlusion-RAF-DB, and Pose-RAF-DB. We   evaluate our RAN on the collected datasets with the default setting ( i.e. ResNet18 with alignment, RB-Loss and fixed cropping). We fine-tune the ResNet18 on original face images as baselines. We present the confusion matrices of our RAN and these baselines in <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_5">Figure 5</ref> to further investigate our improvements. We find that our RAN consistently boosts the "happiness", "superise", and "sadness" categories on all the test sets. It may be explained by that these facial expressions have clear region features, such as action units of "Lip Corner Puller" , "Cheek Raiser", and "Lip Corner Depressor", which can be effectively captured by our RAN.</p><p>We also conduct a fair comparison on the recent occlusion test dataset: FED-RO <ref type="bibr" target="#b36">[37]</ref>. We use the RAN with default setting, and train it using the same training data as <ref type="bibr" target="#b36">[37]</ref>. We finally achieve 67.98% which is clearly better than 66.5% of <ref type="bibr" target="#b36">[37]</ref>.</p><p>Individual regions and their combination. Since our RAN integrates several regions in a single network, we present the performace of individual regions and their score fusion on Occlusion-and Pose-FERPlus in <ref type="table" target="#tab_0">Table III</ref>. To investigating if the improvement of our RAN only comes from augmented data, we also train a traditional model by mixing all the regions and the original images for data augmentation, which is called aug. training. We conclude that i) the performance of individual regions are comparable to each other except for the region I 1 and I 2 , ii) a naive score fusion (i.e. average) and mixing all the regions improve individual performance slightly, and iii) our RAN outperforms the score fusion and Aug. Training by a large margin. Compared to score fusion and fusion training, our RAN takes account of the importance of region features and also emphasises the most important region with RB-Loss.</p><p>What is learned for occlusion and pose variant faces? It may illustrate that the tiny occlusion can not impact the FER system a lot. The attention weights of tiny occlusion face are more likely to be random. Compared to the RB-Loss case, though RAN without RB-Loss can also assign different attention weights similarly, the weights for all the regions from RAN without RB-Loss are smoother. In addition, the original image prefers to have the highest weight without RB-Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study on FERPlus and AffectNet</head><p>To validate the generality of our method, we conduct an ablation study on the full test set of FERPlus and the full validation set of AffectNet with default setting. Face alignment is a standard pre-processing method for face analysis, while a few works do not utilize <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b47">[48]</ref> for FER task. Here we also study the effect of face alignment. Attention modules. We first study the attention modules of our RAN without using RB-Loss. The evaluation results on FERPlus and AffectNet are presented on <ref type="table" target="#tab_0">Table IV and  Table V</ref>, respectively. On FERPlus without face alignment, the self-attention (F m in Eq. (3)) improves the baseline by 0.4%. Adding the relation-attention module, our method outperforms the baseline by 1.13% and 3.05% on FERPlus and AffectNet without face alignment. Face alignment is found to significantly boost the baseline method on both datasets, while its effect is limited when using our proposed RAN. This can be explained by that our method implicitly learns to align facial regions with the attention mechanism as that in machine traslation <ref type="bibr" target="#b3">[4]</ref>. With face alignment, our attention modules improve the baselines by 0.83% and 1.85% on FERPlus and AffectNet, respectively.</p><p>Region biased loss. The RB-Loss is added to the selfattention module with margin 0.02 by default. From <ref type="table" target="#tab_0">Table IV</ref>   and <ref type="table" target="#tab_6">Table V</ref>, we can see that the designed RB-Loss further improves performance on both FERPlus and AffectNet consistently. Specifically, the improvement on AffectNet without face alignment is 0.92%. With oversampling, our RAN with RB-Loss achieves 59.5% on the validation set of AffectNet. It is worth noting that RB-Loss does not increase computational cost in testing. We also evaluate the parameter α of RB-Loss in <ref type="figure" target="#fig_7">Figure  7</ref>. Increasing α from 0 to 0.02 gradually improves the performance while larger α leads to fast degradation, which indicates the original image is also important for FER. As the mater of fact, the result of this experiment is part of our motivation to keep the original face image for our method.</p><p>Evaluation of individual regions and different fusion schemes. We conduct an evaluation of individual regions and different fusion schemes on the full FERPlus test datasets without face alignment. For the fusion schemes, we mainly consider three popular methods, namely feature concatanation, feature average pooling, and score fusion (i.e. score average). The evaluation results are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. Several observations can be concluded as follows. First, all the individual crops are inferior to the original image which indicates the performance gain is not from special enlarged crops. Second, compared to the original image, there is no obvious improvement by concatanating and averaging region features. Third, score fusion slightly improves the baseline by 0.54% while our RAN outperforms the baseline by 1.35%.</p><p>Evaluation of region generation strategies. We evaluate the fixed cropping, landmark-based cropping, and random cropping methods on FERPlus with the default setting for other parameters, the results are shown in <ref type="figure" target="#fig_9">Figure 9</ref>. For random cropping, we randomly generate 3 regions for each  image in each training iteration while generate 6, 30, 60, 80, 120 random regions for test evaluation several times. For landmark-based cropping, we set the radius as 0.4 of the side of image which ensures a similar size as fixed cropping. The fixed cropping strategy consistently outperforms the random cropping even dozens of times more regions are used. The landmark-based cropping performs slightly worse than the fixed cropping. Training with random cropping yet testing with the same fixed cropping has limited effect for random region cropping. Increasing the crops boosts performance in the beginning while degrades after 30 crops. This may be explained by that increasing crops leads to too many suboptimized regions and they dominate the final representation.</p><p>Evaluation of region size. To explore the impact of region sizes for our RAN, we evaluate the region size of fixed cropping scheme on FERPlus with other parameter as default. Since five regions with different sizes are cropped in our default setting, we evaluate these region sizes using a ratio from low to high compared to the default sizes.The evaluation results are shown in <ref type="figure" target="#fig_1">Figure 10</ref>. The performance degrades significantly with the ratio reducing to 0.4. Increasing (i.e. ration:1.1) size upon the default one slightly reduces the performance. It may be explained that the regions of I 4 and I 5 almost degrade to the original image, and the information gain from enlarging regions disappeared if too large regions are used.</p><p>Evaluation of inference time. Since our RAN has five  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with the state-of-the-art methods</head><p>In this section, we compare our best results to several stateof-the-art methods on FERPlus, AffectNet, SFEW, and RAF-DB.</p><p>Comparison on FERPlus. We compare our RAN to several state-of-the-art methods on the FERPlus dataset in <ref type="table" target="#tab_0">Table VI</ref>. Both <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b0">[1]</ref> leverage the label distribution for each face as supervision. <ref type="bibr" target="#b0">[1]</ref> pretrains a SeNet50 <ref type="bibr" target="#b25">[26]</ref> on VGGFace2.0 <ref type="bibr" target="#b9">[10]</ref> which includes amount of large-pose faces. With the KLDiv loss and label distribution supervision, we fine-tune the public VGGFace model (VGG16 pretrained on VGGFace1.0) with our RAN and achieve 89.16% which is a new state of the art to our knowledge.</p><p>Comparison on AffectNet. <ref type="table" target="#tab_0">Table VII</ref> presents the comparison on AffectNet. We obtain 52.97% and 59.5% without and with oversampling, respectively. It is worth noting that <ref type="bibr" target="#b47">[48]</ref> only achieves 47% with upsampling and <ref type="bibr" target="#b65">[66]</ref> uses one more large-scale FER dataset and 80 layers ResNet for training with elaborated loss weights on them.</p><p>Comparison on SFEW. <ref type="table" target="#tab_0">Table VIII</ref> presents the comparison on SFEW. <ref type="bibr" target="#b8">[9]</ref> applies a small CNN with an island loss which is the combination of the Center loss <ref type="bibr" target="#b59">[60]</ref> and an inter-class loss. <ref type="bibr" target="#b64">[65]</ref> ensembles multiple CNNs with each CNN model initialized randomly or pretrained on FER2013. Our RAN with   <ref type="table" target="#tab_0">Table IX</ref> presents the comparison on RAF-DB. RAF-DB is a latest facial expression dataset which not only has basic emotion categories but also compound categories. We report the overall accuracy on the basic emotion categories. <ref type="bibr" target="#b33">[34]</ref> introduces the RAF-DB dataset and uses a locality-preserving loss for network training. <ref type="bibr" target="#b36">[37]</ref> leverages patch-based attention networks and glocal networks. Our proposed RAN achieves 86.9% on RAF-DB with default setting, which are 2.77% and 1.83% better than DLP-CNN <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b36">[37]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we address the facial expression recognition in the real-world occlusion and pose-variant conditions. We build several new FER test datasets on these conditions, and propose the Region Attention Network (RAN) which adaptively adjusts the importance of facial parts. We further design a region Biased loss (RB-Loss) function to encourage high attention weight for the most important region. We evaluate our method on the collected datasets and make extensive studies on FER-Plus and AffectNet. Our proposed method achieves state-ofthe-art results on FERPlus, SFEW, RAF-DB, and AffectNet. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Albanie et al. use the VGGFace model (face recognition model) and fine-tune it on FERPlus with soft probabilities. Meng et al. evaluate different face recognition model architectures and used face recognition datasets for facial expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The framework of our RAN. A face image is cropped into several regions, and these regions are fed into a backbone CNN for feature extraction. The self-attention and relation-attention module are then used to obtain compact face representation. f denotes the sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>An example of our region generation methods. Left: fixed position cropping. Right: random cropping. Upper: landmarkbased cropping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Some examples of our collected occlusion and pose variant test datasets. The left color images are from the test set of AffectNet, and the right gray images are from the test set of FERPlus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The confusion matrices of baseline methods and our RAN on the Occlusion-and Pose-FERPlus test sets. Baseline on Occlusion-AffectNet. RAN on Occlusion-AffectNet. Baseline on Pose-AffectNet. RAN on Pose-AffectNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>The confusion matrices of baseline methods and our RAN on the Occlusion-and Pose-AffectNet test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>(a) RAN with RB-Loss (b) RAN without RB-Loss Illustration of learned attention weights for different regions along with origianl faces. s(·) denotes the softmax function. Red-filled boxes indicate the highest weights while blue-filled ones are the lowest weights. From left to right, the columns represent the original faces, regions I 1 to I 5 . Note that the left and right figures show the weights obtained with and without the RBLoss respectively. Better viewed in PDF.To better explore our RAN, we illustrate the final attention weights for several examples with RB-Loss and without RB-Loss inFigure 6(a) and Figure 6(b), respectively. Occlusion examples are shown in the first two rows, and pose examples in the third and forth rows. We also show a bad example in the last row. For the occlusion examples, our RAN with RB-Loss gets the highest weight on the small center crop (i.e. I 5 ) for the first example. It makes sense since this image suffers from the left and right occlusion. In the second example which suffers from bottom-left occlusion, the RAN with RB-Loss automatically assigns the highest weight to the up-right region while suppresses the bottom-left region. For both pose-variant examples in the third and forth rows, our RAN with RB-Loss gets high attention weights on center regions while gets low weights on the up-right regions. This may be explained by that the up-right regions contain the most of irrelated information on the near-profile faces. With RB-Loss and RAN, the original faces get relatively average attention weights among all the examples. For a bad example in the last row, our RAN with RB-Loss does not assign the regions with high weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>The evaluaiton of the margin (α) in RB-Loss on FERPlus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Performce comparison of individual regions and different aggregation schemes on FERPlus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Evaluation of different region generation strategies and the number of random regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Evaluation of region sizes on FERPlus. The ratios are compared to the default setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Statistics of collected test datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Occlusion</cell><cell></cell><cell cols="2">Pose(pitch/yaw)</cell></row><row><cell></cell><cell>upper</cell><cell>bottom</cell><cell>left/right</cell><cell>glasses/mask</cell><cell>&gt;30</cell><cell>&gt;45</cell></row><row><cell>FERPlus</cell><cell>70</cell><cell>138</cell><cell>213</cell><cell>184</cell><cell>1171</cell><cell>634</cell></row><row><cell>AffectNet</cell><cell>84</cell><cell>183</cell><cell>128</cell><cell>288</cell><cell>1949</cell><cell>985</cell></row><row><cell>RAF-DB</cell><cell>126</cell><cell>151</cell><cell>160</cell><cell>298</cell><cell>1248</cell><cell>558</cell></row><row><cell cols="7">The total numbers of occlusion samples in FERPlus (test) ,</cell></row><row><cell cols="7">AffectNet (validation), and RAF-DB (test) are respectively</cell></row><row><cell cols="7">605, 682 , and 735, which are 16.86%, 17.05%, and 23.9% of</cell></row><row><cell cols="7">their original sets. For the variant pose issue, about one-third</cell></row><row><cell cols="7">of FERPlus (test), about two-fifths of RAF-DB, and about half</cell></row><row><cell cols="7">of AffectNet (validation) have poses larger than 30 degrees (in</cell></row><row><cell cols="2">pitch or yaw).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Performance comparison between the proposed RAN and baseline method with occlusion and variant pose conditions.</figDesc><table><row><cell>FERPlus</cell><cell cols="3">Occlusion Pose(30) Pose(45)</cell></row><row><cell>Baseline</cell><cell>73.33</cell><cell>78.11</cell><cell>75.50</cell></row><row><cell>RAN (w RB-Loss)</cell><cell>83.63</cell><cell>82.23</cell><cell>80.40</cell></row><row><cell>AffectNet</cell><cell cols="3">Occlusion Pose(30) Pose(45)</cell></row><row><cell>Baseline</cell><cell>49.48</cell><cell>50.10</cell><cell>48.50</cell></row><row><cell>RAN (w RB-Loss)</cell><cell>58.50</cell><cell>53.90</cell><cell>53.19</cell></row><row><cell>RAF-DB</cell><cell cols="3">Occlusion Pose(30) Pose(45)</cell></row><row><cell>Baseline</cell><cell>80.19</cell><cell>84.04</cell><cell>83.15</cell></row><row><cell>RAN (w RB-Loss)</cell><cell>82.72</cell><cell>86.74</cell><cell>85.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II</head><label>II</label><figDesc>AffectNet and Pose-RAF-DB, respectively. The gains are improved to 4.9%, 5.4% and 2.05% with pose larger than 45 degrees. Overall, these results demonstrate the effectiveness of our proposed RAN on occlusion and variant pose FER data.</figDesc><table><row><cell>presents the comparison between the</cell></row><row><cell>baselines and our method. Our RAN improves the baseline</cell></row><row><cell>method significantly, with gains 10.3%, 10.02% and 2,53%</cell></row><row><cell>on Occlusion-FERPlus, Occlusion-AffectNet and Occlusion-</cell></row><row><cell>RAF-DB, respectively. On Pose-FERPlus, Pose-AffectNet and</cell></row><row><cell>Pose-RAF-DB, the RAN also outperforms the baseline with a</cell></row><row><cell>large margin. Specifically, with pose larger than 30 degrees,</cell></row><row><cell>the gains are 4.12%, 3.09% and 2.70% on Pose-FERPlus,</cell></row><row><cell>Pose-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>The performance of individual regions with occlusion and variant pose conditions on FERPlus. 'Aug. Training' means that we augment the dataset by combining all the regions and original images and then train the network.</figDesc><table><row><cell>Region</cell><cell cols="3">Occlusion Pose(30) Pose(45)</cell></row><row><cell>Original (I 0 )</cell><cell>73.33</cell><cell>78.11</cell><cell>75.50</cell></row><row><cell>I 1</cell><cell>67.43</cell><cell>74.27</cell><cell>71.40</cell></row><row><cell>I 2</cell><cell>64.13</cell><cell>72.22</cell><cell>70.30</cell></row><row><cell>I 3</cell><cell>72.22</cell><cell>78.48</cell><cell>76.84</cell></row><row><cell>I 4</cell><cell>72.8</cell><cell>78.54</cell><cell>77.00</cell></row><row><cell>I 5</cell><cell>74.54</cell><cell>78.63</cell><cell>75.35</cell></row><row><cell>Score Fusion (I 0 − I 5 )</cell><cell>75.70</cell><cell>79.84</cell><cell>78.45</cell></row><row><cell>Aug. Training (I 0 − I 5 )</cell><cell>79.92</cell><cell>81.24</cell><cell>79.26</cell></row><row><cell>RAN (w RB-Loss)</cell><cell>83.63</cell><cell>82.23</cell><cell>80.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Evaluation of all components of our RAN along with face alignment on FERPlus.</figDesc><table><row><cell cols="5">Align Self-att. Relation-att. RB-Loss Accuracy</cell></row><row><cell>√ √ √ √</cell><cell>√ √ √ √ √ √</cell><cell>√ √ √ √</cell><cell>√ √</cell><cell>86.50 86.90 87.63 87.85 87.60 87.80 88.23 88.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Evaluation of all components of our RAN along with face alignment on AffectNet without oversampling.</figDesc><table><row><cell cols="5">Align Self-att. Relation-att. RB-Loss Accuracy</cell></row><row><cell>√ √ √</cell><cell>√ √ √ √</cell><cell>√ √ √ √</cell><cell>√ √</cell><cell>49.00 52.05 52.97 50.32 52.17 52.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison to the state-of-the-art results on the FERPlus dataset. * These results are trained using label distribution as supervison.times feedforward operations than the baseline, we investigate the inference time on FERPlus test set. We evaluate the average per-image inference time and the run-time is obtained on a TITAN 1080ti GPU of Linux Cluster with a 2.6 GHz Intel(R) Xeon(R) E5-2690 CPU. The average inference time of RAN and the baseline are 0.025s and 0.006s, respectively. Due to the powerful parallel ability of GPU, the increasing time is not linear to the number of regions.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell>Pre-trained Dataset</cell><cell>Year</cell><cell>Performance</cell></row><row><cell>[5]  *</cell><cell>VGG13</cell><cell>/</cell><cell>2016</cell><cell>85.1</cell></row><row><cell>[27]</cell><cell>ResNet18+VGG16</cell><cell>/</cell><cell>2017</cell><cell>87.4</cell></row><row><cell>[1]  *</cell><cell>SeNet50</cell><cell>VGG Face2</cell><cell>2018</cell><cell>88.8</cell></row><row><cell>RAN-ResNet18</cell><cell>ResNet18</cell><cell>MS Celeb 1M</cell><cell>2019</cell><cell>88.55</cell></row><row><cell>RAN-VGG16  *</cell><cell>VGG16</cell><cell>VGG Face</cell><cell>2019</cell><cell>89.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison to the state-of-the-art results on the AffectNet dataset. + Oversampling is used for a final performance report since AffectNet is imbalanced. ‡ RAF-DB is added into training data.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell>Year</cell><cell>Performance</cell></row><row><cell>Up-Sampling [48]</cell><cell>AlexNet</cell><cell>2018</cell><cell>47.0</cell></row><row><cell>Weighted-Loss [48]</cell><cell>AlexNet</cell><cell>2018</cell><cell>58.0</cell></row><row><cell>[66]  ‡</cell><cell cols="2">ResNet80 2018</cell><cell>55.71</cell></row><row><cell>RAN-ResNet18</cell><cell cols="2">ResNet18 2019</cell><cell>52.97</cell></row><row><cell>RAN-ResNet18 +</cell><cell cols="2">ResNet18 2019</cell><cell>59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparison to the state-of-the-art results on the SFEW dataset. single model achieves 54.19% on the validation set which is the best single model to our best of knowledge. Since model ensemble is popular on SFEW, we also conduct a naive model fusion by averaging the scores of ResNet18 and VGG16 which obtains 56.4%.</figDesc><table><row><cell>Method</cell><cell>Pre-trained Dataset</cell><cell>Year</cell><cell>Performance</cell></row><row><cell>Island Loss [9]</cell><cell>FER2013</cell><cell>2018</cell><cell>52.52</cell></row><row><cell>Identity-aware CNN [46]</cell><cell>FER2013</cell><cell>2017</cell><cell>50.98</cell></row><row><cell>Multiple deep CNNs [65]</cell><cell>FER2013</cell><cell>2015</cell><cell>55.96</cell></row><row><cell>RAN-ResNet18</cell><cell>MS Celeb 1M</cell><cell>2019</cell><cell>54.19</cell></row><row><cell>RAN(VGG16+ResNet18)</cell><cell>MS Celeb 1M</cell><cell>2019</cell><cell>56.4</cell></row></table><note>Comparison on RAF-DB.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX :</head><label>IX</label><figDesc>Comparison to the state-of-the-art results on the RAF-DB dataset.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell>Year</cell><cell>Performance</cell></row><row><cell>DLP-CNN [34]</cell><cell>8-layer baseDCNN</cell><cell>2019</cell><cell>84.13</cell></row><row><cell>gACNN [37]</cell><cell>VGG16</cell><cell>2018</cell><cell>85.07</cell></row><row><cell>RAN-ResNet18</cell><cell>ResNet18</cell><cell>2019</cell><cell>86.90</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotion recognition in speech using cross-modal transfer in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Andrea Vedaldi, and Andrew Zisserman. Emotion recognition in speech using cross-modal transfer in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Samuel Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05561</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Openface: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>CMU School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">CMU-CS-16-118</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training deep networks for facial expression recognition with crowd-sourced label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristian Canton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aumpnet: simultaneous action units detection and intensity estimation on multipose facial images using a single convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Júlio César</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vítor</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Olga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Bellon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="866" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial areas and emotional information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of communication</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognition of facial expressions in the presence of occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Bourel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">C</forename><surname>Chibelushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">A</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Island loss for learning discriminative features in facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shehab Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Oreilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Long short-term memorynetworks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>abs/1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse representation for accurate classification of corrupted and occluded facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="838" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted voting of sparse representation classifiers for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing European Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1164" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotiw 2018: Audio-video, student engagement and group-level affect prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanjot</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Ov Ramana Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comprehensive survey on poseinvariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facenet2expnet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprakash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust face analysis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="40" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">Luc</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition using features of salient facial patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurobinda</forename><surname>Sl Happy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining convolutional neural networks for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">URTC, 2017 IEEE MIT</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunjeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><forename type="middle">Chandias</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An analysis of facial expression recognition under partial facial image occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioan</forename><surname>Buciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild via convolutional neural networks and mapped binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep facial expression recognition: A survey. CoRR, abs/1804.08348</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Occlusion aware facial expression recognition using cnn with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partial face recognition: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1193" to="1205" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Au-inspired deep networks for facial expression feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Facial expression recognition under partial occlusion based on gabor multi-orientation features fusion and local gabor binary pattern histogram sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Information Hiding and Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<idno>abs/1711.09550</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The japanese female facial expression (jaffe) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeru</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miyuki</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Gyoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Budynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">frame attention networks for facial expression recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00193</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Identityaware convolutional neural network for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sift: predicting amino acid changes that affect protein function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Henikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3812" to="3814" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Coupled gaussian processes for pose-invariant facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1357" to="1369" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Group emotion recognition with individual facial emotion cnns and global image based cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianzhi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Face attention network: An effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1711.07246</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung Yongxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<idno>abs/1603.05474</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Specialized face perception mechanisms extract both part and spacing information: Evidence from developmental prosopagnosia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galit</forename><surname>Yovel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Duchaine</surname></persName>
		</author>
		<idno type="PMID">16768361</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="580" to="593" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="222" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Gender and smile classification using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianzhi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Facial expression recognition experiments with data from television broadcasts and the world wide web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Chandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Random gabor based templates for facial expression recognition in images with facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Chandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="451" to="464" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Robust facial expression recognition via compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3747" to="3761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
