<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AttaCut: A Fast and Accurate Neural Thai Word Segmenter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pattarawat</forename><surname>Chormai</surname></persName>
							<email>pat.chormai@maxplanckschools.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ponrawee</forename><surname>Prasertsom</surname></persName>
							<email>ponrawee.pra@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><forename type="middle">T</forename><surname>Rutherford</surname></persName>
							<email>attapol.t@chula.ac.th</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck School of Cognition Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Chulalongkorn University</orgName>
								<address>
									<settlement>Bangkok</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Chulalongkorn University</orgName>
								<address>
									<settlement>Bangkok</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AttaCut: A Fast and Accurate Neural Thai Word Segmenter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Word segmentation is a fundamental pre-processing step for Thai Natural Language Processing. The current off-the-shelf solutions are not benchmarked consistently, so it is difficult to compare their trade-offs. We conducted a speed and accuracy comparison of the popular systems on three different domains and found that the state-of-the-art deep learning system is slow and moreover does not use sub-word structures to guide the model. Here, we propose a fast and accurate neural Thai Word Segmenter that uses dilated CNN filters to capture the environment of each character and uses syllable embeddings as features. Our system runs at least 5.6× faster and outperforms the previous state-of-the-art system on some domains. In addition, we develop the first ML-based Thai orthographical syllable segmenter, which yields syllable embeddings to be used as features by the word segmenter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word segmentation presents a fundamental challenge for Thai language processing. Many of the downstream natural language processing tasks require that texts be broken into a sequence of words before applying any models. Like Chinese, the Thai script does not mark word boundaries with spaces, so an automatic word segmentation is often required. The Thai script uses 44 consonant symbols, 15 vowel symbols, and 4 tonal symbols. A word is composed of one or more syllables, and each syllable is formed by a set of intricate orthographical rules for valid sequences of Thai alphabet symbols.</p><p>Word segmentation is challenging because of linguistic ambiguity and out-of-vocabulary cases. A word can be formed by juxtaposing two "words" e.g. เห็ นชอบ (approve) = เห็ น (see) + ชอบ (like). This kind of word formation can be detected with a simple dictionary lookup, but harder cases which require context abound in the language. For example, กอดอกไม can be segmented into กอ|ดอก|ไม (flower bush) or กอด|อก|ไม (hugging wooden human chest), but the latter is nonsensical and very unlikely. The local context is needed to select the right segmentation; therefore, dictionaries only provide partial solutions to this problem. A constant stream of new words and loanwords complicates the task of word segmentation further.</p><p>In recent years, a few open-sourced Thai word segmenters have been introduced and used widely in the industry. Notable examples of open-sourced Thai word segmenters include PythaiNLP <ref type="bibr" target="#b15">[16]</ref>, Sertis <ref type="bibr" target="#b19">[20]</ref>, and DeepCut <ref type="bibr" target="#b6">[7]</ref>, which claim good performance according to their own respective benchmarks. The accuracies of these word segmentation systems are not benchmarked on the same datasets for a rigorous comparison within and across domains. We aim to evaluate the speed and accuracy of these previous solutions and compare the trade-offs with our system proposed in this paper.</p><p>The popular and powerful Thai word segmenters (DeepCut and Sertis) utilize deep convolutional neural networks (CNNs) or recurrent neural networks (RNNs). The accuracy comes at the high cost of speed, however. The fastest neural model takes 2 minutes to process one million characters on a cloud instance 2 . This rate is unsuitably slow as a first preprocessing step when dealing with a large amount of data or streaming data, where the rate of incoming data exceeds the rate of processing. <ref type="figure" target="#fig_0">Figure 1</ref> shows a comparison between speed and segmentation quality of existing Thai word segmenters. In this project, we propose a model that can segment Thai text at least 2× faster than the previous methods.</p><p>Our system takes advantage of the fact that every word can be parsed into orthographical syllables. We hypothesize that syllables provide important features for segmentation because word boundaries are a subset of syllable boundaries. We propose a CNN-based word segmentation model that utilizes character and syllable embeddings as the representation and significantly reduces the number layers and computation time needed to achieve the comparable performance.</p><p>Our contributions can be summarized as follows:</p><p>• We propose a Thai word segmentation model that runs at least 5.6× faster than the previous state-of-the-art system without compromising much segmentation performance.</p><p>• We develop the first ML-based Thai orthographical syllable segmenter that resists typos, which actually benefits the performance of our word segmentation model.</p><p>• We benchmark the existing word segmentation systems, along with our systems, across multiple data genres to make recommendations for practitioners on speed and accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ก า ล เ ว ล า</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem: Thai Word and Syllable Segmentation</head><p>Many downstream NLP models require that text is broken down into words or smaller linguistic units. For example, bag-of-word models or RNN models usually require that the text is represented as a sequence of words. A Thai word is the smallest lexical unit that conveys the meaning <ref type="bibr" target="#b1">[2]</ref>. การบ าน (homework) is one word, not two, although การ (nominalization prefix) and บ าน (house) are also words in other context. Most ambiguous and debatable cases revolve around the degree of compositionality of nouns and verbs. The meaning of การบ าน is not composed of การ (nominalization morpheme) and บ าน (house) and therefore should be treated as one word. As a more uncertain example, it is arguable that the meaning of กอดอก (to cross arms is composed of กอด (to hug) and อก (breast) and therefore should not be treated as one word. Our dataset follows the guidelines that favor the segmentation takes the degree of compositionality into account and not grammatical function changes such as nominalization or verbalization.</p><p>Syllable segmentation task is defined similarly, but we try to find syllable boundaries instead of word boundaries. It is noteworthy that word boundaries are always subset of syllable boundaries because each syllable belongs to exactly one word. We use this fact as a basis for our model architecture. Unlike word boundaries, syllable boundaries are less ambiguous. An orthographical syllable is defined as a substring in a word that can be pronounced as one or one and a half phonological syllable. For example, a word กอดอก can be segmented into two orthographical syllables กอด (/god/) | อก (/ok/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approaches: AttaCut</head><p>Our proposed model follows a typical CNN architecture and similar to the state-of-the-art Thai word segmenter, DeepCut. We perform an investigation on DeepCut's architecture, analyzing which parts of the model are important or superfluous: complete details of the analysis can be found at Appendix 7.3.1. We use the insight to design our proposed models that can perform word segmentation quickly while achieving a good level of quality. We employ the dilation technique <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref> in the convolution layers, which allows the model to use non-redundant convolution layers that cover sufficient context. Unlike Strubell et al. <ref type="bibr" target="#b22">[23]</ref>'s architecture, our convolutional layers have different dilation numbers and kernel widths, and they convolve directly on embeddings. Without hierarchical convolutions, our architecture has less computational dependencies, hence a higher degree of parallelism. In addition, we use syllable embeddings as additional features, which should provide higher-level information than a character type or a character embedding can afford. <ref type="figure" target="#fig_1">Figure 2</ref> shows an overview of our proposed architecture. It is comprised of three one-dimension convolutional layers, pooling layers, and a fully-connected layer. The convolution layers take the concatenation of character and syllable embeddings as input. The concatenation contextualizes the character embedding with its surrounding context provided by the syllable embedding. Therefore, a character will be represented by a different embedding if found in a different syllable. We call this model AttaCut-SC, while our baseline model that use only character embeddings is named AttaCut-C.</p><p>We use max pooling to combine extracted features from these convolution layers. After pooling, we have fully-connected layers to derive the probability that the corresponding character is either a starting-word character (B) or a in-word character (I).</p><p>In general, syllable and word segmentation can be formulated as a sequence labelling problem, where we want to assign a label to each position in the sequence: in segmentation problems, the label is binary, representing whether the position is a segment-initial character. A segment here can represent a word or a syllable, depending on the task.</p><p>Let D be a Thai corpus containing Thai sentences s i and y i represents the segmentation label of s i . The learning objective is to learn a set of suitable parametersθ of a model f parameterized by θ:</p><formula xml:id="formula_0">θ = arg min θ E[L(f (s; θ), y)] (1) ≈ arg min θ 1 |D| |D| ∑ i=1 L(f (s i ; θ), y i ),<label>(2)</label></formula><p>where E[·] denotes expectation and L is a binary cross entropy loss function over all positions in a sequence. We propose a Conditional Random Fields (CRF) syllable segmenter primarily to be used as a preprocessing step for the word segmenter. Historically, automatic syllable segmentation has often been intended for other purposes, such as concatenative speech synthesis <ref type="bibr" target="#b8">[9]</ref>. To the best of our knowledge, syllables have never been used as features for ML-based word segmentation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment 1: Syllable Segmenter</head><p>We use pycrfsuite implementation of CRF <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> and train the model on Thai National Corpus (TNC) <ref type="bibr" target="#b2">[3]</ref>. The TNC contains a subcorpus of 2.56M annotated syllables (around 8M characters). The dataset is split three-way for training, development and testing using the 70:20:10 scheme. The training, development, and test sets contain 1.8M syllables, 0.5M syllables, and 0.25M syllables respectively.</p><p>We hypothesize that CRF is suitable for syllable segmentation because of its inclusion of sequential information. We test this hypothesis by comparing it against a maximum entropy model (MaxEnt), trained using the scikit-learn implementation <ref type="bibr" target="#b13">[14]</ref>. For both algorithms, we experiment with the following features, with N and window size W of 1 to 4: i) individual characters within W places around on both sides of a potential boundary (Chr); ii) two N-grams on both sides (ChrSpan); iii) N-gram features that include all N-grams within W places on both sides.</p><p>Our evaluation employs measures of precision, recall and F 1 on two levels: character-level (CL) and syllable-level (SL). CL measures are standard in evaluating word segmentation. Here, they are essentially the same, except for the fact that they are based on that of correct syllable-initial-rather than word-initial-characters. SL measures are calculated based on the number of correct syllables rather than characters. We compute SL precision and recall as follows:</p><p>• Syllable-level Precision (SL P ) is the ratio between the number of correctly segmented syllables and the number of syllables in prediction;</p><p>• Syllable-level Recall (SL R ) is the ratio between the number of correctly segmented syllables and the number of syllables in the ground truth.</p><p>We use macro-averaging for final statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment 2: Word Segmenter</head><p>We use PyTorch <ref type="bibr" target="#b12">[13]</ref> for implementation. We train AttaCut word segmenters using Adam <ref type="bibr" target="#b5">[6]</ref> and adjust hyperparameters according to training dynamics. We experiment with various layer configurations for both architectures: the best setting for AttaCut-SC contains 158, 993 learning parameters, while AttaCut-C has 173, 533 such parameters.</p><p>Our training data is BEST-2010 <ref type="bibr" target="#b10">[11]</ref>. Annotated with word boundaries and name entities, the corpus contains 415 Thai documents from four categories: news, articles, encyclopedias, and novels, accounting for 134,107 samples (split by line), around 5.11M words, and 18.74M characters. Balancing the distribution of categories, we take 10% of the training set as a development split. We use the official provided test set (officially named as TEST_100K) for evaluation, which has 2,252 samples, about 128K words, and 496K characters.</p><p>Apart from in-domain evaluation on BEST-2010's test set, we also perform cross-domain word segmentation evaluations on another two datasets: i) Thai National Historical Corpus (TNHC) <ref type="bibr" target="#b17">[18]</ref> contains 20,791 samples, around 599K words, and 2.14M characters of Thai classical literature documents with word boundaries annotated by humans, and ii) Wisesight corpus <ref type="bibr" target="#b25">[26]</ref> contains 26,700 social media messages, labelled in four categories including question, positive, negative, and neutral, with official train and test splits. Because it does not have word boundary annotated, we randomly take 1,000 samples (with 7 spam messages removed) from the test split and manually segment them using a word segmentation standard proposed by Aroonmanakun and others <ref type="bibr" target="#b1">[2]</ref>. We call this Wisesight-1000: it has around 22K words, and 75K characters.</p><p>We compare our segmenters with: i) PyThaiNLP <ref type="bibr" target="#b15">[16]</ref> with its maximal matching engine <ref type="bibr" target="#b21">[22]</ref>; ii) Sertis <ref type="bibr" target="#b19">[20]</ref>, a bidirectional RNN with GRUs with 121K trainable parameters <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>; iii) DeepCut <ref type="bibr" target="#b6">[7]</ref>, a character-level CNN <ref type="bibr" target="#b27">[28]</ref> with a stack of 13 convolutional filters, followed by pooling and fully-connected layers, comprising around 500K trainable parameters. DeepCut's architecture is described in Appendix 7.2.</p><p>Evaluating the quality of word segmenters is typically done on a character-level (CL) basis. Standard measured metrics are precision, recall, and F 1 of starting-word characters. However, intuitively, when a word is tokenized wrongly, it would consequentially affect the tokenization of following words.</p><p>Thus, measuring only the character-level metrics would overestimate the tokenization performance of word tokenizers. Therefore, in this work, we also consider these measures at the word level (WL). As shown in <ref type="figure">Figure 3</ref>, we compute these WL metrics the same way we did with SL metrics, substituting numbers of syllables with numbers of words.</p><p>Apart from these quantitative results, we also develop a website 3 that enable qualitative analysis on segmentation results. We refer to Appendix 7.1 for public links to our code that is part of this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Syllable Segmentation Performance</head><p>We evaluate the quality of syllable segmentation using measures discussed in Section 4.1. As <ref type="table" target="#tab_1">Table  1</ref> shows, CRF models outperform MaxEnt, even with less information. These results suggest that our hypothesis is true, i.e. that sequential information encoded in CRF models matters in syllable segmentation.</p><p>Our best model is a CRF model with character and trigram features (N = 3, W = 4 for both feature types), and the model is further used as part of the word segmentation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Segmentation Performance</head><p>We use measures discussed in Section 4.1 to evaluate word segmentation quality. <ref type="table" target="#tab_2">Table 2</ref> shows that the performance on BEST-2010 of our AttaCut-SC is comparable to DeepCut's performance, with only a few percentage points of WL F1 lower. Although our AttaCut-C achieves slightly lower than DeepCut, its performance serves as a strong baseline for our study: moreover, its performance is higher than the performance of Sertis. The difference between the performances of AttaCut-C and AttaCut-SC verifies our hypothesis that syllable knowledge is important for Thai word segmentation: in fact, with syllable knowledge, AttaCut-SC uses fewer parameters than AttaCut-C.</p><p>On cross domain evaluation, DeepCut, Sertis, and AttaCut-SC achieve similar performance on Wisesight-1000. Despite having the lowest word segmenation performance on BEST-2010 and Wisesight-1000, PyThaiNLP is the best word segmenter on TNHC. We refer to Section 5.4 for our explanation about this phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Speed Benchmark</head><p>Because word segmentation is one of the first tasks in NLP pipelines, speed is a key aspect that we have to consider when building a word segmenter. In practical settings, word segmentation is often deployed in distributed and scalable architectures or cloud services. This configuration allows one to operate NLP pipelines at scale while minimizing cost.</p><p>Considering these practicalities, we benchmark the speed of the Thai word segmentation models on two cloud instances: AWS's t2.small and t2.medium. We use Wisesight's training set as a benchmark dataset. The dataset contains around 24,063 documents, amounting to around 2.15M characters. Appendix 7.4 discusses our speed benchmark protocol.  <ref type="figure" target="#fig_3">Figure 4</ref> shows that DeepCut, the state of the art, has the longest execution time for segmenting Wisesight's training set. It takes around 846 and 522 seconds on AWS's t2.small and t2.medium respectively. Sertis is the second slowest, taking about 309 and 185 seconds on the two instances, factors of 2.7× and 2.8× faster than DeepCut, respectively. AttaCut-SC completes the segmentation under 100 seconds on both instances, factors of 9.1× and 5.6× faster than DeepCut. AttaCut-C performs the task under 60 seconds (14.3× and 9.9× faster than DeepCut). PyThaiNLP is the fastest Thai word segmenter, taking less than 10 seconds to achieve the task; however, it suffers from low segmentation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Why do AttaCut and DeepCut fail on TNHC?</head><p>Despite displaying state-of-the-art word segmentation results on BEST-2010, both AttaCut and Deep-Cut models perform poorly on TNHC, while PyThaiNLP performs the best. This poses a questions whether Thai can have a generic word segmenter.</p><p>As in other languages, Thai texts can be written in various styles, depending on their purpose. For example, classic literature was typically written with archaic words, or poems were constructed with additional linguistic structures that are quite different from normal texts. This is the case with the TNHC documents: they are classic literature written with poetic techniques.</p><p>Sentences in TNHC are composed with words (often short) that are harmonically matched but not semantically similar. Filler words without actual meaning are also used in these documents to create appealing rhythms when read aloud. Therefore, the distribution of words in these documents is completely divergent from our training data (BEST-2010), which contains normal Thai written texts. Because AttaCut and DeepCut are learning-based word segmenters, they segment words based on character correlations that they extract from the training data.   <ref type="figure" target="#fig_4">Figure 5</ref> shows three random samples from TNHC that PyThaiNLP's WL F1 is larger than AttaCut's one by 0.2: this threshold is set to account for prediction variability. Comparing to annotations, AttaCut-SC and DeepCut attempt to form words from short words, while PyThaiNLP does not. This is a notable case because these formations look reasonable if found in normal Thai texts. <ref type="table" target="#tab_3">Table 3</ref> shows that PyThaiNLP segments the majority of TNHC samples, whose lengths are fewer than or equal to 20 words, with relatively similar numbers of words, while AttaCut-SC segments them into one word fewer. Although the difference on average is considerably small, using macroaveraging accumulates the large difference in statistics, causing AttaCut-* and DeepCut's WL F1 significantly lower than PyThaiNLP's. Quantitative results from <ref type="table" target="#tab_3">Table 3</ref> support our reasoning that learning-based methods tend to group words together despite compositional irrelevances.</p><p>Although we aimed to build a universal word segmenter for Thai, the result of TNHC shows that this might be partly applicable and more experiments should be conducted. One possibility to mitigate the problem is to use transfer learning, training a model on a large corpus before retraining it on the dataset of interest. With that, we provide our implementation with sufficient documentation, such that practitioners can retrain AttaCut models on datasets at hand conveniently. We believe that transfer learning is a reasonable direction that will help alleviate this writing-style problem in Thai word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Thai word segmentation has posed its challenges since the digital era. Previous approaches can be categorised into two categories: dictionary-based and learning-based. Dictionary-based approaches rely on an exhaustive dictionary. Poowarawan <ref type="bibr" target="#b16">[17]</ref> proposes the first dictionary-based method using a greedy algorithm to decide when a word should be formed. Dictionary-based methods inevitably suffer from unseen words, and hence are harder to generalise to other domains. Sornlertlamvanich <ref type="bibr" target="#b20">[21]</ref> proposes an algorithm, called Maximal Matching, to handle such unseen word cases.</p><p>Meknavin et al. <ref type="bibr" target="#b9">[10]</ref> start formulating word tokenization for Thai as a learning problem. Using handcrafted features, two algorithms learn to solve ambiguous segmentation cases to aid a dictionarybased segmenter. Theeramunkong et al. <ref type="bibr" target="#b24">[25]</ref> present an idea of grouping characters--Thai Character Clusters (TCCs)--into a unit that is inseparable based on Thai writing rules, which help reduce chances of segmenting words incorrectly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Thai word segmentation is a challenging task in which speed is often exchanged for quality. We proposed an efficient CNN-based word segmenter for Thai that utilizes character and syllable embeddings. The segmenter is at least 5.6× faster than previous state-of-the-art segmenters, and it achieved comparable and, in some domains, better performance. In addition, our analysis shows that learning-based approaches suffer an out-of-domain problem with idiosyncratic datasets such as poetry. Future work could experiment with transfer learning to address this issue.</p><p>We hypothesise that these redundant convolution layers are DeepCut's computation bottleneck. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, we set up an experiment in which we manually disable each of these layers one by one and observe its influence on segmentation quality.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Convolution Layer Arrangement</head><p>Based the results described in Section 7.2, we design a special convolution layer arrangement that allows AttaCut models to extract a similar amount of character features as in DeepCut while minimizing overlaps between filters, using the dilation technique <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. In total, we use only three convolution layers, i.e. three different kernel widths and dilation numbers. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, the configuration contains:</p><p>• 1d convolution layer with kernel width 3 and dilation 1 4 ;</p><p>• 1d convolution layer with kernel width 5 and dilation 3;</p><p>• 1d convolution layer with kernel width 9 and dilation 2.</p><p>Our arrangement of convolution layers cover a context that span 8 characters left and right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">AttaCut-SC</head><p>The architecture of AttaCut-SC contains:</p><p>• character embedding layer with 32 dimensions; <ref type="bibr" target="#b3">4</ref> Dilation 1 means no gap in the kernel. • syllable embedding layer with 16 dimensions; • the convolution layers described in Section 7.3 with 64 filters;</p><p>• two fully-connected layers with 32 and 1 neurons respectively.</p><p>The configuration results in 158, 993 learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">AttaCut-C</head><p>AttaCut-C has a similar layer configuration to AttaCut-SC, except it does not have the syllable embedding layer. Overall, AttaCut-C's architecture includes:</p><p>• character embedding layer with 48 dimensions; • the convolution layers described in Section 7.3 with 196 filters;</p><p>• two fully-connected layers with 32 and 1 neurons respectively.</p><p>The configuration results in 173, 533 learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Speed Benchmark</head><p>We benchmark word segmenters on two cloud instances: t2.small and t2.medium. We perform speed benchmarks on those instances because they are standardized machines and typically used in industry for scalable systems. We use a special OS image provided by AWS, which is highly optimized for numerical computation, such as in neural networks. We develop code 5 that orchestrates benchmarking processes: instantiating a testing instance and installing necessary dependencies. Once the instance is ready, one can access it to run a speed benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of segmentation quality and inference time of existing Thai word segmenters. Inference time is measured on a laptop with Intel Core i7 2.2 GHz, 16 GB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>AttaCut-SC: CNN-based word segmenter with character and syllable features. Colors represent different embeddings. Word in figure is กาลเวลา (time). Appendix 7.3.1 provides the details of AttaCut's convolution filters and layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Word starting character ( 1 )Figure 3 :</head><label>13</label><figDesc>Segmentation evaluation metrics. Correct segmentation is ฝน|ตก|ที ่ |ทะเล (rain|fall|at|sea).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Speed comparison between Thai word segmenters on Wisesight's training set. The factors are relative to DeepCut's corresponding values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Short words in TNHC causing learning-base word segmenters to fail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>DeepCut Analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>AttaCut's convolution layer arrangement. Three convolutions layers with different kernel widths as well as dilation numbers. Word in figure is กาลเวลา (time).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Syllable segmentation quality (SL F1 ) on TNC and different methods.</figDesc><table><row><cell>Algorithm</cell><cell>Features</cell><cell>SL F1</cell><cell>CL F1</cell></row><row><cell>CRF</cell><cell cols="3">Chr (W=4), Trigram (W=4) 0.96±0.18 0.99±0.06</cell></row><row><cell>CRF</cell><cell cols="3">Chr (W=3), Trigram (W=3) 0.96±0.18 0.99±0.06</cell></row><row><cell>CRF</cell><cell cols="3">Chr (W=3), ChrSpan (W=3) 0.95±0.20 0.98±0.07</cell></row><row><cell>MaxEnt</cell><cell cols="3">Chr (W=4), Trigram (W=4) 0.94±0.22 0.98±0.08</cell></row><row><cell>MaxEnt</cell><cell>Trigram (W=4)</cell><cell cols="2">0.94±0.22 0.98±0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :0.81 ± 0.18 0.81 ± 0.200.81 ± 0.20</head><label>2</label><figDesc>Word segmentation quality (WL F1 ) on different datasets and methods. 0.80 ± 0.20</figDesc><table><row><cell>( †): Dictionary-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>PyThaiNLP segments the majority of TNHC samples with a similar number of words, while AttaCut-SC does with fewer.</figDesc><table><row><cell cols="2">TNHC Sample Statistics</cell><cell></cell><cell>No. Words</cell><cell></cell></row><row><cell cols="2">Character Length Percentage</cell><cell>Annotation</cell><cell>PyThaiNLP</cell><cell>AttaCut-SC</cell></row><row><cell>[0, 20]</cell><cell>85.50%</cell><cell>8.89 ± 3.78</cell><cell>8.59 ± 3.71</cell><cell>7.22 ± 3.52</cell></row><row><cell>(20, 100]</cell><cell>10.00%</cell><cell>42.38 ± 20.84</cell><cell>36.90 ± 20.01</cell><cell>36.24 ± 18.46</cell></row><row><cell>(100, 200]</cell><cell>1.90%</cell><cell>144.36 ± 30.35</cell><cell>129.12 ± 32.49</cell><cell>123.98 ± 29.13</cell></row><row><cell>(200, 1000]</cell><cell>2.00%</cell><cell>420.12 ± 188.15</cell><cell>381.94 ± 174.96</cell><cell>370.09 ± 169.89</cell></row><row><cell>(1000, 2937]</cell><cell>0.3%</cell><cell cols="3">1517.34 ± 453.75 1362.84 ± 439.72 1350.54 ± 413.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 shows</head><label>4</label><figDesc></figDesc><table /><note>that DeepCut still performs well when it does not have the convolutional layers with width 7, 9, and 10. Without these three layers, DeepCut becomes 24.60% smaller. Hence, the shrunken DeepCut is approximately 20% faster and can be improved by retraining. This result ver- ifies our hypothesis that some convolution layers of DeepCut are redundant and bring unnecessary computational cost to the model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Changes in WL F1 when removing certain layers from DeepCut. Removing the convolutional layers with width 7, 9, and 10 (Conv-7-9-10) affects WL F1 relatively less than other layers, but DeepCut becomes about 24% smaller.</figDesc><table><row><cell>Inactive Layer</cell><cell>WL F1</cell><cell cols="3">% ∆WL F1 Inactive neurons Size (smaller)</cell></row><row><cell cols="2">Original DeepCut 0.96±0.11</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv-1</cell><cell>0.89±0.21</cell><cell>-7.29%</cell><cell>7,605</cell><cell>1.42%</cell></row><row><cell>Conv-2</cell><cell>0.90±0.21</cell><cell>-6.25%</cell><cell>14,005</cell><cell>2.62%</cell></row><row><cell>Conv-3</cell><cell>0.88±0.21</cell><cell>-8.33%</cell><cell>20,405</cell><cell>3.81%</cell></row><row><cell>Conv-4</cell><cell>0.90±0.21</cell><cell>-6.25%</cell><cell>26,805</cell><cell>5.01%</cell></row><row><cell>Conv-5</cell><cell>0.90±0.21</cell><cell>-6.25%</cell><cell>33,205</cell><cell>6.21%</cell></row><row><cell>Conv-6</cell><cell>0.90±0.21</cell><cell>-6.25%</cell><cell>39,605</cell><cell>7.40%</cell></row><row><cell>Conv-7</cell><cell>0.92±0.21</cell><cell>-4.17%</cell><cell>46,005</cell><cell>8.60%</cell></row><row><cell>Conv-8</cell><cell>0.91±0.21</cell><cell>-5.21%</cell><cell>52,405</cell><cell>9.79%</cell></row><row><cell>Conv-9</cell><cell>0.91±0.21</cell><cell>-5.21%</cell><cell>44,105</cell><cell>8.24%</cell></row><row><cell>Conv-10</cell><cell>0.91±0.21</cell><cell>-5.21%</cell><cell>48,905</cell><cell>9.14%</cell></row><row><cell>Conv-11</cell><cell>0.90±0.21</cell><cell>-6.25%</cell><cell>53,705</cell><cell>10.04%</cell></row><row><cell>Conv-12</cell><cell>0.91±0.20</cell><cell>-5.21%</cell><cell>38,500</cell><cell>7.20%</cell></row><row><cell>Conv-7-9-10</cell><cell>0.90±0.21</cell><cell>-6.25%</cell><cell>13,1615</cell><cell>24.60%</cell></row><row><cell cols="2">7.3 AttaCut's Architectures</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">AWS's t2.medium</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pythainlp.github.io/tokenization-benchmark-visualization/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/heytitle/tokenization-speed-benchmark</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Collocation and Thai Word Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aroonmanakun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="68" to="75" />
			<pubPlace>Prachuapkhirikhan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thoughts on word and sentence segmentation in Thai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aroonmanakun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Symposium on Natural language Processing</title>
		<meeting>the Seventh Symposium on Natural language Processing<address><addrLine>Pattaya, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Thai National Corpus: A Progress Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aroonmanakun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tansiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nittayanuparp</surname></persName>
		</author>
		<idno>978-1-932432-56-5</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=1690299.1690321" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Asian Language Resources, ALR7</title>
		<meeting>the 7th Workshop on Asian Language Resources, ALR7</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thai Word Segmentation with Hidden Markov Model and Decision Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bheganan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-642-01307-2_10</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-642-01307-2_10" />
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>T. Theeramunkong, B. Kijsirikul, N. Cercone, and T.-B. Ho</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5476</biblScope>
			<biblScope unit="page" from="74" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepCut: A Thai word tokenization library using Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kittinaradorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Achakulvisut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaovavanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srithaworn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaewkasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruangrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oparad</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3457707</idno>
		<ptr target="http://doi.org/10.5281/zenodo.3457707" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-Candidate Word Segmentation using Bi-directional LSTM Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lapjaturapit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Viriyayudhakom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theeramunkong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Embedded Systems and Intelligent Technology &amp; International Conference on Information and Communication Technology for Embedded Systems (ICESIT-ICICTES)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic syllabification in english: A comparison of different algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Adsett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Damper</surname></persName>
		</author>
		<idno type="DOI">10.1177/0023830908099881</idno>
		<idno type="PMID">19334414</idno>
		<ptr target="https://doi.org/10.1177/0023830908099881" />
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature-based Thai word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meknavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charoenpornsawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kijsirikul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing Pacific Rim Symposium</title>
		<meeting>Natural Language Processing Pacific Rim Symposium</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<ptr target="https://www.nectec.or.th/corpus/index.php" />
		<title level="m">NECTEC. BEST: Benchmark for Enhancing the Standard of Thai Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Crfsuite: a fast implementation of conditional random fields (crfs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<ptr target="http://www.chokkan.org/software/crfsuite/" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Version 1.1.0</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Version 0.20.2</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Python-CRFSuite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korobov</surname></persName>
		</author>
		<ptr target="https://github.com/scrapinghub/python-crfsuite.Version0.9.6" />
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PyThaiNLP: a Python NLP package for Thai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Phatthiyaphaibun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaovavanich</surname></persName>
		</author>
		<ptr target="https://github.com/PyThaiNLP/pythainlp.Version2.0.6" />
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dictionary-based Thai Syllable Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poowarawan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">TNHC: Thai National Historical Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatphol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rutherford</surname></persName>
		</author>
		<ptr target="https://attapol.github.io/tlc.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Thai word segmentation with bi-directional RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertis</forename><surname>Co</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ltd</surname></persName>
		</author>
		<ptr target="https://github.com/sertiscorp/thai-word-segmentation.Commit" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="696" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word segmentation for Thai in machine translation system. Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sornlertlamvanich</surname></persName>
		</author>
		<ptr target="https://ci.nii.ac.jp/naid/10003787432/en/" />
	</analytic>
	<monogr>
		<title level="j">NECTEC</title>
		<imprint>
			<biblScope unit="page" from="556" to="561" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ORCHID: Thai part-of-speech tagged corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sornlertlamvanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Charoenporn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Electronics and Computer Technology Center Technical Report</title>
		<imprint>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1283/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-dictionary-based Thai word segmentation using decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theeramunkong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Usanavasin</surname></persName>
		</author>
		<idno type="DOI">10.3115/1072133.1072209</idno>
		<ptr target="http://portal.acm.org/citation.cfm?doid=1072133.1072209" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Human language technology research -HLT &apos;01</title>
		<meeting>the first international conference on Human language technology research -HLT &apos;01<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character cluster based Thai information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theeramunkong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sornlertlamvanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanhermhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chinnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth international workshop on on Information retrieval with Asian languages</title>
		<meeting>the fifth international workshop on on Information retrieval with Asian languages</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Wisesight Sentiment Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wisesight (thailand) Co</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ltd</surname></persName>
		</author>
		<ptr target="https://github.com/PyThaiNLP/wisesight-sentiment/releases/tag/v1.0" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.07122" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reproducibility Over the course of our project, we have encountered several situations that we wanted to test methods proposed in papers but there was no implementation available, nor concrete details about experiments and datasets. Going through these experiences, we believe that the Thai NLP research community should strive for better reproducibility, paving a common foundation for future advancements. Therefore, we publish every piece of our code and results online, and we strongly encourage other researchers to verify our implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">• Word</forename><surname>Segmenter</surname></persName>
		</author>
		<ptr target="https://github.com/PyThaiNLP/attacut" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<ptr target="https://pythainlp.github.io/tokenization-benchmark-visualization/" />
		<title level="m">• Tokenization Visualization Website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Attacut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Sc</surname></persName>
		</author>
		<ptr target="https://www.floydhub.com/pattt/projects/attacut/50" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Attacut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-C</forename></persName>
		</author>
		<ptr target="https://www.floydhub.com/pattt/projects/attacut/42" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>• Thai Word</surname></persName>
		</author>
		<ptr target="https://github.com/PyThaiNLP/docker-thai-tokenizers" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Why is DeepCut slow?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Despite its high performance on segmentation, using it comes at high computation cost due to a large number of parameters in the model</title>
	</analytic>
	<monogr>
		<title level="m">particular, DeepCut&apos;s structure involves: • character and character-type embedding layers</title>
		<imprint/>
	</monogr>
	<note>DeepCut is the state-of-the-art word segmenter for Thai</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<title level="m">• 1d convolutional layers with kernel widths ranging from 1 to 12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">• flattening and concatenation layers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Because the kernel widths are varied linearly, the filters of these layers overlap each other; hence, they possibly extract redundant features from the embeddings</title>
		<imprint/>
	</monogr>
	<note>The majority of DeepCut&apos;s parameters concentrates in the convolutional layers: 415,760 from total 535,025 parameters</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
