<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege&apos;s Principle</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
							<email>trisha@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Guhan</surname></persName>
							<email>pguhan@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
							<email>uttaranb@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege&apos;s Principle</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present EmotiCon, a learning-based algorithm for context-aware perceived human emotion recognition from videos and images. Motivated by Frege's Context Principle from psychology, our approach combines three interpretations of context for emotion recognition. Our first interpretation is based on using multiple modalities (e.g. faces and gaits) for emotion recognition. For the second interpretation, we gather semantic context from the input image and use a self-attention-based CNN to encode this information. Finally, we use depth maps to model the third interpretation related to socio-dynamic interactions and proximity among agents. We demonstrate the efficiency of our network through experiments on EMOTIC, a benchmark dataset. We report an Average Precision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8 over prior methods. We also introduce a new dataset, GroupWalk, which is a collection of videos captured in multiple real-world settings of people walking. We report an AP of 65.83 across 4 categories on GroupWalk, which is also an improvement over prior methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Perceiving the emotions of people around us is vital in everyday life. Humans often alter their behavior while interacting with others based on their perceived emotions. In particular, automatic emotion recognition has been used for different applications, including human-computer interaction <ref type="bibr" target="#b12">[13]</ref>, surveillance <ref type="bibr" target="#b11">[12]</ref>, robotics, games, entertainment, and more. Emotions are modeled as either discrete categories or as points in a continuous space of affective dimensions <ref type="bibr" target="#b15">[16]</ref>. In the continuous space, emotions are treated as points in a 3D space of valence, arousal, and dominance. In this work, our focus is on recognizing perceived human emotion rather than the actual emotional state of a person in the discrete emotion space.</p><p>Initial works in emotion recognition have been mostly unimodal <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b43">44]</ref> approaches. The unique modality We use three interpretations of context to perform perceived emotion recognition. We use multiple modalities (Context 1) of faces and gaits, background visual information (Context 2) and sociodynamic inter-agent interactions (Context 3) to infer the perceived emotion. EmotiCon outperforms prior context-aware emotion recognition methods. Above is an input sample from the EMOTIC dataset. may correspond to facial expressions, voice, text, body posture, gaits, or physiological signals. This was followed by multimodal emotion recognition <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref>, where various combinations of modalities were used and combined in various manners to infer emotions.</p><p>Although such modalities or cues extracted from a person can provide us with information regarding the perceived emotion, context also plays a very crucial role in the understanding of the perceived emotion. Frege's context principle <ref type="bibr" target="#b44">[45]</ref> urges not asking for the meaning of a word in isolation and instead of finding the meaning in the context of a sentence. We use this notion behind the context principle in psychology for emotion recognition. 'Context' has been interpreted in multiple ways by researchers in psychology, including:</p><p>(a) Context 1 (Multiple Modalities): Incorporating cues from different modalities was one of the initial definitions of context. This domain is also known as Multimodal Emotion Recognition. Combining modalities provides complementary information, which leads to better inference and also performs better on in-the-wild datasets. (b) Context 2 (Background Context): Semantic understanding of the scene from visual cues in the image helps in getting insights about the agent's surroundings and activity, both of which can affect the perceived emotional state of the agent. (c) Context 3 (Socio-Dynamic Inter-Agent Interactions):</p><p>Researchers in psychology suggest that the presence or absence of other agents affects the perceived emotional state of an agent. When other agents share an identity or are known to the agent, they often coordinate their behaviors. This varies when other agents are strangers. Such interactions and proximity to other agents have been less explored for perceived emotion recognition.</p><p>One of our goals is to make Emotion Recognition systems work for real-life scenarios. This implies using modalities that do not require sophisticated equipment to be captured and are readily available. Psychology researchers <ref type="bibr" target="#b2">[3]</ref> have conducted experiments by mixing faces and body features corresponding to different emotions and found that participants guessed the emotions that matched the body features. This is also because of the ease of "mocking" one's facial expressions. Subsequently, researchers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref> found the combination of faces and body features to be a reliable measure of inferring human emotion. As a result, it would be useful to combine such face and body features for contextbased emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Contributions:</head><p>We propose EmotiCon, a contextaware emotion recognition model. The input to Emoti-Con is images/video frames, and the output is a multi-label emotion classification. The novel components of our work include:</p><p>1. We present a context-aware multimodal emotion recognition algorithm called EmotiCon. Consistent with Ferge's Context principle, in this work, we try to incorporate three interpretations of context to perform emotion recognition from videos and images. 2. We also present a new approach to modeling the sociodynamic interactions between agents using a depthbased CNN. We compute a depth map of the image and feed that to the network to learn about the proximity of agents to each other. 3. Though extendable to any number of modalities, we release a new dataset GroupWalk for emotion recognition. To the best of our knowledge, there exist very few datasets captured in uncontrolled settings with both faces and gaits that have emotion label annotations. To enable research in this domain, we make GroupWalk publicly available with emotion annotations. GroupWalk is a collection of 45 videos captured in multiple real-world settings of people walking in dense crowd settings. The videos have about 3544 agents annotated with their emotion labels.</p><p>We compare our work with prior methods by testing our performance on EMOTIC <ref type="bibr" target="#b27">[28]</ref>, a benchmark dataset for context-aware emotion recognition. We report an improved AP score of 35.48 on EMOTIC, which is an improvement of 7 − 8 over prior methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">58]</ref>. We also report AP scores of our approach and prior methods on the new dataset, GroupWalk. We perform ablation experiments on both datasets, to justify the need for the three components of EmotiCon. As per the annotations provided in EMOTIC, we perform a multi-label classification over 26 discrete emotion labels. On GroupWalk too, we perform a multi-label classification over 4 discrete emotions (anger, happy, neutral, sad).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we give a brief overview of previous works on unimodal and multimodal emotion recognition, context-aware emotion recognition, and existing contextaware datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Uni/Multimodal Emotion Recognition</head><p>Prior works in emotion recognition from handcrafted features <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b59">60]</ref> or deep learning networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref> have used single modalities like facial expressions <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b0">1]</ref>, voice, and speech expressions <ref type="bibr" target="#b46">[47]</ref>, body gestures <ref type="bibr" target="#b41">[42]</ref>, gaits <ref type="bibr" target="#b43">[44]</ref>, and physiological signals such as respiratory and heart cues <ref type="bibr" target="#b25">[26]</ref>. There has been a shift in the paradigm, where researchers have tried to fuse multiple modalities to perform emotion recognition, also known as Multimodal Emotion Recognition. Fusion methods like early fusion <ref type="bibr" target="#b48">[49]</ref>, late fusion <ref type="bibr" target="#b20">[21]</ref>, and hybrid fusion <ref type="bibr" target="#b49">[50]</ref> have been explored for emotion recognition from multiple modalities. Multimodal emotion recognition has been motivated by research in psychology and also helped in improving accuracy on in-thewild emotion recognition datasets like IEMOCAP <ref type="bibr" target="#b8">[9]</ref> and CMU-MOSEI [57].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Context-Aware Emotion Recognition in Psychology Research</head><p>Though introduced in the domain of philosophy of language, Frege <ref type="bibr" target="#b44">[45]</ref> proposed that words should never be seen in isolation but in the context of their proposition. Researchers in psychology <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> also agree that just like most psychological processes, emotional processes cannot be interpreted without context. They suggest that context often produces emotion and also shapes how emotion is perceived. Emotion literature that addresses context <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref> suggests several broad categories of contextual features: person, situation, and context. Martinez et al. <ref type="bibr" target="#b35">[36]</ref> conduct experiments about the necessity of context and found that even when the participants' faces and bodies were masked in silent videos, viewers were able to infer the affect successfully. Greenway et al. <ref type="bibr" target="#b19">[20]</ref> organize these contextual features in three levels, ranging from microlevel (person) to macro-level (cultural). In level 2 (situational), they include factors like the presence and closeness <ref type="figure">Figure 2</ref>: EmotiCon: We use three interpretations of context. We first extract features for the two modalities to obtain f1 and f2 and inputs Imask and Idepth from the raw input image, I. These are then passed through the respective neural networks to obtain h1, h2 and h3.</p><p>To obtain h1, we use a multiplicative fusion layer (red color) to fuse inputs from both modalities, faces, and gaits. h1, h2 and h3 are then concatenated to obtain hconcat.</p><p>of other agents. Research shows that the simple presence of another person elicits more expression of emotion than situations where people are alone <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b23">24]</ref>. These expressions are more amplified when people know each other and are not strangers <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Context-Aware Emotion Recognition</head><p>Recent works in context-aware emotion recognition are based on deep-learning network architectures. Kosti et al. <ref type="bibr" target="#b26">[27]</ref> and Lee et al. <ref type="bibr" target="#b29">[30]</ref> present two recent advances in context-aware emotion recognition and they propose similar architectures. Both of them have two-stream architectures followed by a fusion network. One stream focuses on a modality (face for <ref type="bibr" target="#b29">[30]</ref> and body for <ref type="bibr" target="#b26">[27]</ref>) and the other focuses on capturing context. Lee et al. <ref type="bibr" target="#b29">[30]</ref> consider everything other than the face as context, and hence mask the face from the image to feed to the context stream. On the other hand, <ref type="bibr" target="#b29">[30]</ref> uses a Region Proposal Network (RPN) to extract context elements from the image. These elements become the nodes of an affective graph, which is fed into a Graph Convolution Network (GCN) to encode context. Another problem that has been looked into is group emotion recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53]</ref>. The objective here is to label the emotion of the entire set of people in the frame under the assumption that they all share some social identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Context-Aware Emotion Recognition Datasets</head><p>Most of the emotion recognition datasets in the past have either only focused on a single modality, e.g., faces or body features, or have been collected in controlled set-tings. For example, the GENKI database <ref type="bibr" target="#b51">[52]</ref> and the UCD-SEE dataset <ref type="bibr" target="#b50">[51]</ref> are datasets that focus primarily on the facial expressions collected in lab settings. The Emotion Recognition in the Wild (EmotiW) challenges <ref type="bibr" target="#b13">[14]</ref> host three databases: AFEW dataset <ref type="bibr" target="#b14">[15]</ref> (collected from TV shows and movies), SFEW (a subset of AFEW with only face frames annotated), and HAPPEI database, which focuses on the problem of group-level emotion estimation. Some of the recent works have realized the potential of using context for emotion recognition and highlighted the lack of such datasets. Context-Aware Emotion Recognition (CAER) dataset <ref type="bibr" target="#b57">[58]</ref> is a collection of video-clips from TV shows with 7 discrete emotion annotations. EMOTIC dataset <ref type="bibr" target="#b26">[27]</ref> is a collection of images from datasets like MSCOCO <ref type="bibr" target="#b33">[34]</ref> and ADE20K <ref type="bibr" target="#b60">[61]</ref> along with images downloaded from web searches. The dataset is a collection of 23, 571 images, with about 34, 320 people annotated for 26 discrete emotion classes. We have summarised and compared all these datasets in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach: EmotiCon</head><p>In this section, we give an overview of the approach in Section 3.1 and motivate the three context interpretations in Section 3.2, 3.3, and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation and Overview</head><p>We present an overview of our context-aware multimodal emotion recognition model, EmotiCon, in <ref type="figure">Figure 2</ref>. Our input consists of an RGB image, I. We process I to  <ref type="table">Table 1</ref>: Context-Aware Emotion Recognition Dataset Analysis: We compare GroupWalk with existing emotion recognition datasets such as EMOTIC <ref type="bibr" target="#b26">[27]</ref>, AffectNet <ref type="bibr" target="#b40">[41]</ref>, CAER and CAER-S <ref type="bibr" target="#b29">[30]</ref>, and AFEW <ref type="bibr" target="#b14">[15]</ref>.</p><p>generate the input data for each network corresponding to the three contexts. The network for Context 1 consists of n streams corresponding to n distinct modalities denoted as m 1 , m 2 , . . . , m n . Each distinct layer outputs a feature vector, f i . The n feature vectors f 1 , f 2 , . . . , f n are combined via multiplicative fusion <ref type="bibr" target="#b39">[40]</ref> to obtain a feature encoding,</p><formula xml:id="formula_0">h 1 = g(f 1 , f 2 , . . . , f n ), where g(·)</formula><p>corresponds to the multiplicative fusion function. Similarly, h 2 , and h 3 are computed through the networks corresponding to the second and third Contexts. h 1 , h 2 , and h 3 are concatenated to perform multi-label emotion classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context 1: Multiple Modalities</head><p>In real life, people appear in a multi-sensory context that includes a voice, a body, and a face; these aspects are also perceived as a whole. Combining more than one modality to infer emotion is beneficial because cues from different modalities can complement each other. They also seem to perform better on in-the-wild datasets <ref type="bibr" target="#b39">[40]</ref> than other unimodal approaches. Our approach is extendable to any number of modalities available. To validate this claim, other than EMOTIC and GroupWalk, which have two modalities, faces, and gaits, we also show results on the IEMOCAP dataset which face, text, and speech as three modalities. From the input image I, we obtain m 1 , m 2 , . . . , m n using processing steps as explained in Section 4.1. These inputs are then passed through their respective neural network architectures to obtain f 1 , f 2 , . . . , f n . To make our algorithm robust to sensor noise and averse to noisy signals, we combine these features multiplicatively to obtain h 1 . As shown in previous research <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>, multiplicative fusion learns to emphasize reliable modalities and to rely less on other modalities. To train this, we use the modified loss function proposed previously <ref type="bibr" target="#b39">[40]</ref> defined as:</p><formula xml:id="formula_1">L multiplicative = − n i=1 (p e i ) β n−1 log p e i<label>(1)</label></formula><p>where n is the total number of modalities being considered, and p e i is the prediction for emotion class, e, given by the network for the i th modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Context 2: Situational/Background Context</head><p>Our goal is to identify semantic context from images and videos to perform perceived emotion recognition. Semantic context includes the understanding of objects -excluding the primary agent-present in the scene, their spatial extents, keywords, and the activity being performed. For instance, in <ref type="figure" target="#fig_0">Figure 1</ref>, the input image consists of a group of people gathered around with drinks on a bright sunny day. The "bright sunny day", "drink glasses", "hats" and "green meadows" constitute semantic components and may affect judgement of one's perceived emotion.</p><p>Motivated by multiple approaches in the computer vision literature <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b17">18]</ref> surrounding semantic scene understanding, we use an attention mechanism to train a model to focus on different aspects of an image while masking the primary agent, to extract the semantic components of the scene. The mask, I mask ∈ R 224×224 , for an input image I is given as</p><formula xml:id="formula_2">I mask = I(i, j) if I(i, j) ∈ bbox agent , 0 otherwise.<label>(2)</label></formula><p>where bbox agent denotes the bounding box of the agent in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Context 3: Inter-Agent Interactions/Socio-Dynamic Context</head><p>When an agent is surrounded by other agents, their perceived emotions change. When other agents share an identity or are known to the agent, they often coordinate their behaviors. This varies when other agents are strangers. Such interactions and proximity can help us infer the emotion of agents better.</p><p>Prior experimental research has used walking speed, distance, and proximity features to model socio-dynamic interactions between agents to interpret their personality traits. Some of these algorithms, like the social force model <ref type="bibr" target="#b22">[23]</ref>, are based on the assumption that pedestrians are subject to attractive or repulsive forces that drive their dynamics. Nonlinear models like RVO <ref type="bibr" target="#b55">[56]</ref> aim to model collision avoidance among individuals while walking to their individual goals. But, both of these methods do not capture cohesiveness in a group.</p><p>We propose an approach to model these socio-dynamic interactions by computing proximity features using depth maps. The depth map, I depth ∈ R 224×224 , corresponding to input image, I, is represented through a 2D matrix where,</p><formula xml:id="formula_3">I depth (i, j) = d(I(i, j), c)<label>(3)</label></formula><p>d(I(i, j), c) represents the distance of the pixel at the i th row and j th column from the camera center, c. We pass I depth as input depth maps through a CNN and obtain h 3 .</p><p>In addition to depth map-based representation, we also use Graph Convolutional Networks (GCNs) to model the proximity-based socio-dynamic interactions between agents. GCNs have been used to model similar interactions in traffic networks <ref type="bibr" target="#b21">[22]</ref> and activity recognition <ref type="bibr" target="#b54">[55]</ref>. The input to a GCN network consists of the spatial coordinates of all agents, denoted by X ∈ R n×2 , where n represents the number of agents in the image, as well as the unweighted adjacency matrix, A ∈ R n×n , of the agents, which is defined as follows,</p><formula xml:id="formula_4">A(i, j) = e −d(vi,vj ) if d(v i , v j ) &lt; µ, 0 otherwise.<label>(4)</label></formula><p>The function f = e −d(vi,vj ) <ref type="bibr" target="#b6">[7]</ref> denotes the interactions between any two agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network Architecture and Implementation Details</head><p>In this section, we elaborate on the implementation and network architectures of EmotiCon. The data preprocessing for the streams of EmotiCon are presented in 4.1.</p><p>We include details about the network architectures of context 1, context 2, and context 3 in Section 4.2. We explain the early fusion technique we use to fuse the features from the three context streams to infer emotion and the loss function used for training the multi-label classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Processing</head><p>Context1: We use OpenFace <ref type="bibr" target="#b3">[4]</ref> to extract a 144dimensional face modality vector, m 1 ∈ R 144 obtained through multiple facial landmarks. We compute the 2D gait modality vectors, m 2 ∈ R 25×2 using OpenPose <ref type="bibr" target="#b9">[10]</ref> to extract 25-coordinates from the input image I. For each coordinate, we record the x and y pixel values.</p><p>Context2: We use RobustTP <ref type="bibr" target="#b10">[11]</ref>, which is a pedestrian tracking method to compute the bounding boxes for all agents in the scene. These bounding boxes are used to compute I mask according to Equation 2.</p><p>Context3: We use Megadepth <ref type="bibr" target="#b32">[33]</ref> to extract the depth maps from the input image I. The depth map, I depth , is computed using Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>Context1: Given a face vector, m 1 , we use three 1D convolutions (depicted in light green color in <ref type="figure">Figure 2</ref>) with batch normalization and ReLU non-linearity. This is followed by a max pool operation and three fully-connected layers (cyan color in <ref type="figure">Figure 2</ref>) with batch normalization and ReLU. For m 2 , we use the ST-GCN architecture proposed by <ref type="bibr" target="#b7">[8]</ref>, which is currently the SOTA network for emotion classification using gaits. Their method was originally designed to deal with 2D pose information for 16 body joints. We modify their setup for 2D pose inputs for 25 joints. We show the different layers and hyper-parameters used in <ref type="figure">Figure 2</ref>. The two networks give us f 1 and f 2 , which are then multiplicatively fused (depicted in red color in <ref type="figure">Figure 2</ref>) to generate h 1 .</p><p>Context 2: For learning the semantic context of the input image I, we use the Attention Branch Network (ABN) <ref type="bibr" target="#b17">[18]</ref> on the masked image I mask . ABN contains an attention branch which focuses on attention maps to recognize and localize important regions in an image. It outputs these potentially important locations in the form of h 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context 3:</head><p>We perform two experiments using both depth map and a GCN. For depth-based network, we compute the depth map, I depth and pass it through a CNN. The CNN is composed of 5 alternating 2D convolutional layers (depicted in dark green color in <ref type="figure">Figure 2</ref>) and max pooling layers (magenta color in <ref type="figure">Figure 2</ref>). This is followed by two fully connected layers of dimensions 1000 and 26 (cyan color in <ref type="figure">Figure 2</ref>).</p><p>For the graph-based network, we use two graph convolutional layers followed by two linear layers of dimension 100 and 26.</p><p>Fusing Context Interpretations: To fuse the feature vectors from the three context interpretations, we use an early fusion technique. We concatenate the feature vectors before making any individual emotion inferences.</p><formula xml:id="formula_5">h concat = [h 1 , h 2 , h 3 ]</formula><p>We use two fully connected layers of dimensions 52 and 26, followed by a softmax layer. This output is used for computing the loss and the error, and then back-propagating the error back to the network.</p><p>Loss Function: Our classification problem is a multi-label classification problem where we assign one or more than one emotion label to an input image or video. To train this network, we use the multi-label soft margin loss function and denote it by L classification . The loss function optimizes a multi-label one-versus-all loss based on max-entropy between the input x and output y.</p><p>So, we combine the two loss functions, L multiplicative (from Eq. 1) and L classification to train EmotiCon.</p><formula xml:id="formula_6">L total = λ 1 L multiplicative + λ 2 L classification<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets</head><p>In Section 5.1, we give details about the benchmark dataset for context-aware emotion recognition, EMOTIC. We present details about the new dataset, GroupWalk and also perform a comparison with other existing datasets in Section 5.2. Like summarised in <ref type="table">Table 1</ref>, there are a lot more datasets for emotion recognition, but they do not have any context available. Though our approach will work on these datasets, we do not expect any significant improvement over the SOTA on these datasets. Just to reinforce this, we did run our method on IEMOCAP <ref type="bibr" target="#b8">[9]</ref>, which has limited context information, and summarise our results in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">EMOTIC Dataset</head><p>The EMOTIC dataset contains 23,571 images of 34,320 annotated people in unconstrained environments. The annotations consist of the apparent emotional states of the people in the images. Each person is annotated for 26 discrete categories, with multiple labels assigned to each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">GroupWalk Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Annotation</head><p>GroupWalk consists of 45 videos that were captured using stationary cameras in 8 real-world setting including a hospital entrance, an institutional building, a bus stop, a train station, and a marketplace, a tourist attraction, a shopping place and more. The annotators annotated agents with clearly visible faces and gaits across all videos. 10 annotators annotated a total of 3544 agents. The annotations consist of the following emotion labels-Angry, Happy, Neutral, and Sad. Efforts to build on this dataset are still ongoing. The dataset collected and annotated so far can be found at the Project webpage. To prepare train and test splits for the dataset, we randomly selected 36 videos for the training and 9 videos for testing.</p><p>While perceived emotions are essential, other affects such as dominance and friendliness are important for carrying out joint and/or group tasks. Thus, we additionally label each agent for dominance and friendliness. More details about the annotation process, labelers and labels processing are presented in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head><p>In this section, we discuss the experiments conducted for EmotiCon. We present details on hyperparameters and training details in Section 6.1. In section 6.2, we list the prior methods we compare the performance of Emoti-Con with. We present an elaborate analysis of both qualitative and quantitative results in Section 6.3. In Section 6.5, we perform experiments to validate the importance of each component of EmotiCon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Training Details</head><p>For training EmotiCon on the EMOTIC dataset, we use the standard train, val, and test split ratios provided in the dataset. For GroupWalk, we split the dataset into 85% training (85%) and testing (15%) sets. In GroupWalk each sample point is an agent ID; hence the input is all the frames for the agent in the video. To extend EmotiCon on videos, we perform a forward pass for all the frames and take an average of the prediction vector across all the frames and then compute the AP scores and use this for loss calculation and backpropagating the loss. We use a batch size of 32 for EMOTIC and a batchsize of 1 for GroupWalk. We train EmotiCon for 75 epochs. We use the Adam optimizer with a learning rate of 0.0001. All our results were generated on NVIDIA GeForce GTX 1080 Ti GPU. All the code was implemented using PyTorch <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Metrics and Methods</head><p>We use the standard metric Average Precision (AP) to evaluate all our methods. For both EMOTIC and Group-Walk datasets, we compare our methods with the following SOTA methods.</p><p>1. Kosti et al. <ref type="bibr" target="#b26">[27]</ref> propose a two-stream network followed by a fusion network. The first stream encodes context and then feeds the entire image as an input to the CNN. The second stream is a CNN for extracting body features. The fusion network combines features of the two CNNs and estimates the discrete emotion categories. 2. Zhang et al. <ref type="bibr" target="#b57">[58]</ref> build an affective graph with nodes as the context elements extracted from the image. To detect the context elements, they use a Region Proposal Network (RPN). This graph is fed into a Graph Convolutional Network (GCN). Another parallel branch in the network encodes the body features using a CNN. The outputs from both the branches are concatenated to infer an emotion label. 3. Lee et al. <ref type="bibr" target="#b29">[30]</ref> present a network architecture, CAER-Net consisting of two subnetworks, a two-stream encoding network, and an adaptive fusion network. The two-stream encoding network consists of a face stream and a context-stream where facial expression and context (background) are encoded. An adaptive fusion network is used to fuse the two streams.</p><p>We use the publicly available implementation for Kosti et al. <ref type="bibr" target="#b26">[27]</ref> and train the entire model on GroupWalk. Both Zhang et al. <ref type="bibr" target="#b57">[58]</ref> and Lee et al. <ref type="bibr" target="#b29">[30]</ref> do not have publicly available implementations. We reproduce the method by Lee et al. <ref type="bibr" target="#b29">[30]</ref> to the best of our understanding. For Zhang et al. <ref type="bibr" target="#b57">[58]</ref>, while we report their performance on the EMOTIC dataset, with limited implementation details, it was difficult to build their model to test their performance on Group-Walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Analysis and Discussion</head><p>Comparison with SOTA: We summarize the evaluation of the APs for all the methods on the EMOTIC and Group-Walk datasets in <ref type="table">Table 2</ref>. For EmotiCon, we report the AP scores for both GCN-based and Depth Map-based implementations of Context 3. On both the EMOTIC and Group-Walk datasets, EmotiCon outperforms the SOTA. Generalize to more Modalities: A major factor for the success of EmotiCon is its ability to combine different   <ref type="table">Table 2</ref>: Emotion Classification Performance: We report the AP scores on the EMOTIC and the GroupWalk datasets. Emoti-Con outperforms all the three methods for most of the classes and also overall.</p><p>modalities effectively via multiplicative fusion. Our approach learns to assign higher weights to more expressive modalities while suppressing weaker ones. For example, in instances where the face may not be visible, Emoti-Coninfers the emotion from context (See <ref type="figure" target="#fig_1">Figure 3</ref>, middle row(right)). This is in contrast to Lee et al. <ref type="bibr" target="#b29">[30]</ref>, which relies on the availability of face data. Consequently, they perform poorly on both the EMOTIC and GroupWalk datasets, as both datasets contain many examples where the face is not visible clearly. To further demonstrate the ability of EmotiCon to generalize to any modality, we additionally report our performance on the IEMOCAP dataset <ref type="bibr" target="#b8">[9]</ref> in Appendix B. GCN versus Depth Maps: GCN-based methods do not perform as well as depth-based but are a close second. This may be due to the fact that, on average most images of the EMOTIC dataset contain 5 agents. GCN-based methods in the literature have been trained on datasets with a lot more number of agents in each image or video. Moreover, with a depth-based approach, EmotiCon leans a 3D aspect of the scene in general and is not limited to inter-agent interactions. Failure Cases: We show two examples from EMOTIC dataset in <ref type="figure" target="#fig_2">Figure 4</ref> where EmotiCon fails to classify correctly. We also show the ground-truth and predicted emotion labels. In the first image, EmotiConis unable to gather any context information. On the other hand, in the second image, there is a lot of context information like the many visual elements in the image and multiple agents. This leads to an incorrect inference of the perceived emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Qualitative Results</head><p>We show qualitative results for three examples, each from both the datasets, respectively, in <ref type="figure" target="#fig_1">Figure 3</ref>. The first column is the input image marking the primary agents, the second column shows the corresponding extracted face and gait, the third column shows the attention maps learned by (a) Ablation Experiments on EMOTIC Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labels</head><p>Context  examples, the semantic context of the coffin and the child's kite is clearly identified to convey sadness and pleasure, respectively. The depth maps corresponding to the input images capture the idea of proximity and interagent interactions. In the top row example (left) and middle row example (right), the depth map clearly marks the tennis player about to swing to convey anticipation, and the woman coming from the hospital to convey sadness, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Ablation Experiments</head><p>To motivate the importance of Context 2 and Context 3, we run EmotiCon on both EMOTIC and GroupWalk dataset removing the networks corresponding to both contexts, followed by removing either of them one by one. The results of the ablation experiments have been summarized in <ref type="table" target="#tab_3">Table 3</ref>. We choose to retain Context 1 in all these runs because it is only Context 1 that is capturing information from the agent itself.</p><p>We observe from the qualitative results in <ref type="figure" target="#fig_1">Figure 3</ref> that Context 2 seems more expressive in the images of EMOTIC dataset, while Context 3 is more representative in Group-Walk. This is supported by the results reported in <ref type="table" target="#tab_3">Table 3</ref>, columns 2 and 3. To understand why this happens, we analyse the two datasets closely. EMOTIC dataset was collected for the task of emotion recognition with context. it is a dataset of pictures collected from multiple datasets and scraped from the Internet. As a result, most of these images have a rich background context. Moreover we also found that more than half the images of EMOTIC contain at most 3 people. These are the reasons we believe that interpretation 2 helps more in EMOTIC than interpretation 3. In the GroupWalk Dataset, the opposite is true. The number of people per frame is much higher. This density gets captured best in interpretation 3 helping the network to make the better inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion, Limitations, and Future Work</head><p>We present EmotiCon, a context-aware emotion recognition system that borrows and incorporates the context interpretations from psychology. We use multiple modalities (faces and gaits), situational context, and also the sociodynamic context information. We make an effort to use easily available modalities that can be easily captured or extracted using commodity hardware (e.g., cameras). To foster more research on emotion recognition with naturalistic modalities, we also release a new dataset called GroupWalk. Our model has limitations and often confuses between certain class labels. Further, we currently perform multi-class classification over discrete emotion labels. In the future, we would also like to move towards the continuous model of emotions (Valence, Arousal, and Dominance). As part of future work, we would also explore more such context interpretations to improve the accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GroupWalk</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Annotation Procedure</head><p>We present the human annotated GroupWalk data set which consists of 45 videos captured using stationary cameras in 8 real-world setting including a hospital entrance, an institutional building, a bus stop, a train station, and a marketplace, a tourist attraction, a shopping place and more. 10 annotators annotated 3544 agents with clearly visible faces and gaits across all videos. They were allowed to view the videos as many times as they wanted and had to categorise the emotion they perceived looking at the agent into 7 categories -"Somewhat Happy", "Extremely Happy", "Somewhat Sad", Extremely Sad", "Somewhat Angry", "Extremely Angry", "Neutral". In addition to perceived emotions, the annotators were also asked to annotate the agents in terms of dominance (5 categories-"Somewhat Submissive", "Extremely Submissive", "Somewhat Dominant", "Extremely Dominant", "Neutral" ) and friendliness (5 categories-"Somewhat Friendly", "Extremely Friendly", "Somewhat Unfriendly", "Extremely Unfriendly", "Neutral"). Attempts to build the dataset are still ongoing.</p><p>For the sake of completeness, we show the friendliness label distribution and dominance label distribution for every annotator in <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Labels Processing</head><p>4 major labels that have been considered are Angry, Happy, Neutral and Sad. As described in Section A.1, one can observe that the annotations are either "Extreme" or "Somewhat" variants of these major labels (except Neutral). Target labels were now generated for each agent. Each of them are of the size 1 x 4 with the 4 columns representing the 4 emotions being considered and are initially all 0. For a particular agent id, if the annotation by an annotator was an "Extreme" variant of Happy, Sad or Angry, 2 was added to the number in the column representing the corresponding major label. Otherwise for all the other cases, 1 was added to the number in the column representing the corresponding major label. Once we have gone through the entire dataset, we normalize the target label vector so that vector is a combination of only 1s and 0s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Analysis</head><p>We show the emotion label distribution for every annotator in <ref type="figure" target="#fig_3">Figure 5</ref>. To understand the trend of annotator agreement and disagreement across the 10 annotators, we gather agents labeled similarly in majority (more than 50% of annotators annotated the agent with the same labels) and then study the classes they were confused most with. We show this pictorially for two classes Happy and Sad in <ref type="figure" target="#fig_4">Figure 6</ref>. For instance, we see that Happy and Sad labels are often confused with label Neutral. In addition, we also show the label distributions for every annotator for Friendliness as well as Dominance in <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref> respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EmotiCon on IEMOCAP Dataset</head><p>To validate that EmotiCon can be generalised for any number of modalities, we report our performance on IEMO-CAP <ref type="bibr" target="#b8">[9]</ref> in <ref type="table" target="#tab_5">Table 4</ref>. IEMOCAP dataset consists of speech, text and face modalities of 10 actors recorded in the form of conversations (both spontaneous and scripted) using a Motion Capture Camera. The labeled annotations consist of 4 emotions -angry, happy, neutral, and sad. This is a singlelabel classification as opposed to multi-label classification we reported for EMOTIC and GroupWalk. Because of this we choose to report mean classification accuracies rather than AP scores. Most prior work which have shown results on IEMOCAP dataset, report mean classification accuracies too.  As can be seen from the <ref type="table" target="#tab_5">Table 4</ref>, there is not a significant improvement in the accuracy, 84.5% as SOTA works, not essentially based on context have reported an accuracy of 82.7%. We believe that the controlled settings in which the dataset is collected, with minimal context information re-sults in not huge improvements. Moreover we also see that prior works in context, Kosti et al. <ref type="bibr" target="#b26">[27]</ref> and Lee et al. <ref type="bibr" target="#b57">[58]</ref> sort of do not get any context to learn from and hence do not perform so well. Even EmotiCon's performance is a result of incorporating modalities, with small contribution from context. <ref type="figure">Figure 7</ref>: Friendliness Labeler Annotations: We depict the friendliness labels for GroupWalk by 10 labelers. A total of 3341 agents were annotated from 45 videos. <ref type="figure">Figure 8</ref>: Dominance Labeler Annotations: We depict the dominance labels for GroupWalk by 10 labelers. A total of 3341 agents were annotated from 45 videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Context-Aware Multimodal Emotion Recognition:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative Results: We show the classification results on three examples, each from the EMOTIC dataset (left) and Group-Walk Dataset (right), respectively. In the top row example (left) and middle row example (right), the depth map clearly marks the tennis player about to swing to convey anticipation, and the woman coming from the hospital to convey sadness, respectively. In the bottom row (left) and bottom row (middle) examples, the semantic context of the coffin and the child's kite is clearly identified to convey sadness and pleasure, respectively. (a) AP Scores for EMOTIC Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Misclassification by EmotiCon: We show two examples where EmotiCon incorrectly classifies the labels. In the first examples, EmotiCon is confused about the prediction due to lack of any context. In the second example, there is a lot of context available, which also becomes confusing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Annotator Annotations of GroupWalkDataset: We depict the emotion class labels for GroupWalk by 10 annotators. A total of 3544 agents were annotated from 45 videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Annotator Agreement/Disagreement: For two emotion classes (Happy and Sad), we depict the trend of annotator disagreement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation Experiments: Keeping the Context interpretation 1 throughout, we remove the other two Context interpretations one by one and compare the AP scores for emotion classification on both the datasets. the model, and lastly, in the fourth column, we show the depth map extracted from the input image. The heatmaps in the attention maps indicate what the network has learned. In the bottom row (left) and bottom row (middle)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Labels Kosti et al.<ref type="bibr" target="#b26">[27]</ref> Zhang et al.<ref type="bibr" target="#b57">[58]</ref> Lee et al.<ref type="bibr" target="#b29">[30]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EmotiCon</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GCN-Based Depth-Based</cell></row><row><cell>Anger</cell><cell>80.7%</cell><cell>-</cell><cell>77.3%</cell><cell>87.2%</cell><cell>88.2%</cell></row><row><cell>Happy</cell><cell>78.9%</cell><cell>-</cell><cell>72.4%</cell><cell>82.4%</cell><cell>83.4%</cell></row><row><cell>Neutral</cell><cell>73.5%</cell><cell>-</cell><cell>62.8%</cell><cell>75.5%</cell><cell>77.5%</cell></row><row><cell>Sad</cell><cell>81.3%</cell><cell>-</cell><cell>68.7%</cell><cell>88.2%</cell><cell>88.9%</cell></row><row><cell>mAP</cell><cell>78.6%</cell><cell>-</cell><cell>70.3%</cell><cell>83.4%</cell><cell>84.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>IEMOCAP Experiments: Mean Classification Accuracies for IEMOCAP Dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported in part by ARO Grants W911NF1910069 and W911NF1910315 and Intel.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facial emotion recognition for intelligent tutoring environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kah</forename><forename type="middle">Phooi</forename><surname>Kingsley Oryina Akputu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">Li</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMLCS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The future of emotion regulation research: Capturing context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amelia</forename><surname>Aldao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="172" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Body cues, not facial expressions, discriminate between intense positive and negative emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Openface: an open source facial behavior analysis toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context in emotion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa Feldman</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Batja</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="286" to="290" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The context principle. The mind in context, 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa Feldman</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Batja</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliot</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12906</idno>
		<title level="m">Step: Spatial temporal graph convolutional networks for emotion perception from gaits</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robusttp: End-to-end trajectory prediction for heterogeneous road-agents in dense traffic with noisy sensor inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Roncal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computer Science in Cars Symposium</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fear-type emotion recognition for future audio-based surveillance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Ehrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="487" to="503" />
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion recognition in human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Votsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winfried</forename><surname>Fellenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SP Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="32" to="80" />
			<date type="published" when="2001-02" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotiw 2016: Video and group-level emotion recognition challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="427" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Head and body cues in the judgment of emotion: A reformulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace V Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptual and motor skills</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="711" to="724" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprakash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention branch network: Learning of attention mechanism for visual explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsubasa</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayoshi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hironobu</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10705" to="10714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Group emotion recognition using machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samanyou</forename><surname>Garg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Context is everything (in emotion research). Social and Personality Psychology Compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Katharine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elise</forename><forename type="middle">K</forename><surname>Greenaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">A</forename><surname>Kalokerinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">12393</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bi-modal emotion recognition from expressive face and body gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hatice</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1334" to="1345" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Social context effects on facial activity in a negative emotional setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Jakobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agneta</forename><forename type="middle">H</forename><surname>Manstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Affective body expression perception and recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE TAC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Physiological signals and their use in augmenting emotion recognition for human-machine interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwa</forename><surname>R Benjamin Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emotionoriented systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="133" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Context based emotion recognition using emotic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emotion recognition in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluations in their social context: Distance regulates consistency and context dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Ledgerwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social and Personality Psychology Compass</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="436" to="447" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Jungin Park, and Kwanghoon Sohn. Context-aware emotion recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05913</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occlusion aware facial expression recognition using cnn with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2439" to="2450" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2041" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learn to combine modalities in multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context may reveal how you feel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aleix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="7169" to="7171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond positive psychology? toward a contextual view of psychological processes and well-being</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnulty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fincham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">101</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rapid perceptual integration of facial expression and emotional body language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Heijnsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emotions in context: A sociodynamic model of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Batja</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion Review</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="298" to="302" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1831</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Individuality in communicative bodily behaviours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costanza</forename><surname>Navarretta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Behavioural Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="417" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyra</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11884</idno>
		<title level="m">Identifying emotions from walking using affective and deep features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The context principle in frege&apos;s philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>David Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy and Phenomenological Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="365" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1034" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Vocal expression of emotion. Handbook of affective sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundrun</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klasmeyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="433" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiple kernel learning for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karmen</forename><surname>Dykstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchitra</forename><surname>Sathyanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwen</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiple kernel learning for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karmen</forename><surname>Dykstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchitra</forename><surname>Sathyanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwen</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Development of a facs-verified set of basic and selfconscious emotion expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jessica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">W</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><forename type="middle">A</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schriber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">554</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The MPLab GENKI Database</title>
		<ptr target="http://mplab.ucsd.edu" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cascade attention networks for group emotion recognition with face, body and image cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI &apos;18</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction, ICMI &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="640" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The effects of social interaction and personal relationships on facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoko</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Jur van den Berg, Dinesh Manocha, and Ming Lin. Composite agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengchin</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<meeting>the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
			<date type="published" when="2018" />
			<publisher>Long Papers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contextaware affective graph reasoning for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Image captioning with integrated bottom-up and multi-level residual top-down attention for game scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06632</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
