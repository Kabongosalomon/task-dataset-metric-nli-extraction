<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LSMVOS: Long-Short-Term similarity matching for video object segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xuerui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Jiangsu Nanjing</orgName>
								<address>
									<postCode>210094</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Jiangsu Nanjing</orgName>
								<address>
									<postCode>210094</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LSMVOS: Long-Short-Term similarity matching for video object segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective Semi-supervised video object segmentation refers to segmenting the object in subsequent frames given the object label in the first frame. Existing algorithms are mostly based on the objectives of matching and propagation strategies, which often make use of the previous frame with masking or optical flow. This paper explores a new propagation method, uses short-term matching modules to extract the information of the previous frame and apply it in propagation, and proposes the network of Long-Short-Term similarity matching for video object segmentation (LSMOVS) Method: By conducting pixel-level matching and correlation between long-term matching module and short-term matching module with the first frame and previous frame, global similarity map and local similarity map are obtained, as well as feature pattern of current frame and masking of previous frame. After two refine networks, final results are obtained through segmentation network. Results: According to the experiments on the two data sets DAVIS 2016 and 2017, the method of this paper achieves favorable average of region similarity and contour accuracy without online fine tuning, which achieves 86.5% and 77.4% in terms of single target and multiple targets. Besides, the count of segmented frames per second reached 21. Conclusion: The short-term matching module proposed in this paper is more conducive to extracting the information of the previous frame than only the mask. By combining the long-term matching module with the short-term matching module, the whole network can achieve efficient video object segmentation without online fine tuning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">Introduction</head><p>Video object segmentation is an important task in computer vision, which is applied in multiple aspects, such as intelligent monitoring, video editing and environment understanding of robots. Semi-supervised video object segmentation refers to segmenting the object in subsequent frames given the object label in the first frame. There are two types of clues used by semisupervised video object segmentation. One is space clues: combining the current frame with the first frame for object segmentation. Another one is time clue: using the information of the previous frame to calculate the current frame, such as segmenting object of current frame with object mask predicted in the previous frame. According to different clues that are used, there are three ways of semi-supervised video object segmentation: method based on test, method based on propagation and method that involves both of them.</p><p>A typical test-based method is OSVOS proposed by Caelles et al <ref type="bibr" target="#b0">[1]</ref> , which converts video segmentation to pictures segmentation. This method trains models that merely focuses on overfitting of current video based on the first frame with labeled information of every video. VideoMatch method based on matching, proposed by Y.-T. Hu et al <ref type="bibr" target="#b1">[2]</ref> , conducts soft segmentation on the average similarity score of matching feature to generate the smooth results of prediction. These methods do not rely on time sequence information, so they can effectively process occlusion and drifting. But they depend on the first labeled frame, and can't well process frames with obvious changes and similar objects.</p><p>A typical example of propagation-based method is MaskTrack proposed by Perazzi <ref type="bibr" target="#b2">[3]</ref> , which converts the video segmentation to guided examples of segmentation. It uses the forecast mask of previous frame as the guiding information of current frame for object segmentation. X. Li <ref type="bibr" target="#b3">[4]</ref> further put forward the idea of repropagation, which selects high-quality frames from video sequence to propagate forward and backward. As changes between frames are not apparent, forecast masking or optical flow of the previous frame can achieve good effects. But when dealing with cases like occlusion and disappearance, this method may propagate the wrong information to the next frame, thus affecting the results of segmentation.</p><p>Currently the main ways of semi-supervised video segmentation combine the above two methods, and make use of the Information of the first frame and the current frame at the same time. RGMP, proposed by S. Wug Oh <ref type="bibr" target="#b4">[5]</ref> , uses Siamese network to encode the features of the current frame and the first frame. It is divided into two paths on input. One path adds the mask of previous frame to current frame. Another path adds corresponding mask to the first frame. As for the features of the two paths, RGMP merely conducts superposition with no other operation. FAVOS, proposed by J. Cheng <ref type="bibr" target="#b5">[6]</ref> , divides objects of the first frame to multiple parts. For example, a person is divided into head, body and limbs. Then, it keeps track of these parts in subsequent frames and generates segmentation mask based on segmentation network of interested region. Lastly, it calculates the feature distance of segmented parts and the first frame to aggregate the parts. OSMN network, proposed by L.Yang <ref type="bibr" target="#b6">[7]</ref> , designs a modulator, inputs the target location of first frame and previous frame to network to obtain visual modulation parameters and spatial modulation parameters. Visual modulation parameters serve as weight and spatial modulation parameters serve as offset to guide current frame features so that they focus on fixed objects Online training is an important way to enhance the performance of semi-supervised video objective segmentation. It means that after the model is trained, use the label of the first frame for each individual video to train for tens of seconds or even minutes. The longer the training time, the better the effect.. This method is developed on the basis of Lucid Data Dreaming for synthetic video frames, proposed by A. Khoreva <ref type="bibr" target="#b8">[8]</ref> The above methods usually make use of masking or optical flows to propagate the Information of previous frame. Sometimes online training is needed as well. Masking only represents the shapes and positions of objectives in the previous frames. In comparison, optical flow calculation needs to add optical flow detection network, which is quite complex and difficult to achieve end-to-end training. Even though online training can enhance the effects of segmentation, it takes much longer time. For this reason, this method cannot complete tasks that require higher computational efficiency. In order to enhance the real time of model calculation and make better use of time sequence information in videos, this paper puts forward longshort-term similarity matching for video object segmentation model. It uses long-term matching module to resolves issues like occlusion, disappearing and correct errors. Meanwhile, it uses short-term matching module to propagate object features. The method of this paper is an efficient model of end-to-end semi-supervised video object segmentation. Without online fine tuning, it achieves 86.5% of region similarity and 77.4% of contour accuracy in DAVIS2016 and DAVIS2017, and 21 frames per second are segmented.</p><p>This paper makes contributions in the following aspects.</p><p>1) It puts forward a new long-term matching and short-term matching convolutional neural network structure, which propagate information of previous frame to current frame through pixel-level calculation.</p><p>2) Experiment verifies that long-term matching module and short-term matching module are complementary. The combination can effectively increase the accuracy of segmentation.</p><p>3) It designs a rapid model of end-to-end semisupervised video object segmentation, and achieves good test results on DIVIS data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Methods</head><p>Given object segmentation mask of the first frame, the paper designs a model of long-short-term similarity matching for video object segmentation. Long-term matching refers to mask matching between current frame and the first labeled frame. Short-term matching refers to the matching of forecast results between current frame and the first frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">network architecture</head><p>Network structure is shown in <ref type="figure" target="#fig_0">Fig.1</ref> The method of this paper includes four sections: encoding modules for extracting characteristics, long-term matching modules that make use of information of the first frame,shortterm matching modules that make use of information of the previous frame,and decoding modules for segmentation mask.</p><p>To go beyond the limitation of convolutional fixed reception field, and resolve the issue of deformation of nonrigid objects in movement, the paper introduces anisotropic convolution modules (AIC) of Literature <ref type="bibr" target="#b9">[9]</ref> into the model, converting the 3D structure to 2D-AIC for processing single-frame videos. After encoders extract the features, they pass the two branches of 2D-AIC to obtain global features for long-term matching and local features for short-term matching. Then, carry out correlation operation on every pixel feature and key frame of global features of current frame to obtain global similarity map. Next, local similarity map is obtained by operating every pixel feature of local features in current frame and pixel features in corresponding areas of the previous frame. Lastly, global similarity map, local similarity map, previous frame mask and features output by encoders are transmitted to decoding module, thereby obtaining the final results through passing two refined networks. In the following sections, we will describe each module in detail:  Encoding modules use res2net <ref type="bibr" target="#b10">[10]</ref> as the backbone network and remove the fully connected layer. Meanwhile, to make better use of multiple scale features and offer low-level features for and subsequent refined network, structures similar to FPN <ref type="bibr" target="#b11">[11]</ref> is used in this paper. In every layer, features of previous layer carry out twofold up-sampling and add to feature of the player after 1×1 convolutional dimensionality reduction. Then, they are transmitted to a 2D-AIC structural output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Encoding module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Long-term matching</head><p>Relevant operations are widely used in object tracking. For example, for SiamRPN <ref type="bibr" target="#b12">[12]</ref> , target area and search area determine the position of objects through correlation operations. In recent years, some algorithms also introduce correlation operations to video segmentation. For example, for RANet <ref type="bibr" target="#b13">[13]</ref> ，similarity map is forged by pixel-level approaches. Then, a smallsize network gives scores to the graph and chooses Layer 256 with the highest score in segmentation. In order to make use of the information of key frame and avoid occlusion and disappearing of object information in the previous frame, this paper will connect the current frame with the first frame at pixel level, as is shown in <ref type="figure">Fig.4</ref> As for every pixel-level feature of extracted from global features ∈ C×H×W ( and equal 1/8 of original picture size) of current frame, conduct correlation operations on per pixel of global features ∈ C×H×W among key frames to obtain similar picture 1×H×W . As for similar picture , after converting its dimension to ( × ) × × , multiply it by M (H×W)×1×1 , the foreground (or background) of key frames. Lastly, take the maximum N ( set to be 256 in this paper) values to get the corresponding pixel in the global similarity figure ∈ N×H×W , as is shown in Equation (1)</p><formula xml:id="formula_0">= { | = (( × ) · )}<label>(1)</label></formula><p>stands for pixel feature in global similarity map of foreground (or background).K stands for feature of key frame. stands for pixel feature of current frame.M represents truth value of foreground (or background) in key frame. X stands for multiplication of vectors. · represents multiplication of per pixel. represents the selection of maximum N values. The visualization of global similarity map is shown in <ref type="figure" target="#fig_3">Fig.5</ref>. It demonstrates the maximum response from target object in foreground and minimum response from target object in background. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">short-term matching</head><p>Sequence propagation stemmed from MaskTrack <ref type="bibr" target="#b2">[3]</ref> ， which also produced goods effects in other methods. But these methods merely make use of mask and optical flow of previous frame, and transfer it to network for segmentation. Mask simply reveals the position and shape of object in previous frame while ignores the object feature of previous frame. In contrast, optical flow calculation needs detection network, which is rather complex and difficult to conduct end-to-end training. In fact, we can assess which pixels of current frame are foreground or background according to the forecast of previous frame. As the changes between video frames are not apparent, it is feasible to limit the range of movement for every pixel. Inspired by mutual correlation layer of Flownet2.0 <ref type="bibr" target="#b14">[14]</ref> , the paper proposes short-term matching operation, which is similar to longterm matching module.</p><p>As is shown in <ref type="figure" target="#fig_5">Fig.7</ref>, as for every pixel feature , Among the pixel at (i，j) of current frame feature, foreground feature (or background feature) of previous frame, take (i，j) as the center, conduct per pixel calculation on pixel sets on x axis and y axis with distance no greater than k , in order to obtain the similarity value of ( × + )^. Then, select the top N values to form foreground similarity (or background similarity figure)L N×H×W . visualization of local similarity figure is shown in <ref type="figure">Fig. 6</ref>. in comparison with global similarity map, local similarity map gets rid of many interference factors, so the results are more explicit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Decoding module</head><p>Decoding modules include two refined networks for up-sampling and one conv 3 × 3 segmentation network for extracting probability graph of final results.</p><p>Refined network is shown as <ref type="figure" target="#fig_6">Fig.8</ref>. Feature i stands for the same-layer features output by encoders, which will be processed by 2D-AIC. Afterwards, add the features to those of previous layer that passed twofold upsampling. Lastly, after passing two refined networks, segmentation is carried out at 1/2 size of the picture.. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">details of training</head><p>The network in this article is end-to-end, and the backbone is initialized with the res2net50 model parameters pre-trained on the ImageNet dataset. The optimizer is the Adam optimizer, the learning rate is set to a fixed 0.00001, and the loss function is Focal loss. The dataset uses YouTube-VOS and DAVIS, and trains 200,000 steps on 4 NVIDIA GeForce TITAN Xp with the batch size being set to 12. Data augmentation adopts random cropping, random size transformation and random flip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single-target video segmentation</head><p>Single target video separation experiment is carried out on DAVIS2016 <ref type="bibr" target="#b18">[18]</ref> data set. The data set has 50 videos, including 2079 frames of 30 training videos and 1376 frames of 20 testing videos. The first frame of every video displays the labeled Information. The evaluation index involves regional similarity J and contour accuracy F. By comparing the DAVIS2016 verification set with methods in <ref type="table" target="#tab_0">Table 1</ref>, the solutions of this paper achieves advanced levels without online tuning. Average values J&amp;F of regional similarity and contour accuracy reach 86.5%. After model training is completed, the online inference rate reaches 21 FPS on GeForce TITAN Xp computer card, achieving favorable balance in terms of time and accuracy. <ref type="table" target="#tab_1">Table 2</ref> shows the results of ablation experiment. To explore the roles of network structure in various modules, only short-term module and long-term modules are retained for training, and results are shown in the first and second rows of <ref type="table" target="#tab_1">Table 2</ref>. Both of them achieve very good results. But they are significantly different from results in the fourth row, which combine two module training. J&amp;F indexes decrease by 3.7% and 2.9% respectively. Only short-term matching module may reduce the accuracy, as the lack of the first frame correction cause the result that errors transfer from the previous frame to current frame. As is shown in <ref type="figure">Fig.9</ref>, with no correction of true value from the first frame, errors increases along with the growth of frame number. Only long-term matching module may also reduce accuracy. This is because the latter targets show great differences from the first frame. The sheer matching with the first frame pixels is not sufficient to capture targets, especially when the target size changes are excessively significant, as is shown in <ref type="figure" target="#fig_0">Fig.10</ref>. Because it is excessively different from the target of first frame, the network cannot detect the target. In order to determine the contribution of mask and short-term matching module to sequence propagation, this paper gets rid of short-term matching modules and mask to carry out experiment, the result of which are shown in the 3 rd and 4 th rows of <ref type="table" target="#tab_1">Table 2</ref>. J&amp;F with short-term matching removed increases by 2.2% than with mask removed. It proves the validity of short-term matching module proposed in this paper. Even though short-term matching module have very good effects, it cannot sufficiently represent the shape and location of objects, so it still needs the complimentary function of mask. With the additional role of mask, this paper's method increases by 1.6% in terms of J&amp;F index. Note: Red shows the optimum value of each row. Green shows less optimum value of each row. * means online training. ↑ means the higher the better. ↓ means the lower the better. Note: Red shows the optimum value of each row. Green shows less optimum value of each row. * means online training. ↑ means the higher the better. ↓ means the lower the better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiple object video segmentation</head><p>In multiple object video segmentation, many similar targets are very likely to shield or miss each other, which makes this process challenging. In this paper, multiple object video segmentation is considered as single object video segmentation. After all the targets are segmented from every frame, select the category with the maximum probability. The reason is that encoding modules, long-term matching modules and short-term matching modules are relatively timeconsuming. In every frame, the multiple targets can be shared commonly, which consumes approximately 42 milliseconds. Encoding module is the only thing linearly dependent on target numbers, and each target consumes approximately 11 milliseconds. Therefore, the algorithm of this paper still achieves high efficiency in terms of multiple object video segmentation. The experimental results of verification set and testing set on multiple object data set DAVIS2017 <ref type="bibr" target="#b19">[19]</ref> are shown in <ref type="table" target="#tab_2">Table 3</ref> and 4. It can be seen the method of this paper still produces favorable results in multiple objects. J&amp;F indexes of PReMVOS <ref type="bibr" target="#b20">[20]</ref> increase by 0.5% and 4.2% than the method of this paper. This is because it utilizes the method of online training. For every video, the training of the first frame needs to last dozens of seconds and even several minutes. In comparison, the method of this paper can achieve equivalent results without the need for online training.  Note: Red shows the optimum value of each row. Green shows less optimum value of each row. * means online training. ↑ means the higher the better. ↓ means the lower the better. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the segmentation results of every 10 frames of four videos. The first video and second video are respectively about horse riding and race car drifting. These two videos verify the processing capability of the algorithm about deformation and rapid movement. The 3 rd video and 4 th video are respective concerned with five goldfish in the ocean and three pedestrians in the crowd. It can be seen that despite the occlusion of similar targets and complicated background, algorithms of this paper still produce favorable results. More results of video segmentation can be seen in the following websites: https://www.bilibili.com/video/BV1jK4y1Y7yd/ https://www.bilibili.com/video/BV1MC4y1t7R2/ https://www.bilibili.com/video/BV1Bh411d72y/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusion</head><p>In order to deal with under-utilization of previous frame information, this paper puts forward short-term matching modules, whose the favorable effects are verified by experiments. By combining long-term matching module with short-term matching module, a simple and fast method of end-to-end video segmentation is proposed in this paper. It has favorable capability of processing in the following scenarios: target occlusion, deformation, rapid movement. Compared with other methods, the network structure of this paper achieves competitive results in terms of DAVIS2016 and DAVIS2017. Besides, without the need for online training, the speed of segmentation is enhanced significantly. Based on the achievements of this paper, subsequent research and studies will make further use of prediction results of previous frames to segment the objects of current frames. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1</head><label>1</label><figDesc>Network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2</head><label>2</label><figDesc>Encoding module Fig 3 Anisotropic Convolution (2D-AIC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Encoders of this paper have three outputs. Res2 layer output is used to extract global features and local features. The other two features are used to provide low level-features for final refined networks. Encoding modules are shown in Fig.2 2D-AIC Structures are shown in Fig.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigFig 5</head><label>5</label><figDesc>Global similarity map ((a)foreground；(b) background)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>original picture size) of current frame, select pixel sets − with distance − no greater than k (set as 8 in the paper) on x axis and y axis in local features of previous frame − ∈ C×H×W , conduct per pixel operation to obtain similarity figure ∈ 1×(2×k+1)×(2×k+1) . Convert the dimension of similarity figure to ( × + )^× × . Afterwards, multiply it by M (2×k+1)^2×1×1 per pixel in foreground (or background) of previous frame. Then, select maximum N (set as 256 in this paper) values to obtain corresponding pixel in local similarity figure ∈ N×H×W , as is shown in Fig.2 = { | = (( − × ) · ( ∈ ([ − , + ] ∩ [ − , + ]))} (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FigFig 7</head><label>7</label><figDesc>Local similarity map ((a)foreground；(b) background)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 8</head><label>8</label><figDesc>Refined module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig 9 Error propagation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig 11 Qualitive results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Results of different methods on the DAVIS 2016 validation set</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>J&amp;F↑</cell><cell>J mean↑</cell><cell>J recall↑</cell><cell>J decay↓</cell><cell>F mean↑</cell><cell>F recall↑</cell><cell>F decay↓</cell><cell>FPS↑</cell></row><row><cell>OSVOS* [1]</cell><cell>80.2</cell><cell>79.8</cell><cell>93.6</cell><cell>14.9</cell><cell>80.6</cell><cell>92.6</cell><cell>15.0</cell><cell>-</cell></row><row><cell>MaskTrack* [3]</cell><cell>77.6</cell><cell>79.7</cell><cell>93.1</cell><cell>8.9</cell><cell>75.4</cell><cell>87.1</cell><cell>9.0</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Ablation study results</head><label>2</label><figDesc></figDesc><table><row><cell>Strategy.</cell><cell>J&amp;F↑</cell><cell>J mean↑</cell><cell>J recall↑</cell><cell>J decay↓</cell><cell>F mean↑</cell><cell>F recall↑</cell><cell>F decay↓</cell></row><row><cell>Long-term matching modules and mask</cell><cell>81.2</cell><cell>80.6</cell><cell>91.4</cell><cell>7.4</cell><cell>81.8</cell><cell>90.9</cell><cell>6.7</cell></row><row><cell>removed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Short-term matching modules and mask</cell><cell>82.0</cell><cell>81.0</cell><cell>92.7</cell><cell>11.3</cell><cell>83.0</cell><cell>93.1</cell><cell>11.7</cell></row><row><cell>removed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Short-term matching modules removed</cell><cell>82.7</cell><cell>81.5</cell><cell>93.1</cell><cell>9.0</cell><cell>83.9</cell><cell>93.3</cell><cell>10.6</cell></row><row><cell>Mask removed.</cell><cell>84.9</cell><cell>83.9</cell><cell>94.7</cell><cell>6.0</cell><cell>85.9</cell><cell>94.4</cell><cell>6.9</cell></row><row><cell>Methods of this paper.</cell><cell>86.5</cell><cell>85.7</cell><cell>97.1</cell><cell>5.1</cell><cell>87.3</cell><cell>96.1</cell><cell>4.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Results of different methods on the DAVIS 2017 validation set</head><label>3</label><figDesc>Note: Red shows the optimum value of each row. Green shows less optimum value of each row. * means online training. ↑ means the higher the better. ↓ means the lower the better.</figDesc><table><row><cell>Method</cell><cell cols="2">J&amp;F↑ J</cell><cell>J</cell><cell>J</cell><cell>F</cell><cell>F</cell><cell>F</cell></row><row><cell></cell><cell></cell><cell>mean↑</cell><cell>recall↑</cell><cell>decay↓</cell><cell>mean↑</cell><cell>recall↑</cell><cell>decay↓</cell></row><row><cell>OSVOS*</cell><cell>60.3</cell><cell>56.6</cell><cell>63.8</cell><cell>26.1</cell><cell>63.9</cell><cell>73.8</cell><cell>27.0</cell></row><row><cell cols="2">PReMVOS* 77.9</cell><cell>73.9</cell><cell>83.1</cell><cell>16.2</cell><cell>81.8</cell><cell>88.9</cell><cell>19.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 Results of different methods on the DAVIS 2017 test set</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">J&amp;F↑ J</cell><cell>J</cell><cell>J</cell><cell>F</cell><cell>F</cell><cell>F</cell></row><row><cell></cell><cell></cell><cell>mean↑</cell><cell>recall↑</cell><cell>decay↓</cell><cell>mean↑</cell><cell>recall↑</cell><cell>decay↓</cell></row><row><cell>OSVOS*</cell><cell>50.9</cell><cell>47.0</cell><cell>52.1</cell><cell>19.2</cell><cell>54.8</cell><cell>59.7</cell><cell>19.8</cell></row><row><cell cols="2">PReMVOS* 71.6</cell><cell>67.5</cell><cell>76.8</cell><cell>21.7</cell><cell>75.8</cell><cell>84.3</cell><cell>20.6</cell></row><row><cell>RGMP</cell><cell>52.8</cell><cell>51.3</cell><cell>59.0</cell><cell>34.3</cell><cell>54.4</cell><cell>61.9</cell><cell>37.2</cell></row><row><cell>FAVOS</cell><cell>43.6</cell><cell>42.9</cell><cell>48.1</cell><cell>18.1</cell><cell>44.2</cell><cell>51.1</cell><cell>19.8</cell></row><row><cell>OSMN</cell><cell>41.3</cell><cell>37.7</cell><cell>38.9</cell><cell>19.0</cell><cell>44.9</cell><cell>47.4</cell><cell>17.4</cell></row><row><cell>RANet</cell><cell>55.4</cell><cell>53.4</cell><cell>61.9</cell><cell>21.9</cell><cell>57.3</cell><cell>67.7</cell><cell>22.1</cell></row><row><cell>Ours</cell><cell>67.4</cell><cell>63.7</cell><cell>72.7</cell><cell>16.9</cell><cell>71.2</cell><cell>81.4</cell><cell>16.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.565</idno>
		<title level="m">// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</title>
		<meeting><address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
	<note type="report_type">One-Shot Video Object Segmentation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VideoMatch: Matching based Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01237-3_4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV<address><addrLine>Munich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="56" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Video Object Segmentation from Static Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.372</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017<address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3491" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Video Object Segmentation with Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y K</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>EB/OL]. [2020-08-14</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast Video Object Segmentation by Reference-Guided Mask Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkavalli</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S J</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00770</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR<address><addrLine>Salt Lake City</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and Accurate Online Video Object Segmentation via Tracking Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00774</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR<address><addrLine>Salt Lake City</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient Video Object Segmentation via Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno type="DOI">10.1109/CVPR.2018.00680</idno>
		<title level="m">Modulation//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting><address><addrLine>Salt Lake City</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lucid Data Dreaming for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01164-6</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anisotropic Convolutional Networks for 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00341</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3348" to="3356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Res2Net: A New Multi-scale Backbone Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename><forename type="middle">P</forename></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2938758</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belongie</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017<address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High Performance Visual Tracking with Siamese Region Proposal Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X L</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00935</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR<address><addrLine>Salt Lake City</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RANet: Ranking Attention Network for Fast Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><forename type="middle">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00408</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) 2019</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) 2019<address><addrLine>Seoul</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3977" to="3986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dosovitskiy</forename><forename type="middle">A</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017<address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F F</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<title level="m">ImageNet: A large-scale hierarchical image database// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Miami</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollá R P</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858826</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01228-1_36</idno>
		<title level="m">YouTube-VOS: Sequence-to-Sequence Video Object Segmentation// Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting><address><addrLine>Munich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.85</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016<address><addrLine>Las Vegas</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The 2017 DAVIS Challenge on Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelá Ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L V</forename><surname>Gool</surname></persName>
		</author>
		<idno>EB/OL]. [2020-08-14</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposalgeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20870-7_35</idno>
	</analytic>
	<monogr>
		<title level="m">Refinement and Merging for Video Object Segmentation// Asian Conference on Computer Vision(ACCV</title>
		<meeting><address><addrLine>Perth</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
