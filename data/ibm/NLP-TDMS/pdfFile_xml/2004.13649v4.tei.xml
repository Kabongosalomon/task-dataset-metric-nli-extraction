<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
							<email>kostrikov@cs.nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
							<email>denisyarats@cs.nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
							<email>fergus@cs.nyu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">New York University &amp;</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC) <ref type="bibr" target="#b21">[22]</ref>, are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24]</ref> methods and recently proposed contrastive learning <ref type="bibr" target="#b49">[50]</ref>. Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN [43]  and significantly improve its data-efficiency on the Atari 100k <ref type="bibr" target="#b30">[31]</ref> benchmark. An implementation can be found at https://sites. google.com/view/data-regularized-q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sample-efficient deep reinforcement learning (RL) algorithms capable of directly training from image pixels would open up many real-world applications in control and robotics. However, simultaneously training a convolutional encoder alongside a policy network is challenging when given limited environment interaction, strong correlation between samples and a typically sparse reward signal. Naive attempts to use a large capacity encoder result in severe over-fitting (see <ref type="figure">Figure 1a</ref>) and smaller encoders produce impoverished representations that limit task performance.</p><p>Limited supervision is a common problem across AI and a number of approaches are adopted: (i) pretraining with self-supervised learning (SSL), followed by standard supervised learning; (ii) supervised learning with an additional auxiliary loss and (iii) supervised learning with data augmentation. SSL approaches are highly effective in the large data regime, e.g. in domains such as vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> and NLP <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> where large (unlabeled) datasets are readily available. However, in sample-efficient RL, training data is more limited due to restricted interaction between the agent and the environment, resulting in only 10 4 -10 5 transitions from a few hundred trajectories. While there are concurrent * Equal contribution. Author ordering determined by coin flip. Both authors are corresponding.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SAC combined with image augmentation in the form of random shifts. The task performance is now similar for all architectures, regardless of their capacity. There is also a clear performance improvement relative to (a), particularly for the more challenging Walker Walk task.</p><p>efforts exploring SSL in the RL context <ref type="bibr" target="#b49">[50]</ref>, in this paper we take a different approach, focusing on data augmentation.</p><p>A wide range of auxiliary loss functions have been proposed to augment supervised objectives, e.g. weight regularization, noise injection <ref type="bibr" target="#b27">[28]</ref>, or various forms of auto-encoder <ref type="bibr" target="#b33">[34]</ref>. In RL, reconstruction objectives <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b59">60]</ref> or alternate tasks are often used <ref type="bibr" target="#b15">[16]</ref>. However, these objectives are unrelated to the task at hand, thus have no guarantee of inducing an appropriate representation for the policy network.</p><p>Data augmentation methods have proven highly effective in vision and speech domains, where output-invariant perturbations can easily be applied to the labeled input examples. Surprisingly, data augmentation has received relatively little attention in the RL community, and this is the focus of this paper. The key idea is to use standard image transformations to peturb input observations, as well as regularizing the Q-function learned by the critic so that different transformations of the same input image have similar Q-function values. No further modifications to standard actor-critic algorithms are required, obviating the need for additional losses, e.g. based on auto-encoders <ref type="bibr" target="#b59">[60]</ref>, dynamics models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>, or contrastive loss terms <ref type="bibr" target="#b49">[50]</ref>.</p><p>The paper makes the following contributions: (i) we demonstrate how straightforward image augmentation, applied to pixel observations, greatly reduces over-fitting in sample-efficient RL settings, without requiring any change to the underlying RL algorithm. (ii) exploiting MDP structure, we introduce two simple mechanisms for regularizing the value function which are generally applicable in the context of model-free off-policy RL. (iii) Combined with vanilla SAC <ref type="bibr" target="#b21">[22]</ref> and using hyper-parameters fixed across all tasks, the overall approach obtains state-of-the-art performance on the DeepMind control suite <ref type="bibr" target="#b50">[51]</ref>. (iv) Combined with a DQN-like agent, the approach also obtains state-of-the-art performance on the Atari 100k benchmark. (v) It is thus the first effective approach able to train directly from pixels without the need for unsupervised auxiliary losses or a world model. (vi) We also provide a PyTorch implementation of the approach combined with SAC and DQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Reinforcement Learning from Images We formulate image-based control as an infinite-horizon partially observable Markov decision process (POMDP) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>. An POMDP can be described as the tuple (O, A, p, r, γ), where O is the high-dimensional observation space (image pixels), A is the action space, the transition dynamics p = P r(o t |o ≤t , a t ) capture the probability distribution over the next observation o t given the history of previous observations o ≤t and current action a t , r : O × A → R is the reward function that maps the current observation and action to a reward r t = r(o ≤t , a t ), and γ ∈ [0, 1) is a discount factor. Per common practice <ref type="bibr" target="#b42">[43]</ref>, throughout the paper the POMDP is converted into an MDP <ref type="bibr" target="#b5">[6]</ref> by stacking several consecutive image observations into a state s t = {o t , o t−1 , o t−2 , . . .}. For simplicity we redefine the transition dynamics p = P r(s t |s t , a t ) and the reward function r t = r(s t , a t ). We then aim to find a policy π(a t |s t ) that maximizes the</p><formula xml:id="formula_0">cumulative discounted return E π [ ∞ t=1 γ t r t |a t ∼ π(·|s t ), s t ∼ p(·|s t , a t ), s 1 ∼ p(·)]</formula><p>. Soft Actor-Critic The Soft Actor-Critic (SAC) <ref type="bibr" target="#b21">[22]</ref> learns a state-action value function Q θ , a stochastic policy π θ and a temperature α to find an optimal policy for an MDP (S, A, p, r, γ) by optimizing a γ-discounted maximum-entropy objective <ref type="bibr">[62]</ref>. θ is used generically to denote the parameters updated through training in each part of the model.</p><p>Deep Q-learning DQN <ref type="bibr" target="#b42">[43]</ref> also learns a convolutional neural net to approximate Q-function over states and actions. The main difference is that DQN operates on discrete actions spaces, thus the policy can be directly inferred from Q-values. In practice, the standard version of DQN is frequently combined with a set of refinements that improve performance and training stability, commonly known as Rainbow <ref type="bibr" target="#b52">[53]</ref>. For simplicity, the rest of the paper describes a generic actor-critic algorithm rather than DQN or SAC in particular. Further background on DQN and SAC can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sample Efficient Reinforcement Learning from Pixels</head><p>This work focuses on the data-efficient regime, seeking to optimize performance given limited environment interaction. In <ref type="figure" target="#fig_1">Figure 1a</ref> we show a motivating experiment that demonstrates over-fitting to be a significant issue in this scenario. Using three tasks from the DeepMind control suite <ref type="bibr" target="#b50">[51]</ref>, SAC <ref type="bibr" target="#b21">[22]</ref> is trained with the same policy network architecture but using different image encoder architectures, taken from the following RL approaches: NatureDQN <ref type="bibr" target="#b42">[43]</ref>, Dreamer <ref type="bibr" target="#b22">[23]</ref>, Impala <ref type="bibr" target="#b16">[17]</ref>, SAC-AE <ref type="bibr" target="#b59">[60]</ref> (also used in CURL <ref type="bibr" target="#b49">[50]</ref>), and D4PG <ref type="bibr" target="#b3">[4]</ref>. The encoders vary significantly in their capacity, with parameter counts ranging from 220k to 2.4M. The curves show that performance decreases as parameter count increases, a clear indication of over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Augmentation</head><p>A range of successful image augmentation techniques to counter over-fitting have been developed in computer vision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7]</ref>. These apply transformations to the input image for which the task labels are invariant, e.g. for object recognition tasks, image flips and rotations do not alter the semantic label. However, tasks in RL differ significantly from those in vision and in many cases the reward would not be preserved by these transformations. We examine several common image transformations from <ref type="bibr" target="#b6">[7]</ref> in Appendix E and conclude that random shifts strike a good balance between simplicity and performance, we therefore limit our choice of augmentation to this transformation. <ref type="figure" target="#fig_1">Figure 1b</ref> shows the results of this augmentation applied during SAC training. We apply data augmentation only to the images sampled from the replay buffer and not for samples collection procedure. The images from the DeepMind control suite are 84 × 84. We pad each side by 4 pixels (by repeating boundary pixels) and then select a random 84 × 84 crop, yielding the original image shifted by ±4 pixels. This procedure is repeated every time an image is sampled from the replay buffer. The plots show overfitting is greatly reduced, closing the performance gap between the encoder architectures. These random shifts alone enable SAC to achieve competitive absolute performance, without the need for auxiliary losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimality Invariant Image Transformations</head><p>While the image augmentation described above is effective, it does not fully exploit the MDP structure inherent in RL tasks. We now introduce a general framework for regularizing the value function through transformations of the input state. For a given task, we define an optimality invariant state transformation f : S × T → S as a mapping that preserves the Q-values Q(s, a) = Q(f (s, ν), a) for all s ∈ S, a ∈ A and ν ∈ T .</p><p>where ν are the parameters of f (·), drawn from the set of all possible parameters T . One example of such transformations are the random image translations successfully applied in the previous section.</p><p>For every state, the transformations allow the generation of several surrogate states with the same Q-values, thus providing a mechanism to reduce the variance of Q-function estimation. In particular, for an arbitrary distribution of states µ(·) and policy π, instead of using a single sample s * ∼ µ(·), a * ∼ π(·|s * ) estimation of the following expectation</p><formula xml:id="formula_1">E s∼µ(·) a∼π(·|s) [Q(s, a)] ≈ Q(s * , a * )</formula><p>we can instead generate K samples via random transformations and obtain an estimate with lower variance</p><formula xml:id="formula_2">E s∼µ(·) a∼π(·|s) [Q(s, a)] ≈ 1 K K k=1 Q(f (s * , ν k ), a k ) where ν k ∈ T and a k ∼ π(·|f (s * , ν k )).</formula><p>This suggests two distinct ways to regularize Q-function. First, we use the data augmentation to compute the target values for every transition tuple (s i , a i , r i , s i ) as</p><formula xml:id="formula_3">y i = r i + γ 1 K K k=1 Q θ (f (s i , ν i,k ), a i,k ) where a i,k ∼ π(·|f (s i , ν i,k ))<label>(1)</label></formula><p>where ν i,k ∈ T corresponds to a transformation parameter of s i . Then the Q-function is updated using these targets through an SGD update using learning rate λ θ</p><formula xml:id="formula_4">θ ← θ − λ θ ∇ θ 1 N N i=1 (Q θ (f (s i , ν i ), a i ) − y i ) 2 .<label>(2)</label></formula><p>In tandem, we note that the same target from Equation (1) can be used for different augmentations of s i , resulting in the second regularization approach</p><formula xml:id="formula_5">θ ← θ − λ θ ∇ θ 1 N M N,M i=1,m=1 (Q θ (f (s i , ν i,m ), a i ) − y i ) 2 .<label>(3)</label></formula><p>When both regularization methods are used, ν i,m and ν i,k are drawn independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Our approach: Data-regularized Q (DrQ)</head><p>Our approach, DrQ, is the union of the three separate regularization mechanisms introduced above: For the experiments in this paper, we pair DrQ with SAC <ref type="bibr" target="#b21">[22]</ref> and DQN <ref type="bibr" target="#b42">[43]</ref>, popular model-free algorithms for control in continuous and discrete action spaces respectively. We select image shifts as the class of image transformations f , with ν ± 4, as explained in Section 3.1. For target Q and Q augmentation we use [K=2,M=2] respectively. <ref type="figure">Figure 2</ref> shows DrQ and ablated versions, demonstrating clear gains over unaugmented SAC. A more extensive ablation can be found in Appendix F. </p><formula xml:id="formula_6">number of Q augmentations M . for each timestep t = 1..T do a t ∼ π(·|s t ) s t ∼ p(·|s t , a t ) D ← D ∪ (s t , a t , r(s t , a t ), s t ) UPDATECRITIC(D) UPDATEACTOR(D)</formula><p>Data augmentation is applied to the samples for actor training as well. end for procedure UPDATECRITIC(D)</p><formula xml:id="formula_7">{(s i , a i , r i , s i )} N i=1 ∼ D Sample a mini batch ν i,k ν i,k ∼ U(T ), i = 1..N, k = 1..K Sample parameters of target augmentations for each i = 1..N do a i ∼ π(·|s i ) or a i,k ∼ π(·| f (s i , ν i,k )), k = 1..K Q i = Q θ (s i , a i ) orQ i = 1 K K k=1 Q θ (f (s i , ν i,k ), a i,k ) y i ← r(s i , a i ) + γQ i end for {ν i,m |ν i,m ∼ U(T ), i = 1..N, m = 1..M } Sample parameters of Q augmentations J Q (θ) = 1 N N i=1 (Q θ (s i , a i ) − y i ) 2 or J Q (θ) = 1 N M N,M i,m=1 (Q θ (f (s i , ν i,m ), a i ) − y i ) 2 θ ← θ − λ θ ∇ θ J Q (θ) Update the critic θ ← (1 − τ )θ + τ θ</formula><p>Update the critic target end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we evaluate our algorithm (DrQ) on the two commonly used benchmarks based on the DeepMind control suite <ref type="bibr" target="#b50">[51]</ref>, namely the PlaNet <ref type="bibr" target="#b23">[24]</ref> and Dreamer <ref type="bibr" target="#b22">[23]</ref> setups. Throughout these experiments all hyper-parameters of the algorithm are kept fixed: the actor and critic neural networks are trained using the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with default parameters and a mini-batch size of 512. For SAC, the soft target update rate τ is 0.01, initial temperature is 0.1, and target network and the actor updates are made every 2 critic updates (as in <ref type="bibr" target="#b59">[60]</ref>). We use the image encoder architecture from SAC-AE <ref type="bibr" target="#b59">[60]</ref> and follow their training procedure. The full set of parameters is in Appendix B.   <ref type="figure">Figure 3</ref>: The PlaNet benchmark. Our algorithm (DrQ [K=2,M=2]) outperforms the other methods and demonstrates the state-of-the-art performance. Furthermore, on several tasks DrQ is able to match the upper-bound performance of SAC trained directly on internal state, rather than images. Finally, our algorithm not only shows improved sample-efficiency relative to other approaches, but is also faster in terms of wall clock time. Following <ref type="bibr" target="#b26">[27]</ref>, the models are trained using 10 different seeds; for every seed the mean episode returns are computed every 10000 environment steps, averaging over 10 episodes. All figures plot the mean performance over the 10 seeds, together with ± 1 standard deviation shading. We compare our DrQ approach to leading model-free and model-based approaches: PlaNet <ref type="bibr" target="#b23">[24]</ref>, SAC-AE <ref type="bibr" target="#b59">[60]</ref>, SLAC <ref type="bibr" target="#b37">[38]</ref>, CURL <ref type="bibr" target="#b49">[50]</ref> and Dreamer <ref type="bibr" target="#b22">[23]</ref>. The comparisons use the results provided by the authors of the corresponding papers.</p><formula xml:id="formula_8">500k step scores DrQ (Ours) CURL PlaNet SAC-AE SLAC SAC State Finger Spin 938±103 874±151 718±40 914±107 771±203 927±43 Cartpole Swingup 868±10 861±30 787±46 730±152 - 870±7 Reacher Easy 942±71 904±94 588±471 601±135 - 975±5 Cheetah Run 660±96 500±91 568±21 544±50 629±74 772±60 Walker Walk 921±45 906±56 478±164 858±82 865±97 964±8 Ball In Cup Catch 963±9 958±13 939±43 810±121 959±4 979±6 100k step scores Finger Spin 901±104 779±108 560±77 747±130 680±130 672±76 Cartpole Swingup 759±92 592±170 563±73 276±38 - 812±45 Reacher Easy 601±213 517±113 82±174 225±164 - 919±123 Cheetah Run 344±67 307±48 165±123 252±173 391±47 * 228±95 Walker Walk 612±164 344±132 221±43 395±58 428±74 604±317 Ball In Cup Catch 913±53 772±241 710±217 338±196 607±173 957±26</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DeepMind Control Suite Experiments</head><p>PlaNet Benchmark <ref type="bibr" target="#b23">[24]</ref> consists of six challenging control tasks from <ref type="bibr" target="#b50">[51]</ref> with different traits. The benchmark specifies a different action-repeat hyper-parameter for each of the six tasks 2 . Following common practice <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b42">43]</ref>, we report the performance using true environment steps, thus are invariant to the action-repeat hyper-parameter. Aside from action-repeat, all other hyper-parameters of our algorithm are fixed across the six tasks, using the values previously detailed. <ref type="figure">Figure 3</ref> compares DrQ [K=2,M=2] to PlaNet <ref type="bibr" target="#b23">[24]</ref>, SAC-AE <ref type="bibr" target="#b59">[60]</ref>, CURL <ref type="bibr" target="#b49">[50]</ref>, SLAC <ref type="bibr" target="#b37">[38]</ref>, and an upper bound performance provided by SAC <ref type="bibr" target="#b21">[22]</ref> that directly learns from internal states. We use the version of SLAC that performs one gradient update per an environment step to ensure a fair comparison to other approaches. DrQ achieves state-of-the-art performance on this benchmark on all the tasks, despite being much simpler than other methods. Furthermore, since DrQ does not learn a model <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref> or any auxiliary tasks <ref type="bibr" target="#b49">[50]</ref>, the wall clock time also compares favorably to the other methods. In <ref type="table" target="#tab_3">Table 1</ref> we also compare performance given at a fixed number of environment interactions (e.g. 100k and 500k). Furthermore, in Appendix G we demonstrate that DrQ is robust to significant changes in hyper-parameter settings.</p><p>Dreamer Benchmark is a more extensive testbed that was introduced in Dreamer <ref type="bibr" target="#b22">[23]</ref>, featuring a diverse set of tasks from the DeepMind control suite. Tasks involving sparse reward were excluded (e.g. Acrobot and Quadruped) since they require modification of SAC to incorporate multi-step returns <ref type="bibr" target="#b3">[4]</ref>, which is beyond the scope of this work. We evaluate on the remaining 15 tasks, fixing the action-repeat hyper-parameter to 2, as in Dreamer <ref type="bibr" target="#b22">[23]</ref>.</p><p>We compare DrQ [K=2,M=2] to Dreamer <ref type="bibr" target="#b22">[23]</ref> and the upper-bound performance of SAC <ref type="bibr" target="#b21">[22]</ref> from states <ref type="bibr" target="#b2">3</ref> . Again, we keep all the hyper-parameters of our algorithm fixed across all the tasks. In <ref type="figure">Figure 4</ref>, DrQ demonstrates the state-of-the-art results by collectively outperforming Dreamer <ref type="bibr" target="#b22">[23]</ref>, although Dreamer is superior on 3 of the 15 tasks (Walker Run, Cartpole Swingup Sparse and Pendulum Swingup). On many tasks DrQ approaches the upper-bound performance of SAC <ref type="bibr" target="#b21">[22]</ref> trained directly on states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Atari 100k Experiments</head><p>We evaluate DrQ [K=1,M=1] on the recently introduced Atari 100k <ref type="bibr" target="#b30">[31]</ref> benchmark -a sampleconstrained evaluation for discrete control algorithms. The underlying RL approach to which DrQ is applied is a DQN, combined with double Q-learning <ref type="bibr" target="#b52">[53]</ref>, n-step returns <ref type="bibr" target="#b41">[42]</ref>, and dueling critic architecture <ref type="bibr" target="#b55">[56]</ref>. As per common practice <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b53">54]</ref>, we evaluate our agent for 125k environment steps at the end of training and average its performance over 5 random seeds. <ref type="figure" target="#fig_3">Figure 5</ref> shows the median human-normalized episode returns performance (as in <ref type="bibr" target="#b42">[43]</ref>) of the underlying model, which we refer to as Efficient DQN, in pink. When DrQ is added there is a significant increase in performance (cyan), surpassing OTRainbow <ref type="bibr" target="#b31">[32]</ref> and Data Efficient Rainbow <ref type="bibr" target="#b53">[54]</ref>. DrQ is also superior to CURL <ref type="bibr" target="#b49">[50]</ref> that uses an auxiliary loss built on top of a hybrid between OTRainbow and Efficient rainbow. DrQ combined with Efficient DQN thus achieves state-of-the-art performance, despite being significantly simpler than the other approaches. The experimental setup is detailed in Appendix C and full results can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Computer Vision Data augmentation via image transformations has been used to improve generalization since the inception of convolutional networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>  <ref type="figure">Figure 4</ref>: The Dreamer benchmark. Our method (DrQ [K=2,M=2]) again demonstrates superior performance over Dreamer on 12 out 15 selected tasks. In many cases it also reaches the upper-bound performance of SAC that learns directly from states.</p><p>Regularization in RL Some early attempts to learn RL function approximators used 2 regularization of the Q <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref> function. Another approach is entropy regularization [62, <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">57]</ref>, where causal entropy is added to the rewards, making the Q-function smoother and facilitating optimization <ref type="bibr" target="#b1">[2]</ref>. Prior work has explored regularization of the neural network approximator in deep RL, e.g. using dropout <ref type="bibr" target="#b18">[19]</ref> and cutout <ref type="bibr" target="#b10">[11]</ref> techniques. See <ref type="bibr" target="#b39">[40]</ref> for a comprehensive evaluation of different network regularization methods. In contrast, our approach directly regularizes the Q-function in a data-driven way that incorporates knowledge of task invariances, as opposed to generic priors. Median human normalized returns Generalization between Tasks and Domains A range of datasets have been introduced with the explicit aim of improving generalization in RL through deliberate variation of the scene colors/textures/backgrounds/viewpoints. These include Robot Learning in Homes <ref type="bibr" target="#b20">[21]</ref>, Meta-World <ref type="bibr" target="#b60">[61]</ref>, the ProcGen benchmark <ref type="bibr" target="#b9">[10]</ref>. There are also domain randomization techniques <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b48">49]</ref> which synthetically apply similar variations, but assume control of the data generation procedure, in contrast to our method. Furthermore, these works address generalization between domains (e.g. synthetic-toreal or different game levels), whereas our work focuses on a single domain and task. In concurrent work, RAD <ref type="bibr" target="#b35">[36]</ref> also demonstrates that image augmentation can improve sample efficiency and generalization of RL algorithms. However, RAD represents a specific instantiation of our algorithm when [K=1,M=1] and different image augmentations are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Control from Pixels</head><p>There are a variety of methods addressing the sample-efficiency of RL algorithms that directly learn from pixels. The most prominent approaches for this can be classified into two groups, model-based and model-free methods. The model-based methods attempt to learn the system dynamics in order to acquire a compact latent representation of high-dimensional observations to later perform policy search <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23]</ref>. In contrast, the model-free methods either learn the latent representation indirectly by optimizing the RL objective <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref> or by employing auxiliary losses that provide additional supervision <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b15">16]</ref>. Our approach is complementary to these methods and can be combined with them to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced a simple regularization technique that significantly improves the performance of SAC trained directly from image pixels on standard continuous control tasks. Our method is easy to implement and adds a negligible computational burden. We compared our method to state-of-the-art approaches on both DeepMind control suite, where we demonstrated that it outperforms them on the majority of tasks, and Atari 100k benchmarks, where it outperforms other methods in the median metric. Furthermore, we demonstrate the method to be robust to the choice of hyper-parameters.</p><p>[62] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence -Volume 3, 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extended Background</head><p>Reinforcement Learning from Images We formulate image-based control as an infinite-horizon partially observable Markov decision process (POMDP) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>. An POMDP can be described as the tuple <ref type="figure">(O, A, p, r, γ)</ref>, where O is the high-dimensional observation space (image pixels), A is the action space, the transition dynamics p = P r(o t |o ≤t , a t ) capture the probability distribution over the next observation o t given the history of previous observations o ≤t and current action a t , r : O × A → R is the reward function that maps the current observation and action to a reward r t = r(o ≤t , a t ), and γ ∈ [0, 1) is a discount factor. Per common practice <ref type="bibr" target="#b42">[43]</ref>, throughout the paper the POMDP is converted into an MDP <ref type="bibr" target="#b5">[6]</ref> by stacking several consecutive image observations into a state s t = {o t , o t−1 , o t−2 , . . .}. For simplicity we redefine the transition dynamics p = P r(s t |s t , a t ) and the reward function r t = r(s t , a t ). We then aim to find a policy π(a t |s t ) that maximizes the</p><formula xml:id="formula_9">cumulative discounted return E π [ ∞ t=1 γ t r t |a t ∼ π(·|s t ), s t ∼ p(·|s t , a t ), s 1 ∼ p(·)].</formula><p>Soft Actor-Critic The Soft Actor-Critic (SAC) <ref type="bibr" target="#b21">[22]</ref> learns a state-action value function Q θ , a stochastic policy π θ and a temperature α to find an optimal policy for an MDP (S, A, p, r, γ) by optimizing a γ-discounted maximum-entropy objective <ref type="bibr">[62]</ref>. θ is used generically to denote the parameters updated through training in each part of the model. The actor policy π θ (a t |s t ) is a parametric tanh-Gaussian that given s t samples a t = tanh(µ θ (s t ) + σ θ (s t ) ), where ∼ N (0, 1) and µ θ and σ θ are parametric mean and standard deviation.</p><p>The policy evaluation step learns the critic Q θ (s t , a t ) network by optimizing a single-step of the soft Bellman residual</p><formula xml:id="formula_10">J Q (D) = E (st,at,s t )∼D a t ∼π(·|s t ) [(Q θ (s t , a t ) − y t ) 2 ] y t = r(s t , a t ) + γ[Q θ (s t , a t ) − α log π θ (a t |s t )],</formula><p>where D is a replay buffer of transitions, θ is an exponential moving average of the weights as done in <ref type="bibr" target="#b38">[39]</ref>. SAC uses clipped double-Q learning <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b19">20]</ref>, which we omit from our notation for simplicity but employ in practice.</p><p>The policy improvement step then fits the actor policy π θ (a t |s t ) network by optimizing the objective</p><formula xml:id="formula_11">J π (D) = E st∼D [D KL (π θ (·|s t )|| exp{ 1 α Q θ (s t , ·)})].</formula><p>Finally, the temperature α is learned with the loss</p><formula xml:id="formula_12">J α (D) = E st∼D at∼π θ (·|st) [−α log π θ (a t |s t ) − αH],</formula><p>whereH ∈ R is the target entropy hyper-parameter that the policy tries to match, which in practice is usually set toH = −|A|.</p><p>Deep Q-learning DQN <ref type="bibr" target="#b42">[43]</ref> also learns a convolutional neural net to approximate Q-function over states and actions. The main difference is that DQN operates on discrete actions spaces, thus the policy can be directly inferred from Q-values. The parameters of DQN are updated by optimizing the squared residual error</p><formula xml:id="formula_13">J Q (D) = E (st,at,s t )∼D [(Q θ (s t , a t ) − y t ) 2 ] y t = r(s t , a t ) + γ max a Q θ (s t , a ).</formula><p>In practice, the standard version of DQN is frequently combined with a set of tricks that improve performance and training stability, wildly known as Rainbow <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The DeepMind Control Suite Experiments Setup</head><p>Our PyTorch SAC <ref type="bibr" target="#b21">[22]</ref> implementation is based off of <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Actor and Critic Networks</head><p>We employ clipped double Q-learning <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b19">20]</ref> for the critic, where each Q-function is parametrized as a 3-layer MLP with ReLU activations after each layer except of the last. The actor is also a 3-layer MLP with ReLUs that outputs mean and covariance for the diagonal Gaussian that represents the policy. The hidden dimension is set to 1024 for both the critic and actor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Encoder Network</head><p>We employ an encoder architecture from <ref type="bibr" target="#b59">[60]</ref>. This encoder consists of four convolutional layers with 3 × 3 kernels and 32 channels. The ReLU activation is applied after each conv layer. We use stride to 1 everywhere, except of the first conv layer, which has stride 2. The output of the convnet is feed into a single fully-connected layer normalized by LayerNorm <ref type="bibr" target="#b2">[3]</ref>. Finally, we apply tanh nonlinearity to the 50 dimensional output of the fully-connected layer. We initialize the weight matrix of fully-connected and convolutional layers with the orthogonal initialization <ref type="bibr" target="#b45">[46]</ref> and set the bias to be zero.</p><p>The actor and critic networks both have separate encoders, although we share the weights of the conv layers between them. Furthermore, only the critic optimizer is allowed to update these weights (e.g. we stop the gradients from the actor before they propagate to the shared conv layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Training and Evaluation Setup</head><p>Our agent first collects 1000 seed observations using a random policy. The further training observations are collected by sampling actions from the current policy. We perform one training update every time we receive a new observation. In cases where we use action repeat, the number of training observations is only a fraction of the environment steps (e.g. a 1000 steps episode at action repeat 4 will only results into 250 training observations). We evaluate our agent every 10000 true environment steps by computing the average episode return over 10 evaluation episodes. During evaluation we take the mean policy action instead of sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 PlaNet and Dreamer Benchmarks</head><p>We consider two evaluation setups that were introduced in PlaNet <ref type="bibr" target="#b23">[24]</ref> and Dreamer <ref type="bibr" target="#b22">[23]</ref>, both using tasks from the DeepMind control suite <ref type="bibr" target="#b50">[51]</ref>. The PlaNet benchmark consists of six tasks of various traits. Importantly, the benchmark proposed to use a different action repeat hyper-parameter for each task, which we summarize in <ref type="table">Table 2</ref>.</p><p>The Dreamer benchmark considers an extended set of tasks, which makes it more difficult that the PlaNet setup. Additionally, this benchmark requires to use the same set hyper-parameters for each task, including action repeat (set to 2), which further increases the difficulty.  <ref type="table">Table 2</ref>: The action repeat hyper-parameter used for each task in the PlaNet benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Pixels Preprocessing</head><p>We construct an observational input as an 3-stack of consecutive frames <ref type="bibr" target="#b42">[43]</ref>, where each frame is a RGB rendering of size 84 × 84 from the 0th camera. We then divide each pixel by 255 to scale it down to [0, 1] range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Other Hyper Parameters</head><p>Due to computational constraints for all the continuous control ablation experiments in the main paper and appendix we use a minibatch size of 128, while for the main results we use minibatch of size 512. In <ref type="table" target="#tab_7">Table 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The Atari 100k Experiments Setup</head><p>For ease of reproducibility in <ref type="table" target="#tab_8">Table 4</ref> we report the hyper-parameter settings used in the Atari 100k experiments. We largely reuse the hyper-parameters from OTRainbow <ref type="bibr" target="#b31">[32]</ref>, but adapt them for DQN <ref type="bibr" target="#b42">[43]</ref>. Per common practise, we average performance of our agent over 5 random seeds. The evaluation is done for 125k environment steps at the end of training for 100k environment steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Full Atari 100k Results</head><p>Besides reporting in <ref type="figure" target="#fig_3">Figure 5</ref> median human-normalized episode returns over the 26 Atari games used in <ref type="bibr" target="#b30">[31]</ref>, we also provide the mean episode return for each individual game in <ref type="table">Table 5</ref>. Human/Random scores are taken from <ref type="bibr" target="#b53">[54]</ref> to be consistent with the established setup.  <ref type="table">Table 5</ref>: Mean episode returns on each of 26 Atari games from the setup in <ref type="bibr" target="#b30">[31]</ref>. The results are recorded at the end of training and averaged across 5 random seeds (the CURL's results are averaged over 3 seeds as reported in <ref type="bibr" target="#b49">[50]</ref>). On each game we mark as bold the highest score. Our method demonstrates better overall performance (as reported in <ref type="figure" target="#fig_3">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Image Augmentations Ablation</head><p>Following <ref type="bibr" target="#b6">[7]</ref>, we evaluate popular image augmentation techniques, namely random shifts, cutouts, vertical and horizontal flips, random rotations and imagewise intensity jittering. Below, we provide a comprehensive overview of each augmentation. Furthermore, we examine effectiveness of these techniques in <ref type="figure">Figure 6</ref>.</p><p>Random Shift We bring our attention to random shifts that are commonly used to regularize neural networks trained on small images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. In our implementation of this method images of size 84 × 84 are padded each side by 4 pixels (by repeating boundary pixels) and then randomly cropped back to the original 84 × 84 size.</p><p>Cutout Cutouts introduced in <ref type="bibr" target="#b13">[14]</ref> represent a generalization of Dropout <ref type="bibr" target="#b27">[28]</ref>. Instead of masking individual pixels cutouts mask square regions. Since image pixels can be highly correlated, this technique is proven to improve training of neural networks.</p><p>Horizontal/Vertical Flip This technique simply flips an image either horizontally or vertically with probability 0.1.</p><p>Rotate Here, an image is rotated by r degrees, where r is uniformly sampled from [−5, −5].</p><p>Intensity Each N × C × 84 × 84 image tensor is multiplied by a single scalar s, which is computed as s = µ + σ · clip(r, −2, 2), where r ∼ N (0, 1). For our experiments we use µ = 1.0 and σ = 0.1.   <ref type="figure">Figure 6</ref>: Various image augmentations have different effect on the agent's performance. Overall, we conclude that using image augmentations helps to fight overfitting. Moreover, we notice that random shifts proven to be the most effective technique for tasks from the DeepMind control suite.</p><p>Implementation Finally, we provide Python-like implementation for the aforementioned augmentations powered by Kornia <ref type="bibr" target="#b44">[45]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F K and M Hyper-parameters Ablation</head><p>We further ablate the K,M hyper-parameters from Algorithm 1 to understand their effect on performance. In <ref type="figure" target="#fig_7">Figure 7</ref> we observe that increase values of K,M improves the agent's performance. We choose to use the [K=2,M=2] parametrization as it strikes a good balance between performance and computational demands.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Robustness Investigation</head><p>To demonstrate the robustness of our approach <ref type="bibr" target="#b26">[27]</ref>, we perform a comprehensive study on the effect different hyper-parameter choices have on performance. A review of prior work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref> shows consistent values for discount γ = 0.99 and target update rate τ = 0.01 parameters, but variability on network architectures, mini-batch sizes, learning rates. Since our method is based on SAC <ref type="bibr" target="#b21">[22]</ref>, we also check whether the initial value of the temperature is important, as it plays a crucial role in the initial phase of exploration. We omit search over network architectures since <ref type="figure" target="#fig_1">Figure 1b</ref> shows our method to be robust to the exact choice. We thus focus on three hyper-parameters: mini-batch size, learning rate, and initial temperature.   <ref type="figure">Figure 8</ref>: A robustness study of our algorithm (DrQ) to changes in mini-batch size, learning rate, and initial temperature hyper-parameters on three different tasks from <ref type="bibr" target="#b50">[51]</ref>. Each row corresponds to a different mini-batch size. The low variance of the curves and heat-maps shows DrQ to be generally robust to exact hyper-parameter settings. <ref type="figure">Figure 8</ref> shows performance curves for each configuration as well as a heat map over the mean performance of the final evaluation episodes, similar to <ref type="bibr" target="#b41">[42]</ref>. Our method demonstrates good stability and is largely invariant to the studied hyper-parameters. We emphasize that for simplicity the experiments in Section 4 use the default learning rate of Adam <ref type="bibr" target="#b32">[33]</ref> (0.001), even though it is not always optimal.</p><p>H Improved Data-Efficient Reinforcement Learning from Pixels</p><p>Our method allows to generate many various transformations from a training observation due to the data augmentation strategy. Thus, we further investigate whether performing more training updates per an environment step can lead to even better sample-efficiency. Following <ref type="bibr" target="#b54">[55]</ref> we compare a single update with a mini-batch of 512 transitions with 4 updates with 4 different mini-batches of size 128 samples each. Performing more updates per an environment step leads to even worse over-fitting on some tasks without data augmentation (see <ref type="figure">Figure 9a</ref>), while our method DrQ, that takes advantage of data augmentation, demonstrates improved sample-efficiency (see <ref type="figure">Figure 9b</ref>).  <ref type="figure">Figure 9</ref>: In the data-efficient regime, where we measure performance at 100k environment steps, DrQ is able to enhance its efficiency by performing more training iterations per an environment step. This is because DrQ allows to generate various transformations for a training observation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Enc (0.22M) SAC + DQN Enc (0.24M) SAC + Dreamer Enc (0.80M) SAC + SAC-AE Enc (1.91M) SAC + D4PG Enc (2.44M) Enc (0.22M) SAC + DQN Enc (0.24M) SAC + Dreamer Enc (0.80M) SAC + SAC-AE Enc (1.91M) SAC + D4PG Enc (2.44M) Enc (0.22M) SAC + DQN Enc (0.24M) SAC + Dreamer Enc (0.80M) SAC + SAC-AE Enc (1.91M) SAC + D4PG Enc (2.44M) (b) SAC with image shift augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The performance of SAC trained from pixels on the DeepMind control suite using image encoder networks of different capacity (network architectures taken from recent RL algorithms, with parameter count indicated). (a): unmodified SAC. Task performance can be seen to get worse as the capacity of the encoder increases, indicating over-fitting. For Walker Walk (right), all architectures provide mediocre performance, demonstrating the inability of SAC to train directly from pixels on harder problems. (b):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The Atari 100k benchmark. Compared to a set of leading baselines, our method (DrQ [K=1,M=1], combined with Efficient DQN) achieves the state-of-the-art performance, despite being considerably simpler. Note the large improvement that results from adding DrQ to Efficient DQN (pink vs cyan). By contrast, the gains from CURL, that utilizes tricks from both Data Efficient Rainbow and OTRainbow, are more modest over the underlying RL methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>import torch import torch.nn as nn import kornia.augmentation as aug random_shift = nn.Sequential(nn.ReplicationPad2d(4),aug.RandomCrop((84, 84))) cutout = aug.RandomErasing(p=0.5) h_flip = aug.RandomHorizontalFlip(p=0.1) v_flip = aug.RandomVerticalFlip(p=0.1) rotate = aug.RandomRotation(degrees=5.0) intensity = Intensity(scale=0.05) class Intensity(nn.Module): def __init__(self, scale): super().__init__() self.scale = scale def forward(self, x): r = torch.randn((x.size(0), 1, 1, 1), device=x.device) noise = 1.0 + (self.scale * r.clamp(-2.0, 2.0)) return x * noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 , M = 1 ]</head><label>11</label><figDesc>= 1, M = 1] DrQ [K = 2, M = 2] DrQ [K = 4, M = 4] DrQ [K = 8, M = 8] = 1, M = 1] DrQ [K = 2, M = 2] DrQ [K = 4, M = 4] DrQ [K = 8, M = 8] 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 ) = 1, M = 1] DrQ [K = 2, M = 2] DrQ [K = 4, M = 4] DrQ [K = 8, M = 8] DrQ [K = 2, M = 2] DrQ [K = 4, M = 4] DrQ [K = 8, M = 8]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Increasing values of K,M hyper-parameters generally correlates positively with the agent's performance, especially on the harder tasks, such as Cheetah Run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Our method DrQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Total number of environment steps T , mini-batch size N , learning rate λ θ , target network update rate τ , image transformation f , number of target Q augmentations K,</figDesc><table><row><cell>Episode Return</cell><cell>0 100 200 300 400 500 600 800 700</cell><cell>0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 ) Cheetah Run Augmentation SAC No Augmentation DrQ [K = 1, M = 1] DrQ [K = 2, M = 1] DrQ [K = 2, M = 2]</cell><cell>Episode Return</cell><cell>1200 0 200 400 600 800 1000</cell><cell>Reacher Easy 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 ) Augmentation SAC No Augmentation DrQ [K = 1, M = 1] DrQ [K = 2, M = 1] DrQ [K = 2, M = 2]</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>Walker Walk Environment Steps (×10 6 ) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Augmentation SAC No Augmentation DrQ [K = 1, M = 1] DrQ [K = 2, M = 1] DrQ [K = 2, M = 2]</cell></row><row><cell cols="9">Figure 2: Different combinations of our three regularization techniques on tasks from [51] using</cell></row><row><cell cols="9">SAC. Black: standard SAC. Blue: DrQ [K=1,M=1], SAC augmented with random shifts. Red: DrQ</cell></row><row><cell cols="9">[K=2,M=1], random shifts + Target Q augmentations. Purple: DrQ [K=2,M=2], random shifts</cell></row><row><cell cols="9">+ Target Q + Q augmentations. All three regularization methods correspond to Algorithm 1 with</cell></row><row><cell cols="9">different hyperparameters K,M and independently provide beneficial gains over unaugmented SAC.</cell></row><row><cell cols="9">Note that DrQ [K=1,M=1] exactly recovers the concurrent work of RAD [36] up to a particular</cell></row><row><cell cols="6">choice of hyper-parameters and data augmentation type.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Algorithm 1 DrQ: Data-regularized Q applied to a generic off-policy actor critic algorithm.</cell></row><row><cell cols="6">Black: unmodified off-policy actor-critic.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Orange: image transformation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Green: target Q augmentation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Blue: Q augmentation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hyperparameters:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The PlaNet benchmark at 100k and 500k environment steps. Our method (DrQ [K=2,M=2]) outperforms other approaches in both the data-efficient (100k) and asymptotic performance (500k) regimes.* : SLAC uses 100k exploration steps which are not counted in the reported values. By contrast, DrQ only uses 1000 exploration steps which are included in the overall step count.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. Following AlexNet<ref type="bibr" target="#b34">[35]</ref>, they have become a standard part of training pipelines. For object classification tasks, the transformations are selected to avoid changing the semantic category, i.e. translations, scales, color shifts, etc. Perturbed versions of input examples are used to expand the training set and no adjustment to the training algorithm is needed. While a similar set of transformations are potentially applicable to control tasks, the RL context does require modifications to be made to the underlying algorithm.Data augmentation methods have also been used in the context of self-supervised learning.<ref type="bibr" target="#b14">[15]</ref> use per-exemplar perturbations in a unsupervised classification framework. More recently, a several approaches<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref> have used invariance to imposed image transformations in contrastive learning schemes, producing state-of-the-art results on downstream recognition tasks. By contrast, our scheme addresses control tasks, utilizing different types of invariance.</figDesc><table><row><cell></cell><cell cols="2">1200</cell><cell></cell><cell>Ball In Cup Catch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cartpole Balance</cell><cell></cell><cell></cell><cell>1200</cell><cell></cell><cell>Cartpole Balance Sparse</cell></row><row><cell></cell><cell cols="2">1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell></cell></row><row><cell>Episode Return</cell><cell></cell><cell>200 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4</cell><cell>0.5 SAC State Agent Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell></cell><cell>200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4</cell><cell>0.5 SAC State Agent Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell>0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4</cell><cell>0.5 SAC State Agent Dreamer DrQ (Ours)</cell></row><row><cell>Episode Return</cell><cell cols="2">0 200 400 600 1000 800</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Cartpole Swingup</cell><cell>1.0 SAC State Agent Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell cols="2">1000 800 200 0 200 400 600</cell><cell>0.0</cell><cell>Cartpole Swingup Sparse 0.2 0.4 0.6 0.8 Environment Steps (×10 6 )</cell><cell>1.0 SAC State Agent Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell cols="2">Cheetah Run Environment Steps (×10 6 ) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Agent SAC State Dreamer DrQ (Ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Finger Spin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hopper Hop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hopper Stand</cell></row><row><cell></cell><cell cols="2">1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell></cell></row><row><cell>Episode Return</cell><cell></cell><cell>0 200 400 600 800</cell><cell></cell><cell></cell><cell>Agent SAC State Dreamer DrQ (Ours)</cell><cell cols="2">Episode Return</cell><cell>0 100 200 300</cell><cell></cell><cell></cell><cell>Agent SAC State Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell>0 200 400 600 800</cell><cell></cell><cell>Agent SAC State Dreamer DrQ (Ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 )</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 )</cell><cell></cell><cell></cell><cell cols="2">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pendulum Swingup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Reacher Easy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Reacher Hard</cell></row><row><cell>Episode Return</cell><cell cols="2">200 0 200 400 600 1000 800</cell><cell cols="3">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 ) Agent SAC State Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell cols="2">1200 1000 200 0 200 400 600 800</cell><cell cols="3">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 ) Agent SAC State Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell>1200 1000 200 0 200 400 600 800</cell><cell cols="2">Environment Steps (×10 6 ) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Agent SAC State Dreamer DrQ (Ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Walker Run</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Walker Stand</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Walker Walk</cell></row><row><cell></cell><cell></cell><cell>800</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell></cell></row><row><cell cols="2">Episode Return</cell><cell>0 200 400 600</cell><cell cols="3">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Environment Steps (×10 6 ) Agent SAC State Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell></cell><cell>200 400 600 800</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8</cell><cell>1.0 SAC State Agent Dreamer DrQ (Ours)</cell><cell>Episode Return</cell><cell>0 200 400 600 800</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8</cell><cell>1.0 SAC State Agent Dreamer DrQ (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>we provide a comprehensive overview of all the other hyper-parameters. An overview of used hyper-parameters in the DeepMind control suite experiments.</figDesc><table><row><cell>Parameter</cell><cell>Setting</cell></row><row><cell>Replay buffer capacity</cell><cell>100000</cell></row><row><cell>Seed steps</cell><cell>1000</cell></row><row><cell>Ablations minibatch size</cell><cell>128</cell></row><row><cell>Main results minibatch size</cell><cell>512</cell></row><row><cell>Discount γ</cell><cell>0.99</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Learning rate</cell><cell>10 −3</cell></row><row><cell>Critic target update frequency</cell><cell>2</cell></row><row><cell>Critic Q-function soft-update rate τ</cell><cell>0.01</cell></row><row><cell>Actor update frequency</cell><cell>2</cell></row><row><cell>Actor log stddev bounds</cell><cell>[−10, 2]</cell></row><row><cell>Init temperature</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>A complete overview of hyper parameters used in the Atari 100k experiments.</figDesc><table><row><cell>Parameter</cell><cell>Setting</cell></row><row><cell>Data augmentation</cell><cell>Random shifts and Intensity</cell></row><row><cell>Grey-scaling</cell><cell>True</cell></row><row><cell>Observation down-sampling</cell><cell>84 × 84</cell></row><row><cell>Frames stacked</cell><cell>4</cell></row><row><cell>Action repetitions</cell><cell>4</cell></row><row><cell>Reward clipping</cell><cell>[−1, 1]</cell></row><row><cell>Terminal on loss of life</cell><cell>True</cell></row><row><cell>Max frames per episode</cell><cell>108k</cell></row><row><cell>Update</cell><cell>Double Q</cell></row><row><cell>Dueling</cell><cell>True</cell></row><row><cell>Target network: update period</cell><cell>1</cell></row><row><cell>Discount factor</cell><cell>0.99</cell></row><row><cell>Minibatch size</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Optimizer: learning rate</cell><cell>0.0001</cell></row><row><cell>Optimizer: β 1</cell><cell>0.9</cell></row><row><cell>Optimizer: β 2</cell><cell>0.999</cell></row><row><cell>Optimizer:</cell><cell>0.00015</cell></row><row><cell>Max gradient norm</cell><cell>10</cell></row><row><cell>Training steps</cell><cell>100k</cell></row><row><cell>Evaluation steps</cell><cell>125k</cell></row><row><cell>Min replay size for sampling</cell><cell>1600</cell></row><row><cell>Memory size</cell><cell>Unbounded</cell></row><row><cell>Replay period every</cell><cell>1 step</cell></row><row><cell>Multi-step return length</cell><cell>10</cell></row><row><cell>Q network: channels</cell><cell>32, 64, 64</cell></row><row><cell>Q network: filter size</cell><cell>8 × 8, 4 × 4, 3 × 3</cell></row><row><cell>Q network: stride</cell><cell>4, 2, 1</cell></row><row><cell>Q network: hidden units</cell><cell>512</cell></row><row><cell>Non-linearity</cell><cell>ReLU</cell></row><row><cell>Exploration</cell><cell>-greedy</cell></row><row><cell>-decay</cell><cell>5000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Due to computational demands, experiments are restricted to a subset of tasks from<ref type="bibr" target="#b50">[51]</ref>: Walker Walk, Cartpole Swingup, and Finger Spin. These were selected to be diverse, requiring different behaviors including locomotion and goal reaching. A grid search is performed over minibatch sizes {128, 256, 512}, learning rates {0.0001, 0.0005, 0.001, 0.005}, and initial temperatures {0.005, 0.01, 0.05, 0.1}. We follow the experimental setup from Appendix B, except that only 3 seeds are used due to the computation limitations, but since variance is low the results are representative.</figDesc><table><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 948 919 942 932 941 907 935 930 927 958 814 956 897 942 902 805 Batch Size=128</cell><cell>0 200 400 600 800 1000</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.01</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.05</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell cols="2">0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0005 LR=0.005 LR=0.001 LR=0.0001 Init Temperature=0.1</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 948 963 954 952 964 946 927 925 943 955 924 933 940 937 906 873 Batch Size=256</cell><cell>0 200 400 600 800 1000</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.01</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.05</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell cols="2">0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0005 LR=0.005 LR=0.001 LR=0.0001 Init Temperature=0.1</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 959 936 950 958 932 951 942 956 934 956 791 957 932 921 930 941 Batch Size=512</cell><cell>0 200 400 600 800 1000</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.01</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005 Init Temperature=0.05</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell cols="2">0.2 Environment Steps (×10 6 ) 0.4 0.6 0.8 Learning Rate 1.0 LR=0.0005 LR=0.005 LR=0.001 LR=0.0001 Init Temperature=0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Walker Walk.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 732 831 864 775 835 839 714 806 838 854 780 791 831 850 823 811 Batch Size=128</cell><cell>0 200 400 600 800 1000</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.005 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.01 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.05 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.1</cell><cell>0.4 Learning Rate 0.5 LR=0.0005 LR=0.005 LR=0.001 LR=0.0001</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 817 852 863 831 839 863 865 830 840 845 851 831 851 858 855 835 Batch Size=256</cell><cell>0 200 400 600 800 1000</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.005 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.01 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.05 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.1</cell><cell>0.4 Learning Rate 0.5 LR=0.0005 LR=0.005 LR=0.001 LR=0.0001</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 837 851 860 845 853 856 827 849 855 850 856 831 855 838 832 860 Batch Size=512</cell><cell>0 200 400 600 800 1000</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.005 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.01 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.05 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>0 200 400 600 800 1000</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 Init Temperature=0.1</cell><cell>0.4 Learning Rate 0.5 LR=0.0005 LR=0.005 LR=0.001 LR=0.0001</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Cartpole Swingup.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Batch Size=128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.005</cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.01</cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.05</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Init Temperature=0.1</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 982 982 874 977 816 971 980 878 982 971 980 931 874 982 981 950</cell><cell></cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell cols="2">0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell></row><row><cell></cell><cell>Batch Size=256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.005</cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.01</cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.05</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Init Temperature=0.1</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 914 977 981 980 871 767 979 799 819 874 792 873 970 876 981 950</cell><cell></cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell cols="2">0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell></row><row><cell></cell><cell>Batch Size=512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.005</cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.01</cell><cell></cell><cell></cell><cell></cell><cell>Init Temperature=0.05</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Init Temperature=0.1</cell></row><row><cell>0.1 0.05 0.01 0.005 Init Temperature</cell><cell>0.0001 0.0005 0.001 0.005 Learning Rate 978 840 956 972 874 980 979 874 941 981 872 872 980 978 875 945</cell><cell></cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell>0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell><cell>Episode Return</cell><cell>1000 0 200 400 600 800</cell><cell>0.0</cell><cell cols="2">0.1 Environment Steps (×10 6 ) 0.2 0.3 0.4 Learning Rate 0.5 LR=0.0001 LR=0.0005 LR=0.001 LR=0.005</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This means the number of training observations is a fraction of the environment steps (e.g. an episode of 1000 steps with action-repeat 4 results in 250 training observations).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">No other publicly reported results are available for the other methods due to the recency of the Dreamer<ref type="bibr" target="#b22">[23]</ref> benchmark.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We would like to thank Danijar Hafner, Alex Lee, and Michael Laskin for sharing performance data for the Dreamer <ref type="bibr" target="#b22">[23]</ref> and PlaNet <ref type="bibr" target="#b23">[24]</ref>, SLAC <ref type="bibr" target="#b37">[38]</ref>, and CURL <ref type="bibr" target="#b49">[50]</ref> baselines respectively. Furthermore, we would like to thank Roberta Raileanu for helping with the architecture experiments. Finally, we would like to thank Ankesh Anand for helping us finding an error in our evaluation script for the Atari 100k benchmark experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06920</idno>
		<title level="m">Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding the impact of entropy on policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafarali</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11214</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dhruva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in randomdot stereograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A markovian decision process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indiana Univ. Math. J</title>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Dan C Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1102.0183</idno>
		<title level="m">High-performance neural networks for visual object classification</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Leveraging procedural generation to benchmark reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01588</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Quantifying generalization in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02341</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning actionable representations from visual observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymir</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01561</idno>
		<title level="m">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generalization and regularization in dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Farebrother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
		<idno>arXiv abs/1810.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressing function approximation error in actor-critic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Herke Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmassan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robot learning in homes: Improving generalization and reducing dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithyavairavan</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakashchand</forename><surname>Dhiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerrel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sehoon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05905</idno>
		<title level="m">Pieter Abbeel, et al. Soft actor-critic algorithms and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01603</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ali Eslami, and Aäron van den Oord. Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference On Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony R</forename><surname>Michael L Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Model-based reinforcement learning for atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blazej</forename><surname>Osinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00374</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Do recent advancements in model-based deep reinforcement learning really improve data efficiency? openreview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Kacper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kielak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerrel</forename><surname>Pinto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14990</idno>
		<title level="m">Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<pubPlace>Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Regularization matters in policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv abs</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01991</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bridging the gap between value and policy based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<editor>Icdar</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Robust visual domain randomization for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reda</forename><surname>Bahi Slaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Toth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv abs</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefrancq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00690</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. Deepmind control suite</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">When to use parametric models in reinforcement learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aslanides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">When to use parametric models in reinforcement learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hado P Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aslanides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Function optimization using connectionist reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<title level="m">Byron Boots, and Vikas Sindhwani. Manifold regularization for kernelized lstd. arXiv abs/1710.05387</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Soft actor-critic (sac) implementation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<ptr target="https://github.com/denisyarats/pytorch_sac" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Improving sample efficiency in model-free reinforcement learning from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01741</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10897</idno>
		<title level="m">Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
