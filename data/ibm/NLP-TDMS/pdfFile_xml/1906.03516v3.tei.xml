<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiCENet: Dimension-wise Convolutions for Efficient Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
						</author>
						<title level="a" type="main">DiCENet: Dimension-wise Convolutions for Efficient Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Neural Network</term>
					<term>Image Classification</term>
					<term>Object Detection</term>
					<term>Semantic Segmentation</term>
					<term>Efficient Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4% higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE basic building layer at the heart of convolutional neural networks (CNNs) is a convolutional layer that encodes spatial and channel-wise information simultaneously <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Learning representations using this layer is computationally expensive. Improving the efficiency of CNN architectures as well as convolutional layers is an active area of research. Most recent attempts have focused on improving the efficiency of CNN architectures using compression-and quantization-based methods (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>). Recently, several factorization-based methods have been proposed to improve the efficiency of standard convolutional layers (e.g. <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>). In particular, depth-wise separable convolutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> have gained a lot of attention (see <ref type="figure" target="#fig_1">Figure 1a</ref>). These convolutions have been used in several efficient state-of-the-art architectures, including neural search-based architectures <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>Separable convolutions factorize the standard convolutional layer in two steps: (1) a light-weight convolutional filter is applied to each input channel using depth-wise convolutions <ref type="bibr" target="#b8">[9]</ref> to learn spatial representations, and (2) a point-wise (1 × 1) convolution is applied to learn linear combinations between spatial representations. Though depth-wise convolutions are efficient, they do not encode channel-wise relationships. Therefore, separable convolutions rely on point-wise convolutions to encode channel-wise relationships. This puts a significant computational load on pointwise convolutions and makes them a computational bottleneck. For example, point-wise convolutions account for about 90% of total operations in ShuffleNetv2 <ref type="bibr" target="#b16">[16]</ref> and MobileNetv2 <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this paper, we introduce DiCENet, Dimension-wise Convolutions for Efficient Networks that encodes spatial and channel-wise representations efficiently. Our main contribution is the novel and generic module, the DiCE unit <ref type="figure" target="#fig_1">(Figure 1a</ref>), that is built using Dimension-wise Convolutions (DimConv) and</p><p>Top-1 accuracy (ImageNet) <ref type="bibr" target="#b22">21</ref> Manual-SepConv NAS-SepConv Manual-DiCE (b) Network with the DiCE unit (DiCENet) has better tasklevel generalization properties compared to networks with separable convolutions (MobileNet <ref type="bibr" target="#b6">[7]</ref>, MobileNetv2 <ref type="bibr" target="#b9">[10]</ref>, MobileNetv3 <ref type="bibr" target="#b13">[14]</ref>, MixNet <ref type="bibr" target="#b15">[15]</ref>, MNASNet <ref type="bibr" target="#b11">[12]</ref>). Here, Manual-SepConv and NAS-SepConv represents the models that uses separable convolution without and with neural architecture search (NAS), respectively. Manual-DiCE represents the DiCENet without NAS. On the ImageNet dataset, these networks have about 200-300 million floating point operations. See Section 6 for more details. Dimension-wise Fusion (DimFuse). The DimConv applies a light-weight convolutional filter across "each dimension" of the input tensor to learn local dimension-wise representations while DimFuse efficiently combines these dimension-wise representations to incorporate global information.</p><p>With DimConv and DimFuse, we build an efficient convolutional unit, the DiCE unit, that can be easily integrated into existing or new CNN architectures to improve their performance and efficiency. <ref type="figure" target="#fig_1">Figure 1</ref> shows that the DiCE unit is effective in comparison to widely-used separable convolutions. Compared to state-of-the-art manually designed networks (e.g., MobileNet <ref type="bibr" target="#b6">[7]</ref>, MobileNetv2 <ref type="bibr" target="#b9">[10]</ref>, and ShuffleNetv2 <ref type="bibr" target="#b16">[16]</ref>), DiCENet delivers significantly better performance. For example, for a network with about 300 million floating point operations (FLOPs), DiCENet is about 4% more accurate than MobileNetv2. Importantly, DiCENet, a manually designed network, delivers similar or better performance than neural architecture search (NAS) based methods. For example, DiCENet is 1.7% more accurate than MNASNet for a network with about 300 MFLOPs.</p><p>We empirically demonstrate in Section 6 and Section 7 that the DiCENet network, built by stacking DiCE units, achieves significant improvements on standard benchmarks across different tasks over existing networks. Compared to existing efficient networks, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices. For instance, DiCENet achieves about 3% higher mean average precision than MobileNetv3 <ref type="bibr" target="#b13">[14]</ref> and MixNet <ref type="bibr" target="#b15">[15]</ref> on the MS-COCO object detection task with SSD <ref type="bibr" target="#b17">[17]</ref> as a detection pipeline ( <ref type="figure" target="#fig_1">Fig. 1b</ref>). Our source code in PyTorch is open-source and is available at https://github.com/sacmehta/EdgeNets/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>CNN architecture designs: Recent success in visual recognition tasks, including classification, detection, and segmentation, can be attributed to the exploration of different CNN designs (e.g., <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b6">7]</ref>). The basic building layer in these networks is a standard convolutional layer, which is computationally expensive. Factorization-based methods improve the efficiency of these layers. For instance, flattened convolutions <ref type="bibr" target="#b5">[6]</ref> approximate a standard convolutional layer with three point-wise convolutions that are applied sequentially, one point-wise convolution per tensor dimension. These convolutions ignore spatial relationships between pixels and do not generalize across a wide variety of computer vision tasks (e.g., detection and segmentation) and large scale datasets (e.g., ImageNet and MS-COCO). To improve the efficiency of standard convolutions while maintaining the performance and generalization ability at scale, depth-wise separable convolutions <ref type="bibr" target="#b6">[7]</ref> are proposed that factorizes the standard convolutional layer into depth-wise and point-wise convolution layers. Most of the efficient CNN architectures are built using these separable convolutions, including MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, Shuf-fleNets <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">18]</ref>, and ESPNetv2 <ref type="bibr" target="#b10">[11]</ref>. In this work, we introduce dimension-wise convolutions that generalize depth-wise convolutions to all dimensions of the input tensor. We also introduce an efficient way for combining these dimension-wise representations. As confirmed by our experiments in Section 5, 6 and 7, the DiCE unit is more effective than separable convolutions. Neural architecture search: Recently, neural architecture searchbased methods have been proposed to automatically construct network architectures (e.g., <ref type="bibr">[12-14, 19, 20]</ref>). These methods search over a large network space (e.g., MNASNet <ref type="bibr" target="#b11">[12]</ref> searches over 8K different design choices) using a dictionary of pre-defined search space parameters, including different types of convolutional layers and kernel sizes, to find a heterogeneous network structure that satisfies optimization constraints, such as inference time. The proposed unit is novel and cannot be discovered using existing neural search-based methods (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">21]</ref>). However, we believe that better neural architectures can be discovered by adding the DiCE unit in neural search dictionary. Quantization, compression, and distillation: Network quantization-based approaches <ref type="bibr" target="#b23">[22]</ref><ref type="bibr" target="#b24">[23]</ref><ref type="bibr" target="#b25">[24]</ref><ref type="bibr" target="#b26">[25]</ref> approximate 32-bit full precision convolution operations with fewer bits. This improves inference speed and reduces the amount of memory required for storing network weights. Network compression-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">[26]</ref><ref type="bibr" target="#b28">[27]</ref><ref type="bibr" target="#b29">[28]</ref> improve the efficiency of a network by removing redundant weights and connections. Unlike network quantization and compression, distillation-based approaches <ref type="bibr" target="#b30">[29]</ref><ref type="bibr" target="#b31">[30]</ref><ref type="bibr" target="#b32">[31]</ref> improve the accuracy of (usually shallow) networks by supervising the training with large pre-trained networks. These approaches are effective for improving the efficiency of a network, including efficient architecture designs (e.g., <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>) and are orthogonal to our work. We believe that the efficiency of DiCENet can be further improved using these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DICENE T</head><p>Standard convolutions encode spatial and channel-wise information simultaneously, but they are computationally expensive. To improve the efficiency of standard convolutions, separable (or depth-wise separable) convolutions are introduced <ref type="bibr" target="#b6">[7]</ref>, where spatial and channel-wise information is encoded separately using depth-wise and point-wise convolutions, respectively. Though this factorization is effective, it puts a significant computational load on point-wise convolutions and makes them a computational bottleneck (see <ref type="figure">Figure 2</ref>).</p><p>To encode spatial and channel-wise information efficiently, we introduce the DiCE unit and is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The DiCE unit factorizes standard convolution using Dimensionwise Convolution (DimConv, Section 3.1) and Dimension-wise Fusion (DimFuse, Section 3.2). DimConv applies a lightweight filtering across each dimension of the input tensor to learn local dimension-wise representations. DimFuse efficiently combines these representations from different dimensions and incorporates global information. The ability to encode local spatial and channel-wise information from all dimensions using  DimConv enables the DiCE unit to use DimFuse instead of computationally expensive point-wise convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dimension-wise Convolution (DimConv)</head><p>We use dimension-wise convolutions (DimConv) to encode depth-, width-, and height-wise information independently. To achieve this, DimConv extends depth-wise convolutions to all dimensions of the input tensor X ∈ R D×H×W , where W , H, and D corresponds to width, height, and depth of X. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, DimConv has three branches, one branch per dimension. These branches apply D depth-wise convolutional kernels k D ∈ R 1×n×n along depth, W width-wise convolutional kernels k W ∈ R n×n×1 along width, and H height-wise convolutional kernels k H ∈ R n×1×n kernels along height to produce outputs Y D , Y W , and Y H ∈ R D×H×W that encode information from all dimensions of the input tensor. The outputs of these independent branches are concatenated along the depth dimension, such that the first spatial plane of Y D , Y W , and Y H are put together and so on, to produce the output</p><formula xml:id="formula_0">Y Dim = {Y D , Y W , Y H } ∈ R 3D×H×W .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dimension-wise Fusion (DimFuse)</head><p>The dimension-wise convolutions encode local information from different dimensions of the input tensor, but do not capture global information. A standard approach to combine features globally in CNNs is to use a point-wise convolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. A point-wise convolutional layer applies D point-wise kernels k p ∈ R 3D×1×1 and performs 3D 2 HW operations to combine dimension-wise representations of Y Dim ∈ R 3D×H×W and produce an output Y ∈ R D×H×W . This is computationally expensive. Given the ability of DimConv to encode spatial and channel-wise information (though independently), we introduce a fusion module, Dimension-wise fusion (DimFuse), that allows us to combine representations of Y Dim efficiently. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, DimFuse factorizes the point-wise convolution in two steps: (1) local fusion and (2) global fusion.</p><p>Y Dim ∈ R 3D×H×W concatenates spatial planes along depth dimension from Y D , Y W , and Y H (see <ref type="figure" target="#fig_2">Figure 3</ref>). Therefore, Y Dim can be viewed as a tensor with D groups, each group with three spatial planes (one from each dimension). DimFuse uses a group point-wise convolutional layer to combine dimension-wise information contained in Y Dim . In particular, this group convolutional layer applies D point-wise convolutional kernels k G ∈ R 3×1×1 to Y Dim and produces an output Y G ∈ R D×H×W . Since D kernels in k G operates independently on D groups in Y Dim , we call this local fusion operation.</p><p>To efficiently encode the global information in Y G , DimFuse learns spatial and channel-wise representations independently and then propagate channel-wise encodings to spatial encodings using an element-wise multiplication. Specifically, DimFuse encodes spatial representations by applying D depthwise convolutional kernels k S ∈ R 1×n×n to Y G to produce an output Y S . 1 Motivated by Squeeze-Excitation (SE) unit <ref type="bibr" target="#b35">[34]</ref>, we squeeze spatial dimensions of Y G and encode channel-wise representations using two fully connected (FC) layers. The first FC layer reduces the input dimension from D to D 4 while the second FC layer expands dimensionality from D 4 to D. To allow these fully connected layers to learn non-linear representations, a ReLU activation is added in between these two layers. Similar to the SE unit, spatial representations Y G are then scaled using these channel-wise representations to produce output Y.</p><p>The computational cost of DimFuse is HW D(3 + n 2 + D). Effectively, DimFuse reduces the computational cost of pointwise convolutions by a factor of 3D 3+n 2 +D . DimFuse uses n = 3, so the computational cost is approximately 3× smaller than that of the point-wise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DiCE Unit for Arbitrary Sized Inputs</head><p>The DiCE unit stacks DimConv and DimFuse to encode spatial and channel-wise information in the input tensor efficiently.  However, the two kernels (i.e., k H and k W ) in DimConv unit correspond to spatial dimensions of the input tensor. This may pose a challenge when spatial dimensions of the input tensor are different from the ones with which the network is trained. To make DiCE units invariant to spatial dimensions of the input tensor, we dynamically scale (either up-sample or down-sample) the height or width dimension of the input tensor to the height or width of the input tensor used in the pretrained network. The resultant tensors are then scaled (either down-sampled or upsampled) back to their original size before being fed to DimFuse; this makes the DiCE unit invariant to input tensor size. <ref type="figure" target="#fig_3">Figure 5</ref> sketches the DiCE unit with dynamic scaling. Results in Section 7, especially object detection and semantic segmentation, show that the DiCE unit can handle arbitrary sized inputs.</p><formula xml:id="formula_1">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Down-sample or up-sample</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DiCENet Architecture</head><p>DiCE units are generic and can be easily integrated in any existing network. <ref type="figure">Figure 4</ref>  To illustrate the performance benefits and generic nature of the DiCE unit over separable convolutions, we replace separable convolutions with the DiCE unit in these architectures. Our empirical results in Section 5 shows that the DiCE unit with ShuffleNetv2's architecture delivers the best performance. Therefore, we choose the ShuffleNetv2 <ref type="bibr" target="#b16">[16]</ref> architecture and call the resultant network DiCENet (ShuffleNetv2 with the DiCE unit). CUDA Implementation: DimConv applies D, W , and H depthwise, width-wise and height-wise convolutional kernels to the input tensor X to aggregate information from different dimensions of the tensor, respectively. A standard solution would be to apply each kernel independently to the tensor and then concatenate their results, as shown in <ref type="figure">Figure 6a</ref>. Another solution would be to apply all kernels simultaneously, as shown in <ref type="figure">Figure 6b</ref>. Compared to three CUDA kernel calls in the former solution, the later one requires one kernel call, thus reducing the kernel launch time. Also, each thread in the CUDA kernel process 3n 2 elements compared to n 2 elements in the former solution for n × n convolutional kernels. This maximizes the work done per thread and improves speed. Our results in Section 6 shows that DiCENet is accurate and fast compared to state-of-the-art methods, including neural search-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SET-UP</head><p>Following most architecture designs (e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>), we evaluate the generic nature of the DiCE unit on the ImageNet dataset in Section 5. We integrate the DiCE unit in different image classification architectures <ref type="figure">(Figure 4</ref>) and study the impact on efficiency and accuracy. We also study the importance of the two main components of the DiCE unit, i.e. DimConv and DimFuse, and show that DiCE units are more effective than separable convolutions <ref type="bibr" target="#b6">[7]</ref>. In Section 6, we evaluate the image classification performance of DiCENet on the ImageNet dataset and show that DiCENet delivers similar or better performance than state-of-the-art efficient networks, including neural searchbased methods. In Section 7, we evaluate task-level generalization ability of DiCENet on three different visual recognition tasks, i.e. object detection, semantic segmentation, and multi-object classification, that are often used in resource-constrained devices. We demonstrate that DiCENet generalizes better than existing efficient networks that are built using separable convolutions. Datasets: We use following datasets in our experiments.  <ref type="figure">Fig. 6</ref>: Implementation of dimension-wise convolution (DimConv). In (a), each kernel is applied to a pixel (represented by red dot) independently. In (b), all kernels are applied to a pixel simultaneously, allowing us to aggregate the information from tensor efficiently. Convolutional kernels are highlighted in color (depth-, width-, and height-wise).</p><p>Image classification: For single label image classification, we use ImageNet-1K classification dataset <ref type="bibr" target="#b36">[35]</ref>. This dataset consists of 1.28M training and 50K validation images. All networks on this dataset are trained from scratch. For multi-label classification, we use MS-COCO dataset <ref type="bibr" target="#b37">[36]</ref> that has 2.9 labels (on an average) per image. We use the same training and validation splits as in <ref type="bibr" target="#b10">[11]</ref>. Object detection: We use MS-COCO <ref type="bibr" target="#b37">[36]</ref> and PASCAL VOC 2007 <ref type="bibr" target="#b38">[37]</ref> datasets for evaluating on the task of object detection. Following a standard convention for training on PASCAL VOC 2007 dataset, we augment it with PASCAL VOC 2012 <ref type="bibr" target="#b39">[38]</ref> and PASCAL VOC 2007 trainval set for training and evaluate the performance on PASCAL VOC 2007 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation:</head><p>We use PASCAL VOC 2012 <ref type="bibr" target="#b39">[38]</ref> dataset for this task. Following a standard convention, we use additional images for training from <ref type="bibr" target="#b40">[39]</ref> and <ref type="bibr" target="#b37">[36]</ref>. Similar to MobileNetv2 <ref type="bibr" target="#b9">[10]</ref>, we evaluate the performance on the validation set. Efficiency metric: We measure efficiency in terms of the number of floating point operations (FLOPs) and inference time. We use PyTorch for training our networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATING DICE UNIT ON IMAGENET</head><p>We first evaluate the two important properties of the DiCE unit, i.e. generic and efficiency, in Section 5.1. To evaluate this, we replace separable convolutions <ref type="bibr" target="#b6">[7]</ref> in different architectures with the DiCE unit ( <ref type="figure">Figure 4</ref>). We then study the importance of each component of the DiCE unit, DimConv and DimFuse, in Section 5.2. Recent studies (e.g., MobileNetv3 <ref type="bibr" target="#b13">[14]</ref> and MNASNet <ref type="bibr" target="#b11">[12]</ref>) uses several different methods, such as exponential moving average (EMA) and large batch sizes, to improve the performance. In Section 5.3, we study the effect of these methods on the performance of DiCENet.</p><p>In these experiment, we use the ImageNet dataset <ref type="bibr" target="#b36">[35]</ref>. In Section 5.1 and Section 5.2, we follow the experimental setup of ESPNetv2 <ref type="bibr" target="#b10">[11]</ref> and ShuffleNetv2 <ref type="bibr" target="#b16">[16]</ref> (fewer training epochs with smaller batch size) while in Section 5.3, we follow experimental set-up similar to MobileNets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> (longer training with larger batch size).  accuracy and efficiency. Since this architecture does not employ any advanced methods (e.g., residual connections and channel shuffle) to improve performance, it allows us to understand the "true" gains of the DiCE unit over separable convolutions. When we replace separable convolutions with the DiCE unit in ResNet and ShuffleNetv2, we observe significant improvements especially for small-(25-60 MFLOPs) and medium-sized (120-170 MFLOPs) models. For instance, the DiCE unit improved the performance of ShuffleNetv2 by about 3% for small-sized model (about 40 MFLOPs). Similarly, ShuffleNetv2 with the DiCE unit requires 24 million fewer FLOPs to achieve the same accuracy as with separable convolutions for medium-sized model (120-170 MFLOPs). These results suggests that the DiCE unit is generic and learns better representations than separable convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DiCE Unit vs. Separable Convolutions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Importance of DimConv and DimFuse</head><p>To understand the significance of each component of the DiCE unit, we replace DimConv with depth-wise convolution and DimFuse with different fusion methods, including point-wise convolutions and squeeze-excitation (SE) unit <ref type="bibr" target="#b35">[34]</ref> and study their  combinations for two architectures (ResNet and ShuffleNetv2) <ref type="bibr" target="#b2">3</ref> .</p><p>In these experiments, we study efficient models by restricting the computational budget between 120 and 150 MFLOPs. Importance of DimConv: We replace depth-wise convolutional layers with DimConv in ResNet and ShuffleNetv2 architectures. <ref type="table" target="#tab_5">Table 2a</ref> shows that these networks with DimConv require about 10-11 million fewer FLOPs to achieve similar accuracy as the depth-wise convolution. These results suggest that encoding spatial and channel-wise information independently in DimConv helps learning better representations compared to encoding only spatial information in depth-wise convolution. Importance of DimFuse: To understand the effect of DimFuse, we replace DimFuse with two widely used fusion operations, i.e., point-wise convolution and SE unit. <ref type="table" target="#tab_5">Table 2b</ref> summarizes the results. Compared to the widely used combination of depthwise and point-wise convolutions (or separable convolution), the combination of DimConv and DimFuse (or the DiCE unit) is the most effective and improves the efficiency of networks by 15-20% with little or no impact on accuracy.</p><p>The combination of depth-wise and DimFuse is not as effective as DimConv and DimFuse. DimConv encodes local spatial and channel-wise information, which enables the DiCE unit to use a less complex fusion method (DimFuse) for encoding global information. Unlike DimConv, depth-wise convolutions only encode local spatial information and require computationally expensive point-wise convolutions to encode global information.</p><p>When point-wise convolutions are replaced with SE unit, the performance of networks with depth-wise and DimConv convolutions dropped significantly. This is because SE unit relies on an existing convolutional unit, such as ResNext <ref type="bibr" target="#b41">[40]</ref>, to encode global spatial and channel-wise information. When SE unit is used as a replacement for point-wise convolutions, it fails to effectively encode this information; resulting in significant performance drop.</p><p>DimFuse replacing all point-wise convolutions: The first layer <ref type="bibr" target="#b2">3</ref> MobileNet's performance is significantly lower than ResNet and Shuf-fleNetv2, therefore, we do not use MobileNet for these experiments.  in ShuffleNetv2 and ResNet is a point-wise convolution ( <ref type="figure">Figure  4</ref>; Section 3.4). <ref type="table" target="#tab_5">Table 2b</ref> shows that DimFuse is an effective replacement for point-wise convolutions. A natural question arises if we can replace the first point-wise convolutional layer in these architectures with DimFuse. <ref type="table" target="#tab_7">Table 3</ref> shows the effect of replacing point-wise convolutions with DimFuse. For a fixed network width, the number of FLOPs are reduced by 45% when point-wise convolutions are replaced with DimFuse, however, the accuracy drops by about 5-7%. For similar number of FLOPs, networks with DimFuse achieves higher accuracy. However, such networks (R3 and R6) are about 4× wider than the networks with point-wise convolutions (R1 and R4); this poses memory constraints for resource-constrained devices. Therefore, we only replace separable convolutions with the DiCE unit while keeping the remaining architecture intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of MobileNet's training hyper-parameters</head><p>In previous experiments, we follow the experimental setup similar to ESPNetv2 and ShuffleNetv2 i.e., each model is trained for 150 epochs using a batch size of 512 and minimizes cross-entropy loss using SGD. Recent efficient models (e.g., MobileNetv3 and MNASNet) are trained longer (600 epochs) with extremely large With +, we indicate that the component is added to the previous configuration. Here, UnOpt represents the un-optimized DiCENet trained for 150 epochs with a batch size of 512 with cross entropy and Opt represents DiCENet with custom CUDA kernel. 300 ep denotes that model is trained for 300 epochs, CE-LS denotes that label-smooth cross-entropy is used, EMA denoted that exponential moving average is used, and LBSz denotes that large batch size (2048 images) is used for training. Here, inference time is measured on NVIDIA GTX 1080 Ti GPU and is an average across 100 trials for a batch of 32 RGB images, each with a spatial dimension of 224 × 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Type FLOP ranges (in millions)   batch size (4096) using cross-entropy with label smoothing (CE-LS) and exponential moving average (EMA). We also trained DiCENet with CE-LS and EMA, with an exception to batch size (2048) and number of epochs (300). For training with larger batch size, we accumulated the gradients for 4 iterations. This resulted in an effective batch size of 2048 (128 images per NVIDIA GTX 1080 Ti GPU × 4 GPUs × accumulation frequency of 4). <ref type="figure" target="#fig_6">Figure 7</ref> shows the effect of these changes. Similar to stateof-the-art models (e.g., MobileNetv2, MobileNetv3, MixNet, and MNASNet), DiCENet also benefits from these hyper-parameters and yields a top-1 accuracy of 75.7 on the ImageNet. Importantly, the optimized CUDA kernel ( <ref type="figure">Figure 6</ref>) improved the inference speed drastically over the unoptimized version. This is because the optimized kernel launches one kernel for these three branches compared to one per branch in the unoptimized one and also, maximizes work done per CUDA thread. This reduces latency. With optimized CUDA kernels, DiCENet models (10-300 MFLOPs) takes between one and three days for training on the ImageNet dataset on 4 NVIDIA GTX 1080 Ti GPUs with an effective batch size of 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATING DICENE T ON THE IMAGENET</head><p>In this section, we evaluate the performance of DiCENet on the ImageNet dataset and show that DiCENet delivers significantly better performance than state-of-the-art efficient networks, including neural search architectures. Recall that the DiCENet is ShuffleNetv2 with the DiCE unit (Section 3.4). Implementation details: We scale the number of output channels by a width scaling factor s to obtain DiCENet models at different complexity levels, ranging from 6 MFLOPs to 500+ MFLOPs (see Appendix A for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics and baselines:</head><p>We use 224 × 224 single crop top-1 accuracy to evaluate the performance on the validation set. The performance of DiCENet is compared with state-ofthe-art manually designed efficient networks (MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, ShuffleNetv2 <ref type="bibr" target="#b16">[16]</ref>, CondenseNet <ref type="bibr" target="#b42">[41]</ref>, and ESPNetv2 <ref type="bibr" target="#b10">[11]</ref>) and automatically designed networks (MNASNet <ref type="bibr" target="#b11">[12]</ref>, FBNet <ref type="bibr" target="#b12">[13]</ref>, MixNet <ref type="bibr" target="#b15">[15]</ref>, and MobileNetv3 <ref type="bibr" target="#b13">[14]</ref>).</p><p>Results: Recent studies (e.g., MobileNetv3) have shown that longer training with extremely large batch sizes improves performance. To have fair comparisons with state-of-the-art methods, we report the performance of DiCENet on two settings. The first setting, DiCENet-E150-B512, is similar to networks like ESPNetv2, ShuffleNetv2, and CondenseNet where DiCENet is trained for fewer epochs (150) with a smaller batch size (512) without EMA and label smoothing. The second setting, DiCENet-E300-B2048, is similar to networks like MobileNets, MNASNet, and MixNet, where DiCENet is trained for 300 epochs with a batch size of 2048. <ref type="table" target="#tab_9">Table 4</ref> compares the performance of DiCENet with stateof-the-art efficient architectures at different FLOP ranges.</p><p>Compared to networks that are trained with smaller batch sizes and fewer epochs, e.g., ESPNetv2 (epochs: 300; batch size: 512) and ShuffleNetv2 (epochs: 240 and batch size: 1024), DiCENet delivers better performance across different FLOP ranges. Similarly, when DiCENet is trained for longer with larger batch sizes, it delivered similar or better performance than state-ofthe-art methods, including neural architecture search (NAS)-based methods. For about 300 MFLOPs, DiCENet is 4% more accurate than MobileNetv2. Specifically, we observe that DiCENet is very effective when model size is small (FLOPs &lt; 150 M). For example, DiCENet outperforms MNASNet <ref type="bibr" target="#b11">[12]</ref>, FBNet <ref type="bibr" target="#b12">[13]</ref>, and MobileNetv3 <ref type="bibr" target="#b13">[14]</ref> by 6.1%, 3.2%, and 1.1% for network size of about 70 MFLOPs, respectively.</p><p>Overall, these results shows that DiCE unit learns better representations than separable convolutions. We believe that incorporating the DiCE unit with NAS would yield better network. Inference speed: We measure the inference time on two GPUs: (1) embedded or mobile GPU (NVIDIA GTX 960M) and <ref type="bibr" target="#b1">(2)</ref> desktop GPU (NVIDIA GTX 1080 Ti) <ref type="bibr" target="#b3">4</ref> . DiCENet is as fast as ShuffleNetv2 but delivers better performance. Compared to MobileNetv2 and MobileNetv3, DiCENet has low latency while delivering similar or better performance. We observe that Mo-bileNetv2 and MobileNetv3 models are slow in comparison to ShuffleNetv2 and DiCENet when the batch size increases. This is because the number of channels in the depth-wise convolution in the inverted residual block of MobileNetv2 and MobileNetv3 are very large as compared to DiCENet and ShuffleNetv2. For example, the maximum number of channels in depth-wise convolution in MobileNetv2 (300 MFLOPs) are 960 while the maximum number of channels in DimConv and depth-wise convolution in DiCENet (298 MFLOPs) and ShuffleNetv2 (292 MFLOPs) are 576 and 352, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">TASK-LEVEL GENERALIZATION OF DICENE T</head><p>Several previous works (e.g., <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b45">44]</ref>) have shown that high accuracy on the Imagenet dataset does not necessarily correlates with high accuracy on visual scene understanding tasks (e.g., object detection and semantic segmentation). Since these tasks are widely used in real-world applications (e.g., autonomous wheel chair and robots) and often run on resource-constrained devices (e.g., embedded devices), it is important that efficient networks generalizes well on these tasks. Therefore, we evaluate the performance of DiCENet on three different tasks: (1) object detection (Section 7.1), (2) semantic segmentation (Section 7.2), and (3) multi-object classification (Section 7.3). Compared to existing efficient networks that are built using separable convolutions (e.g., MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>, MixNet <ref type="bibr" target="#b15">[15]</ref>, and ESPNetv2 <ref type="bibr" target="#b10">[11]</ref>), DiCENet delivers better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Object Detection on VOC and MS-COCO</head><p>Implementation details: For object detection, we use Single Shot object Detection (SSD) <ref type="bibr" target="#b17">[17]</ref> pipeline. We use DiCENet (298 MFLOPs) pretrained on the ImageNet as a base feature extractor instead of VGG <ref type="bibr" target="#b1">[2]</ref>. We fine-tune our network using SGD with smooth L1 and cross-entropy losses for object localization and classification, respectively. Evaluation metrics and baselines: We evaluate the performance using mean Average Precision (mAP). For MS-COCO, we report mAP@IoU of 0.50:0.95. For SSD as a detection pipeline, we compare DiCENet's performance with two types of base feature extractors: (1) manual (VGG <ref type="bibr" target="#b1">[2]</ref>, MobileNet <ref type="bibr" target="#b6">[7]</ref>, MobileNetv2 <ref type="bibr" target="#b9">[10]</ref>, and ESPNetv2 <ref type="bibr" target="#b10">[11]</ref>) and (2) NAS-based (MNASNet <ref type="bibr" target="#b11">[12]</ref>, MixNet <ref type="bibr" target="#b15">[15]</ref>, and MobileNetv3 <ref type="bibr" target="#b13">[14]</ref>). Results: <ref type="table" target="#tab_12">Table 6</ref> compares quantitative results of SSD with different backbone networks on the PASCAL VOC 2007 and <ref type="bibr" target="#b3">4</ref> We do not measure the inference speed on Smartphones because efficient implementations of DiCE unit are not yet available for such devices.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Semantic Segmentation on PASCAL VOC</head><p>Implementation details: We adapt DiCENet to ESPNetv2's <ref type="bibr" target="#b10">[11]</ref> encoder-decoder architecture. We choose this network because it delivers competitive performance to existing methods even with low resolution images (e.g. 256×256 vs. 512×512). We replace the encoder in ESPNetv2 (pretrained on ImageNet) with the DiCENet and follow the same training procedure for fine-tuning as ESPNetv2. We do not change the decoder. Evaluation metrics and baselines: The performance at different complexity levels (FLOPs) is evaluated using mean intersection over union (mIOU). Results: <ref type="figure">Figure 8</ref> compares the performance of DiCENet with ESPNetv2 on the PASCAL VOC 2012 validation set.  Here, † indicates that results are from <ref type="bibr" target="#b10">[11]</ref>.</p><p>DiCENet significantly improves the segmentation performance i.e., for similar FLOPs, DiCENet achieves higher mIOU while for similar mIOU, DiCENet requires significantly fewer FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Multi-object Classification on MS-COCO</head><p>Implementation details: Following <ref type="bibr" target="#b10">[11]</ref>, we fine-tune DiCENet using the binary cross-entropy loss. Evaluation metrics and baselines: Similar to <ref type="bibr" target="#b10">[11]</ref>, we evaluate the performance using overall and per-class F1 score and compare with two efficient architectures, i.e., ESPNetv2 and ShuffleNetv2.</p><p>Results: <ref type="table" target="#tab_14">Table 7</ref> shows that DiCENet outperforms existing efficient networks by a significant margin (e.g., ESPNetv2 and ShuffleNetv2 by 4.1% and 5.8% respectively) on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We introduce a novel and generic convolutional unit, the DiCE unit, that uses dimension-wise convolutions and dimensionwise fusion module to learn spatial and channel-wise representations efficiently. Our empirical results suggest that the DiCE unit is more effective than separable convolutions. Moreover, when we stack DiCE units to build DiCENet model, we observe significant improvements across different computer vision tasks. We have shown that the DiCE unit is effective. Future work involves adding the DiCE unit in neural search space to discover a better neural architecture, particularly with <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">15]</ref>.</p><p>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A NETWORK ARCHITECTURE</head><p>The overall architecture at different network complexities is given in <ref type="table" target="#tab_15">Table 8</ref>. The first layer is a standard 3 × 3 convolution with a stride of two while the second layer is a max pooling layer. All convolutional layers are followed by a batch normalization layer <ref type="bibr" target="#b46">[45]</ref> and a PReLU non-linear activation layer <ref type="bibr" target="#b47">[46]</ref>, except for the last layer that feeds into a softmax for classification. Following previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">18]</ref>, we scale the number of output channels by a width scaling factor s to construct networks at different FLOPs. We initialize weights of our network using the same method as in <ref type="bibr" target="#b47">[46]</ref>.   <ref type="figure" target="#fig_3">Figure 5</ref> on the paper (MobileNetv1 and ResNet), we replace blocks 1, 2, and 3 with the corresponding blocks. (c) Results in the wild (top row: Input image, middle row: predictions, last row: predictions overlayed on the input image). These images do not have ground truth labels. Therefore, to reflect the segmentation quality, we overlayed segmentation masks on the input images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B QUALITATIVE RESULTS FOR OBJECT DETECTION AND SEMANTIC SEGMENTATION</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Separable convolutions vs. the DiCE unit. arXiv:1906.03516v3 [cs.CV] 30 Nov 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>DiCE unit efficiently encodes the spatial and channel-wise information in the input tensor X using dimension-wise convolutions (DimConv) and dimension-wise fusion (DimFuse) to produce an output tensor Y. For simplicity, we show kernel corresponding to each dimension independently. However, in practice, these three kernels are executed simultaneously, leading to faster run-time. See Section 3.4 and 6 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>DiCE unit for arbitrary sized input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>visualizes the DiCE unit with different architectures: (1) MobileNet<ref type="bibr" target="#b6">[7]</ref> stacks separable convolutions (depthwise convolution followed by point-wise convolution) to learn representations. (2) ResNet<ref type="bibr" target="#b2">[3]</ref> introduces the bottleneck unit with residual connections to train very deep networks. The bottleneck unit is a stack of three convolutional layers: one 3 × 3 depth-wise convolutional layer 2 surrounded by two point-wise convolutions. This block can be viewed as a point-wise convolution followed by separable convolution. (3) ShuffleNetv2<ref type="bibr" target="#b16">[16]</ref> is a state-of-theart efficient network that outperforms other efficient networks, including MobileNetv2<ref type="bibr" target="#b9">[10]</ref>. ShuffleNetv2's unit stacks a pointwise convolution and separable convolution. It also uses channel split and shuffle to promote feature reuse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Unoptimized (Left to right: depth-wise, height-wise, and width-wise)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Impact of different components in the training of DiCENet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9</head><label>9</label><figDesc>provides qualitative results for object detection while Figures 10 provide results for semantic segmentation on the PASCAL VOC 2012 dataset as well as in the wild. These results suggest that the DiCENet is able to detect and segment objects in diverse settings, including different backgrounds and image sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Object detection using DiCENet under diverse background conditions. (a) PASCAL VOC Colormap (b) Results on the PASCAL VOC 2012 validation set (top row: Input image, middle row: ground truth, last row: DiCENet predictions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Semantic segmentation results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>DiCE unit in different architecture designs for the task of image classification on the ImageNet dataset. Green and boxes are with and without stride, respectively. Here, N i = {3, 7, 3} for i = {1, 2, 3}. See Appendix A for detailed architecture specification at different complexity levels.</figDesc><table><row><cell></cell><cell>Input Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell></cell><cell>DiCE Unit</cell><cell>1x1 Conv</cell><cell>Channel split</cell><cell></cell></row><row><cell></cell><cell>Max Pool</cell><cell></cell><cell>DimConv 3 x3</cell><cell>DiCE Unit</cell><cell></cell><cell>1x1 Conv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DimConv 3 x3</cell><cell></cell><cell>DiCE Unit</cell></row><row><cell>ůŽĐŬ ݅</cell><cell>Convolutional Unit Unit Convolutional</cell><cell>ൈ ܰ</cell><cell>DimFuse</cell><cell>DimFuse</cell><cell>Concatenate</cell><cell>DimConv 3 x3 DimFuse</cell></row><row><cell></cell><cell>Pooling, FC</cell><cell></cell><cell>Addition</cell><cell></cell><cell>Channel shuffle</cell><cell></cell></row><row><cell></cell><cell>Logits</cell><cell></cell><cell>DŽďŝůĞEĞƚ|ϭ</cell><cell cols="3">ZĞƐEĞƚ^ŚƵĨĨůĞEĞƚ|Ϯ</cell></row><row><cell cols="2">Depth-wise Conv Fig. 4: Width-wise Up-sample or Conv Down-sample</cell><cell>Down-sample or up-sample</cell><cell>DimFuse</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Up-sample or down-sample</cell><cell>Height-wise Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table><row><cell>FLOP</cell><cell cols="2">Separable conv</cell><cell cols="2">DiCE unit</cell><cell cols="2">Absolute difference</cell></row><row><cell>Range</cell><cell></cell><cell>(SC)</cell><cell cols="2">(DU)</cell><cell cols="2">(DU -SC)</cell></row><row><cell cols="3">(in millions) Top-1 FLOPs</cell><cell cols="2">Top-1 FLOPs</cell><cell>Top-1</cell><cell>FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MobileNet [7]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>25-60</cell><cell>49.80</cell><cell>41 M</cell><cell>52.55</cell><cell>29 M</cell><cell>+2.75</cell><cell>-12 M</cell></row><row><cell>120-170</cell><cell>65.30</cell><cell>162 M</cell><cell>69.05</cell><cell>167 M</cell><cell>+3.75</cell><cell>+5 M</cell></row><row><cell>270-320</cell><cell>68.40</cell><cell>317 M</cell><cell>70.83</cell><cell>277 M</cell><cell>+2.43</cell><cell>-40 M</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet [3]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>25-60</cell><cell>59.30</cell><cell>59 M</cell><cell>61.35</cell><cell>52 M</cell><cell>+2.05</cell><cell>-7 M</cell></row><row><cell>120-170</cell><cell>67.80</cell><cell>142 M</cell><cell>67.90</cell><cell>122 M</cell><cell>+0.10</cell><cell>-20 M</cell></row><row><cell>270-320</cell><cell>70.67</cell><cell>302 M</cell><cell>71.80</cell><cell>300 M</cell><cell>+1.13</cell><cell>-2 M</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ShuffleNetv2 [16]</cell><cell></cell><cell></cell></row><row><cell>25-60</cell><cell>59.69</cell><cell>41 M</cell><cell>62.80</cell><cell>46 M</cell><cell>+3.11</cell><cell>+5 M</cell></row><row><cell>120-170</cell><cell>68.14</cell><cell>146 M</cell><cell>68.21</cell><cell>122 M</cell><cell>+0.07</cell><cell>-24 M</cell></row><row><cell>270-320</cell><cell>71.80</cell><cell>292 M</cell><cell>72.90</cell><cell>298 M</cell><cell>+1.10</cell><cell>+6 M</cell></row></table><note>the performance of the DiCE unit with different architectures at different FLOP ranges. When separable convolu- tions are replaced with the DiCE unit in MobileNet architecture, we observe significant gains in performance both in terms of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison between the DiCE unit and separable con- volutions on the ImageNet dataset across different architectures. Models with the DiCE unit requires fewer channels compared to models with separable convolution in order to obtain similar performance. Thus, models with the DiCE unit has fewer FLOPs compared to separable convolutions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Importance of DimConv. DWise denotes depth-wise conv.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ResNet</cell><cell></cell><cell>ShuffleNetv2</cell></row><row><cell>Layer</cell><cell cols="3">FLOPs Top-1</cell><cell></cell><cell>FLOPs Top-1</cell></row><row><cell>DWise + Point-wise</cell><cell cols="2">142 M</cell><cell>67.80</cell><cell></cell><cell>146 M</cell><cell>68.14</cell></row><row><cell>DimConv + Point-wise</cell><cell cols="2">132 M</cell><cell>68.10</cell><cell></cell><cell>135 M</cell><cell>68.45</cell></row><row><cell cols="5">(a) ResNet</cell><cell>ShuffleNetv2</cell></row><row><cell>Layer</cell><cell></cell><cell cols="3">FLOPs Top-1</cell><cell>FLOPs Top-1</cell></row><row><cell>DWise + Point-wise (Separable)</cell><cell></cell><cell></cell><cell>142 M</cell><cell>67.80</cell><cell>146 M</cell><cell>68.14</cell></row><row><cell>DWise + SE</cell><cell></cell><cell></cell><cell>137 M</cell><cell>63.90</cell><cell>140 M</cell><cell>64.70</cell></row><row><cell>DWise + Point-wise + SE</cell><cell></cell><cell></cell><cell>142 M</cell><cell>68.20</cell><cell>146 M</cell><cell>68.60</cell></row><row><cell>DWise + DimFuse</cell><cell></cell><cell></cell><cell>136 M</cell><cell>65.90</cell><cell>139 M</cell><cell>66.80</cell></row><row><cell>DimConv + Point-wise</cell><cell></cell><cell></cell><cell>132 M</cell><cell>68.10</cell><cell>135 M</cell><cell>68.45</cell></row><row><cell>DimConv + SE</cell><cell></cell><cell></cell><cell>134 M</cell><cell>64.80</cell><cell>138 M</cell><cell>65.40</cell></row><row><cell>DimConv + Point-wise + SE</cell><cell></cell><cell></cell><cell>132 M</cell><cell>67.90</cell><cell>135 M</cell><cell>68.30</cell></row><row><cell cols="2">DimConv + DimFuse (DiCE unit)</cell><cell cols="2">122 M</cell><cell>67.90</cell><cell>122 M</cell><cell>68.21</cell></row><row><cell cols="6">(b) Importance of DimFuse. DWise denotes depth-wise conv.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Evaluating DiCE unit on the ImageNet dataset. Top-1 accuracy is reported on the validation set. Models with DiCE unit requires fewer channels compared to models with separable convolution in order to obtain similar performance. Therefore, models with DiCE unit has fewer FLOPs compared to separable convolutions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 :</head><label>3</label><figDesc>Impact of replacing all pointwise convolutions with DimFuse. Here, DWise denotes depth-wise convolution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 4 :</head><label>4</label><figDesc>Results on the ImageNet dataset. DiCENet delivers similar or better performance than state-of-the-art methods, including neural architecture search (NAS)-based methods. Here, each entry is represented as top-1 accuracy and FLOPs within parentheses. DiCENet-E150-B512 models are trained for 150 epochs with a batch size of 512 (without EMA and label smoothing) while DiCENet-E300-B2048 models are trained for 300 epochs with an effective batch size of 2048 (with EMA and label smoothing).</figDesc><table><row><cell></cell><cell></cell><cell>Network statistics</cell><cell></cell><cell cols="2">Device: GTX-960 M (Memory = 4GB)</cell><cell cols="3">Device: GTX-1080 Ti (Memory = 11 GB)</cell></row><row><cell>Model</cell><cell cols="3"># Params # FLOPs Top-1 Accuracy</cell><cell>Batch size = 1</cell><cell>Batch size = 32</cell><cell cols="3">Batch size = 1 Batch size = 32 Batch size = 64</cell></row><row><cell>MobileNetv2</cell><cell>3.5 M</cell><cell>300 M</cell><cell>71.8</cell><cell>5.6 ± 0.2 ms</cell><cell>114 ± 0.1 ms</cell><cell>5.9 ± 0.1 ms</cell><cell>22.8 ± 0.1 ms</cell><cell>44.3 ± 0.8 ms</cell></row><row><cell>ShuffleNetv2</cell><cell>3.5 M</cell><cell>300 M</cell><cell>71.8</cell><cell>5.8 ± 0.2 ms</cell><cell>80.7 ± 0.6 ms</cell><cell>5.8 ± 0.2 ms</cell><cell>12.9 ± 0.1 ms</cell><cell>24.1 ± 0.4 ms</cell></row><row><cell>MobileNetv3</cell><cell>5.5 M</cell><cell>220 M</cell><cell>75.2</cell><cell>8.5 ± 0.2 ms</cell><cell>Out-of-memory</cell><cell>9.0 ± 0.3 ms</cell><cell>20.9 ± 0.1 ms</cell><cell>40.2 ± 0.2 ms</cell></row><row><cell>DiCENet (Ours)</cell><cell>5.1 M</cell><cell>297 M</cell><cell>75.7</cell><cell>5.9 ± 0.1 ms</cell><cell>79 ± 0.1 ms</cell><cell>5.7 ± 0.1 ms</cell><cell>12.8 ± 0.1 ms</cell><cell>24.1 ± 0.6 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table /><note>Inference speed. DiCENet and ShuffleNetv2 are comparatively faster than MobileNetv2 and MobileNetv3 on both devices (Mobile GPU: GTX-960M and Desktop GPU: GTX-1080 Ti). Inference results are an average over 100 trials for RGB input images of size 224 × 224. We used PyTorch with CUDA 10.2 for measuring speed. MobileNetv2 and ShuffleNetv2 implementations are taken from official PyTorch repository while MobileNetv3's implementation is taken from [42]. Since efficient implementations of EESP module in ESPNetv2 and mixed depth-wise convolution in MixNet are not available in PyTorch, we do not compare with these works.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6 :</head><label>6</label><figDesc>Object detection results of SSD [17] with different backbones on PASCAL VOC 2007 and MS-COCO. On the MS-COCO dataset, total network parameters in SSD with different backbones are about 5 million, except VGG. See Appendix B for qualitative results. Semantic segmentation results on the PASCAL VOC 2012 validation set. Here, mIOU represents mean intersection over union. See Appendix B for qualitative results. the MS-COCO datasets. DiCENet significantly improves the performance of SSD-based object detection pipeline and delivers 1-4% higher mAP than other existing efficient variants of SSD, including NAS-based backbones such as MobileNetv3 (22.0 vs. 25.1) and MixNet (22.3 vs. 25.1). Compared to standard SSD with VGG as backbone, DiCENet achieves higher mAP while being 45× and 38× more efficient on the PASCAL VOC 2007 and the MS-COCO dataset, respectively.</figDesc><table><row><cell>Model</cell><cell># Params # FLOPs mIOU</cell></row><row><cell>ESPNetv2</cell><cell>0.79 M 0.76 M 67.01</cell></row><row><cell>DiCENet</cell><cell>1.1 M 0.68 M 70.90</cell></row><row><cell>Fig. 8:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 7 :</head><label>7</label><figDesc>Multi-object classification results on the MS-COCO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 8 :</head><label>8</label><figDesc>Overall architecture of DiCENet (ShuffleNetv2 with DiCE unit) at different network complexities for the ImageNet classification. We use 4 groups in grouped fully connected (FC) layer. For other architecture designs in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When the depth of Y Dim is different from Y, then k S is a group convolution, where number of groups is the greatest common divisor between the depth of Y Dim and Y.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our focus is on efficient network. Therefore, we replace standard convolutional layer with depth-wise convolutional layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS-1616112, IIS1252835), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, and Amazon. Authors would also like to thank members of the PRIOR team at the Allen Institute for Artificial Intelligence (AI2), the H2Lab at the University of Washington, Seattle, and anonymous reviewers for their valuable feedback and comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constrained optimization based lowrank approximation of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Flattened convolutional neural networks for feedforward acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks,&quot; in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Es-pnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03443</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mixconv: Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8697" to="8710" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Discovering neural wirings,&quot; in NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Xnor-net: Imagenet classification using binary convolutional neural networks,&quot; in ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Binarized neural networks,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yodann: An architecture for ultralow power binary-weight cnn acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2704" to="2713" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Haq: Hardwareaware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7132" to="7141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge,&quot; IJCV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">PyTorch Image Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object detection with deep learning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pyramidal recurrent unit for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
