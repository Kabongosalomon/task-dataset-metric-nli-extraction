<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On learning to localize objects with minimal supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
							<email>song@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
							<email>julien.mairal@inria.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><forename type="middle">Harchaoui@inria</forename><surname>Fr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
						</author>
						<title level="a" type="main">On learning to localize objects with minimal supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The classical paradigm for learning object detection models starts by annotating each object instance, in all training images, with a bounding box. However, this exhaustive labeling approach is costly and error prone for large-scale datasets. The massive amount of textually annotated visual data available online inspires a different, more challenging, research problem. Can weakly-labeled imagery, without bounding boxes, be used to reliably train object detectors?</p><p>In this alternative paradigm, the goal is to learn to localize objects with minimal supervision <ref type="bibr" target="#b41">(Weber et al., 2000a;</ref>. We focus on the case where the learner has access to binary image labels that encode whether an image contains the target object or not, without access to any instance level annotations (i.e., bounding boxes).</p><p>Our approach starts by reducing the set of possible image locations that contain the object of interest from millions to thousands per image, using the selective search window proposal technique introduced by <ref type="bibr" target="#b40">Uijlings et al. (2013)</ref>. Then, we formulate a discriminative submodular cover algorithm to discover an initial set of image windows that are likely to contain the target object. After training a detection model with this initial set, we refine the detector using a novel smoothed formulation of latent SVM <ref type="bibr" target="#b1">(Andrews et al., 2003;</ref><ref type="bibr" target="#b17">Felzenszwalb et al., 2010)</ref>. We employ recently introduced object detection features, based on deep convolutional neural networks <ref type="bibr" target="#b21">Girshick et al., 2014)</ref>, to represent the window proposals for clustering and detector training.</p><p>Compared to prior work on weakly-supervised detector training, we show substantial improvements on the standard evaluation metric (detection average precision on PASCAL VOC). Quantitatively, our approach achieves a 50% relative improvement in mean average precision over the current state-of-the-art for weakly-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our work is related to three active research areas: (1) weakly-supervised learning, (2) unsupervised discovery of mid-level visual elements, and (3) co-segmentation.</p><p>We build on a number of previous approaches for training object detectors from weakly-labeled data. In nearly all cases, the task is formulated as a multiple instance learning (MIL) problem <ref type="bibr" target="#b29">(Long &amp; Tan, 1996)</ref>. In this formulation, the learner has access to an image-level label indicating the presence or absence of the target class, but not its location (if it is present). The challenge faced by the learner is to find the sliver of signal present in the positive images, but absent from the negative images. The implicit assumption is that this signal will correspond to the positive class.</p><p>Although there have been recent works on convex relaxations <ref type="bibr" target="#b28">(Li et al., 2013;</ref><ref type="bibr" target="#b22">Joulin &amp; Bach, 2012)</ref>, most MIL algorithms start from an initialization and then perform some form of local optimization. Early efforts, such as <ref type="bibr">(Weber arXiv:1403</ref><ref type="bibr">.1024v4 [cs.CV] 15 May 2014</ref><ref type="bibr" target="#b41">et al., 2000a</ref><ref type="bibr" target="#b20">Galleguillos et al., 2008;</ref><ref type="bibr" target="#b18">Fergus et al., 2007;</ref><ref type="bibr" target="#b7">Crandall &amp; Huttenlocher, 2006;</ref><ref type="bibr" target="#b6">Chum &amp; Zisserman, 2007;</ref><ref type="bibr" target="#b4">Chen et al., 2013)</ref>, focused on datasets with strong objectin-the-center biases (e.g. Caltech-101). This simplified setting enabled clarity and focus on the MIL formulation, image features, and classifier design, but masked the vexing problem of finding a good initialization in data where such helpful biases are absent.</p><p>More recent work, such as <ref type="bibr" target="#b38">(Siva &amp; Xiang, 2011;</ref><ref type="bibr" target="#b39">Siva et al., 2012)</ref>, attempts to learn detectors, or simply automatically generate bounding box annotations from much more challenging datasets such as PASCAL VOC <ref type="bibr" target="#b16">(Everingham et al., 2010)</ref>. In this data regime, focusing on initialization is crucial and carefully designed heuristics, such as shrinking bounding boxes <ref type="bibr" target="#b36">(Russakovsky et al., 2012)</ref>, are often employed.</p><p>Recent literature on unsupervised mid-level visual element discovery <ref type="bibr" target="#b11">(Doersch et al., 2012;</ref><ref type="bibr" target="#b37">Singh et al., 2012;</ref><ref type="bibr" target="#b14">Endres et al., 2013;</ref><ref type="bibr" target="#b24">Juneja et al., 2013;</ref><ref type="bibr" target="#b34">Raptis et al., 2012)</ref> uses weak labels to discover visual elements that occur commonly in positive images but not in negative images. Discovered visual element representation were shown to successfully provide discriminative information in classifying images into scene types. The most recent work <ref type="bibr" target="#b12">(Doersch et al., 2013)</ref> presents a discriminative mode seeking formulation and draws connections between discovery and meanshift algorithms <ref type="bibr" target="#b19">(Fukunaga &amp; Hostetler, 1975)</ref>.</p><p>The problem of finding common structure is related to the challenging setting of co-segmentation <ref type="bibr" target="#b35">(Rother et al., 2006;</ref><ref type="bibr" target="#b23">Joulin et al., 2010;</ref><ref type="bibr" target="#b0">Alexe et al., 2010)</ref>, which is the unsupervised segmentation of an object that is present in multiple images. While in this paper we do not address pixel-level segmentation, we employ ideas from co-segmentation: the intuition behind our submodular cover framework in Section 4 is shared with CoSand <ref type="bibr" target="#b25">(Kim et al., 2011)</ref>. Finally, submodular covering ideas have recently been applied to (active) filtering of hypothesis after running a detector, and without the discriminative flavor we propose <ref type="bibr" target="#b2">(Barinova et al., 2012;</ref><ref type="bibr" target="#b5">Chen et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem formulation</head><p>Our goal is to learn a detector for a visual category from a set of images, each with a binary label. We model an image as a set of overlapping rectangular windows and follow a standard approach to detection: reduce the problem of detection to the problem of binary classification of image windows. However, at training time we are only given image-level labels, which leads to a classic multiple instance learning (MIL) problem. We can think of each image as a "bag" of instances (rectangular windows) and the binary image label y = 1 specifies that the bag contains at least one instance of the target category. The label y = −1 specifies that the image contains no instances of the category. During training, no instance labels are available.</p><p>MIL problems are typically solved (locally) by finding a local minimum of a non-convex objective function, such as MI-SVM <ref type="bibr" target="#b1">(Andrews et al., 2003)</ref>. In practice, the quality of the local solution depends heavily on the quality of the initialization. We therefore focus extensively on finding a good initialization. In Section 4, we develop an initialization method by formulating a discriminative set multicover problem that can be solved approximately with a greedy algorithm. This initialization, without further MIL refinement, already produces good object detectors, validating our approach. However, we can further improve these detectors by optimizing the MIL objective. We explore two alternative MIL objectives in Section 5. The first is the standard Latent SVM (equivalently MI-SVM) objective function, which can be optimized by coordinate descent on an auxiliary objective that upper-bounds the LSVM objective. The second method is a novel technique that smoothes the Latent SVM objective and can be solved more directly with unconstrained smooth optimization techniques, such as L-BFGS <ref type="bibr" target="#b32">(Nocedal &amp; Wright, 1999)</ref>. Our experimental results show modest improvements from our smoothed LSVM formulation on a variety of MIL datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Finding objects via submodular cover</head><p>Learning with LSVM is a chicken and egg problem: The model weights are needed to infer latent annotations, but the latent annotations are needed to estimate the model weights. To initialize this process, we approximately identifying jointly present objects in a weakly supervised manner. The experiments show a significant effect from this initialization. Our procedure implements two essential assumptions: (i) the correct boxes are similar, in an appropriate feature space, across positive images (or there are few modes), and (ii) the correct boxes do not occur in the negative images. In short, in the similarity graph of all boxes we seek dense subgraphs that only span the positive images. Finding such subgraphs is a nontrivial combinatorial optimization problem.</p><p>The problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar <ref type="bibr" target="#b8">(Darrell et al., 1990;</ref><ref type="bibr" target="#b27">Leibe et al., 2004;</ref><ref type="bibr" target="#b30">Micolajczyk et al., 2006;</ref><ref type="bibr" target="#b25">Kim et al., 2011)</ref>. These approaches share the idea that a small number of exemplars or clusters should well encode the shared information we are interested in. We formalize this intuition as a flexible submodular cover problem. However, we also have label information at hand that can help identify correct boxes. We therefore integrate into our covering framework the relevance <ref type="figure">Figure 1</ref>. Illustration of the graph G with V (top row) and U (bottom row). Each box b ∈ V is connected to its closest neighbors from positive images (one from each image). Non-discriminative boxes occur in all images equally, and may not even have any boxes from positive images among their closest neighbors -and consequently no connections to U. Picking the green-framed box v in V "covers" its (green) highlighted neighbors Γ(b).</p><p>for positively versus negatively labeled images, generalizing ideas from <ref type="bibr" target="#b11">(Doersch et al., 2012)</ref>. This combination allows us to find multiple modes of the object appearance distribution.</p><p>Let P be the set of all positive images. Each image contains a set B I = {b 1 , . . . , b m } of candidate bounding boxes generated from selective search region proposals <ref type="bibr" target="#b40">(Uijlings et al., 2013)</ref>. In practice, there are about 2000 region proposal boxes per image and about 5000 training images in the PASCAL VOC dataset. Ultimately, we will define a function F (S) on sets S of boxes that measures how well the set S represents P. For each box b, we find its nearest neighbor box in each (positive and negative) image. We sort the set N (b) of all such neighbors of b in increasing order by their distance to b. This can be done in parallel. We will define a graph using these nearest neighbors that allows us to optimize for a small set of boxes S that are (i) relevant (occur in many positive images); (ii) discriminative (dissimilar to the boxes in the negative images); and (iii) complementary (capture multiple modes).</p><p>We construct a bipartite graph G = (V, U, E) whose nodes V and U are all boxes occurring in P (each b occurs once in V and once in U). The nodes in U are partitioned into groups B I : B I contains all boxes from image I ∈ P. The edges E are formed by connecting each node (box) b ∈ V to its top k neighbors in N (b) ⊆ U from positive images. <ref type="figure">Figure 1</ref> illustrates the graph. Connecting only to the top k neighbors (instead of all) implements discriminativeness: the neighbors must compete. If b occurs in positively and negatively labeled images equally, then many top-k closest neighbors in N (b) stem from negative images. Consequently, b will not be connected to many nodes (boxes from P) in G. We denote the neighborhood of a set of nodes</p><formula xml:id="formula_0">S ⊆ V by Γ(S) = {b ∈ U | ∃(v, b) ∈ E with v ∈ S}.</formula><p>Let S ⊆ V denote a set of selected boxes. We define a covering score cov I,t (S) for each I that is determined by a covering threshold t and a scalar, nondecreasing concave function g :</p><formula xml:id="formula_1">R + → R + : cov I,t (S) = g(min{t, |Γ(S) ∩ B I |}).</formula><p>(1)</p><p>This score measures how many boxes in B I are neighbors of S and thus "covered". We gain from covering up to t boxes from B I -anything beyond that is considered redundant. The total covering score of a set S ⊆ V is then</p><formula xml:id="formula_2">F (S) = I∈P cov I,t (S).<label>(2)</label></formula><p>The threshold t balances relevance and complementarity: let, for simplicity, g = id. If t = 1, then a set that maximizes cov I,t (S) contains boxes from many different images, and few from a single image. The selected neighborhoods are very complementary, but some of them may not be very relevant and cover outliers. If t is large, then any additionally covered box yields a gain, and the best boxes b ∈ V are those with the largest degree. A box has large degree if many of its closest neighbors in N (b) are from positive images. This also means b is discriminative and relevant for P.</p><formula xml:id="formula_3">Lemma 1. The function F : 2 V → R + defined in Equa- tion (2) is nondecreasing and submodular. A set function is submodular if it satisfies diminishing marginal returns: for all v and S ⊆ T ⊆ V \ {v}, it holds that F (S ∪ {v}) − F (S) ≥ F (T ∪ {v}) − F (T ).</formula><p>Proof. First, the function S → |Γ(S) ∩ B I | is a covering function and thus submodular: let S ⊂ T ⊆ V \ b. Then Γ(S) ⊆ Γ(T ) and therefore</p><formula xml:id="formula_4">|Γ(T ∪ {b})| − |Γ(T )| = |Γ(b) \ Γ(T )| (3) ≤ |Γ(b) \ Γ(S)| (4) = |Γ(S ∪ {b})| − |Γ(S)|. (5)</formula><p>The same holds when intersecting with B I . Thus, cov t,I (S) is a nondecreasing concave function of a submodular function and therefore submodular. Finally, F is a sum of submodular functions and hence also submodular. Monotonicity is obvious.</p><p>We aim to select a representative subset S ⊆ V with minimum cardinality:</p><formula xml:id="formula_5">min S⊆V |S| s.t. F (S) ≥ αF (V)<label>(6)</label></formula><p>for α ∈ (0, 1]. We optimize this via a greedy algorithm: let S 0 = ∅ and, in each step τ , add the node v that maximizes the marginal gain F (S τ ∪ {v}) − F (S τ ). Lemma 2. The greedy algorithm solves Problem (6) within an approximation factor of 1 + log</p><formula xml:id="formula_6">kg(1) g(t)−g(t−1) = O(log k).</formula><p>Lemma 2 says that the algorithm returns a set S with F ( S) ≥ αF (V) and | S| ≤ O(log k)|S * |, where S * is an optimal solution. This result follows from the analysis by <ref type="bibr" target="#b43">Wolsey (1982)</ref> (Thm. 1) adapted to our setting. To get a better intuition for the formulation (6) we list some special cases: Min-cost cover. With t = 1 and g(a) = a being the identity, Problem 6 becomes a min-cost cover problem. Such straightforward covering formulations have been used for filtering after running a detector <ref type="bibr" target="#b2">(Barinova et al., 2012)</ref>. Maximum relevance. A minimum-cost cover merely focuses on complementarity of the selected nodes S, which may include rare outliers. At the other extreme (t large), we would merely select by the number of neighbors <ref type="bibr" target="#b11">(Doersch et al. (2012)</ref> choose one single N (b) that way). Multi-cover. To smoothly move between the two extremes, one may choose t &gt; 1 and g to be sub-linear. This trades off representation, relevance, and discriminativeness.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we visualize top 5 nearest neighbors with positive labels in the first chosen cluster S 1 for all 20 classes on the PASCAL VOC data. Our experiments in Section 6 show the benefits of our framework. Potentially, the results might improve even further when using the complementary mode shifts of <ref type="bibr" target="#b12">(Doersch et al., 2013)</ref> as a pre-selection step before covering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Iterative refinement with latent variables</head><p>In this section, we review the latent SVM formulation, and we propose a simple smoothing technique enabling us to use classical techniques for unconstrained smooth optimization. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates our multiple instance learning analogy for object detection with one-bit labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Review of latent SVM</head><p>For a binary classification problem, the latent SVM formulation consists of learning a decision function involving a maximization step over a discrete set of configurations Z. Given a data point x in R p that we want to classify, and some learned model parameters w in R d , we select a label y in {−1, +1} as follows:</p><formula xml:id="formula_7">y = sign max z∈Z w φ(x, z) ,<label>(7)</label></formula><p>where z is called a "latent variable" chosen among the set Z. For object detection, Z is typically a set of bounding boxes, and maximizing over Z amounts to finding a bounding box containing the object. In deformable part models <ref type="bibr" target="#b17">(Felzenszwalb et al., 2010)</ref>, the set Z contains all possible part configurations, each part being associated to a position in the image. The resulting set Z has exponential size, but <ref type="formula" target="#formula_7">(7)</ref> can be solved efficiently with dynamic programming techniques for particular choices of φ.</p><p>Learning the model parameters w is more involved than solving a simple SVM problem. We are given some training data {(x i , y i )} n i=1 , where the vectors x i are in R p and the scalars y i are binary labels in {1, −1}. Then, the latent SVM formulation becomes</p><formula xml:id="formula_8">min w∈R d 1 2 w 2 2 + C n i=1 y i , max z∈Z w φ(x i , z) ,<label>(8)</label></formula><p>where : R × R → R is the hinge loss defined as (y,ŷ) = max(0, 1−yŷ), which encourages the decision function for each training example to be the same as the corresponding label. Similarly, other loss functions can be used such as the logistic or squared hinge loss.</p><p>Problem <ref type="formula" target="#formula_8">(8)</ref> is nonconvex and nonsmooth, making it hard to tackle. A classical technique to obtain an approximate solution is to use a difference of convex (DC) programming technique, called concave-convex procedure <ref type="bibr" target="#b45">(Yuille &amp; Rangarajan, 2003;</ref><ref type="bibr" target="#b44">Yu &amp; Joachims, 2009)</ref>. We remark that the part of (8) corresponding to negative examples is convex with respect to w. It is indeed easy to show that each corresponding term can be written as a pointwise maximum of convex functions, and is thus convex (see <ref type="bibr" target="#b3">Boyd &amp; Vandenberghe, 2004)</ref>:</p><formula xml:id="formula_9">when y i = −1, (y i , max z∈Z w φ(x i , z)) = max z∈Z (y i , w φ(x i , x)).</formula><p>On the other hand, the part corresponding to positive examples is concave, making the objective (8) suitable to DC programming. Even though such a procedure does not have any theoretical guarantee about the quality of the optimization, it monotonically decreases the value of the objective and performs relatively well when the problem is well initialized <ref type="bibr" target="#b17">(Felzenszwalb et al., 2010)</ref>.</p><p>We propose a smooth formulation of latent SVM, with two main motives. First, smoothing the objective function of latent SVM allows the use of efficient second-order optimization algorithms such as quasi-Newton <ref type="bibr" target="#b32">(Nocedal &amp; Wright, 1999</ref>) that can leverage curvature information to speed up convergence. Second, as we show later, smoothing the latent SVM boils down to considering the top-N configurations in the maximization step in place of the top-1 configuration in the regular latent SVM. As a result, the smooth latent SVM training becomes more robust to unreliable configurations in the early stages, since a larger set of plausible configurations is considered at each maximization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Smooth formulation of LSVM</head><p>In the objective (8), the hinge loss can be easily replaced by a smooth alternative, e.g., squared hinge, or logistic loss. However, the non-smooth points induced by the following functions are more difficult to handle  We propose to use a smoothing technique studied by <ref type="bibr" target="#b31">Nesterov (2005)</ref> for convex functions.</p><formula xml:id="formula_10">f xi (w) := max z∈Z w φ(x i , z).<label>(9)</label></formula><p>Nesterov's smoothing technique We only recall here the simpler form of Nesterov's results that is relevant for our purpose. Consider a non-smooth function that can be written in the following form:</p><formula xml:id="formula_11">g(w) := max u∈∆ Aw, u ,<label>(10)</label></formula><p>where u ∈ R m , A is in R m×d , and ∆ denotes the probability simplex, ∆ = {x :</p><formula xml:id="formula_12">m i=1 x i = 1, x i ≥ 0}.</formula><p>Smoothing here consists of adding a strongly convex function ω in the maximization problem</p><formula xml:id="formula_13">g µ (w) := max u∈∆ Aw, u − µ 2 ω(u) .<label>(11)</label></formula><p>The resulting function g µ is differentiable for all µ &gt; 0, and its gradient is</p><formula xml:id="formula_14">∇g µ (w) = A u (w),<label>(12)</label></formula><p>where u (w) is the unique solution of (11). The parameter µ controls the amount of smoothing. Clearly, g µ (w) → g(w) for all w ∈ W as µ → 0. As <ref type="bibr" target="#b31">Nesterov (2005)</ref> shows, for a given target approximation accuracy , there is an optimal amount of smoothing µ( ) that can be derived from a convex optimization perspective using the strong convexity parameter of ω(·) on ∆ and the (usually unknown) Lipschitz constant of g. In the experiments, we shall simply learn the parameter µ from data.</p><p>Smoothing the latent SVM We now apply Nesterov's smoothing technique to the latent SVM objective function. As we shall see, the smoothed objective takes a simple form, which can be efficiently computed in the latent SVM framework. Furthermore, smoothing latent SVM implicitly models uncertainty in the selection of the best configuration z in Z, as shown by <ref type="bibr" target="#b26">Kumar et al. (2012)</ref> for a different smoothing scheme.</p><p>In order to smooth the functions f xi defined in <ref type="formula" target="#formula_10">(9)</ref>, we first notice that</p><formula xml:id="formula_15">f xi (w) = max u∈∆ A xi w, u ,<label>(13)</label></formula><p>where A xi is a matrix of size |Z| × d such that the j-th row of A xi is the feature vector φ(x i , z j ) and z j is the j-th element of Z. Considering any strongly convex function ω and parameter µ &gt; 0, the smoothed latent SVM objective is obtained by replacing in <ref type="formula" target="#formula_8">(8)</ref> • the functions f xi by their smoothed counterparts f xi,µ obtained by applying (11) to (13);</p><p>• the non-smooth hinge-loss function l by any smooth loss.</p><p>Objective and gradient evaluations An important issue remains the computational tractability of the new formulation in terms of objective and gradient evaluations, in order to use quasi-Newton optimization techniques. The choice of the strongly convex function ω is crucial in this respect.</p><p>There are two functions known to be strongly convex on the simplex: i) the Euclidean norm, ii) the entropy. In the case of the Euclidean-norm ω(u) = u 2 2 , it turns out that the smoothed counterpart can be efficiently computed using a projection on the simplex, as shown below.</p><formula xml:id="formula_16">u (w) = arg min u∈∆ 1 µ Aw − u 2 2 ,<label>(14)</label></formula><p>where u (w) is the solution of (11). Computing Aw requires a priori O(|Z|d) operations. The projection can be computed in O(|Z|) (see, e.g., . Once u is obtained, computing the gradient requires O(d u 0 ) operations, where u 0 is the number of non-zero entries in u .</p><p>When the set Z is large, these complexities can be improved by leveraging two properties. First, the projection on the simplex is known to produce sparse solutions, the smoothing parameter µ controlling the sparsity of u ; second, the projection preserves the order of the variables. As a result, the following heuristic can be justified. Assume that for some N &lt; |Z|, we can obtain the top-N entries of Aw without exhaustively exploring Z. Then, performing the projection on these reduced set of N variables yields a vector u which can be shown to be optimal for the original problem (14) whenever u 0 &lt; N . In other words, whenever N is large enough and µ small enough, computing the gradient of f xi,µ can be done in O(N d) operations. We use this heuristic in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We performed two sets of experiments, one on a multiple instance learning dataset <ref type="bibr" target="#b1">(Andrews et al., 2003)</ref> and the other on the PASCAL VOC 2007 data <ref type="bibr">(Everingham et al.)</ref>.</p><p>The first experiment was designed to compare the multiple instance learning bag classification performance of LSVM with Smooth LSVM (SLSVM). The second experiment evaluates detection accuracy (measured in average precision) of our framework in comparison to baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Multiple instance learning datasets</head><p>We evaluated our method in Section 5 on standard multiple instance learning datasets <ref type="bibr" target="#b1">(Andrews et al., 2003)</ref>. For preprocessing, we centered each feature dimension and 2 normalize the data. For fair comparison with <ref type="bibr" target="#b1">(Andrews et al., 2003)</ref>, we use the same initialization, where the initial weight vector is obtained by training an SVM with all  <ref type="bibr" target="#b39">(Siva et al., 2012)</ref> vs our method. Red bounding boxes are constructed positive windows from <ref type="bibr" target="#b39">(Siva et al., 2012)</ref>. Green bounding boxes are constructed positive windows from our method. On learning to localize objects with minimal supervision the negative instances and bag-averaged positive instances. For this experiment, we performed 10 fold cross validation on C and µ. <ref type="table">Table 1</ref> shows the experimental results. Without the bias, our method significantly performs better than LSVM method and with the bias, our method shows modest improvement in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Weakly-supervised object detection</head><p>To implement our weakly-supervised detection system we need suitable image features for computing the nearest neighbors of each image window in Section 4 and for learning object detectors. We use the recently proposed R-CNN <ref type="bibr" target="#b21">(Girshick et al., 2014)</ref> detection framework to compute features on image windows in both cases. Specifically, we use the convolutional neural network (CNN) distributed with DeCAF , which is trained on the Im-ageNet ILSVRC 2012 dataset (using only image-level annotations). We avoid using the better performing CNN that is fine-tuned on PASCAL data, as described in <ref type="bibr" target="#b21">(Girshick et al., 2014)</ref>, because fine-tuning requires instance-level annotations.</p><p>We report detection accuracy as average precision on the standard benchmark dataset for object detection, PASCAL VOC 2007 test <ref type="bibr">(Everingham et al.)</ref>. We compare to five different baseline methods that learn object detectors with limited annotations. Note that other baseline methods use additional information besides the one-bit image-level annotations. <ref type="bibr" target="#b9">Deselaers et al. (2010;</ref>  To evaluate the efficacy of our initialization, we compare it to the state-of-the-art algorithm recently proposed by <ref type="bibr" target="#b39">(Siva et al., 2012)</ref>. Their method constructs a set of positive windows by looping over each positive image and picking the instance that has the maximum distance to its nearest neighbor over all negative instances (and thus the name negative data mining algorithm). For a fair comparison, we used the same window proposals, the same features <ref type="bibr" target="#b21">(Girshick et al., 2014)</ref>, the same L2 distance metric, and the same PASCAL 2007 detection evaluation criteria. The class mean average precision for the mining algorithm was 11.6% compared to 29.0% obtained by our initialization procedure. <ref type="figure" target="#fig_3">Figure  4</ref> visualizes some command failure modes in our implementation of <ref type="bibr" target="#b39">(Siva et al., 2012)</ref>. Since the negative mining method does not take into account the similarity among positive windows (in contrast to our method) our intuition is that the method is less robust to intra-class variations and background clutter. Therefore, it often latches onto background objects (i.e. hurdle in horse images, street signs in bus images), onto parts of the full objects (i.e. wheels of bicycles), or merges two different objects (i.e. rider and motorcycle). It is worth noting that <ref type="bibr" target="#b33">Pandey &amp; Lazebnik (2011)</ref>; <ref type="bibr" target="#b39">Siva et al. (2012)</ref> use the CorLoc metric 1 as the evaluation metric to report results on PASCAL test set. In contrast, in our experiments, we exactly follow the PAS-CAL VOC evaluation protocol (and use the PASCAL VOC devkit scoring software) and report detection average precision. <ref type="table" target="#tab_0">Table 3</ref> shows the detection result on the full PASCAL 2007 dataset. There are two baseline methods <ref type="bibr" target="#b38">(Siva &amp; Xiang, 2011;</ref><ref type="bibr" target="#b36">Russakovsky et al., 2012)</ref> which report the result on the full dataset. Unfortunately, we were not able to obtain the per-class average precision data from the authors of <ref type="bibr" target="#b36">(Russakovsky et al., 2012)</ref> except the class mean average precision (mAP) of 15.0%. As shown in <ref type="table" target="#tab_0">Table 3</ref>, the initial detector model trained from the constructed set of positive windows already produces good object detectors but we can provide further improvement by optimizing the MIL objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We developed a framework for learning to localize objects with one-bit object presence labels. Our results show that the proposed framework can construct a set of positive windows to train initial detection models and improve the models with the refinement optimization method. We achieve state-of-the-art performance for object detection with minimal supervision on the standard benchmark object detection dataset. Source code will be available on the author's website.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&amp;CP volume 32. Copyright 2014 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visualizations of top 5 nearest neighbor proposal boxes with positive labels in the first cluster, S1 for all 20 classes in PASCAL VOC dataset. From left toright, aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike,  person, plant, sheep, sofa, train, and tvmonitor.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>In the refinement stage, we formulate a multiple instance learning bag per image and bag instances correspond to each window proposals from selective search. Binary bag labels correspond to image-level annotations of whether the target object exists in the image or not. (Left) ground truth bounding boxes color coded with category labels. green: person, yellow: dog, and magenta: sofa, (Right) visualization of 100 random subset of window proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of some common failure cases of constructed positive windows by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>use a set of 799 images with bounding box annotations as meta-training data. In addition to bounding box annotations, Deselaers et al. (2010; 2012); Pandey &amp; Lazebnik (2011) use extra instance level annotations such as pose, difficult and truncated. Siva et al. (2012);<ref type="bibr" target="#b36">Russakovsky et al. (2012)</ref> use difficult instance annotations but not pose or truncated. First, we report the detection average precision on 6 subsets of classes in table 2 to compare with<ref type="bibr" target="#b9">Deselaers et al. (2010;</ref>;<ref type="bibr" target="#b33">Pandey &amp; Lazebnik (2011)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 .</head><label>3</label><figDesc>Detection average precision (%) on full PASCAL VOC 2007 test set.</figDesc><table><row><cell></cell><cell cols="8">Dataset LSVM w/o bias SLSVM w/o bias LSVM w/ bias SLSVM w/ bias</cell></row><row><cell></cell><cell>musk1</cell><cell cols="2">70.8 ± 14.4</cell><cell>80.3 ± 10.3</cell><cell cols="2">81.7 ± 14.5</cell><cell></cell><cell>79.2 ± 13.4</cell></row><row><cell></cell><cell>musk2</cell><cell cols="2">51.0 ± 10.9</cell><cell>79.5 ± 10.4</cell><cell cols="2">80.5 ± 9.9</cell><cell></cell><cell>84.3 ± 11.4</cell></row><row><cell></cell><cell>fox</cell><cell cols="2">51.5 ± 7.5</cell><cell>63.0 ± 11.8</cell><cell cols="2">57.0 ± 8.9</cell><cell></cell><cell>61.0 ± 12.6</cell></row><row><cell></cell><cell cols="3">elephant 81.5 ± 6.3</cell><cell>88.0 ± 6.7</cell><cell cols="2">81.5 ± 4.1</cell><cell></cell><cell>87.0 ± 6.3</cell></row><row><cell></cell><cell>tiger</cell><cell cols="2">79.5 ± 8.6</cell><cell>85.5 ± 6.4</cell><cell cols="2">86.0 ± 9.1</cell><cell></cell><cell>87.5 ± 7.9</cell></row><row><cell></cell><cell>trec1</cell><cell cols="2">94.3 ± 2.9</cell><cell>95.5 ± 2.6</cell><cell cols="2">95.3 ± 3.0</cell><cell></cell><cell>95.3 ± 2.8</cell></row><row><cell></cell><cell>trec2</cell><cell cols="2">69.0 ± 6.8</cell><cell>83.0 ± 6.5</cell><cell cols="2">86.5 ± 5.7</cell><cell></cell><cell>83.8 ± 7.4</cell></row><row><cell></cell><cell>trec3</cell><cell cols="2">77.5 ± 5.8</cell><cell>90.0 ± 5.8</cell><cell cols="2">85.5 ± 6.3</cell><cell></cell><cell>86.0 ± 6.5</cell></row><row><cell></cell><cell>trec4</cell><cell cols="2">77.3 ± 8.0</cell><cell>85.0 ± 5.1</cell><cell cols="2">85.3 ± 3.6</cell><cell></cell><cell>86.3 ± 5.2</cell></row><row><cell></cell><cell>trec7</cell><cell cols="2">74.5 ± 9.8</cell><cell>83.8 ± 4.0</cell><cell cols="2">82.5 ± 7.0</cell><cell></cell><cell>81.5 ± 5.8</cell></row><row><cell></cell><cell>trec9</cell><cell cols="2">66.8 ± 5.0</cell><cell>70.3 ± 5.7</cell><cell cols="2">68.8 ± 8.0</cell><cell></cell><cell>71.5 ± 6.4</cell></row><row><cell></cell><cell>trec10</cell><cell cols="2">71.0 ± 9.9</cell><cell>84.3 ± 5.4</cell><cell cols="2">80.8 ± 6.6</cell><cell></cell><cell>82.8 ± 7.3</cell></row><row><cell cols="9">Table 1. 10 fold average and standard deviation of the test accuracy on MIL dataset. The two methods start from the same initialization</cell></row><row><cell cols="2">introduced in (Andrews et al., 2003)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="6">aeroplane left right left right left right left right left right left right bicycle boat bus horse motorbike mAP</cell></row><row><cell cols="2">(Deselaers et al., 2010)</cell><cell></cell><cell cols="2">9.1 23.6 33.4 49.4 0.0</cell><cell></cell><cell cols="3">0.0 0.0 16.4 9.6</cell><cell>9.1 20.9 16.1</cell><cell>16.0</cell></row><row><cell cols="2">(Pandey &amp; Lazebnik, 2011)</cell><cell></cell><cell cols="2">7.5 21.1 38.5 44.8 0.3</cell><cell></cell><cell>0.5 0.0</cell><cell cols="2">0.3 45.9 17.3 43.8 27.2</cell><cell>20.8</cell></row><row><cell cols="2">(Deselaers et al., 2012)</cell><cell></cell><cell cols="2">5.3 18.1 48.6 61.6 0.0</cell><cell></cell><cell cols="3">0.0 0.0 16.4 29.1 14.1 47.7 16.2</cell><cell>21.4</cell></row><row><cell cols="2">(Russakovsky et al., 2012)</cell><cell></cell><cell>30.8</cell><cell>25.0</cell><cell>3.6</cell><cell cols="2">26.0</cell><cell>21.3</cell><cell>29.9</cell><cell>22.8</cell></row><row><cell cols="3">(Siva et al., 2012) with our features</cell><cell>23.2</cell><cell>15.4</cell><cell>5.1</cell><cell>2.0</cell><cell></cell><cell>6.2</cell><cell>17.4</cell><cell>11.6</cell></row><row><cell>Cover + SVM</cell><cell></cell><cell></cell><cell>23.4</cell><cell>43.5</cell><cell>8.1</cell><cell cols="2">33.9</cell><cell>24.7</cell><cell>40.2</cell><cell>29.0</cell></row><row><cell>Cover + LSVM</cell><cell></cell><cell></cell><cell>28.2</cell><cell>47.2</cell><cell>9.6</cell><cell cols="2">34.7</cell><cell>25.2</cell><cell>39.8</cell><cell>30.8</cell></row><row><cell cols="9">Table 2. Detection average precision (%) on PASCAL VOC 2007-6x2 test set. First three baseline methods report results limited to left</cell></row><row><cell cols="3">and right subcategories of the objects.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VOC2007 test</cell><cell cols="8">aero bike bird boat bottle bus car cat chair cow table dog horse mbike pson plant sheep sofa train tv mAP</cell></row><row><cell cols="9">(Siva &amp; Xiang, 2011) 13.4 44.0 3.1 3.1 0.0 31.2 43.9 7.1 0.1 9.3 9.9 1.5 29.4 38.3 4.6 0.1 0.4 3.8 34.2 0.0 13.9</cell></row><row><cell>Cover + SVM</cell><cell cols="8">23.4 43.5 22.4 8.1 6.2 33.9 33.8 30.4 0.1 17.9 11.5 17.1 24.7 40.2 2.4 14.8 21.4 15.1 31.9 6.2 20.3</cell></row><row><cell>Cover + LSVM</cell><cell cols="8">28.2 47.2 17.6 9.6 6.5 34.7 35.5 31.5 0.3 21.7 13.2 20.7 25.2 39.8 12.6 18.6 21.2 18.6 31.7 10.2 22.2</cell></row><row><cell>Cover + SLSVM</cell><cell cols="8">27.6 41.9 19.7 9.1 10.4 35.8 39.1 33.6 0.6 20.9 10.0 27.7 29.4 39.2 9.1 19.3 20.5 17.1 35.6 7.1 22.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">CorLoc was proposed by to evaluate the detection results on PASCAL train set</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Yong Jae Lee for helpful insights and discussions. H. Song was supported by Samsung Scholarship Foundation. J. Mairal and Z. Harchaoui were funded by the INRIA-UC Berkeley associated team "Hyperion", a grant from the France-Berkeley fund, the Gargantua project under program Mastodons of CNRS, and the LabEx PERSYVAL-Lab (ANR-11-LABX-0025). This work was partially supported by ONR N00014-11-1-0688, NSF, DARPA, and Toyota.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classcut for unsupervised class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="106" />
		</imprint>
	</monogr>
	<note>Support vector machines for multiple-instance learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active detection via adaptive submodularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An exemplar model for learning object classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of part-based spatial models for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation by minimal description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mid-level visual element discovery as discriminative mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning collections of part models for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoeim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>VOC2007) Results</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weakly supervised scale-invariant learning of models for visual recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The estimation of the gradient of a density function, with applications in pattern recognition. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hostetler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with stable segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convex relaxation for weakly supervised classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blocks that shout: Distinctive parts for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling latent variable uncertainty for loss-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Combined object categorization and segmentation with an implicit chape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convex and scalable weakly labeled svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PAC learning axis aligned rectangles with respect to product distributions from multiple-instance examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comp. Learning Theory</title>
		<meeting>Comp. Learning Theory</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiple object class detection with a generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Micolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovering discriminative action parts from mid-level video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching incorporating a global constraint into MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object-centric spatial pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards automatic discovery of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of models for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An analysis of the greedy algorithm for the submodular set covering problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="393" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
