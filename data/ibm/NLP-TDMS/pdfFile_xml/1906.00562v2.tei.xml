<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Self-Train for Semi-Supervised Few-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-29">29 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhe</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Self-Train for Semi-Supervised Few-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-29">29 Sep 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called learning to self-train (LST) that leverages unlabeled data and specifically metalearns how to cherry-pick and label such unsupervised data to further improve performance. To this end, we train the LST model through a large number of semi-supervised few-shot tasks. On each task, we train a few-shot model to predict pseudo labels for unlabeled data, and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning. We additionally learn a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization. We evaluate our LST method on two ImageNet benchmarks for semisupervised few-shot classification and achieve large improvements over the stateof-the-art method. Code is at github.com/xinzheli1217/learning-to-self-train.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today's deep neural networks require large amounts of labeled data for supervised training and best performance <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>. Their potential applications to the small-data regimes are thus limited. There has been growing interest in reducing the required amount of data, e.g. to only 1-shot <ref type="bibr" target="#b11">[12]</ref>. One of the most powerful methods is meta-learning that transfers the experience learned from similar tasks to the target task <ref type="bibr" target="#b2">[3]</ref>. Among different meta strategies, gradient descent based methods are particularly promising for today's neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25]</ref>. Another intriguing idea is to additionally use unlabeled data. Semi-supervised learning using unlabeled data with a relatively small set of labeled ones has obtained good performance on standard datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. A classic, intuitive and simple method is e.g. self-training. It first trains a supervised model with labeled data, and then enlarges the labeled set based on the most confident predictions (called pseudo labels) on unlabeled data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref>. It can outperform regularization based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>, especially when labeled data is scarce.</p><p>The focus of this paper is thus on the semi-supervised few-shot classification (SSFSC) task. Specifically, there are few labeled data and a much larger amount of unlabeled data for training classifiers. To tackle this problem, we propose a new SSFSC method called learning to self-train (LST) that successfully embeds a well-performing semi-supervised method, i.e. self-training, into the meta gradient descent paradigm. However, this is non-trivial, as directly applying self-training recursively may result in gradual drifts and thus adding noisy pseudo-labels <ref type="bibr" target="#b39">[40]</ref>. To address this issue, we propose both to meta-learn a soft weighting network (SWN) to automatically reduce the effect of noisy labels, as well as to fine-tune the model with only labeled data after every self-training step.</p><p>Specifically, our LST method consists of inner-loop self-training (for one task) and outer-loop metalearning (over all tasks). LST meta-learns both to initialize a self-training model and how to cherrypick from noisy labels for each task. An inner loop starts from the meta-learned initialization by which a task-specific model can be fast adapted with few labeled data. Then, this model is used to predict pseudo labels, and labels are weighted by the meta-learned soft weighting network (SWN). Self-training consists of re-training using weighted pseudo-labeled data and fine-tuning on few labeled data. In the outer loop, the performance of these meta-learners are evaluated via an independent validation set, and parameters are optimized using the corresponding validation loss.</p><p>In summary, our LST method learns to accumulate self-supervising experience from SSFSC tasks in order to quickly adapt to a new few-shot task. Our contribution is three-fold. (i) A novel selftraining strategy that prevents the model from drifting due to label noise and enables robust recursive training. (ii) A novel meta-learned cherry-picking method that optimizes the weights of pseudo labels particularly for fast and efficient self-training. (iii) Extensive experiments on two versions of ImageNet benchmarks -miniImageNet <ref type="bibr" target="#b33">[34]</ref> and tieredImageNet <ref type="bibr" target="#b21">[22]</ref>, in which our method achieves top performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Few-shot classification (FSC). Most FSC works are based on supervised learning. They can be roughly divided into four categories: (1) data augmentation based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> generate data or features in a conditional way for few-shot classes; (2) metric learning methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> learn a similarity space of image features in which the classification should be efficient with few examples; (3) memory networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref> design special networks to record training "experience" from seen tasks, aiming to generalize that to the learning of unseen ones; and (4) gradient descent based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref> learn a meta-learner in the outer loop to initialize a base-learner for the inner loop that is then trained on a novel few-shot task. In our LST method, the outer-inner loop optimization is based on the gradient descent method. Different to previous works, we propose a novel meta-learner that assigns weights to pseudo-labeled data, particularly for semi-supervised few-shot learning.</p><p>Semi-supervised learning (SSL). SSL methods aim to leverage unlabeled data to obtain decision boundaries that better fit the underlying data structure <ref type="bibr" target="#b17">[18]</ref>. The Π-Model applies a simple consistency regularization <ref type="bibr" target="#b8">[9]</ref>, e.g. by using dropout, adding noise and data augmentation, in which data is automatically "labeled". Mean Teacher is more stable version of the Π-Model by making use of a moving average technique <ref type="bibr" target="#b31">[32]</ref>. Visual Adversarial Training (VAT) regularizes the network against the adversarial perturbation, and it has been shown to be an effective regularization <ref type="bibr" target="#b14">[15]</ref>. Another popular method is Entropy Minimization that uses a loss term to encourage low-entropy (more confident) predictions for unlabeled data, regardless of their real classes <ref type="bibr" target="#b5">[6]</ref>. Pseudo-labeling is a self supervised learning method that relies on the predictions of unlabeled data, i.e. pseudo labels <ref type="bibr" target="#b1">[2]</ref>. It can outperform regularization based methods, especially when labeled data is scarce <ref type="bibr" target="#b17">[18]</ref> as in our envisioned setting. We thus use this method in our inner loop training.</p><p>Semi-supervised few-shot classification (SSFSC). Semi-supervised learning on FSC tasks aims to improve the classification accuracy by adding a large number of unlabeled data in training. Ren et al. proposed three semi-supervised variants of ProtoNets <ref type="bibr" target="#b28">[29]</ref>, basically using Soft k-Means method to tune clustering centers with unlabeled data. A more recent work used the transductive propagation network (TPN) <ref type="bibr" target="#b36">[37]</ref> to propagate labels from labeled data to unlabeled ones, and meta-learned the key hyperparameters of TPN. Differently, we build our method based on the simple and classical self-training <ref type="bibr" target="#b38">[39]</ref> and meta gradient descent method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref> without requiring a new design of a semi-supervised network. Rohrbach et al. <ref type="bibr" target="#b22">[23]</ref> proposed to further leverage external knowledge, such as the semantic attributes of categories, to solve not only few-shot but also zero-shot problems. Similarly, we expect further gains of our approach when using similar external knowledge in our future work. <ref type="figure" target="#fig_1">Figure 1</ref>: The pipeline of the proposed LST method on a single (2-class, 3-shot) task. The prototype of a class is the mean feature in the class, and SWN is the soft weighting network whose optimization procedure is given in <ref type="figure" target="#fig_0">Figure 2</ref> and Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem definition and denotation</head><p>In conventional few-shot classification (FSC), each task has a small set of labeled training data called support set S, and another set of unseen data for test, called query set Q. Following <ref type="bibr" target="#b21">[22]</ref>, we denote another set of unlabeled data as R to be used for semi-supervised learning (SSL). R may or may not contain data of distracting classes (not included in S).</p><p>Our method follows the uniform episodic formulation of meta-learning <ref type="bibr" target="#b33">[34]</ref> that is different to traditional classification in three aspects. (1) Main phases are meta-train and meta-test (instead of train and test), each of which includes training (and self-training in our case) and test. (2) Samples in meta-train and meta-test are not datapoints but episodes (SSFSC tasks in our case). (3) Meta objective is not to classify unseen datapoints but to fast adapt the classifier on a new task. Let's detail the denotations. Given a dataset D for meta-train, we first sample SSFSC tasks {T } from a distribution p(T ) such that each T has few samples from few classes, e.g. 5 classes and 1 sample per class. T has a support set S plus an unlabeled set R (with a larger number of samples) to train a task-specific SSFSC model, and a query set Q to compute a validation loss used to optimize meta-learners. For meta-test, given an unseen new dataset D un , we sample a new SSFSC task T un . "Unseen" means there is no overlap of image classes (including distracting classes) between meta-test and meta-train tasks . We first initialize a model and weight pseudo labels for this unseen task, then self-train the model on S un and R un . We evaluate the self-training performance on a query set Q un . If we have multiple unseen tasks, we report average accuracy as the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning to self-train (LST)</head><p>The computing flow of applying LST to a single task is given in <ref type="figure" target="#fig_1">Figure 1</ref>. It contains pseudo-labeling unlabeled samples by a few-shot model pre-trained on the support set; cherry-picking pseudo-labeled samples by hard selection and soft weighting; re-training on picked "cherries", followed by a finetuning step; and the final test on a query set. On a meta-train task, final test acts as a validation to output a loss for optimizing meta-learned parameters of LST, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pseudo-labeling &amp; cherry-picking unlabeled data</head><p>Pseudo-labeling. This step deploys a supervised few-shot method to train a task-specific classifier θ on the support set S. Pseudo labels of the unlabeled set R are then predicted by θ. Basically, we can use different methods to learn θ. We choose a top-performing one -meta-transfer learning (MTL) <ref type="bibr" target="#b29">[30]</ref> (for fair comparison we also evaluate this method as a component of other semisupervised methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>) that is based on simple and elegant gradient descent optimization <ref type="bibr" target="#b2">[3]</ref>. In the outer loop meta-learning, MTL learns scaling and shifting parameters Φ ss to fast adapt a largescale pre-trained network Θ (e.g. for 64 classes and 600 images per class on miniImageNet <ref type="bibr" target="#b33">[34]</ref>) to a new learning task. In the inner loop base-learning, MTL takes the last fully-connected layer as classifier θ and trains it with S.</p><p>In the following, we detail the pseudo-labeling process on a task T . Given the support set S, its loss is used to optimize the task-specific base-learner (classifier) θ by gradient descent:</p><formula xml:id="formula_0">θ t ← θ t−1 − α∇ θt−1 L S; [Φ ss , θ t−1 ] ,<label>(1)</label></formula><p>where t is the iteration index and t ∈ {1, ..., T }. The initialization θ 0 is given by θ ′ which is metalearned (see Section 4.2). Once trained, we feed θ T with unlabeled samples R to get pseudo labels Y R as follows,</p><formula xml:id="formula_1">Y R = f [Φss,θT ] (R),<label>(2)</label></formula><p>where f indicates the classifier function with parameters θ T and feature extractor with parameters Φ ss (the frozen Θ is omitted for simplicity).</p><p>Cherry-picking. As directly applying self-training on pseudo labels Y R may result in gradual drifts due to label noises, we propose two countermeasures in our LST method. The first is to meta-learn the SWN that automatically reweighs the data points to up-weight the more promising ones and down-weighs the less promising ones, i.e. learns to cherry-pick. Prior to this step we also perform hard selection to only use the most confident predictions <ref type="bibr" target="#b32">[33]</ref>. The second countermeasure is to fine-tune the model with only labeled data (in S) after every self-training step (see Section 4.2).</p><p>Specifically, we refer to the confident scores of Y R to pick-up the top Z samples per class. Therefore, we have ZC samples from C classes in this pseudo-labeled dataset, namely R p . Before feeding R p to re-training, we compute their soft weights by a meta-learned soft weighting network (SWN), in order to reduce the effect of noisy labels. These weights should reflect the relations or distances between pseudo-labeled samples and the representations of C classes. We refer to a supervised method called RelationNets <ref type="bibr" target="#b30">[31]</ref> which makes use of relations between support and query samples for traditional few-shot classification.</p><p>First, we compute the prototype feature of each class by averaging the features of all its samples. In the 1-shot case, we use the unique sample feature as prototype. Then, given a pseudo-labeled sample (x i , y i ) ∈ R p , we concatenate its feature with C prototype features, then feed them to SWN. The weight on the c-th class is as follows,</p><formula xml:id="formula_2">w i,c = f Φswn f Φss (x i ); k f Φss (x c,k ) K ,<label>(3)</label></formula><p>where c is the class index and c ∈ [1, ..., C], k is the sample index in one class and k ∈ [1, ..., K], x c,k ∈ S, and Φ swn denotes the parameters of SWN whose optimization procedure is given in Section 4.2. Note that {w i,c } have been normalized over C classes through a softmax layer in SWN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-training on cherry-picked data</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref> (inner loop), our self-training contains two main stages. The first stage contains a few steps of re-training on the pseudo-labeled data R p in conjunction with support set S, and the second are fine-tuning steps with only S.</p><p>We first initialize the classifier parameters as θ 0 ← θ ′ , where θ ′ is meta-optimized by previous tasks in the outer loop. We then update θ 0 by gradient descent on R p and S. Assuming there are T iterations, re-training takes the first 1 ∼ m iterations and fine-tuning takes the rest m + 1 ∼ T . For t ∈ {1, ..., m}, we have where α is the base learning rate. L denotes the classification losses that are different for samples from different sets, as follows,</p><formula xml:id="formula_3">θ t ← θ t−1 − α ▽ θt−1 L S ∪ R p ; [Φ swn , Φ ss , θ t−1 ] ,<label>(4)</label></formula><formula xml:id="formula_4">L S ∪ R p ; [Φ swn , Φ ss , θ t ] = L ce f [Φswn,Φss,θt] (x i ), y i ), if (x i , y i ) ∈ S, L ce w i ⊙ f [Φswn,Φss,θt] (x i ), y i ), if (x i , y i ) ∈ R p ,<label>(5)</label></formula><p>where L ce is the cross-entropy loss. It is computed in a standard way on S. For a pseudo-labeled sample in R p , its predictions are weighted by</p><formula xml:id="formula_5">w i = {w i,c } C c=1 before going into the softmax layer. For t ∈ {m + 1, ..., T }, θ t is fine-tuned on S as θ t ← θ t−1 − α ▽ θt−1 L(S; [Φ swn , Φ ss , θ t−1 ]).<label>(6)</label></formula><p>Iterating self-training using fine-tuned model. Conventional self-training often follows an iterative procedure, aiming to obtain a gradually enlarged labeled set <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. Similarly, our method can be iterated once a fine-tuned model θ T is obtained, i.e. to use θ T to predict better pseudo labels on R and re-train θ again. There are two scenarios: (1) the size of R is small, e.g. 10 samples per class, so that self-training can only be repeated on the same data; and (2) that size is infinite (at least big enough, e.g. 100 samples per class), we can split it into multiple subsets (e.g. 10 subsets and each one has 10 samples) and do the recursive learning each time on a new subset. In this paper, we consider the second scenario. We also validate in experiments that first splitting subsets and then recursive training is better than using the whole set for one re-training round.</p><p>Meta-optimizing Φ swn , Φ ss and θ ′ . Gradient descent base methods typically use θ T to compute the validation loss on query set Q used for optimizing meta-learner <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref>. In this paper, we have multiple meta-learners with the parameters Φ swn , Φ ss and θ ′ . We propose to update them by the validation losses calculated at different self-training stages, aiming to optimize them particularly towards specific purposes. Φ ss and θ ′ work for feature extraction and final classification affecting on the whole self-training. We optimize them by the loss of the final model θ T . While, Φ swn produces soft weights to refine the re-training steps, and its quality should be evaluated by re-trained classifier θ m . We thus use the loss of θ m to optimize it. Two optimization functions are as follows,</p><formula xml:id="formula_6">Φ swn =: Φ swn − β 1 ▽ Φswn L(Q; [Φ swn , Φ ss , θ m ]),<label>(7)</label></formula><formula xml:id="formula_7">[Φ ss , θ ′ ] =: [Φ ss , θ ′ ] − β 2 ▽ [Φss,θ ′ ] L(Q; [Φ swn , Φ ss , θ T ]),<label>(8)</label></formula><p>where β 1 and β 2 are meta learning rates that are manually set in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed LST method in terms of few-shot image classification accuracy in semisupervised settings. Below we describe the two benchmarks we evaluate on, details of settings, comparisons to state-of-the-art methods, and an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and implementation details</head><p>Datasets. We conduct our experiments on two subsets of ImageNet <ref type="bibr" target="#b23">[24]</ref>. miniImageNet was firstly proposed by Vinyals et al. <ref type="bibr" target="#b33">[34]</ref> and has been widely used in supervised FSC works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5]</ref>, as well as semi-supervised works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b21">22]</ref>. In total, there are 100 classes with 600 samples of 84 × 84 color images per class. In the uniform setting, these classes are divided into 64, 16, and 20 respectively for meta-train, meta-validation, and meta-test. tieredImageNet was proposed by Ren et al. <ref type="bibr" target="#b21">[22]</ref>. It includes a larger number of categories, 608 classes, than miniImageNet. These classes are from 34 super-classes which are divided into 20 for meta-train (351 classes), 6 for meta-validation (97 classes), and 8 for meta-test (160 classes). The average image number per class is 1281, which is much bigger than that on miniImageNet. All images are resized to 84 × 84. On both datasets, we follow the semi-supervised task splitting method used in previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>. We consider the 5-way classification, and sample 5-way, 1-shot (5-shot) task to contain 1 (5) samples as the support set S and 15 samples (a uniform number) samples as the query set Q. Then, on the 1-shot (5-shot) task, we have 30 (50) unlabeled images per class in the unlabeled set R. After hard selection, we filter out 10 (20) samples and only use the rest 20 (30) confident ones to do soft weighting and then re-training. In the recursive training, we use a larger unlabeled data pool containing 100 samples from which each iteration we can sample a number of samples, i.e. 30 (50) samples for 1-shot (5-shot).</p><p>Network architectures of Θ and Φ ss are based on ResNet-12 (see details of MTL <ref type="bibr" target="#b29">[30]</ref>) which consist of 4 residual blocks and each block has 3 CONV layers with 3 × 3 kernels. At the end of each block, a 2 × 2 max-pooling layer is applied. The number of filters starts from 64 and is doubled every next block. Following residual blocks, a mean-pooling layer is applied to compress the feature maps to a 512-dimension embedding. The architecture of SWN consists of 2 CONV layers with 3 × 3 kernels in 64 filters, followed by 2 FC layers with the dimensionality of 8 and 1, respectively.</p><p>Hyperparameters. We follow the settings used in MTL <ref type="bibr" target="#b29">[30]</ref>. Base-learning rate α (in Eq. 1, Eq. 4 and Eq. 6) is set to 0.01. Meta-learning rates β 1 and β 2 (in Eq. 7 and Eq. 8) are set to 0.001 initially and decay to the half value every 1k meta iterations until a minimum value 0.0001 is reached. We use a meta-batch size of 2 and run 15k meta iterations. In recursive training, we use 6 (3) recursive stages for 1-shot (5-shot) tasks. Each recursive stage contains 10 re-training and 30 fine-tuning steps.</p><p>Comparing methods. In terms of SSFSC, we have two methods, namely Soft Masked k-Means <ref type="bibr" target="#b21">[22]</ref> and TPN <ref type="bibr" target="#b36">[37]</ref> to compare with. Their original models used a shallow, i.e. 4CONV <ref type="bibr" target="#b2">[3]</ref> trained from scratch, for feature extraction. For fair comparison, we implement the MTL as a component of their models in order to use deeper nets and pre-trained models which have been proved better. In addition, we run these experiments using the maximum budget of unlabeled data, i.e. 100 samples per class. We also compare to the state-of-the-art supervised FSC models which are closely related to ours. They are based on either data augmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> or gradient descent <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Ablative settings. In order to show the effectiveness of our LST method, we design following settings belonging to two groups: with and without meta-training. Following are the detailed ablative settings. no selection denotes the baseline of once self-training without any selection of pseudo labels. hard denotes hard selection of pseudo labels. hard with meta-training means meta-learning only [Φ ss , θ T ]. soft denotes soft weighting on selected pseudo labels by meta-learned SWN. recursive applies multiple iterations of self-training based on fine-tuned models, see Section 4.2. Note that this recursive is only for the meta-test task, as the meta-learned SWN can be repeatedly used. We also have a comparable setting to recursive called mixing in which we mix all unlabeled subsets used in recursive and run only one re-training round (see the last second paragraph of Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and analyses</head><p>We conduct extensive experiments on semi-supervised few-shot classification. In <ref type="table" target="#tab_2">Table 1</ref>, we present our results compared to the state-of-the-art FSC methods, respectively on miniImageNet and tiered-ImageNet. In <ref type="table">Table 2</ref>, we provide experimental results for ablative settings and comparisons with the state-of-the-art SSFSC methods. In <ref type="figure">Figure 3</ref>, we show the effect of using different numbers of re-training steps (i.e. varying m in <ref type="figure" target="#fig_0">Figure 2</ref>).  <ref type="bibr" target="#b40">[41]</ref> ResNet-12 52.71 ± 0.64 68.63 ± 0.67 adaResNet, <ref type="bibr" target="#b16">[17]</ref> ResNet-12 ‡ 56.88 ± 0.62 71.94 ± 0.57 LEO, <ref type="bibr" target="#b24">[25]</ref> WRN-28-10 (pre) 61.76 ± 0.08 77.59 ± 0.12 MTL, <ref type="bibr" target="#b29">[30]</ref> ResNet-12 (pre) 61.2 ± 1.8 75.5 ± 0.9 MetaOpt-SVM, <ref type="bibr">[</ref>   <ref type="table">Table 2</ref>: Classification accuracy (%) in ablative settings (middle blocks) and related SSFSC works (bottom block), on miniImageNet ("mini") and tieredImageNet ("tiered"). "fully supervised" means the labels of unlabeled data are used. "w/D" means using unlabeled data from 3 distracting classes that are excluded in the support set <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b21">22]</ref>. The results of using a small unlabeled set, 5 per class <ref type="bibr" target="#b21">[22]</ref>, are given in the supplementary materials.</p><p>Overview for two datasets with FSC methods. In the upper part of <ref type="table" target="#tab_2">Table 1</ref>, we present SSFSC results on miniImageNet. We can see that LST achieves the best performance for the 1-shot (70.1%) setting, compared to all other FSC methods. Besides, it tackles the 5-shot episodes with an accuracy of 78.7%. This result is slightly better than 78.6% reported by <ref type="bibr" target="#b9">[10]</ref>, which uses various regularization techniques like data augmentation and label smoothing. Compared to the baseline method MTL <ref type="bibr" target="#b29">[30]</ref>, LST improves the accuracies by 8.9% and 3.2% respectively for 1-shot and 5-shot, which proves the efficiency of LST using unlabeled data. In the lower part of <ref type="table" target="#tab_2">Table 1</ref>, we present the results on tieredImageNet. Our LST performs best in both 1-shot (77.7%) and 5-shot (85.2%) and surpasses (a) (b) (c) <ref type="figure">Figure 3</ref>: Classification accuracy on 1-shot miniImageNet using different numbers of re-training steps, e.g. m = 2 means using 2 steps f re-training and 38 steps (40 steps in total) of fine-tuning at every recursive stage. Each curve shows the results obtained at the final stage. Methods are (a) our LST; (b) recursive, hard (no meta) with MTL <ref type="bibr" target="#b29">[30]</ref>; and (c) recursive, hard (no meta) simply initialized by pre-trained ResNet-12 model <ref type="bibr" target="#b29">[30]</ref>. Results on tieredImageNet are given in the supplementary.</p><p>the state-of-the-art method [10] by 11.7% and 3.6% respectively for 1-shot and 5-shot. Compared to MTL <ref type="bibr" target="#b29">[30]</ref>, LST improves the results by 12.1% and 6.6% respectively for 1-shot and 5-shot.</p><p>Hard selection. In <ref type="table">Table 2</ref>, we can see that the hard selection strategy often brings improvements. For example, compared to no selection, hard can boost the accuracies of 1-shot and 5-shot by 3.3% and 1.1% respectively on miniImageNet, 2.4% and 0.4% respectively on tieredImageNet. This is due to the fact that selecting more reliable samples can relieve the disturbance brought by noisy labels. Moreover, simply repeating this strategy (recursive,hard) brings about 1% average gain.</p><p>SWN. The meta-learned SWN is able to reduce the effect of noisy predictions in a soft way, leading to better performance. When using SWN individually, soft achieves comparable results with two previous SSFSC methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>. When using SWN in cooperation with hard selection (hard,soft) achieves 0.9% improvement on miniImageNet for both 1-shot and 5-shot compared to hard(Φ ss , θ ′ ), which also shows that SWN and the hard selection strategy are complementary.</p><p>Recursive self-training. Comparing the results of recursive,hard with hard, we can see that by doing recursive self-training when updating θ, the performances are improved in both "meta" and "no meta" scenarios. E.g., it boosts the results by 5.1% when applying recursive training to hard,soft for miniImageNet 1-shot. However, when using mixing,hard,soft that learns all unlabeled data without recursive, the improvement reduces by 3.9%. These observations show that recursive self-training can successfully leverage unlabeled samples. However, this method sometimes brings undesirable results in the cases with distractors. E.g., compared to hard, the recursive,hard brings 0.4% and 0.5% reduction for 1-shot on miniImagenet and tieredImagenet respectively, which might be due to the fact that disturbances caused by distractors in early recursive stages propagate to later stages.</p><p>Comparing with the state-of-the-art SSFSC methods. We can see that Masked Soft k-Means <ref type="bibr" target="#b21">[22]</ref> and TPN <ref type="bibr" target="#b36">[37]</ref> improve their performances by a large margin (more than 10% for 1-shot and 7% for 5shot) when they are equipped with MTL and use more unlabeled samples (100 per class). Compared with them, our method (recursive,hard,soft) achieves more than 7.4% and 4.5% improvements respectively for 1-shot and 5-shot cases with the same amount of unlabeled samples on miniImagenet. Similarly, our method also surpasses TPN by 5.6% and 1.9% for 1-shot and 5-shot on tieredImagenet. Even though our method is slightly more effected when adding distractors to the unlabeled dataset, we still obtain the best results compared to others.</p><p>In order to better understand our method and validate the robustness, we present more in-depth results regarding the key components, namely re-training steps, distracting classes, pseudo labeling accuracies, and using different architectures as backbone, in the following texts.</p><p>Number of re-training steps. In <ref type="figure">Figure 3</ref>, we present the results for different re-training steps. <ref type="figure">Figure 3</ref>(a), (b) and (c) show different settings respectively: LST; recursive,hard that uses the offthe-shelf MTL method; and recursive,hard that replaces MTL with pre-trained ResNet-12 model. All three figures show that re-training indeed achieves better results, but too many re-training steps may lead to drifting problems and cause side effects on performance. The first two settings reach best performance at 10 re-training steps while the third one needs 20 re-training steps. That means   <ref type="figure">Figure 4</ref>: Classification accuracy on miniImageNet 1-shot (a) and tieredImageNet 1-shot (b), using different numbers of distracting classes.</p><p>MTL-based methods (LST and the recursive,hard) achieve faster convergence compared to the one directly using pre-trained ResNet-12 model.</p><p>Quantitative analyses on the number of distracting classes. In <ref type="figure">Figure 4</ref>, we show the effects of distracting classes on our LST and related methods (improved versions with MTL) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref>. More distracting classes cause more performance deduction for all methods. Our LST achieves the top performance, especially more than 2% higher than TPN <ref type="bibr" target="#b36">[37]</ref> in the hardest case with 7 distracting classes. Among our different settings, we can see that LST with less re-training steps, i.e., a smaller m value, works better for reducing the effect from a larger number of distracting classes.</p><p>Displaying the performance of pseudo-labeling.. Taking the miniImageNet 1-shot as an example, we record the accuracy of pseudo labeling for meta-training and meta-test (based on our best model recursive, hard, soft), in <ref type="table" target="#tab_5">Table 3</ref> and <ref type="table" target="#tab_6">Table 4</ref>, respectively. In meta-training, we can see the accuracy grows from 47.0% (iter=0) to 71.5% (iter=15k), and it reaches saturation after 2k iterations. There are 6 recursive stages during meta-test. From stage-2 to stage-6, the average accuracy of 600 metatest tasks using our best method increases from 59.8% to 68.8%.   Generalization ability. Our LST approach is in principle able to generalize to other optimizationbased FSC methods. To validate this, we replace MTL with a classical method called MAML <ref type="bibr" target="#b2">[3]</ref>. We implement the experiments of MAML-based LST (using recursive,hard,soft) and compare with the same 4CONV-arch model TPN <ref type="bibr" target="#b36">[37]</ref>. On miniImagenet 1-shot, our method gets the accuracy of 54.8% (52.0% for w/D), outperforming TPN by 2.0% (1.6% for w/D). On the more challenging dataset tieredImageNet (1-shot) we achieve even higher superiority, i.e., 2.9% (2.0% for w/D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a novel LST approach for semi-supervised few-shot classification. A novel recursivelearning-based self-training strategy is proposed for robust convergence of the inner loop, while a cherry-picking network is meta-learned to select and label the unsupervised data optimized in the outer loop. Our method is general in the sense that any optimization-based few-shot method with different base-learner architectures can be employed. On two popular few-shot benchmarks, we found consistent improvements over both state-of-the-art FSC and SSFSC methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary materials</head><p>These supplementary materials include the additional results of using different numbers of stages in the recursive training in our LST method (recursive, hard, soft) ( §A), the supplementary results (on the tieredImageNet dataset) of <ref type="figure">Figure 3</ref> in the main paper ( §B), and the comparable results using a very limited number of unlabeled data, i.e. 5 unlabeled samples per class ( §C). There are also experimental results about when our LST method is equiped with different backbones ( §D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Using different numbers of recursive stages</head><p>During meta-validation, we test our method using different numbers of recursive stages, and show the results in <ref type="figure">Figure 5</ref>. We observe that the performance of our method is saturated when running after e.g. 6 stages. In experiments, we split 100 samples (per class) as the unlabeled dataset. At each recursive stage, we sample a subset, i.e. 30 for 1-shot and 50 for 5-shot. After a few stages, the model has sampled and learned all unlabeled samples, therefore, its performance gets saturated. We choose the peak values, so we use 6 stages for 1-shot and 3 for 5-shot during meta-test, on both miniImageNet and tieredImageNet.</p><p>(a) (b) <ref type="figure">Figure 5</ref>: Meta-validation results (classification accuracy) using different numbers of recursive stages, in the 1-shot (a) and 5-shot (b) settings on the miniImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Using different numbers of re-training steps</head><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, we report the results on tieredImageNet 1-shot, using different numbers of re-training steps, as the supplementary of <ref type="figure">Figure 3</ref> in the main paper. The same as in <ref type="figure">Figure 3</ref>, each curve shows the results obtained at the final recursive stage. Corresponding methods are (a) our LST, (b) our ablative method recursive, hard (no meta) with off-the-shelf MTL model <ref type="bibr" target="#b29">[30]</ref>, and (c) the recursive, hard (no meta) that directly uses pre-trained ResNet-12 model <ref type="bibr" target="#b29">[30]</ref>. We can observe that very few re-training steps, i.e. 2 steps, are enough for our LST model to converge to the best performance, similar to the conclusion drawn from the results on miniImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Using a small number of unlabeled samples</head><p>We also consider using limited number of unlabeled samples (5 per class) in the experiments. In this setting, we evaluate our LST method (the version without recursive due to few unlabeled data) as well as related methods, Masked Soft k-Means and TPN. Note that same with <ref type="table">Table 2</ref> in the main paper, these related methods are equipped with MTL, i.e. using pre-trained ResNet-12 as backbone, and using more efficient meta operations (scaling and shifting) in the feature extraction part. As shown in <ref type="table">Table 5</ref>, our method achieves the best performance compared to other two methods, on both benchmarks.   <ref type="table">Table 5</ref>: Classification accuracy (%) using a limited number of unlabeled samples (5 per class) on two benchmarks -miniImageNet ("mini") and tieredImageNet ("tiered"). "w/D" means using unlabeled data from distracting classes that are excluded in the support set <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Generalization ability</head><p>We incorporate the 4CONV arch. of MAML <ref type="bibr" target="#b2">[3]</ref> and the recent FSC method LEO <ref type="bibr" target="#b24">[25]</ref> into our LST, respectively. The results are shown in <ref type="table">Table 6</ref>. For example, on tieredImageNet 1-shot, LST-MAML-4CONV outperforms TPN-4CONV [37] by 2.9% and 2.0% (w/D). LST-LEO-ResNet12 outperforms TPN-ResNet12 by 3.8% and 2.8% (w/D).</p><p>MAML <ref type="bibr" target="#b2">[3]</ref> LEO <ref type="bibr" target="#b24">[25]</ref> mini <ref type="formula">(</ref>  <ref type="table">Table 6</ref>: 5-way, 1-shot classification accuracy (%) by replacing our base network MTL(ResNet-12) <ref type="bibr" target="#b29">[30]</ref> with MAML(4CONV) <ref type="bibr" target="#b2">[3]</ref> and LEO(ResNet-12) <ref type="bibr" target="#b24">[25]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Outer-loop and inner-loop training procedures in our LST method. The inner loop in the red box contains the m steps of re-training (with S and R p ) and T − m steps of fine-tuning (with only S). In recursive training, the fine-tuned θ T replaces the initial MTL learned θ T (see Section 4.1) for the pseudo-labeling at the next stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Classification accuracy in the 1-shot tieredImageNet, using different numbers of retraining steps, e.g. m = 2 means using 2 steps of re-training and 38 steps (40 steps in total) of fine-tuning at every recursive stage. Each curve shows the results obtained at the final stage. Methods are (a) our LST; (b) recursive, hard (no meta) with MTL<ref type="bibr" target="#b29">[30]</ref>; and (c) recursive, hard (no meta) simply initialized by pre-trained ResNet-12 model<ref type="bibr" target="#b29">[30]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The 5-way, 1-shot and 5-shot classification accuracy (%) on miniImageNet and tieredIma-geNet datasets. "pre" means pre-trained for a single classification task using all training datapoints.</figDesc><table><row><cell></cell><cell></cell><cell>mini</cell><cell></cell><cell>tiered</cell><cell></cell><cell cols="2">mini w/D</cell><cell>tiered w/D</cell></row><row><cell></cell><cell></cell><cell>1(shot)</cell><cell>5</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell></row><row><cell cols="2">fully supervised (upper bound)</cell><cell>80.4</cell><cell>83.3</cell><cell cols="2">86.5 88.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>no selection</cell><cell>59.7</cell><cell>75.2</cell><cell cols="2">67.4 81.1</cell><cell cols="2">54.4 73.3</cell><cell>66.1 79.4</cell></row><row><cell>no meta</cell><cell>hard</cell><cell>63.0</cell><cell>76.3</cell><cell cols="2">69.8 81.5</cell><cell cols="2">61.6 75.3</cell><cell>68.8 81.1</cell></row><row><cell></cell><cell>recursive,hard</cell><cell>64.6</cell><cell>77.2</cell><cell cols="2">72.1 82.4</cell><cell cols="2">61.2 75.7</cell><cell>68.3 81.1</cell></row><row><cell></cell><cell>hard (Φss, θ ′ )</cell><cell>64.1</cell><cell>76.9</cell><cell cols="2">74.7 83.2</cell><cell cols="2">62.9 75.4</cell><cell>73.4 82.5</cell></row><row><cell></cell><cell>soft</cell><cell>62.8</cell><cell>75.9</cell><cell cols="2">73.1 82.8</cell><cell cols="2">61.1 74.6</cell><cell>72.1 81.7</cell></row><row><cell>meta</cell><cell>hard,soft</cell><cell>65.0</cell><cell>77.8</cell><cell cols="2">75.4 83.4</cell><cell cols="2">63.7 76.2</cell><cell>74.1 82.9</cell></row><row><cell></cell><cell>recursive,hard,soft</cell><cell>70.1</cell><cell>78.7</cell><cell cols="2">77.7 85.2</cell><cell cols="2">64.1 77.4</cell><cell>73.5 83.4</cell></row><row><cell></cell><cell>mixing,hard,soft</cell><cell>66.2</cell><cell>77.9</cell><cell cols="2">75.6 84.6</cell><cell cols="2">64.5 76.5</cell><cell>73.6 83.8</cell></row><row><cell cols="2">Masked Soft k-Means with MTL</cell><cell>62.1</cell><cell>73.6</cell><cell cols="2">68.6 81.0</cell><cell cols="2">61.0 72.0</cell><cell>66.9 80.2</cell></row><row><cell cols="2">TPN with MTL</cell><cell>62.7</cell><cell>74.2</cell><cell cols="2">72.1 83.3</cell><cell cols="2">61.3 72.4</cell><cell>71.5 82.7</cell></row><row><cell cols="2">Masked Soft k-Means [22]</cell><cell>50.4</cell><cell>64.4</cell><cell cols="2">52.4 69.9</cell><cell cols="2">49.0 63.0</cell><cell>51.4 69.1</cell></row><row><cell>TPN [37]</cell><cell></cell><cell>52.8</cell><cell>66.4</cell><cell cols="2">55.7 71.0</cell><cell cols="2">50.4 64.9</cell><cell>53.5 69.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Pseudo-labeling accuracies (%) during the meta-training process, on miniImageNet 1-shot.</figDesc><table><row><cell>Stage</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell cols="7">Accuracy 59.8 63.6 65.1 66.9 67.9 68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Pseudo-labeling accuracies (%) at six recursive stages of meta-test, on miniImageNet 1-shot.</figDesc><table /><note>Stage-1 is initialization.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is part of NExT research which is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative. It is also partially supported by German Research Foundation (DFG CRC 1223), and National Natural Science Foundation of China (61772359, 61671289, 61771301, 61521062).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Dong-Hyun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative adversarial residual pairwise networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambedkar</forename><surname>Dukkipati</surname></persName>
		</author>
		<idno>1703.08033</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Snail: A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Virtual adversarial training for semisupervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chapelle</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schölkopf</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zien</forename><surname>Alexander</surname></persName>
		</author>
		<idno>978-0-262-03358-9</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TADAM: task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-labeled techniques for semisupervised learning: taxonomy, software and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="284" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Zeynep Akata. f-VAEGAN-D2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Liu Yanbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lecun</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinton</forename><surname>Geoffrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Grahahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
