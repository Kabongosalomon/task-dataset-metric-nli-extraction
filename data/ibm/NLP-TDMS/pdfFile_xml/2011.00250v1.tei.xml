<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Smoothing for 3D Human Pose Estimation and Localization for Occluded People</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Véges</surname></persName>
							<email>vegesm@inf.elte.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">Eötvös Loránd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lőrincz</surname></persName>
							<email>lorincz@inf.elte.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">Eötvös Loránd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Smoothing for 3D Human Pose Estimation and Localization for Occluded People</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>human activity recognition · pose estimation · temporal · absolute pose</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In multi-person pose estimation actors can be heavily occluded, even become fully invisible behind another person. While temporal methods can still predict a reasonable estimation for a temporarily disappeared pose using past and future frames, they exhibit large errors nevertheless. We present an energy minimization approach to generate smooth, valid trajectories in time, bridging gaps in visibility. We show that it is better than other interpolation based approaches and achieves state of the art results. In addition, we present the synthetic MuCo-Temp dataset, a temporal extension of the MuCo-3DHP dataset. Our code is made publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of 3D human pose estimation is to predict the coordinates of certain body joints based on an input image or video. The potential applications are numerous, including augmented reality, sport analytics and physiotherapy. In a multiperson settings, 3D poses may help analyzing the interactions between the actors.</p><p>Recent results on the popular Human3.6M database <ref type="bibr" target="#b8">[9]</ref> are starting to saturate <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> and there is an increasing interest in more natural settings. The standard evaluation protocol in Human3.6M uses hip-relative coordinates. It makes the prediction task easier, as the localization of the pose is not required. However, in multi-person settings the distance between the subjects can be important too. This led to the introduction of absolute pose estimation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, where the relative pose prediction is supplemented with the localization task.</p><p>Note that this problem is underdefined when only a single camera provides input. It is impossible to estimate the scale of the real scene based on one image, if semantic information is not available. Therefore, we use a scale-free error metric that calculates the prediction error up to a scalar multiplier. Additionally, our method is temporal, that lets us discern special cases such as jumping. An imagebased method does not have enough information to decide, whether the jumping person is closer to the camera or higher above the ground. Temporal methods can exploit information in neighboring frames. When a person disappears for a short time (for instance someone walks in front of him/her), these methods could fill in the void based on the previous and following frames. Still, the output is often noisy and even a simple linear interpolation yields better results (see <ref type="table" target="#tab_2">Table 2</ref>).</p><p>To overcome the occlusion problem, we propose two contributions. First, we introduce the MuCo-Temp synthetic dataset. It is a temporal extension of the MuCo-3DHP dataset <ref type="bibr" target="#b17">[18]</ref>. The latter database is commonly used as the training set for MuPoTS-3D <ref type="bibr" target="#b17">[18]</ref>. The MuPoTS-3D dataset is a multi-person 3D human pose database, containing 20 video sequences. It consists of a test set only so the authors of <ref type="bibr" target="#b17">[18]</ref> introduced MuCo-3DHP, that is generated from the single person MPI-INF-3DHP dataset <ref type="bibr" target="#b16">[17]</ref>. Each image in MuCo-3DHP is a composition of four poses from MPI-INF-3DHP. This new synthetic dataset contains occlusions typical to multi-person settings but has images only while our method is temporal. Thus, we created the MuCo-Temp dataset, containing videos composited from sequences in the MPI-INF-3DHP database. To keep compatibility, we used the same algorithm as the authors of MuCo-3DHP.</p><p>Our second contribution is an energy minimization based smoothing function, targeting specifically those frames where a person became temporarily invisible. It adaptively smoothes the prediction stronger at frames where the pose is occluded and weaker when the pose is visible. This way large noises during heavy occlusions can be filtered without 'over-smoothing' unoccluded frames. The method does not require additional training and can be applied after any temporal pose estimation algorithm.</p><p>To summarize, we introduce an approach to predict 3D human poses even when the person is temporarily invisible. It achieves state-of-the-art results on the MuPoTS-3D dataset, showing its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D Pose estimation Before the widespread usage of deep learning, various approaches were explored for 3D human pose prediction, such as conditional random fields <ref type="bibr" target="#b1">[2]</ref> or dictionary based methods <ref type="bibr" target="#b22">[23]</ref>. However, recent algorithms are all based on neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>. The primary difficulty in 3D human pose estimation is the lack of accurate in the wild datasets. Accurate measurements require a studio setting, such as Human3.6M <ref type="bibr" target="#b8">[9]</ref> or MPI-INF-3DHP <ref type="bibr" target="#b16">[17]</ref>. To overcome this, several approaches were proposed to use auxiliary datasets. Zhou et al. <ref type="bibr" target="#b32">[33]</ref> uses 2D pose datasets with a reprojection loss. Pavlakos et al. <ref type="bibr" target="#b19">[20]</ref> employed depthwise ordering of joints as additional supervision signal. An adversarial loss added to the regular regression losses ensure the plausibility of generated 3D poses <ref type="bibr" target="#b5">[6]</ref>. It requires no paired 2D-3D data.</p><p>More importantly, Martinez et al. <ref type="bibr" target="#b14">[15]</ref> introduced a two step prediction approach: first the 2D pose is predicted from the image, then the 3D pose is estimated solely from the 2D joint coordinates. This places the task of handling image features on the 2D pose estimator, for which large, diverse datasets exist <ref type="bibr" target="#b12">[13]</ref>. The 2D-to-3D regression part can be a simple feed-forward network <ref type="bibr" target="#b14">[15]</ref>, one that uses recurrent layers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> or a graph convolutional network <ref type="bibr" target="#b31">[32]</ref>. Additionally to employing off-the-shelf algorithms <ref type="bibr" target="#b26">[27]</ref>, the joint training of the 2D pose estimator and the 2D-to-3D part is possible via soft-argmax <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>. The soft-argmax function is a differentiable approximation of the argmax operation that lets gradients flow to the 2D estimator. In our paper we follow the two step approach with a pretrained 2D pose estimator <ref type="bibr" target="#b26">[27]</ref>.</p><p>Temporal methods While 3D poses can be inferred from an image only, for videos temporal methods provide better performance than simple frameby-frame approaches. A natural idea is to use recurrent layers over per-frame estimates <ref type="bibr" target="#b7">[8]</ref>. However, even with LSTMs, RNN based methods exploit only a small temporal neighborhood of a frame. This can be solved with 1D convolution <ref type="bibr" target="#b20">[21]</ref>. To increase the receptive field without adding extra parameters, the authors used dilated convolutions. Additionally, bundle adjustment <ref type="bibr" target="#b0">[1]</ref> was used to refine body mesh prediction in time. Our work is closest to <ref type="bibr" target="#b0">[1]</ref>, however their method smoothes coordinates for visible poses only and does not handle occlusions.</p><p>Instead of working directly on joint coordinates, Lin et al. first decomposed the trajectories with discrete cosine transform <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b30">[31]</ref> the authors introduced a special motion loss that encourages realistic motion paths. Finally, Cai et al <ref type="bibr" target="#b2">[3]</ref> use a spatio-temporal graph convolutional network. The standard uniform convolution was changed such that the kernels became different for different joints.</p><p>Localization and pose estimation Most work focuses on relative pose estimation: the joint coordinates are predicted relative to a root joint, usually the hip. This is sufficient for many use cases, however for multi-person images, the locations of the actors might be important too. The joint estimation of location and relative pose is also called absolute pose estimation <ref type="bibr" target="#b28">[29]</ref>. In <ref type="bibr" target="#b29">[30]</ref>, the authors propose a direct regression approach that learns from RGB-D data as well. Moon et al. <ref type="bibr" target="#b18">[19]</ref> uses two separate networks to predict the location and the 3D pose.</p><p>Additional performance can be gained by using multiple cameras. The authors of <ref type="bibr" target="#b21">[22]</ref> predict the joint locations based on many viewpoints. They employ a  modified pictorial structure model that recursively increases its resolution around the joint locations, improving accuracy while keeping speed.</p><p>Occlusions Occlusions can be grouped in two categories: self-occlusions and occlusions caused by other objects or people. In typical single-pose estimation datasets, only the first type arises. Mehta et al. introduces Occlusion Robust Heatmaps, that store joint location coordinates as a 2D heatmap <ref type="bibr" target="#b17">[18]</ref>. Augmenting datasets with synthetic occlusions also helps. Sarandi et al. overlaid rectangles and objects from MS-COCO on individual frames <ref type="bibr" target="#b25">[26]</ref>. Cylindrical man models can be used to predict visibility from 3D joint coordinates <ref type="bibr" target="#b4">[5]</ref>. The visibility flags combined with 2D joint detection score can form the basis of an effective regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our algorithm predicts the coordinates of the joints of a person in a camera relative coordinate system, where the origin is at the camera center and the axes are parallel to the camera plane. The estimation of coordinates is separated into two tasks: localization and relative pose estimation, similar to previous work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref>. That is, we predict the location of the root joint (the hip) and the location of the other joints relative to the hip. We denote the former P loc , the latter P rel .</p><p>The general outline of our algorithm is as follows (see <ref type="figure" target="#fig_0">Figure 1</ref>): firstŜ k,t , the 2D position of the kth person on frame t, is predicted using an off-the-shelf algorithm <ref type="bibr" target="#b26">[27]</ref>. Then, a temporal pose network (TPN ) predicts the 3D location and pose of the person (P loc k,t andP rel k,t respectively), see Section 3.1 for details. Finally, the pose refinement step smoothes the predictions of TPN, producing the final predictionsP loc k,t andP rel k,t (see Section 3.2). The TPN and the pose refinement are run for each person separately. For brevity, we drop the index k in the discussions below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporal PoseNet</head><p>The Temporal PoseNet takes the 2D joint coordinates produced by the 2D pose estimator and predicts the 3D location and pose. To be robust against the different cameras in the training and test set, the 2D joints are normalized by the inverse of the camera calibration matrix. Then, for frame t the joint locations are predicted based on a window of size 2w + 1:</p><formula xml:id="formula_0">P loc t ;P rel t = f T P N [K −1Ŝ i ] t+w i=t−w ,</formula><p>where K is the intrinsic camera calibration matrix and f T P N is the Temporal PoseNet.</p><p>The network architecture of TPN was inspired by <ref type="bibr" target="#b20">[21]</ref>, <ref type="figure" target="#fig_1">Figure 2</ref> shows an overview. It is a 1D convolutional network, that has three residual modules. Each module contains a convolution of size 3 followed by a convolution of size 1. The convolutional layers are followed by a Batch Normalization layer, a ReLU activation function and a Dropout layer. The input and the output of the network are normalized by removing the mean and dividing by the standard deviation.</p><p>The loss function is the 1 loss, to be robust against outliers. The final loss is:</p><formula xml:id="formula_1">L T P N = t P loc t − P loc t 1 + P rel t − P rel t 1 ,</formula><p>where P loc t and P rel t are the ground-truth location and the relative pose coordinates for frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pose Refinement</head><p>While the input of the TPN is a large temporal window of frames, it still makes mistakes, especially during heavy occlusions. We propose an energy optimization based smoothing method that adaptively filters the keypoint trajectories. The objective function is calculated separately for the location and relative pose. It is formulated as:</p><formula xml:id="formula_2">E ref (P ) = vE pred (P ) + (1 − v)λ 1 E smooth (P , τ 1 ) + λ 2 E smooth (P , τ 2 ),<label>(1)</label></formula><p>whereP is is eitherP loc orP rel , indicating whether the objective function optimizes the location or the relative pose; v is the visibility score, E pred is an error function ensuring the smoothed pose is close to the original prediction, and E smooth is a smoothness error. The parameter τ of E smooth controls the smoothing frequency. A small value corresponds to a low-pass filter with a high frequency threshold, while a large value corresponds to a lower frequency threshold (see below the definition of E smooth ). We choose τ 1 τ 2 . The visibility score v is close to 1 when the pose is visible, and to 0 when the pose is occluded. If the person is fully invisible, v is set to 0. Thus, in Equation <ref type="formula" target="#formula_2">(1)</ref>, when the pose is visible, only E pred is taken into account and the (stronger, τ 1 parametrized) smoothing is ignored. In other words, when the pose was visible, the objective function does not override the predictions of TPN. On the other hand, when the pose is heavily occluded, only E smooth is active and the optimization ensures that the predicted pose will be smooth.</p><p>The prediction error is</p><formula xml:id="formula_3">E pred P = t min P t −P t 2 2 , m ,</formula><p>whereP t is either the location or the pose predicted by the Temporal PoseNet at frame t, andP t is the target of the optimization. The min function ensures that large outliers do not affect the objective function. This is essential because the predicted poseP can be noisy when the person is occluded. m is a fixed parameter.</p><p>The smoothing error is a zero velocity loss <ref type="bibr" target="#b0">[1]</ref>, and τ controls the scale of the smoothing:</p><formula xml:id="formula_4">E smooth P , τ = t P t −P t−τ 2 2 .</formula><p>The error encourages that the pose changes smoothly over time. If τ is small, then the filtering works in a small window, removing local noise. If τ is large, the error function looks at a larger timescale.</p><p>Finally, the visibility score v is the confidence score predicted by the 2D pose estimator. Since the 2D estimator returns per-joint confidences, these are averaged. Finally, a median filter is applied on v.</p><p>The total energy function evaluates Equation (1) both for the location and relative position:</p><formula xml:id="formula_5">E total (P loc ) = E ref (P loc ) + λ rel E ref (P rel )</formula><p>where λ rel is a weighting parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated our method on the multi-person 3D pose dataset MuPoTS-3D <ref type="bibr" target="#b17">[18]</ref>. It contains videos taken in indoor and outdoor environments. Since MuPoTS-3D contains only test sequences and has no training set, commonly the MPI-INF-3DHP <ref type="bibr" target="#b16">[17]</ref> and MuCo-3DHP <ref type="bibr" target="#b17">[18]</ref> databases serve as the training set. However, MuCo-3DHP contains single frames only, so temporal models can not be trained on it. Therefore, we created the MuCo-Temp synthetic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuCo-Temp Dataset</head><p>The MuCo-3DHP database contains synthetic images, composited from frames of MPI-INF-3DHP. The latter dataset was recorded in a green-screen studio, so segmenting of the actors is easy. Each synthesized frame contains four persons copied on a single image. Optionally, the background can be augmented with arbitrary images.</p><p>MuCo-Temp is using the same generation algorithm as MuCo-3DHP but it consists of videos instead of frames. Each video contains 4 person and 2000 frames. In total, we generated 77 videos for training. The validation set was created using the same process. We did not augmented the background, as the 2D pose estimator was already trained on a visually diverse dataset.</p><p>Our method was trained on the concatenation of the MPI-INF-3DHP and MuCo-Temp datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We calculate various metrics to evaluate the performance of our algorithm. The following list briefly summarizes each:</p><p>-MRPE or Mean Root Position Error, the average error of the root joint (the hip) <ref type="bibr" target="#b18">[19]</ref>. -MPJPE or Mean Per Joint Position Error, the mean Euclidean error averaged over all joints and all poses, calculated on relative poses <ref type="bibr" target="#b14">[15]</ref>. -3D-PCK or Percentage of Correct Keypoints, the percentage of joints that are closer than 150mm to the ground truth. This metric is also calculated on relative poses only.</p><p>Since our method uses a single a camera, predicting the poses and locations of people is an underdefined problem. The above metrics do not take into account this, and calculate every error in mm. Therefore, we also include unit-less variants of the above metrics called N-MRPE and N-MPJPE <ref type="bibr" target="#b23">[24]</ref>. The definition of N-MRPE is min</p><formula xml:id="formula_6">s∈R 1 N k,t sP hip k,t − P hip k,t 2 2 ,</formula><p>whereP hip k,t and P hip k,t are the predicted and ground-truth locations of the hip for person k on frame t. The total number of poses is N . The formula above finds an optimal scaling parameter s to minimize the error. In other words, the scaling of the prediction comes from the ground truth. The scaling constant s is calculated over all people and frames of the video: that is, the size of two predicted skeletons must be correct relative to each other, while the absolute size is still unknown.</p><p>MuPoTS has two kinds of pose annotations: universal and normal. The normal coordinates are the joint locations in millimeters. The universal coordinates are like the normal ones, but each person is rescaled from their hip to have a normalized height. Following previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>, the 3D-PCK metric is calculated on the universal coordinates of MuPoTS-3D while the (N-)MRPE and (N-)MPJPE errors are calculated on the normal coordinates. All the metrics are calculated over each sequence separately, then averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>The half-window size w in the Temporal PoseNet was 40, thus the full input window length was 81 frames. The dropout rate was set to 0.25. The training algorithm was Adam with a learning rate of 10 −3 , decayed by a multiplier of 0.95 on each epoch. The network was trained for 80 epochs.</p><p>To avoid overfitting, the training set was augmented by scaling the 2D skeletons. When the focal length of the camera remains unchanged, this roughly corresponds to zooming the image. This step is essential, otherwise the TPN may overfit to the y location of joints.</p><p>The optimization algorithm minimizing E total was Adam, with a learning rate of 10 −2 . The optimization ran for 500 iterations. The temporal timestep τ 1 was 20, τ 2 was 1, and the threshold m was 1. The values of the weighting parameters: λ rel = 0.1, λ 1 = 0.1 and λ 2 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>First, we compare the results of our model to the state of the art <ref type="table" target="#tab_1">(Table 1)</ref>. We improve on all metrics, however, this is not a fair comparison, since these models are single-frame algorithms, while ours is a temporal model. Also, MRPE and MPJPE errors can be calculated on detected poses only for these models To have a fair comparison, we also test against other baselines <ref type="table" target="#tab_2">(Table 2</ref>). First, we use a simple linear interpolation for frames when a person is occluded (Interpolation). While on relative pose estimation metrics it achieves similar performance as our model, the localization performance is considerably worse (272 vs 252 in MRPE, a 7.4% relative drop). Additionally, we apply the 1-Euro filter <ref type="bibr" target="#b3">[4]</ref> on the interpolated poses (1-Euro). This filter was applied in previous work to reduce high-frequency noises <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>. It is noticeable, that the filter had no additional effect over the interpolation, the metrics either did not change or marginally worsened. That is, our smoothing procedure performs better than the 1-Euro method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies</head><p>We performed an ablation study to confirm that each component of our algorithm contributes positively, the results are presented in  We hypothesized that our model improves the estimation of occluded poses. To show that the pose refinement process does not hurt the accuracy of visible joints, we evaluated the model on those poses, where the visibility score reached a threshold (0.1 in our experiments). The results are shown in <ref type="table" target="#tab_4">Table 3b</ref>. The addition of the refinement step changed the performance only marginally, indicating that a) our method improves prediction on heavily occluded poses b) does not decrease performance on unoccluded poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Vertical Location and Depth</head><p>One may assume that the TPN overfitted and simply calculates the depth based on the y coordinates. However, this is mitigated by the fact that the training and test set have different camera viewpoints. Moreover, we applied an augmentation that effectively zooms the the cameras, further increasing the diversity of the inputs (see Section 4.3). We also show an example sequence in <ref type="figure" target="#fig_2">Figure 3</ref>. It contains the trajectory of the hip of a person jumping backward, taken from MuPoTS Sequence 15. The figure demonstrates, that even though the y coordinate of the joint is increasing, the z coordinate still follows the ground truth. If the TPN was overfitted to the y location, we would expect a jump in the trajectory of the z coordinate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a pose refinement approach, that corrects predictions of heavily occluded poses. Our method improves both localization and pose estimation performance, achieving state-of-the-art results on the MuPoTS dataset. We demonstrated that it does not impair performance on unoccluded poses. Our algorithm could be further extended by making the refinement process part of the pose estimation network, in an end-to-end fashion. Also, one drawback of our approach is that it does not include tracking, the combination with a tracking algorithm remains future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our algorithm. First 2D pose estimations (Ŝt) are generated for all frames. Then the Temporal PoseNet (TPN ) converts the 2D estimates to initial 3D poses (Pt). Finally, the pose refinement step smoothes out the trajectories, producing the final 3D pose estimates (Pt).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Temporal PoseNet architecture. The network has three residual blocks, each block contains two convolutional layer followed by BatchNorm and Dropout layers. The activation function was ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Trajectory of hip during a jump. During a jump, the trajectory follows the ground-truth, showing that the model did not overfit to vertical location. Left plot shows the vertical coordinate of the hip in a camera centered coordinate system. The right plot shows the depth of the joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art MPJPE and MRPE errors are in mm. * Non-temporal methods. † Error is calculated on detected frames only.</figDesc><table><row><cell></cell><cell cols="3">MRPE MPJPE 3D-PCK</cell></row><row><cell>LCR-Net++ [25]*</cell><cell>-</cell><cell>-</cell><cell>70.6</cell></row><row><cell>Moon et al. [19]*</cell><cell>277  †</cell><cell>-</cell><cell>81.8</cell></row><row><cell>Veges et al. [30]*</cell><cell>238  †</cell><cell>120  †</cell><cell>78.2</cell></row><row><cell>Ours  †</cell><cell>225  †</cell><cell>100  †</cell><cell>86.8  †</cell></row><row><cell>Ours</cell><cell>252</cell><cell>103</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with baselines Interpolation uses simple linear interpolation for unseen poses. 1-Euro applies a 1-Euro filter on interpolated poses. (N-)MPJPE and MRPE errors are in mm.</figDesc><table><row><cell></cell><cell cols="5">MRPE MPJPE 3D-PCK N-MRPE N-MPJPE</cell></row><row><cell>Ours w/o refine</cell><cell>340</cell><cell>107</cell><cell>83.4</cell><cell>307</cell><cell>113</cell></row><row><cell>Interpolation</cell><cell>272</cell><cell>103</cell><cell>85,1</cell><cell>243</cell><cell>109</cell></row><row><cell>1-Euro</cell><cell>273</cell><cell>104</cell><cell>85.1</cell><cell>243</cell><cell>109</cell></row><row><cell>Ours</cell><cell>252</cell><cell>103</cell><cell>85.3</cell><cell>221</cell><cell>108</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3a .</head><label>3a</label><figDesc>The Baseline corresponds to the Temporal PoseNet only, trained on MPI-INF-3DHP. Adding MuCo-Temp improves on all performance metrics. Moreover, adding Pose Refinement further decreases errors by a large amount (MRPE goes from 340 to 252, a 25% drop).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of ablation studies. a) Results when components are turned on sequentially. b) Errors calculated on visible poses only.</figDesc><table><row><cell cols="3">a) Performance of components</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">MRPE MPJPE 3D-PCK N-MRPE N-MPJPE</cell></row><row><cell>Baseline</cell><cell>372</cell><cell>116</cell><cell>81,2</cell><cell>332</cell><cell>122</cell></row><row><cell>+ MuCo-Temp</cell><cell>340</cell><cell>107</cell><cell>83.4</cell><cell>307</cell><cell>113</cell></row><row><cell>+ Pose Refinement</cell><cell>252</cell><cell>103</cell><cell>85.3</cell><cell>221</cell><cell>108</cell></row><row><cell cols="2">b) Results on visible poses</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">MRPE MPJPE 3D-PCK N-MRPE N-MPJPE</cell></row><row><cell>Ours w/o refine</cell><cell>227</cell><cell>100</cell><cell>86.5</cell><cell>191</cell><cell>106</cell></row><row><cell>Ours</cell><cell>225</cell><cell>100</cell><cell>86.6</cell><cell>189</cell><cell>106</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vegesm/pose_refinement arXiv:2011.00250v1 [cs.CV] 31 Oct 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>MV received support from the European Union and co-financed by the European Social Fund (EFOP-3.6.3-16-2017-00002). AL was supported by the National Research, Development and Innovation Fund of Hungary via the Thematic Excellence Programme funding scheme under Project no. ED 18-1-2019-0030 titled Application-specific highly reliable IT solutions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">1e filter: a simple speed-based low-pass filter for noisy input in interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2527" to="2530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename></persName>
		</author>
		<title level="m">Can 3d pose be learned from 2d projections alone? ECCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08585</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Propagating LSTM: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context. Lecture Notes in Computer Science p</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sporri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">LCR-Net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 ECCV PoseTrack challenge on 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sarandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04987</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lorincz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJCNN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-person absolute 3d human pose estimation with weak depth supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lorincz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03989</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13985</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reducing footskate in human motion reconstruction with ground contact constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
