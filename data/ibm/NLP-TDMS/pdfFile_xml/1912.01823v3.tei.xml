<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain-independent Dominance of Adaptive Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Savarese</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudarshan</forename><surname>Babu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
						</author>
						<title level="a" type="main">Domain-independent Dominance of Adaptive Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, Im-ageNet) and character-level language modelling (Penn Treebank) tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep network architectures are becoming increasingly complex, often containing parameters that can be grouped according to multiple functionalities, such as gating, attention, convolution, and generation. Such parameter groups should arguably be treated differently during training, as their gradient statistics might be highly distinct. Adaptive gradient methods designate parameter-wise learning rates based on gradient histories, treating such parameters groups differently and, in principle, promise to be better suited for training complex neural network architectures.</p><p>Nonetheless, advances in neural architectures have not been matched by progress in adaptive gradient descent algorithms. SGD is still prevalent, in spite of the development of seemingly more sophisticated adaptive alternatives, such as RM-SProp <ref type="bibr" target="#b2">(Dauphin et al., 2015)</ref> and Adam <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref>. Such adaptive methods have been observed to yield poor generalization compared to SGD in classification tasks <ref type="bibr" target="#b24">(Wilson et al., 2017)</ref>, and hence have been mostly adopted for training complex models <ref type="bibr" target="#b23">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017)</ref>. For relatively simple architectures, such as ResNets <ref type="bibr" target="#b7">(He et al., 2016a)</ref> and DenseNets <ref type="bibr" target="#b10">(Huang et al., 2017)</ref>, SGD is still the dominant choice.</p><p>At a theoretical level, concerns have also emerged about the current crop of adaptive methods. Recently, <ref type="bibr" target="#b18">Reddi et al. (2018)</ref> has identified cases, even in the stochastic convex setting, where Adam <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref> fails to converge. Modifications to Adam that provide convergence guarantees have been formulated, but have shortcomings. AMSGrad  requires non-increasing learning rates, while AdamNC  and AdaBound <ref type="bibr" target="#b14">(Luo et al., 2019)</ref> require that adaptivity be gradually eliminated during training. Moreover, while most of the recently proposed variants do not provide formal guarantees for nonconvex problems, the few current convergence rate analyses in the literature <ref type="bibr" target="#b27">(Zaheer et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2019)</ref> do not match SGD's. Section 3 fully details the convergence rates of the most popular Adam variants, along with their shortcomings.</p><p>Our contribution is marked improvements to adaptive optimizers, from both theoretical and practical perspectives. At the theoretical level, we focus on convergence guarantees, deriving new algorithms:</p><p>• Delayed Adam. Inspired by <ref type="bibr" target="#b27">Zaheer et al. (2018)</ref>'s analysis of Adam, Section 4 proposes a simple modification for adaptive gradient methods which yields a provable convergence rate of O(1/ √ T ) in the stochastic nonconvex setting -the same as SGD. Our modification can be implemented by swapping two lines of code and preserves adaptivity without incurring extra memory costs. To illustrate these results, we present a non-convex problem where Adam fails to converge to a stationary point, while Delayed Adam -Adam with our proposed modification -provably converges with a rate of O(1/ √ T ).</p><p>• AvaGrad. Inspecting the convergence rate of Delayed Adam, we show that it would improve with an adaptive global learning rate, which self-regulates based on global statistics of the gradient second moments. Following this insight, Section 5 proposes a new adaptive method, AvaGrad, whose hyperparameters decouple learning rate and adaptability.</p><p>arXiv:1912.01823v3 [cs.</p><p>LG] 17 Mar 2020</p><p>Through extensive experiments, Section 6 demonstrates that AvaGrad is not merely a theoretical exercise. AvaGrad performs as well as both SGD and Adam in their respectively favored usage scenarios. Along this experimental journey, we happen to disprove some conventional wisdom, finding adaptive optimizers, including Adam, to be superior to SGD for training CNNs. The caveat is that, excepting AvaGrad, these methods need extensive grid search to outperform SGD, often requiring unconventional hyperparameter values to even yield competitive performance.</p><p>AvaGrad is a uniquely attractive adaptive optimizer, as it decouples the learning rate and its adaptability parameter, making hyperparameter search significantly faster. In particular, given a computational budget similar to SGD's, AvaGrad yields near best results over a wide range of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>For vectors a = [a 1 , a 2 , . . .</p><formula xml:id="formula_0">], b = [b 1 , b 2 , .</formula><p>. . ] ∈ R d , we use the following notation: 1 a for element-wise divi-</p><formula xml:id="formula_1">sion ( 1 a = [ 1 a1 , 1 a2 , . . . ]), √ a for element-wise square root ( √ a = [ √ a 1 , √ a 2 , . . . ]), a + b for element-wise addition (a + b = [a 1 + b 1 , a 2 + b 2 , . . . ]), a b for element-wise multiplication (a b = [a 1 b 1 , a 2 b 2 , . . . ]).</formula><p>Moreover, a is used to denote the 2 -norm: other norms will be specified whenever used (e.g., a ∞ ).</p><p>For subscripts and vector indexing, we adopt the following convention: the subscript t is used to denote an object related to the t-th iteration of an algorithm (e.g., w t ∈ R d denotes the iterate at time step t); the subscript i is used for indexing: w i ∈ R denotes the i-th coordinate of w ∈ R d . When used together, t precedes i: w t,i ∈ R denotes the i-th coordinate of w t ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stochastic Non-Convex Optimization</head><p>In the stochastic non-convex setting, we are concerned with the optimization problem:</p><formula xml:id="formula_2">min w∈R d f (w) = E s∼D [f s (w)]<label>(1)</label></formula><p>where D is a probability distribution over a set S of "data points". We also assume that f is M -smooth in w, as is typically done in non-convex optimization:</p><formula xml:id="formula_3">∀ w, w f (w ) ≤ f (w)+ ∇f (w), w −w + M 2 w − w 2<label>(2)</label></formula><p>Methods for stochastic non-convex optimization are evaluated in terms of number of iterations or gradient evaluations required to achieve small loss gradients. This differs from the stochastic convex setting where convergence is measured w.r.t. suboptimality f (w) − min w∈R d f (w). We assume that the algorithm takes a sequence of data points S = (s 1 , . . . , s T ) from which it deterministically computes a sequence of parameter settings w 1 , . . . , w T together with a distribution P over {1, . . . , T }. We say an algorithm has a</p><formula xml:id="formula_4">convergence rate of O(g(T )) if E S∼D T t∼P(t|S) ∇f (w t ) 2 ≤ O(g(T )) where, as defined above, f (w) = E s∼D [f s (w)].</formula><p>We also assume that the functions f s have bounded gradients: there exists some G ∞ such that ∇f s (w) ∞ ≤ G ∞ for all s ∈ S and w ∈ R d . Throughout the paper, we also let G 2 denote an upper bound on ∇f s (w) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Here we present a brief overview of optimization methods commonly used for training neural networks, along with their convergence rate guarantees for stochastic smooth nonconvex problems. We consider methods which, at each iteration t, receive or compute a gradient estimate:</p><formula xml:id="formula_5">g t := ∇f st (w t ), s t ∼ D<label>(3)</label></formula><p>and perform an update of the form:</p><formula xml:id="formula_6">w t+1 = w t − α t · η t m t<label>(4)</label></formula><p>where α t ∈ R is the global learning rate, η t ∈ R d are the parameter-wise learning rates, and m t ∈ R d is the update direction, typically defined as:</p><formula xml:id="formula_7">m t = β 1,t m t−1 + (1 − β 1,t )g t and m 0 = 0. (5)</formula><p>Non-momentum methods such as SGD, AdaGrad, and RM-SProp <ref type="bibr" target="#b2">(Dauphin et al., 2015;</ref><ref type="bibr" target="#b3">Duchi et al., 2011)</ref> have m t = g t (i.e., β 1,t = 0), while momentum SGD and Adam <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref> have β 1,t ∈ (0, 1). Note that while α t can always be absorbed into η t , representing the update in this form will be convenient throughout the paper.</p><p>SGD uses the same learning rate for all parameters, i.e., η t = 1. Although SGD is simple and offers no adaptation, it has a convergence rate of O(1/ √ T ) with either constant, increasing, or decreasing learning rates <ref type="bibr" target="#b5">(Ghadimi &amp; Lan, 2013)</ref>, and is widely used when training deep networks, especially CNNs <ref type="bibr" target="#b7">(He et al., 2016a;</ref><ref type="bibr" target="#b10">Huang et al., 2017)</ref>. At the heart of its convergence proof is the fact that</p><formula xml:id="formula_8">E st [α t · η t g t ] = α t · ∇f (w t ).</formula><p>Popular adaptive methods such as RMSProp <ref type="bibr" target="#b2">(Dauphin et al., 2015)</ref>, AdaGrad <ref type="bibr" target="#b3">(Duchi et al., 2011)</ref>, and Adam <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref> </p><formula xml:id="formula_9">have η t = 1 √ vt+ , where v t ∈ R d is given by: v t = β 2,t v t−1 + (1 − β 2,t )g 2 t and v 0 = 0. (6)</formula><p>As v t is an estimate of the second moments of the gradients, the optimizer designates smaller learning rates for parameters with larger uncertainty in their stochastic gradients.</p><p>However, in this setting η t and s t are no longer independent,</p><formula xml:id="formula_10">hence E st [α t · η t g t ] = α t · E st [η t ] ∇f (w t ).</formula><p>This "bias" can cause RMSProp and Adam to present convergence issues, even in the stochastic convex setting .</p><p>Recently, <ref type="bibr" target="#b27">Zaheer et al. (2018)</ref> showed that, with a constant learning rate, RMSProp and Adam have a convergence rate of</p><formula xml:id="formula_11">O(σ 2 + 1/T ), where σ 2 = sup w∈R d E s∼D ∇f s (w) − ∇f (w) 2 , hence their result</formula><p>does not generally guarantee convergence. <ref type="bibr" target="#b1">Chen et al. (2019)</ref> showed that AdaGrad and AMSGrad enjoy a convergence rate of O(log T / √ T ) when a decaying learning rate is used. Note that both methods constrain η t in some form, the former with β 2,t = 1 − 1/t (adaptability diminishes with t), and the latter explicitly enforces v t ≥ v j for all j &lt; t (η t is point-wise non-increasing). In both cases, the method is less adaptive than Adam, and the rates above are worse than SGD's O(1/ √ T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SGD-like Convergence without Constrained Rates</head><p>We first take a step back to note the following: to show that Adam might not converge in the stochastic convex setting, <ref type="bibr" target="#b18">Reddi et al. (2018)</ref> provide a stochastic linear problem where Adam fails to converge w.r.t. suboptimality. Since non-convex optimization is evaluated w.r.t. norm of the gradients, a different instance is required to characterize Adam's behavior in this setting.</p><p>The following result shows that even for a quadratic problem, Adam indeed does not converge to a stationary point: Theorem 1. For any ≥ 0 and constant β 2,t = β 2 ∈ [0, 1), there is a stochastic convex optimization problem for which Adam does not converge to a stationary point.</p><p>Proof. The full proof is given in Appendix A 1 . The argument follows closely from <ref type="bibr" target="#b18">Reddi et al. (2018)</ref>, where we explicitly present a stochastic optimization problem:</p><formula xml:id="formula_12">min w∈[0,1] f (w) := E s∼D [f s (w)] f s (w) = C w 2 2 , with probability p := 1+δ C+1 −w, otherwise<label>(7)</label></formula><p>We show that, for large enough C (as a function of δ, , β 2 ), Adam will move towards w = 1 where ∇f (1) = δ, and that the constraint w ∈ [0, 1] does not make w = 1 a stationary point.</p><p>This result, like the one in <ref type="bibr" target="#b18">Reddi et al. (2018)</ref>, relies on the fact that η t and s t are correlated: upon a draw of the rare 1 See suplementary material for Appendices sample C w 2 2 , the learning rate η t decreases significantly and Adam takes a small step in the correct direction. On the other hand, a sequence of common samples increases η t and Adam moves faster towards w = 1.</p><p>Instead of enforcing η t to be point-wise non-increasing in t , which forces the optimizer to take small steps even for a long sequence of common samples, we propose to simply have η t be independent of s t . As an extra motivation for this approach, note that successful proof strategies <ref type="bibr" target="#b27">(Zaheer et al., 2018)</ref> to analyzing adaptive methods include the following step:</p><formula xml:id="formula_13">E st [η t g t ] = E st [(η t−1 + η t − η t−1 ) g t ] = η t−1 ∇f (w t ) + E st [(η t − η t−1 ) g t ]<label>(8)</label></formula><p>where</p><formula xml:id="formula_14">bounding E st [(η t − η t−1 ) g t ],</formula><p>seen as a form of bias, is a key part of recent convergence analyses. Replacing η t by η t−1 in the update equation of Adam removes this bias and can be implemented by simply swapping lines of code (updating η after w), yielding a simple convergence analysis without hindering the adaptability of the method in any way.</p><p>Algorithm 1 provides pseudo-code when applying this modification to Adam, yielding Delayed Adam. The following Theorem shows that this modification is enough to guarantee a SGD-like convergence rate of O(1/ √ T ) in the stochastic non-convex setting for general adaptive gradient methods.</p><p>Theorem 2. Consider any optimization method which updates parameters as follows:</p><formula xml:id="formula_15">w t+1 = w t − α t · η t g t (9) where g t := ∇f st (w t ), s t ∼ D, and α t , η t are indepen- dent of s t . Assume that f (w 1 ) − f (w ) ≤ D, f (w) = E s∼D [f s (w)] is M -smooth, and ∇f s (w) ∞ ≤ G ∞ for all s ∈ S, w ∈ R d . Moreover, let Z = T t=1 α t min i η t,i . For α t = γ t 2D T M G 2 ∞ , if p(Z|s t ) = p(Z) for all s t ∈ S, then: E S∼D T t∼P(t|S) ∇f (w t ) 2 ≤ M DG 2 ∞ 2T · E S∼D T T t=1 1 + γ 2 t η t 2 T t=1 γ t min i η t,i<label>(10)</label></formula><p>where P assigns probabilities p(t) ∝ α t · min i η t,i .</p><p>Proof. The full proof is given in Appendix B, along with analysis for the case with momentum β 1,t ∈ (0, 1) in Appendix B.1, and in particular β 1,t = β 1 / √ t, which yields a similar rate.</p><formula xml:id="formula_16">Algorithm 1 DELAYED ADAM Input: w 1 ∈ R d , α t , &gt; 0, β 1,t , β 2,t ∈ [0, 1) 1: Set m 0 = 0, v 0 = 0 2: for t = 1 to T do 3: Draw s t ∼ D 4: Compute g t = ∇f st (w t ) 5: m t = β 1,t m t−1 + (1 − β 1,t )g t 6: η t = 1 √ vt−1+ 7: w t+1 = w t − α t · η t m t 8: v t = β 2,t v t−1 + (1 − β 2,t )g 2 t 9: end for</formula><p>The convergence rate depends on η t and min i η t,i , which are random variables for Adam-like algorithms. However, if there are constants H and L such that</p><formula xml:id="formula_17">0 &lt; L ≤ η t,i ≤ H &lt; ∞<label>(11)</label></formula><p>for all i and t, then a rate of</p><formula xml:id="formula_18">O(1/ √ T ) is guaranteed.</formula><p>This is the case for Delayed Adam, where 1/(G 2 + ) ≤ η t,i ≤ 1/ for all t and i. Theorem 2 also requires that α t and η t are independent of s t , which can be assured to hold by applying a "delay" to their respective computations, if necessary (i.e., replacing η t by η t−1 , as in Delayed Adam).</p><p>Additionally, the assumption that p(Z|s t ) = p(Z), meaning that a single sample should not affect the distribution of Z = T t=1 α t min i η t,i , is required since P is conditioned on the samples S (unlike in standard analysis, where Z = T t=1 α t and α t is deterministic), and is expected to hold as T → ∞.</p><p>Practitioners typically use the last iterate w T or perform early-stopping: in this case, whether the assumption holds or not does not affect the behavior of the algorithm. Nonetheless, we also show in Appendix B.2 a similar rate that does not require this assumption to hold, which also yields a O(1/ √ T ) convergence rate taken that the parameter-wise learning rates are bounded from above and below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AvaGrad: An Adaptive Method with Adaptive Variance</head><p>Now, we consider the implications of Theorem 2 for Delayed Adam, where η t = 1 √ v t−1 + , and hence 1/(G 2 + ) ≤ η t,i ≤ 1/ for all t and i.</p><p>For a fixed γ t = γ, chosen a-priori (that is, without knowledge of the realization of {η t } T t=1 ), we can optimize γ to minimize the worst-case rate using η t 2 ≤ d/ 2 and min i η t,i ≥ G 2 + . This yields γ * = O( ), and a convergence rate linear in 1/ , suggesting that, at least in the worst case, should be chosen to be as large as possible, and the learning rate α should scale linearly with .</p><formula xml:id="formula_19">Algorithm 2 AVAGRAD Input: w 1 ∈ R d , α t , &gt; 0, β 1,t , β 2,t ∈ [0, 1) 1: Set m 0 = 0, v 0 = 0 2: for t = 1 to T do 3: Draw s t ∼ D 4: Compute g t = ∇f st (w t ) 5: m t = β 1,t m t−1 + (1 − β 1,t )g t 6: η t = 1 √ vt−1+ 7: w t+1 = w t − α t · ηt ηt/ √ d 2 m t 8: v t = β 2,t v t−1 + (1 − β 2,t )g 2 t 9: end for</formula><p>What if we allow γ t to vary in each time step? For example, choosing γ t = 1/ η t yields a convergence rate of</p><formula xml:id="formula_20">M DG 2 ∞ 2T · E S∼D T   1 2T T t=1 min i η t,i η t −1   . (12)</formula><p>Using 1/(G 2 + ) ≤ η t,i ≤ 1/ we see that in the worst-case this is also linear in 1/ . However, this dependence differs from the one with fixed γ t = γ in a few aspects. Most notably, if we consider different scalings of η t = 1 √ vt−1+ (e.g., small and scaling v t−1 ), the convergence rate with fixed γ can get arbitrarily worse: in (10), we get that the numerator grows quadratically with the scaling, while the denominator only grows linearly.</p><p>On the other hand, for γ t = 1/ η t the convergence rate remains unchanged since in (12), both min i η t,i and η t grow linearly with the scaling and hence its effect is cancelled out.</p><p>For the particular case d = 1, we have mini ηt,i ηt = 1 in (12), yielding the exact same convergence rate as SGD including constant factors. However, a constant γ results in a dependence on the scale of η t , which again can be large if the second moment estimate is either large or small. Lastly, normalizing the learning rate by 1/ η t removes its dependence on in the worst-case setting, making the two hyperparameters more separable.</p><p>Motivated by the above observations, the choice of γ t = √ d/ η t , where √ d is added for normalization effects, yields a method which we name AvaGrad -Adaptive VAriance Gradients, presented as pseudo-code in Algorithm 2. We call it an "adaptive variance" method since, if we scale up or down the variance of the gradients, the convergence guarantee in Theorem 2 does not change, while for global learning rates that are independent of η t (as in Adam and other adaptive methods), it can be arbitrarily bad. 6. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Synthetic Data</head><p>To illustrate empirically the implications of Theorem 1 and Theorem 2, we set up a synthetic stochastic optimization problem with the same form as the one used in the proof of Theorem 1: </p><p>This function has a stationary point w = 1−0.002 999·0.002 ≈ 0.5, and it satisfies Theorem 1 for β 1 = 0, β 2 = 0.99, = 10 −8 . We proceed to perform stochastic optimization with Adam, AMSGrad, and Delayed Adam, with constant learning rate α t = 10 −5 . For simplicity, we let P be uniform over (1, . . . , T ), since α t is constant. <ref type="figure" target="#fig_0">Figure 1</ref> shows the progress of 1 t t t =1 w t and 1 t t t =1 ∇f (w t ) 2 for each iteration t: as expected, Adam fails to converge to the stationary point w , while both AMSGrad and Delayed Adam converge. Note that Delayed Adam converges significantly faster, likely because it has no constraint on the learning rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Image Classification on CIFAR</head><p>Our theory suggests that, in the worst case, should be chosen as large as possible, at which point the learning rate α should scale linearly with it. As a first experiment to assess this hypothesis, we analyze the interaction between α and when training a Wide ResNet 28-4 (Zagoruyko &amp; Komodakis, 2016) on the CIFAR dataset <ref type="bibr" target="#b12">(Krizhevsky, 2009</ref>). The CIFAR-10 and CIFAR-100 datasets consist of 60,000 RGB images with 32 × 32 pixels and comes with a standard train/test split of 50,000 and 10,000 images, respectively. <ref type="formula" target="#formula_2">(2016)</ref>, we pre-process the dataset by performing channel-wise normalization using statistics computed from the training set. We also flip each image horizontally with 50% probability and perform random cropping by first padding 4 black pixels to each image and then extracting a random 32 × 32 crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Following Zagoruyko &amp; Komodakis</head><p>We use a validation set of 5,000 images to evaluate the performance of SGD and different adaptive gradient methods: Adam, AMSGrad, AdaBound <ref type="bibr" target="#b14">(Luo et al., 2019;</ref><ref type="bibr" target="#b20">Savarese, 2019)</ref>, AdaShift <ref type="bibr" target="#b28">(Zhou et al., 2019)</ref>, and our proposed algorithm, AvaGrad. We also assess whether performing weight decay as proposed in <ref type="bibr" target="#b13">Loshchilov &amp; Hutter (2019)</ref> instead of standard L 2 regularization positively impacts the performance of adaptive methods: we do this by evaluating AdamW and AvaGradW.</p><p>The learning rate is decayed by a factor of 5 at epochs 60, 120 and 160, and the model is trained for a total of 200 epochs with a weight decay of 0.0005. We use a mini-batch size of 128, and each model is trained on a single GPU. For SGD, we use a momentum of 0.9, while for adaptive methods we use the default β 1 = 0.9 and β 2 = 0.999. For AdaBound, we use the default final learning rate α * = 0.1 and γ = 10 −3 for the bound functions. Finally, for AdaShift, we use the default stack size n = 10.</p><p>We run each adaptive method with different powers of 10 for , multiplied by 1 and 2, from 10 −8 up to 100, a value large enough such that adaptability can be effectively ignored. We also vary the learning rate α of each method with different powers of 10, multiplied by 1 and 5, from 5 × 10 −7 up to 5000. In total, we evaluate 441 different hyperparameter settings for each adaptive method. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results for Adam and AvaGrad. Our main findings are twofold:</p><p>• The optimal for every adaptive method is considerably larger than the values typically used in practice, ranging from 0.1 (Adam, AMSGrad, AvaGradW) to 10.0 (Ava-Grad, AdamW). For Adam and AMSGrad, the optimal learning rate is α = = 0.1, a value 100 times larger than the default.</p><p>• All adaptive methods, except for AdaBound, outperform SGD in terms of validation performance. Note that for SGD the optimal learning rate is α = 0.1, matching the value used in work such as <ref type="bibr" target="#b7">He et al. (2016a)</ref>; <ref type="bibr" target="#b26">Zagoruyko &amp; Komodakis (2016)</ref>; <ref type="bibr" target="#b25">Xie et al. (2017)</ref>, which presented state-of-the-art results at time of publication.</p><p>However, the fact that adaptive methods outperform SGD in this setting is not conclusive, since they are executed with more hyperparameter settings (varying as well as α). Moreover, the main motivation for adaptive methods is to be less sensitive to hyperparameter values; performing an extensive grid search defeats their purpose.</p><p>Aiming for a fair comparison between SGD and adaptive methods, we also train a Wide ResNet 28-10 on both CIFAR-10 and CIFAR-100, evaluating the test performance of each adaptive method with its optimal values for α and found in the previous experiment. For SGD, we confirmed that the learning rate α = 0.1 still yielded the best validation performance with the new architecture, hence the fact that we transfer hyperparameters from the Wide ResNet 28-4 runs does not unfairly advantage adaptive methods in the comparison with SGD. With a larger network and a different task (CIFAR-100), this experiment should also capture how hyperparameters of adaptive methods transfer between tasks and models.</p><p>On CIFAR-10, SGD achieves 3.86% test error (reported as 4% in <ref type="bibr" target="#b26">Zagoruyko &amp; Komodakis (2016)</ref>) and is outperformed by both Adam (3.64%) and AvaGrad (3.80%). On CIFAR-100, SGD (19.05%) is outperformed by Adam (18.96%), AMSGrad (18.97%), AdaShift (18.88%), Ava-Grad (18.76%), and AvaGradW (19.04%). We believe these results are surprising, as they show that adaptive methods can yield state-of-the-art performance when training CNNs as long as their adaptability is correctly controlled with .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Image Classification on ImageNet</head><p>As a final evaluation of the role of adaptability when training convolutional networks, we repeat the previous experiment on the ImageNet dataset <ref type="bibr" target="#b19">(Russakovsky et al., 2015)</ref>, a challenging benchmark composed of 1.2M training and 50,000 validation RGB images sampled from a total of 1,000 classes. We follow <ref type="bibr" target="#b6">Gross &amp; Wilber (2016)</ref> for data augmentation (scale and color transformations) and use 224x224 single-crops to compute the top-1 accuracy on the validation set.</p><p>We train a ResNet-50 <ref type="bibr" target="#b8">(He et al., 2016b)</ref> with SGD and different adaptive methods, transferring the hyperparameters from our original CIFAR-10 results. The network is trained  for 100 epochs with a batch size of 256 on 4 GPUs (batch size of 64 per GPU), the learning rate is decayed by a factor of 10 at epochs 30, 60 and 90, and a weight decay of 0.0001 is applied.</p><p>SGD yields 24.01% top-1 validation error, underperforming Adam (23.45%), AMSGrad (23.46%), AvaGrad (23.58%) and AvaGradW (23.49%) -a total of 4 out of the 6 adaptive methods evaluated on the dataset. <ref type="table" target="#tab_0">Table 1</ref> summarizes the results.</p><p>In contrast to numerous papers that surpassed the state-ofthe-art on ImageNet by training networks with SGD <ref type="bibr" target="#b21">(Simonyan &amp; Zisserman, 2015;</ref><ref type="bibr" target="#b22">Szegedy et al., 2015;</ref><ref type="bibr" target="#b7">He et al., 2016a;</ref><ref type="bibr" target="#b26">Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b25">Xie et al., 2017)</ref>, our results show that adaptive methods can yield superior results in terms of generalization performance.</p><p>Most strikingly, we observed that Adam outperformed more sophisticated methods such as AMSGrad, AdaBound, and AdamW. Note that the hyperparameter values we used for SGD match the ones in <ref type="bibr" target="#b7">He et al. (2016a)</ref>, <ref type="bibr" target="#b8">He et al. (2016b)</ref> and <ref type="bibr" target="#b6">Gross &amp; Wilber (2016)</ref>: an initial learning rate of 0.1 with a momentum of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Language Modelling with RNNs</head><p>It is perhaps not very surprising that to perform optimally in the image classification tasks studied previously, adaptive gradient methods required large values of , and hence were barely adaptive.</p><p>Here, we consider a task where state-of-the-art results are not typically achieved by SGD, but by adaptive methods with low values for : language modelling with recurrent networks. In particular, we perform character-level language modelling on the Penn Treebank dataset <ref type="bibr" target="#b15">(Marcus et al., 1994;</ref><ref type="bibr" target="#b17">Mikolov et al., 2010)</ref>, which consists of 5.01M/393k/442k training/validation/test tokens, respectively, and a vocabulary size of 10,000.</p><p>Following <ref type="bibr" target="#b16">Merity et al. (2018)</ref>, we train 3-layer LSTMs <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997)</ref> with a character embedding size of 200 and varying size for the LSTM cells. The model is trained for a total of 500 epochs, and the learning rate is decayed by 10 at epochs 300 and 400. We use a batch size of 128, a BPTT length of 150, and weight decay of 1.2 × 10 −6 .</p><p>As in <ref type="bibr" target="#b16">Merity et al. (2018)</ref>, we apply weight dropout with p = 0.5 to the LSTM's hidden-to-hidden matrix, variational dropout <ref type="bibr" target="#b4">(Gal &amp; Ghahramani, 2016)</ref> with p = 0.1 for the input/output layers, p = 0.25 for the LSTM layers, and p = 0.1 to the columns of the embedding matrix (embedding dropout).</p><p>We first evaluate the validation performance of SGD, Adam, AMSGrad, AdaShift, AdaBound, AdamW, AvaGrad and AvaGradW with varying learning rate α and adaptability parameter , when training a 3-layer LSTM with 300 hidden units in each layer. We vary the learning rate α in powers of 10 multiplied by 2: from 0.0002 up to 20; for the adaptability parameter , we vary the values in powers of 10, multiplied by 1 and 5, from 10 −8 up to 100. <ref type="figure" target="#fig_3">Figure 3</ref> shows that, in this task, smaller values for are indeed optimal: Adam, AMSGrad and AvaGrad performed best with = 10 −8 . The optimal learning rates for both Adam and AMSGrad, α = 0.002, agree with the value used in <ref type="bibr" target="#b16">Merity et al. (2018)</ref>. Both AvaGrad and AvaGradW performed best with α = 200: the former with = 10 −8 , the latter with = 10 −5 .</p><p>Next, we train a larger model: a 3-layer LSTM with 1000 hidden units per layer (the same model used in <ref type="bibr" target="#b16">Merity et al. (2018)</ref>, where it was trained with Adam), choosing values for α, which yielded the best validation performance in the previous experiment. For SGD, we again confirmed that a learning rate of 20 performed best on the validation set. When combined with the previous results, we see that adaptive methods actually dominate SGD across tasks of different domains. In particular, both Adam and AvaGrad outperformed SGD in all 4 considered tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Hyperparameter Separability and Domain-Independence</head><p>We observed that, given enough budget for hyperparameter tuning, Adam can actually outperform SGD in tasks such as image classification with CNNs, where adaptive methods have traditionally found little success. But can we decrease the cost of hyperparameter search?</p><p>One of the main motivations behind AvaGrad is that it removes the dependence between the learning rate α and the adaptability parameter , at least in the worst-case rate of Theorem 2. Observing the heatmaps in <ref type="figure" target="#fig_2">Figure 2</ref> and 3, we can see that indeed AvaGrad offers great separability between α and , unlike Adam.</p><p>In particular, for values larger than 0.0001, has little to no interaction with the learning rate α, as opposed to Adam where the optimal α increases linearly with . For language modelling on Penn Treebank, the optimal learning rate for AvaGrad was α = 200 for every choice of , while for image classification on CIFAR-10, we had α = 1.0 for all except two values of . This shows that AvaGrad enables a grid search over α and (naively, with quadratic complexity) to be broken into two line searches over α and separately (linear complexity). In the context of Section 6.2, this leads to a decrease from 21 2 = 441 to 2 × 21 = 42 total trials for hyperparameter search, only twice as many as SGD's budget. The cost of optimizing both SGD and Adam with a fixed is the same as fully optimizing AvaGrad. Therefore, unless is chosen optimally a-priori, AvaGrad dominates both SGD and Adam given the same budget and coarseness for hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>As neural architectures become more complex, with parameters having highly heterogeneous roles, parameter-wise learning rates are often necessary for training. However, adaptive methods have both theoretical and empirical gaps, with SGD outperforming them in some tasks and having stronger theoretical convergence guarantees. In this paper, we close this gap, by first providing a convergence rate guarantee that matches SGD's, and by showing that, with proper hyperparameter tuning, adaptive methods can dominate in both computer vision and natural language processing tasks. Key to our finding is AvaGrad, our proposed optimizer whose adaptability is decoupled from its learning rate.</p><p>Our experimental results show that proper tuning of the learning rate together with the adaptability of the method is necessary to achieve optimal results in different domains, where distinct neural network architectures are used across tasks. By enabling this tuning to be performed in linear time, AvaGrad takes a leap towards efficient domain-agnostic training of general neural architectures.</p><p>Proof. Consider the following stochastic optimization problem:</p><formula xml:id="formula_22">min w∈[0,1] f (w) := E s∼D [f s (w)] f s (w) = C w 2 2 , with probability p := 1+δ C+1 −w, otherwise<label>(14)</label></formula><p>where C &gt; 1−p p &gt; 1 + w1 √ 1−β2 . Note that ∇f (w) = pCw − (1 − p), and f is minimized at w = 1−p Cp = C−δ C(1+δ) . The proof follows closely from <ref type="bibr" target="#b18">Reddi et al. (2018)</ref>'s linear example for convergence in suboptimality. We assume w.l.o.g. that β 1 = 0. Consider:</p><formula xml:id="formula_23">∆ t = w t+1 − w t = −η g t √ v t + = −η g t β 2 v t−1 + (1 − β 2 )g 2 t + (15) E [∆ t ] η = E [w t+1 − w t ] η = −E g t β 2 v t−1 + (1 − β 2 )g 2 t + = pE      −Cw t β 2 v t−1 + (1 − β 2 )C 2 w 2 t + T1      + (1 − p)E      1 β 2 v t−1 + (1 − β 2 ) + T2     <label>(16)</label></formula><p>where the expectation is over all the randomness in the algorithm up to time t, as all expectations to follow in the proof. Note that T 1 = 0 for w t = 0. For w t &gt; 0 we bound T 1 as follows:</p><formula xml:id="formula_24">T 1 ≥ −Cw t (1 − β 2 )C 2 w 2 t = −1 √ 1 − β 2<label>(17)</label></formula><p>Hence, T 1 ≥ min(0, −1 √ 1−β2 ) = −1 √ 1−β2 . As for T 2 , we have, from Jensen's inequality:</p><formula xml:id="formula_25">E [T 2 ] ≥ 1 β 2 E [v t−1 ] + 1 − β 2 +<label>(18)</label></formula><p>Now, remember that v t−1 = (1 − β 2 )</p><formula xml:id="formula_26">t−1 i=1 β t−i−1 2 g 2 i , hence: E [v t−1 ] = (1 − β 2 ) t−1 i=1 β t−i−1 2 E g 2 i = (1 − β 2 ) t−1 i=1 β t−i−1 2 1 − p + pC 2 E w 2 t ≤ (1 − β 2 ) t−1 i=1 β t−i−1 2 1 − p + pC 2 ≤ (1 − β t−1 2 ) 1 − p + pC 2 ≤ (1 + δ)C 2<label>(19)</label></formula><p>and thus:</p><formula xml:id="formula_27">E [T 2 ] ≥ 1 β 2 (1 + δ)C + 1 − β 2 +<label>(20)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Plots of Adam, AMSGrad, and Delayed Adam trained on the synthetic example in Equation 13, with a stationary point at w ≈ 0.5. Left: The expected iterate sampled uniformly from {w 1 , . . . , w t }, for each iteration t. As predicted by our theoretical results, Adam moves towards w = 1 with ∇f (w) = 1, while Delayed Adam converges to w . Right: The expected norm squared of the gradient, for w randomly sampled from {w 1 , . . . , w t }. Delayed Adam converges significantly faster than AMSGrad, while Adam fails to converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Validation error of a Wide ResNet 28-4 trained on the CIFAR-10 dataset with Adam (left) and AvaGrad (right), for different values of the learning rate α and parameter , where larger yields less adaptability. Best performance is achieved with small adaptability ( &gt; 0.001).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Validation bits-per-character (lower is better) of a 3-layer LSTM with 300 hidden units, trained on the Penn Treebank dataset with Adam (left) and AvaGrad (right), for different values of the learning rate α and parameter , where larger yields less adaptability. Best performance is achieved with high adaptability ( &lt; 0.0001).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test performance of SGD and popular adaptive methods in benchmark tasks. Red indicates results with the recommended optimizer, following the paper that proposed each model, and any improved performance is given in blue. The best result for each task is in bold, and numbers in parentheses present standard deviations of 3 runs for CIFAR.</figDesc><table><row><cell>Method</cell><cell>CIFAR-10 (Test Err %)</cell><cell>CIFAR-100 (Test Err %)</cell><cell>ImageNet (Top-1 Val Err %)</cell><cell>Penn Treebank (Test Bits per Character)</cell></row><row><cell>SGD</cell><cell cols="2">3.86 (0.08) 19.05 (0.24)</cell><cell>24.01</cell><cell>1.238</cell></row><row><cell>Adam</cell><cell cols="2">3.64 (0.06) 18.96 (0.21)</cell><cell>23.45</cell><cell>1.182</cell></row><row><cell>AMSGrad</cell><cell cols="2">3.90 (0.17) 18.97 (0.09)</cell><cell>23.46</cell><cell>1.187</cell></row><row><cell>AdaBound</cell><cell cols="2">5.40 (0.24) 22.76 (0.17)</cell><cell>27.99</cell><cell>2.863</cell></row><row><cell>AdaShift</cell><cell cols="2">4.08 (0.11) 18.88 (0.06)</cell><cell>N/A</cell><cell>1.274</cell></row><row><cell>AdamW</cell><cell cols="2">4.11 (0.17) 20.13 (0.22)</cell><cell>27.10</cell><cell>1.230</cell></row><row><cell>AvaGrad</cell><cell cols="2">3.80 (0.02) 18.76 (0.20)</cell><cell>23.58</cell><cell>1.179</cell></row><row><cell cols="3">AvaGradW 3.97 (0.02) 19.04 (0.37)</cell><cell>23.49</cell><cell>1.175</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 (</head><label>1</label><figDesc>right column) reports all results. In this setting, AvaGrad and AvaGradW outperform Adam, achieving bitper-characters of 1.179 and 1.175 compared to 1.182. The poor performance of AdaBound could be caused by convergence issues or due to the default values for its hyperparameters:<ref type="bibr" target="#b20">Savarese (2019)</ref> showed that that the bound functions strongly affect the optimizer's behavior and might require careful tuning.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">TTI-Chicago 2 University of Chicago. Correspondence to: Pedro Savarese &lt;savarese@ttic.edu&gt;.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Proof of Theorem 1</head><p>Plugging in the bounds for T 1 and T 2 in Equation 16:</p><p>Hence, for large enough C, and C δ, w ≈ 1 1+δ while the above quantity becomes non-negative, and hence E [w t ] ≥ w 1 . In other words, Adam will, in expectation, drift away from the stationary point, towards w = 1, at which point ∇f (1) 2 = δ.</p><p>For example, δ = 1 implies that lim T →∞</p><p>To see that w = 1 is not a stationary point due to the feasibility constraints, check that ∇f (1) = 1 &gt; 0: that is, the negative gradient points towards the feasible region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 2</head><p>Proof. Throughout the proof we use the following notation for clarity:</p><p>We start from the fact that f is M -smooth:</p><p>and use the update</p><p>where in the first step we used the fact that m t η t</p><p>the second we used m t = β 1,t m t−1 + (1 − β 1,t )g t , in the third we used Cauchy-Schwarz, and in the fourth we used</p><p>Now, taking the expectation over s t , and using the fact that E st [g t ] = ∇f (w t ), and that η t , α t are both independent of s t :</p><p>where in the second step we used β 1,t ≤ β 1 and ∇f</p><p>Re-arranging, we get:</p><p>Now, defining p(t) = αtLt Z , where Z =</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the convergence of a class of adam-type algorithms for non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rmsprop and equilibrated adaptive learning rates for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">corrL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic First-and Zeroth-order Methods for Nonconvex Stochastic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/fb.resnet.torch" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adaptive gradient methods with dynamic bound of learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuanhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The penn treebank: Annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology</title>
		<meeting>the Workshop on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An Analysis of Neural Language Modeling at Multiple Scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocký</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model. INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the Convergence of AdaBound and its Connection to SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04457</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Marginal Value of Adaptive Gradient Methods in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive methods for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adashift: Decorrelation and convergence of adaptive learning rate methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
