<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and Colonoscopy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Molecular and Clinical Medicine</orgName>
								<orgName type="institution" key="instit1">Sahlgrenska Academy</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kumar Tomar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharib</forename><surname>Ali</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Håvard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Molecular and Clinical Medicine</orgName>
								<orgName type="institution" key="instit1">Sahlgrenska Academy</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Johansen</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Molecular and Clinical Medicine</orgName>
								<orgName type="institution" key="instit1">Sahlgrenska Academy</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>De Lange</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Medical Research</orgName>
								<orgName type="institution">Baerum Hospital</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Augere Medical AS</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Molecular and Clinical Medicine</orgName>
								<orgName type="institution" key="instit1">Sahlgrenska Academy</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Oslo Metropolitan University</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Molecular and Clinical Medicine</orgName>
								<orgName type="institution" key="instit1">Sahlgrenska Academy</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simulamet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norway</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Molecular and Clinical Medicine</orgName>
								<orgName type="institution" key="instit1">Sahlgrenska Academy</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Medical Department</orgName>
								<orgName type="institution">Sahlgrenska University Hospital-Mölndal Hospital</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and Colonoscopy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video capsule endoscopy</term>
					<term>colonoscopy</term>
					<term>deep learning</term>
					<term>segmentation</term>
					<term>tool segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning in gastrointestinal endoscopy can assist to improve clinical performance and be helpful to assess lesions more accurately. To this extent, semantic segmentation methods that can perform automated real-time delineation of a region-of-interest, e.g., boundary identification of cancer or precancerous lesions, can benefit both diagnosis and interventions. However, accurate and real-time segmentation of endoscopic images is extremely challenging due to its high operator dependence and high-definition image quality. To utilize automated methods in clinical settings, it is crucial to design lightweight models with low latency such that they can be integrated with low-end endoscope hardware devices. In this work, we propose NanoNet, a novel architecture for the segmentation of video capsule endoscopy and colonoscopy images. Our proposed architecture allows real-time performance and has higher segmentation accuracy compared to other more complex ones. We use video capsule endoscopy and standard colonoscopy datasets with polyps, and a dataset consisting of endoscopy biopsies and surgical instruments, to evaluate the effectiveness of our approach. Our experiments demonstrate the increased performance of our architecture in terms of a trade-off between model complexity, speed, model parameters, and metric performances. Moreover, the resulting models size is relatively tiny, with only nearly 36,000 parameters compared to traditional deep learning approaches having millions of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Gastrointestinal (GI) endoscopy is a widely used technique to diagnose and treat anomalies in the upper (esophagus, stomach, and duodenum) and the lower (large bowel and anus) GI tract. Among the other GI tract organs, colorectal cancer (CRC) has the highest cancer incidences and mortality rate <ref type="bibr" target="#b0">[1]</ref>. There are several CRC screening options. Theses are usually divided into two categories, namely, invasive (visual examination-based test) and non-invasive based tests (stool, blood, and radiological test). Colonoscopy, the gold standard for examining the large bowel (colon and rectum), is an invasive examination used to detect, observe, and remove abnormalities (such as polyps). It detects colorectal cancer with both high sensitivity and specificity. Sigmoidscopy is another invasive test. Computed Tomography(CT) Colonoscopy, Fecal Occult Blood Test (FOBT) Fecal Immunochemical Test (FIT), and Video Capsule Endoscopy (VCE) are non-invasive tests. VCE is a technology for capturing the video inside the GI tract. It has evolved as an important tool for detecting small bowel diseases <ref type="bibr" target="#b1">[2]</ref>.</p><p>Deep Learning (DL) methods have made a significant breakthrough in several medical domain such as lung cancer detection <ref type="bibr" target="#b2">[3]</ref>, diabetic retinopathy progression <ref type="bibr" target="#b3">[4]</ref>, and obstructive hypertrophic cardiomyopathy detection <ref type="bibr" target="#b4">[5]</ref>. It has provided new opportunities to solve challenges such as bleeding, light over/underexposure, smoke, and reflections <ref type="bibr" target="#b5">[6]</ref>. However, DL normally needs a large annotated dataset for the implementation of methods. It is difficult to obtain a labeled medical dataset. First, it needs collaborations with the hospitals. For data collection, the doctors require approval from various authorities and patient consent. They need to set protocols for the collection, and the collected data must be anonymized and cleaned with the help of data engineers. Domain experts must label raw data, and after labeling, the annotations must be done depending upon the need of the task. The whole process requires an significant amount of expert time and is costly. Additionally, it is an operator-dependent process. The quality of the data labeling and annotation depends on the expertise of the clinicians. Therefore, it is challenging to curate a larger dataset.</p><p>One way of solving the dataset issue is to create synthetic images using Genearative Adversarial Network (GAN) <ref type="bibr" target="#b6">[7]</ref>. However, generated synthetic images may not always capture all the properties and characteristics of real endoscopic images. Consequently, the model may only learn to predict the properties from the synthetic images and may not perform well on a real endoscopic dataset. Another solution could be domain adaptation from a similar endoscopic dataset. However, we lack large publicly available labeled endoscopic datasets. Thus, a viable and compelling approach to solve the semantic segmentation task is to reuse ImageNet pre-trained encoders in the segmentation model <ref type="bibr" target="#b7">[8]</ref>. The predicted masks from the algorithm can provide reliable information to the endoscopic model.</p><p>A lightweight Convolutional Neural Network (CNN) model can be essential for the development of real-time and efficient semantic segmentation methods. Usually, lightweight models are computationally efficient and require less memory. A smaller number of parameters makes the network less redundant. Lightweight CNN models are mainly being deployed in mobile applications <ref type="bibr" target="#b8">[9]</ref>. A lightweight model can play a crucial role from a system perspective with a limited resource constraint for real-time prediction in clinics. Consequently, we propose a novel architecture, NanoNet, optimized for faster inference and high accuracy. An extremely lightweight model with very few trainable parameters, faster inference, and higher performance would require less memory footprint to be incorporated with any devices. Therefore, we put forward this approach to address the challenges in endoscopy.</p><p>The main contributions of this work include the following: 1) We proposed a novel architecture, named NanoNet, to segment video capsule endoscopy and colonoscopy images in real-time with high accuracy. The proposed architecture is very lightweight, and the model size is smaller, requiring less computational cost. 2) VCE datasets are difficult to obtain with pixel-wise annotations. In this context, we have annotated 55 polyps from the "polyp" class of the Kvasir-Capsule dataset with the help of an expert gastroenterologist. We have made this dataset public and provided the benchmark. 3) NanoNet achieves promising performance on the KvasirCapsule-SEG, Kvasir-SEG <ref type="bibr" target="#b9">[10]</ref>, 2020 Medico automatic polyp segmentation challenge <ref type="bibr" target="#b10">[11]</ref>, 2020 Endo-Tect challenge <ref type="bibr" target="#b11">[12]</ref>, and Kvasir-Instrument <ref type="bibr" target="#b12">[13]</ref> datasets. All experiments conform with state-of-the-art (SOTA) in terms of parameter uses (size), speed, computation, and performance metrics. 4) The model can be integrated with mobile and embedded devices because of fewer parameters used in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic segmentation of endoscopic images</head><p>Semantic segmentation of endoscopic images has been a well-established topic in medical image segmentation. Earlier work mostly relied on the handcrafted descriptors for feature learning <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The handcrafted features such as color, shape, texture, and edges were extracted and fed to the Machine Learning (ML) classifier, which separates lesions from the background. However, the traditional ML methods based on handcrafted features suffer from low performance <ref type="bibr" target="#b15">[16]</ref>. The recent works on polyp segmentation using both video capsule endoscopy and colonoscopy mostly relied on Deep Neural Network (DNN) <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b22">[23]</ref>.</p><p>With the DNN methods, there is progress in the performance for segmenting endoscopic images (for example, polyps). However, the network architectures are often complex and requires high-end GPUs for training, and is computationally expensive <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Additionally, real-time lesion segmentation has often been ignored. Although there is some recent initiation for the real-time detection of endoscopic images, they have mostly used private datasets <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref> for the experimentation. It is difficult to compare the new methods on these datasets and extend the benchmark. Therefore, there is a need for a benchmark on publicly available datasets to minimize the research gap towards building a clinically relevant model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lightweight model</head><p>There are few works in the literature that have proposed lightweight models for image segmentation. Ni et al. <ref type="bibr" target="#b28">[29]</ref> presented a novel bilinear attention network-based approach with an adaptive receptive field for the segmentation of surgical instruments. Wang et al. <ref type="bibr" target="#b29">[30]</ref> proposed a lightweight encoder-decoder network (LEDNet), an encoder-decoder network that uses ResNet50 in the encoder block and attention pyramidal network in the decoder block. Beheshti et al. <ref type="bibr" target="#b30">[31]</ref> proposed SqueezeNet. The architecture of the SqueezeNet is inspired by UNet <ref type="bibr" target="#b31">[32]</ref>. The proposed model obtained a 12× reduction in model size and showed efficient performance in multiplication accumulation (mac) and memory uses.</p><p>From the above-related work, we identify a need for a realtime polyp segmentation method. A real-time polyp segmentation method can be achieved by building a lightweight network architecture by designing an efficient network with blocks that require fewer parameters. A lower number of network parameters will reduce the network complexity, leading to real-time or faster inference. In this respect, we propose NanoNet, which uses a lightweight pre-trained network MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, and simple convolutional blocks such as residual block and squeeze and excite block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NETWORK ARCHITECTURE</head><p>The architecture of NanoNet follows an encoder-decoder approach as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the network architecture uses a pre-trained model as an encoder, followed by the three decoder blocks. Using pre-trained Ima-geNet <ref type="bibr" target="#b33">[34]</ref> models for transfer learning has become the best choice for many CNN architectures <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b24">[25]</ref>. It helps the model converge much faster and achieves high performance compared to the non-pre-trained model. The proposed architecture uses a MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> model pre-trained on the ImageNet <ref type="bibr" target="#b33">[34]</ref> dataset as the encoder. The decoder is built using a modified version of the residual block, which was initially introduced by He et al. <ref type="bibr" target="#b34">[35]</ref>. The encoder is used to capture the required contextual information from the input, whereas the decoder is used to generate the final output by using the contextual information extracted by the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MobileNetV2</head><p>The MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> is an architecture that is primarily designed for mobile and embedded devices. The architecture performed well on a variety of different datasets while maintaining high accuracy, despite having fewer parameters. The architecture of MobileNetV2 is based on the architecture of MobileNetV1, which uses depth-wise separable convolutions as the main building block. A depth-wise separable convolution consists of depth-wise convolution followed by a pointwise convolution. The MobileNetV2 introduces two main ideas: inverted residual block and linear bottleneck block <ref type="bibr" target="#b32">[33]</ref>. The inverted residual block is based on the bottleneck residual block as described in the <ref type="bibr" target="#b34">[35]</ref>, which consists of three standard convolutions, which are 1 × 1, 3 × 3, and 1 × 1. Every convolution layer is followed by a Rectified Linear Unit (ReLU) non-linearity. In the first 1 × 1 standard convolution, the number of feature channels are reduced, and in the last 1 × 1 standard convolution, the number of feature channels are expanded. After that, an element-wise addition with the identity mapping is performed. The inverted residual block also has three convolution layers: a 1 × 1 standard convolution, a 3 × 3 depth-wise convolution, and a 1 × 1 standard convolution. Every convolution has a ReLU activation function. Here, the exact opposite of the bottleneck residual block is performed. The first 1 × 1 standard convolution expands the number of feature channels, and the last 1 × 1 standard convolution reduces the number of feature channels. Due to this opposite functionality, it is referred to as an inverted residual block. The linear bottleneck block is the same as the inverted residual block, except the last 1 × 1 standard convolution has a linear activation before an elementwise addition is performed with the identity mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modified Residual Block</head><p>The original residual block uses two 3 × 3 standard convolutions, where the first convolution is followed by a batchnormalization and a ReLU activation function. After that, the second convolution is followed only by a batch-normalization. An element-wise addition is performed between the output of the batch-normalization and the identity mapping, followed by another ReLU activation function. An identity mapping consists of a 1×1 standard convolution and a batch-normalization over the original input.</p><p>We have modified the residual block for our network. The modified residual block starts with a 1×1 convolution followed by a 3 × 3 convolution. In both of these convolutions, we reduce the number of filters by <ref type="bibr">1 4</ref> , which are then followed by the batch normalization and the ReLU activation function. We have a 3 × 3 convolution with batch normalization. Now, we perform an element-wise addition with the identity mapping. Finally, we apply a ReLU activation function followed by the squeeze and excitation block. The squeeze and excitation block improves the quality of feature maps by increasing their sensitivity towards essential features.</p><p>C. The NanoNet architecture <ref type="figure" target="#fig_0">Figure 1</ref> shows the block diagram of the NanoNet architecture. The NanoNet architecture starts with a pre-trained MobileNetV2 as an encoder followed by a decoder. There is a modified residual block between the encoder and the decoder, which acts like a bridge that connects the encoder and the decoder. In the first step, we feed the image data into the pre-trained encoder. The pre-trained encoder starts with a standard convolution with 32 feature channels, followed by the bottleneck layer with ReLU6 as the activation function. All the convolution operations use a standard 3 × 3 kernel size. The entire encoder network progressively downsamples the feature maps by using strided convolution and slowly increases the number of feature channels alternatively.</p><p>The output from the pre-trained encoder passes through the modified residual block, which is fed to the decoder. Every step in the decoder uses a bilinear upsampling to increase the spatial dimension (height and width) of the input feature maps. After that, it is concatenated with the appropriate feature maps from the pre-trained encoder using the skip connections. These skip connections pass information that may be lost sometimes between the layers and are used to improve the quality of the feature maps. These concatenated feature maps are passed through the modified residual block, which further increases the generalization capacity of the decoder. After the feature maps pass through all the three decoder block, the output of the last decoder block is fed to a 1 × 1 convolution with a number of classes as the feature channels. This is followed by the sigmoid activation if it is a binary segmentation task, else we use the softmax activation function.</p><p>We have demonstrated three different NanoNet architectures: NanoNet-A, NanoNet-B, and NanoNet-C. Each architecture consists of different feature channels in its decoder  In NanoNet-B, the number of feature channels is reduced to 32, 64, and 96. In NanoNet-C, these feature channels are further reduced to 16, 24, and 32. The reduction in the number of feature channels leads to less trainable parameters, which simplifies the model complexity leading to a light-weight network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section, we will describe the dataset, evaluation metrics, implementation details, and data augmentation techniques used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To address the polyp segmentation problem from video capsule endoscopy images, we have selected the polyp class from labelled images folder of the Kvasir-Capsule dataset <ref type="bibr" target="#b36">[37]</ref> and annotated it with the help of an expert gastroenterologist. The Kvasir-Capsule is an open-access dataset that contains 13 classes of labelled anomalies and findings. It only includes 55 polyp frames out of 44,228 medically verified video capsule frames present in the Kvasir-Capsule. We have annotated the polyp class of Kvasir-Capsule and generated corresponding ground truth masks. Examples of polyps and their corresponding masks from KvasirCapsule-SEG can be found in <ref type="figure" target="#fig_1">Figure 2</ref>. Furthermore, we also provide bounding box information to be used for video capsule endoscopy detection and localization tasks. The Kvasir-Capsule can be downloaded from here 1 and KvasirCapsule-SEG can be downloaded from here 2 .  <ref type="table" target="#tab_0">Table I</ref> also has the corresponding ground truth. The link for each of the datasets is provided in the table. The standard setting for the "Medico automatic polyp segmentation challenge" and "Endotect challenge" is that they use the Kvasir-SEG for training. The challenge organizers have provided unseen 160 images in the "Medico automatic polyp segmentation challenge" and released 200 images in the "Endotect challenge" to test the participant's approaches. For the Kvasir-instrument dataset, we experimented with the official split provided by the organizers. The detail explanation of these datasets and the baseline results can be found in <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>For the evaluation of our model, we have chosen standard computer vision metrics such as Dice Coefficient (DSC), mean Intersection over Union (mIoU), Precision, Recall, Specificity, Accuracy, and Frame-per-second (FPS). More explanation of these metrics can be found in <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>We have implemented the NanoNet using Keras 3 with TensorFlow <ref type="bibr" target="#b39">[40]</ref> as backend. The experiments were run on the Experimental Infrastructure for Exploration of Exascale Computing (eX3), NVIDIA DGX-2 machine. The code implementation of NanoNet can be found here 4 . As the model has very few low trainable parameters, we have set a batch size of 16. We have resized the dataset images to 256 × 256 pixels for better utilization of the GPU, and it also helps to reduce the training time. The model is trained on 200 epochs with the Nadam optimizer <ref type="bibr" target="#b40">[41]</ref> and dice coefficient as the loss function. The learning rate for the optimizer is set to 1e −4 . We prefer to choose a low learning rate to update the parameters slowly and carefully. The learning rate is reduced by a factor of 0.1 when the validation loss does not decrease in 10 consecutive epochs. It helps to improve model performance. Additionally, we have used an early stopping mechanism to prevent over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data augmentation</head><p>We use data-augmentation on the training set to increase diversity and to improve the generalization of our model. Data     augmentation techniques such as random cropping, random rotation, horizontal flipping, vertical flipping, grid distortion, and many more are used. We have used an offline data augmentation technique. The validation and testing set is not augmented and is directly resized into 256 × 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULT AND DISCUSSION</head><p>In this section, we provide the experimental results for the segmentation task of the endoscopic image dataset. For the evaluation, we have used performance metrics such as DSC and mIoU, and FPS as the main evaluation metrics. We also calculate recall, precision, F2, and overall accuracy to support a complete set of metrics. <ref type="table" target="#tab_0">Table II, Table III, Table IV,  Table V, and Table VI</ref> show the results of the NanoNet model experiments using different parameters. The results are compared with the recent SOTA computer vision methods.</p><p>The quantitative results in these tables show that NanoNet consistently outperforms or performs nearly equal to its competitors in terms of performance. The quantitative results also show that NanoNet can produce real-time segmentation (i.e., produces at least close to 30 FPS for each dataset present in the Tables). This is one of the major contributions of the work. The other strength of the work lies in the parameter use. From <ref type="table" target="#tab_0">Table II</ref>, we can observe that the best performing NanoNet (i.e., NanoNet-A) uses nearly 35 times less parameters as ResUNet <ref type="bibr" target="#b37">[38]</ref>. Similarly, NanoNet-C uses 225 times less parameters as compared to that of ResUNet and also produces better DSC, mIoU and FPS with the Kvasir-SEG.</p><p>The qualitative results are displayed in <ref type="figure">Figure 3</ref>. The first, second, and third columns show the image, ground truth, and prediction masks, respectively. Similarly, the name of the dataset is provided on the left side. One example image for each dataset is shown. The qualitative results with diversified classes of medical datasets show that NanoNet can produce accurate segmentation results with different types of lesions (polyps) and therapeutic tools. The example images and the prediction also show that NanoNet produces good segmentation masks for large, medium, and small polyps (see <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3: Qualitative results of NanoNet-A on five different datasets</head><p>From the qualitative results, we can derive and conclude that NanoNet produces good results with small-sized polyps but produces over-segmentation for the large-sized lesions upon detail dissection. For future work, one could create a specific dataset consisting of a set of small and large-sized polyps to explore this further.</p><p>From both evaluation metrics and qualitative results, the improvement is remarkable. Thus, the proposed NanoNet architecture is simple, compact, and provides a robust solution for real-time applications, as it produces satisfactory performance despite having fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a novel lightweight architecture for real-time video capsule endoscopy and colonoscopy image segmentation. The proposed NanoNet architecture utilizes a pre-trained MobileNetV2 model and a modified residual block. The depthwise separable convolution is the main building block of the network and allows the model to achieve high performance with minuscule trainable parameters. The experimental results on varied endoscopy datasets demonstrate the strength of our model compared to state-of-the-art models with respect to their speed and performance. The presented model has the potential to enable easier roll out of deep learning models in clinical systems due to fewer parameters, competitive accuracy, and low-latency. In addition, the model does not require any sort of initialization, post-processing, or temporal regularization, considered as another strength of this work. In the future, we will design an encoder lighter than the currently used pre-trained MobilNetV2. Moreover, we aspire to utilize the currently built segmentation module in the clinic and study the efficacy of our designed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed NanoNet architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Polyps and corresponding masks from KvasirCapsule-SEG block. NanoNet-A consists of 32, 64 and 128 feature channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Publicly available endoscopic datasets used in our experiments</figDesc><table><row><cell>Dataset</cell><cell>No. of Images</cell><cell>Imaging Type</cell><cell>Availability</cell></row><row><cell>KvasirCapsule-SEG</cell><cell>55</cell><cell>Video capsule endoscopy</cell><cell>https://www.dropbox.com/sh/hr46vieykbmvmkk/</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AAAs V8ECG0wq51Fpw3rYU 5a?dl=0</cell></row><row><cell>Kvasir-SEG [10]</cell><cell>1000</cell><cell>Colonoscopy</cell><cell>https://datasets.simula.no/kvasir-seg/</cell></row><row><cell>2020 Medico automatic polyp segmen-</cell><cell>160</cell><cell>Colonoscopy</cell><cell>https://multimediaeval.github.io/editions/2020/tasks/</cell></row><row><cell>tation challenge [11]</cell><cell></cell><cell></cell><cell>medico/</cell></row><row><cell>Endotect Challenge Dataset [12]</cell><cell>200</cell><cell>Colonoscopy</cell><cell>https://endotect.com/</cell></row><row><cell>Kvasir-Instrument [36]</cell><cell>590</cell><cell>Colonoscopy</cell><cell>https://datasets.simula.no/kvasir-instrument/</cell></row><row><cell>test images</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I</head><label>I</label><figDesc>shows the detailed information about the open imaging dataset used in our experiments. Each of the datasets presented in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Performance evaluation of the proposed networks and recent SOTA methods on KvasirCapsule-SEG</figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>F2</cell><cell>Accuracy</cell><cell>FPS</cell></row><row><cell>ResUNet (GRSL'18) [38]</cell><cell>8,227,393</cell><cell>0.9532</cell><cell>0.9137</cell><cell>0.9785</cell><cell>0.9325</cell><cell>0.9677</cell><cell>0.9386</cell><cell>17.96</cell></row><row><cell>ResUNet++ (ISM'19) [24]</cell><cell>4,070,385</cell><cell>0.9499</cell><cell>0.9087</cell><cell>0.9762</cell><cell>0.9296</cell><cell>0.9648</cell><cell>0.9334</cell><cell>15.39</cell></row><row><cell>NanoNet-A (Ours)</cell><cell>235,425</cell><cell>0.9493</cell><cell>0.9059</cell><cell>0.9693</cell><cell>0.9325</cell><cell>0.9609</cell><cell>0.9351</cell><cell>28.35</cell></row><row><cell>NanoNet-B (Ours)</cell><cell>132,049</cell><cell>0.9474</cell><cell>0.9028</cell><cell>0.9682</cell><cell>0.9308</cell><cell>0.9593</cell><cell>0.9324</cell><cell>27.39</cell></row><row><cell>NanoNet-C (Ours)</cell><cell>36,561</cell><cell>0.9465</cell><cell>0.9021</cell><cell>0.9754</cell><cell>0.9238</cell><cell>0.9629</cell><cell>0.9297</cell><cell>29.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Performance evaluation of the proposed networks and recent SOTA methods on Kvasir-SEG<ref type="bibr" target="#b9">[10]</ref> </figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>F2</cell><cell>Accuracy</cell><cell>FPS</cell></row><row><cell>ResUNet (GRSL'18) [38]</cell><cell>8,227,393</cell><cell>0.7203</cell><cell>0.6106</cell><cell>0.7602</cell><cell>0.7624</cell><cell>0.7327</cell><cell>0.9251</cell><cell>17.72</cell></row><row><cell>ResUNet++ (ISM'19) [24]</cell><cell>4,070,385</cell><cell>0.7310</cell><cell>0.6363</cell><cell>0.7925</cell><cell>0.7932</cell><cell>0.7478</cell><cell>0.9223</cell><cell>19.79</cell></row><row><cell>NanoNet-A (Ours)</cell><cell>235,425</cell><cell>0.8227</cell><cell>0.7282</cell><cell>0.8588</cell><cell>0.8367</cell><cell>0.8354</cell><cell>0.9456</cell><cell>26.13</cell></row><row><cell>NanoNet-B (Ours)</cell><cell>132,049</cell><cell>0.7860</cell><cell>0.6799</cell><cell>0.8392</cell><cell>0.8004</cell><cell>0.8067</cell><cell>0.9365</cell><cell>29.73</cell></row><row><cell>NanoNet-C (Ours)</cell><cell>36,561</cell><cell>0.7494</cell><cell>0.6360</cell><cell>0.8081</cell><cell>0.7738</cell><cell>0.7719</cell><cell>0.9290</cell><cell>32.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Performance evaluation of the proposed networks and recent SOTA methods on the Medico 2020 dataset<ref type="bibr" target="#b10">[11]</ref> </figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>F2</cell><cell>Accuracy</cell><cell>FPS</cell></row><row><cell>ResUNet (GRSL'18) [38]</cell><cell>8,227,393</cell><cell>0.6846</cell><cell>0.5599</cell><cell>0.7235</cell><cell>0.7236</cell><cell>0.6961</cell><cell>0.9231</cell><cell>18.54</cell></row><row><cell>ResUNet++ (ISM'19) [24]</cell><cell>4,070,385</cell><cell>0.6925</cell><cell>0.5849</cell><cell>0.8249</cell><cell>0.6840</cell><cell>0.7434</cell><cell>0.8995</cell><cell>19.47</cell></row><row><cell>NanoNet-A (Ours)</cell><cell>235,425</cell><cell>0.7364</cell><cell>0.6319</cell><cell>0.8566</cell><cell>0.7310</cell><cell>0.7804</cell><cell>0.9166</cell><cell>28.07</cell></row><row><cell>NanoNet-B (Ours)</cell><cell>132,049</cell><cell>0.7378</cell><cell>0.6247</cell><cell>0.8283</cell><cell>0.7373</cell><cell>0.7685</cell><cell>0.9223</cell><cell>29.04</cell></row><row><cell>NanoNet-C (Ours)</cell><cell>36,651</cell><cell>0.7070</cell><cell>0.5866</cell><cell>0.8095</cell><cell>0.7089</cell><cell>0.7432</cell><cell>0.9148</cell><cell>32.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance evaluation of the proposed networks and recent SOTA methods on the Endotect 2020 dataset<ref type="bibr" target="#b11">[12]</ref> </figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>F2</cell><cell>Accuracy</cell><cell>FPS</cell></row><row><cell>ResUNet (GRSL'18) [39]</cell><cell>8,227,393</cell><cell>0.6640</cell><cell>0.5408</cell><cell>0.7510</cell><cell>0.6841</cell><cell>0.6943</cell><cell>0.9075</cell><cell>26.55</cell></row><row><cell>ResUNet++ (ISM'19) [24]</cell><cell>4,070,385</cell><cell>0.6940</cell><cell>0.5838</cell><cell>0.8797</cell><cell>0.6591</cell><cell>0.7597</cell><cell>0.8841</cell><cell>18.58</cell></row><row><cell>NanoNet-A (Ours)</cell><cell>235,425</cell><cell>0.7508</cell><cell>0.6466</cell><cell>0.8238</cell><cell>0.7744</cell><cell>0.7773</cell><cell>0.9255</cell><cell>27.19</cell></row><row><cell>NanoNet-B (Ours)</cell><cell>132,049</cell><cell>0.7362</cell><cell>0.6238</cell><cell>0.8109</cell><cell>0.7532</cell><cell>0.7646</cell><cell>0.9252</cell><cell>29.91</cell></row><row><cell>NanoNet-C (Ours)</cell><cell>36,651</cell><cell>0.7001</cell><cell>0.5792</cell><cell>0.8000</cell><cell>0.7159</cell><cell>0.7380</cell><cell>0.9091</cell><cell>32.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Performance evaluation of the proposed networks and recent SOTA methods on Kvasir-Instrument<ref type="bibr" target="#b12">[13]</ref> </figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>F2</cell><cell>Accuracy</cell><cell>FPS</cell></row><row><cell>UNet (Baseline) [39]</cell><cell>-</cell><cell>0.9158</cell><cell>0.8578</cell><cell>0.9487</cell><cell>0.8998</cell><cell>0.9320</cell><cell>0.9864</cell><cell>20.46</cell></row><row><cell>DoubleUNet (Baseline) [25]</cell><cell>-</cell><cell>0.9038</cell><cell>0.8430</cell><cell>0.9275</cell><cell>0.8966</cell><cell>0.9147</cell><cell>0.9838</cell><cell>10.00</cell></row><row><cell>ResUNet++ (ISM'19) [24]</cell><cell>4,070,385</cell><cell>0.9140</cell><cell>0.8635</cell><cell>0.9103</cell><cell>0.9348</cell><cell>0.9140</cell><cell>0.9866</cell><cell>17.87</cell></row><row><cell>NanoNet-A (Ours)</cell><cell>235,425</cell><cell>0.9251</cell><cell>0.8768</cell><cell>0.9142</cell><cell>0.9540</cell><cell>0.9251</cell><cell>0.9887</cell><cell>28.00</cell></row><row><cell>NanoNet-B (Ours)</cell><cell>132,049</cell><cell>0.9284</cell><cell>0.8790</cell><cell>0.9205</cell><cell>0.9482</cell><cell>0.9284</cell><cell>0.9875</cell><cell>29.82</cell></row><row><cell>NanoNet-C (Ours)</cell><cell>36,561</cell><cell>0.9139</cell><cell>0.8600</cell><cell>0.9037</cell><cell>0.9452</cell><cell>0.9139</cell><cell>0.9863</cell><cell>32.18</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://osf.io/dv2ag/ 2 https://www.dropbox.com/sh/hr46vieykbmvmkk/AAAs V8ECG0wq51Fpw3rYU 5a?dl=0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://keras.io/ 4 https://github.com/DebeshJha/NanoNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The research is partially funded by the PRIVATON project (263248) and the Autocap project (282315) from the Research Council of Norway (RCN). Our experiments were performed on the Experimental Infrastructure for Exploration of Exascale Computing (eX3) system, which is financially supported by RCN under contract 270053.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: a cancer journal for clinicians</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video capsule endoscopy in inflammatory bowel disease: past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inflammatory Bowel Diseases</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="278" to="285" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end lung cancer screening with threedimensional deep learning on low-dose chest computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="954" to="961" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning algorithm predicts diabetic retinopathy progression in individual patients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arcadu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine learning detection of obstructive hypertrophic cardiomyopathy using a wearable biosensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02475</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06530</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Multimedia Modeling (MMM)</title>
		<meeting>of International Conference on Multimedia Modeling (MMM)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Medico multimedia task at mediaeval 2020: Automatic polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Emanuelsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Proceedings of MediaEval Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The endotect 2020 challenge: Evaluation and comparison of classification, segmentation and inference time for endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPR 2020 Workshops and Challenges</title>
		<meeting>ICPR 2020 Workshops and Challenges</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kvasir-instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Multimedia Modeling (MMM)</title>
		<meeting>of Multimedia Modeling (MMM)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computer-aided tumor detection in endoscopic video using color wavelet features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Karkanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Maroulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tzivras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information technology in biomedicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="152" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Texturebased polyp detection in colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ameling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bildverarbeitung für die Medizin</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="346" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wireless capsule endoscopy: A new tool for cancer screening in the colon with deeplearning-based polyp recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Polyp detection and segmentation from video capsule endoscopy: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prasath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fanet: A feedback attention network for improved biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17235</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polyp segmentation with fully convolutional deep neural networks-extended evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matuszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for detection and segmentation of artefact and disease instances in gastrointestinal endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">102002</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>of International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ResUNet++: An Advanced Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Symposium on Multimedia (ISM)</title>
		<meeting>of IEEE International Symposium on Multimedia (ISM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Multimedia Modeling (MMM)</title>
		<meeting>of International Conference on Multimedia Modeling (MMM)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time detection of colon polyps during colonoscopy using deep learning: systematic validation with four independent datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y O</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Development of a real-time endoscopic image diagnosis support system using deep learning technology in colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ai-doscopist: a real-time deep-learning-based algorithm for localising polyps in colonoscopy videos with edge computing devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Barnet: Bilinear attention network with adaptive receptive field for surgical instrument segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Ni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07093</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lednet: A lightweight encoder-decoder network for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Image Processing</title>
		<meeting>of IEEE International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1860" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeeze u-net: A memory and energy efficient image segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Beheshti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="364" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition</title>
		<meeting>of IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition</title>
		<meeting>of IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Kvasir-capsule, a video capsule endoscopy dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer Nature Scientific Data</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Medical image computing and computer-assisted intervention (MICCAI)</title>
		<meeting>of International Conference on Medical image computing and computer-assisted intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>of USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incorporating nesterov momentum into adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
