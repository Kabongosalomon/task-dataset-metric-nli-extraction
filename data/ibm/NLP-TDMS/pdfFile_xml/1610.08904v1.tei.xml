<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Similarity-Aware Deep Feature Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
							<email>chuang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Change</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loy</forename><forename type="middle">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Local Similarity-Aware Deep Feature Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.</p><p>We question the effectiveness of using a single and global Euclidean distance metric for finding hard samples, especially for real-world vision tasks that exhibit complex feature variations due to pose, lighting, and appearance. As shown in a fine-grained bird image retrieval example in <ref type="figure">Figure 1(a)</ref>, the diversity of feature patterns learned for each class throughout the feature space can easily lead 30th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep embedding methods aim at learning a compact feature embedding f (x) ∈ R d from image x using a deep convolutional neural network (CNN). They have been increasingly adopted in a variety of vision tasks such as product visual search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> and face verification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. The embedding objective is usually in a Euclidean sense: the Euclidean distance D i,j = f (x i ) − f (x j ) 2 between two feature vectors should preserve their semantic relationship encoded pairwise (by contrastive loss <ref type="bibr" target="#b0">[1]</ref>), in triplets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> or even higher order relationships (e.g., by lifted structured loss <ref type="bibr" target="#b28">[29]</ref>).</p><p>It is widely observed that an effective data sampling strategy is crucial to ensure the quality and learning efficiency of deep embedding, as there are often many more easy examples than those meaningful hard examples. Selecting overly easy samples can in practice lead to slow convergence and poor performance since many of them satisfy the constraint well and give nearly zero loss, without exerting any effect on parameter update during the back-propagation <ref type="bibr" target="#b2">[3]</ref>. Hence hard example mining <ref type="bibr" target="#b6">[7]</ref> becomes an indispensable step in state-of-the-art deep embedding methods. These methods usually choose hard samples by computing the convenient Euclidean distance in the embedding space. For instance, in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>, hard negatives with small Euclidean distances are found online in a mini-batch. An exception is <ref type="bibr" target="#b32">[33]</ref> where an online reservoir importance sampling scheme is proposed to sample discriminative triplets by relevance scores. Nevertheless, these scores are computed offline with different hand-crafted features and distance metrics, which is suboptimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Euclidean metric</head><p>Triplet loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>   <ref type="bibr" target="#b18">[19]</ref>) of the CUB-200-2011 <ref type="bibr" target="#b31">[32]</ref> test set. The intraclass distance can be larger than the interclass distance under the global Euclidean metric, which can mislead the hard sample mining and consequently deep embedding learning. We propose a PDDM unit that incorporates the absolute position (i.e., feature mean denoted by the red triangle) to adapt metric to the local feature structure. (b) Overlapped similarity distribution by the Euclidean metric (the similarity scores are transformed from distances by a Sigmoid-like function) vs. the well-separated distribution by PDDM. (c) PDDM-guided hard sample mining and embedding learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position-Dependent Deep Metric</head><p>to a larger intraclass Euclidean distance than the interclass distance. Such a heterogeneous feature distribution yields a highly overlapped similarity score distribution for the positive and negative pairs, as shown in the left chart of <ref type="figure" target="#fig_0">Figure 1</ref>(b). We observed similar phenomenon for the global Mahalanobis metric <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> in our experiments. It is not difficult to see that using a single and global metric would easily mislead the hard sample mining. To circumvent this issue, Cui et al. <ref type="bibr" target="#b2">[3]</ref> resorted to human intervention for harvesting genuinely hard samples.</p><p>Mitigating the aforementioned issue demands an improved metric that is adaptive to the local feature structure. In this study we wish to learn a local-adaptive similarity metric online, which will be exploited to search for high-quality hard samples in local neighborhood to facilitate a more effective deep embedding learning. Our key challenges lie in the formulation of a new layer and loss function that jointly consider the similarity metric learning, hard samples selection, and deep embedding learning. Existing studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> only consider the two latter objectives but not together with the first. To this end, we propose a new Position-Dependent Deep Metric (PDDM) unit for similarity metric learning. It is readily pluggable to train end-to-end with an existing deep embedding learning CNN. We formulate the PDDM such that it learns locally adaptive metric (unlike the global Euclidean metric), through a non-linear regression on both the absolute feature difference and feature mean (which encodes absolute position) of a data pair. As depicted in the right chart of <ref type="figure" target="#fig_0">Figure 1</ref>(b), the proposed metric yields a similarity score distribution that is more distinguishable than the conventional Euclidean metric. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c), hard samples are mined from the resulting similarity score space and used to optimize the feature embedding space in a seamless manner. The similarity metric learning in PDDM and embedding learning in the associated CNN are jointly optimized using a novel large-margin double-header hinge loss.</p><p>Image retrieval experiments on two challenging real-world vision datasets, CUB-200-2011 <ref type="bibr" target="#b31">[32]</ref> and CARS196 <ref type="bibr" target="#b14">[15]</ref>, show that our local similarity-aware feature embedding significantly outperforms state-of-the-art deep embedding methods that come without the online metric learning and associated hard sample mining scheme. Moreover, the proposed approach incurs a far lower computational cost and encourages faster convergence than those structured embedding methods (e.g., <ref type="bibr" target="#b28">[29]</ref>), which need to compute a fully connected dense matrix of pairwise distances in a mini-batch. We further demonstrate our learned embedding is generalizable to new classes in large open set scenarios. This is validated in the transfer learning and zero-shot learning (using the ImageNet hierarchy as auxiliary knowledge) tasks on ImageNet 2010 and ImageNet-10K <ref type="bibr" target="#b4">[5]</ref> datasets.  </p><formula xml:id="formula_0">( ) i f x L2 ( ) j f x L2 ( ) ( ) i j f x f x  L2 ( ) ( ) 2 i j f x f x </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Hard sample mining in deep learning: Hard sample mining is a popular technique used in computer vision for training robust classifier. The method aims at augmenting a training set progressively with false positive examples with the model learned so far. It is the core of many successful vision solutions, e.g. pedestrian detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. In a similar spirit, contemporary deep embedding methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> choose hard samples in a mini-batch by computing the Euclidean distance in the embedding space. For instance, Schroff et al. <ref type="bibr" target="#b26">[27]</ref> selected online the semi-hard negative samples with relatively small Euclidean distances. Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed an online reservoir importance sampling algorithm to sample triplets by relevance scores, which are computed offline with different distance metrics. Similar studies on image descriptor learning <ref type="bibr" target="#b27">[28]</ref> and unsupervised feature learning <ref type="bibr" target="#b33">[34]</ref> also select hard samples according to the Euclidean distance-based losses in their respective CNNs. We argue in this paper that the global Euclidean distance is a suboptimal similarity metric for hard sample mining, and propose a locally adaptive metric for better mining.</p><p>Metric learning: An effective similarity metric is at the core of hard sample mining. Euclidean distance is the simplest similarity metric, and it is widely used by current deep embedding methods where Euclidean feature distances directly correspond the similarity. Similarities can be encoded pairwise with a contrastive loss <ref type="bibr" target="#b0">[1]</ref> or in more flexible triplets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>. Song et al. <ref type="bibr" target="#b28">[29]</ref> extended to even higher order similarity constraints by lifting the pairwise distances within a mini-batch to the dense matrix of pairwise distances. Beyond Euclidean metric, one can actually turn to the parametric Mahalanobis metric instead. Representative works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref> minimize the Mahalanobis distance between positive sample pairs while maximizing the distance between negative pairs. Alternatives directly optimize the Mahalanobis metric for nearest neighbor classification via the method of Neighbourhood Component Analysis (NCA) <ref type="bibr" target="#b10">[11]</ref>, Large Margin Nearest Neighbor (LMNN) <ref type="bibr" target="#b34">[35]</ref> or Nearest Class Mean (NCM) <ref type="bibr" target="#b20">[21]</ref>. However, the common drawback of the Mahalanobis and Euclidean metrics is that they are both global and are far from being ideal in the presence of heterogeneous feature distribution (see <ref type="figure" target="#fig_0">Figure 1(a)</ref>). An intuitive remedy would be to learn multiple metrics <ref type="bibr" target="#b8">[9]</ref>, which would be computationally expensive though. Xiong et al. <ref type="bibr" target="#b36">[37]</ref> proposed a single adaptive metric using the absolute position information in random forest classifiers. Our approach shares the similar intuition, but incorporates the position information by a deep CNN in a more principled way, and can jointly learn similarity-aware deep features instead of using hand-crafted ones as in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Local similarity-aware deep embedding</head><p>Let X = {(x i , y i )} be an imagery dataset, where y i is the class label of image x i . Our goal is to jointly learn a deep feature embedding f (x) from image x into a feature space R d , and a similarity metric</p><formula xml:id="formula_1">S i,j = S(f (x i ), f (x j )) ∈ R 1 ,</formula><p>such that the metric can robustly select hard samples online to learn a discriminative, similarity-aware feature embedding. Ideally, the learned features (f (x i ), f (x j )) from the set of positive pairs P = {(i, j)|y i = y j } should be close to each other with a large similarity score S i,j , while the learned features from the set of negative pairs N = {(i, j)|y i = y j } should be pushed far away with a small similarity score. Importantly, this relationship should hold independent of the (heterogeneous) feature distribution in R d , where a global metric S i,j can fail. To adapt S i,j to the latent structure of feature embeddings, we propose a Position-Dependent Deep Metric (PDDM) unit that can be trained end-to-end, see <ref type="figure" target="#fig_2">Figure 2</ref></p><formula xml:id="formula_2">(b).</formula><p>The overall network architecture is shown in <ref type="figure" target="#fig_2">Figure 2</ref>(a). First, we use PDDM to compute similarity scores for the mini-batch samples during a particular forward pass. The scores are used to select one hard quadruplet from the local sets of positive pairsP ∈ P and negative pairsN ∈ N in the batch. Then each sample in the hard quadruplet is separately fed into four identical CNNs with shared parameters W to extract d-dimensional features. Finally, a discriminative double-header hinge loss is applied to both the similarity score and feature embeddings. This enables us to jointly optimize the two that benefit each other. We will provide the details in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PDDM learning and hard sample mining</head><p>Given a feature pair (f W (x i ), f W (x j )) extracted from images x i and x j by an embedding function f W (·) parameterized by W , we wish to obtain an ideal similarity score y i,j = 1 if (i, j) ∈ P , and y i,j = 0 if (i, j) ∈ N . Hence, we seek the optimal similarity metric S * (·, ·) from an appropriate function space H, and also seek the optimal feature embedding parameters W * :</p><formula xml:id="formula_3">(S * (·, ·), W * ) = argmin S(·,·)∈H,W 1 |P ∪ N | (i,j)∈P ∪N l (S(f W (x i ), f W (x j )), y i,j ) ,<label>(1)</label></formula><p>where l(·) is some loss function. We will omit the parameters W of f (·) in the following for brevity.</p><p>Adapting to local feature structure. The standard Euclidean or Mahalanobis metric can be seen as a special form of function S(·, ·) that is based solely on the feature difference vector u = |f (x i )−f (x j )| or its linearly transformed version. These metrics are suboptimal in a heterogeneous embedding space, thus could easily fail the searching of genuinely hard samples. On the contrary, the proposed PDDM leverages the absolute feature position to adapt the metric throughout the embedding space. Specifically, inspired by <ref type="bibr" target="#b36">[37]</ref>, apart from the feature difference vector u, we additionally incorporate the feature mean vector v = (f (x i ) + f (x j ))/2 to encode the absolute position. Unlike <ref type="bibr" target="#b36">[37]</ref>, we formulate a principled learnable similarity metric from u and v in our CNN.</p><p>Formally, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b), we first normalize the features f (x i ) and f (x j ) onto the unit hypersphere, i.e., f (x) 2 = 1, in order to maintain feature comparability. Such normalized features are used to compute their relative and absolute positions encoded in u and v, each followed by a fully connected layer, an elementwise ReLU nonlinear function σ(ξ) = max(0, ξ), and again, 2 -normalization r(x) = x/ x 2 . To treat u and v differently, the fully connected layers applied to them are not shared, parameterized by</p><formula xml:id="formula_4">W u ∈ R d×d , b u ∈ R d and W v ∈ R d×d , b v ∈ R d , respectively.</formula><p>The nonlinearities ensure the model is not trivially equivalent to be the mapping from f (x i ) and f (x j ) themselves. Then we concatenate the mapped u and v vectors and pass them through another fully connected layer parameterized by W c ∈ R 2d×d , b c ∈ R d and the ReLU function, and finally map to a score</p><formula xml:id="formula_5">S i,j = S(f (x i ), f (x j )) ∈ R 1 via W s ∈ R d×1 , b s ∈ R 1 . To summarize: u = |f (x i ) − f (x j )| , v = (f (x i ) + f (x j )) /2, u = r (σ(W u u + b u )) , v = r (σ(W v v + b v )) , c = σ W c u v + b c , S i,j = W s c + b s .<label>(2)</label></formula><p>In this way, we transform the seeking of the similarity metric function S(·, ·) into the joint learning of</p><formula xml:id="formula_6">CNN parameters (W u , W v , W c , W s , b u , b v , b c , b</formula><p>s for the PDDM unit, and W for feature embeddings). The parameters collectively define a flexible nonlinear regression function for the similarity score.</p><p>Double-header hinge loss. To optimize all these CNN parameters, we can choose a standard regression loss function l(·), e.g., logistic regression loss. Or alternatively, we can cast the problem as a binary classification one as in <ref type="bibr" target="#b36">[37]</ref>. However, in both cases the CNN is prone to overfitting, because the supervisory binary similarity labels y i,j ∈ {0, 1} tend to independently push the scores towards two single points. While in practice, the similarity scores of positive and negative pairs live on a 1-D manifold following some distribution patterns on heterogeneous data, as illustrated in <ref type="figure" target="#fig_0">Figure 1(b)</ref>.</p><p>This motivates us to design a loss function l(·) to separate the similarity distributions, instead of in an independent pointwise way that is noise-sensitive. One intuitive option is to impose the Fisher criterion <ref type="bibr" target="#b19">[20]</ref> on the similarity scores, i.e., maximizing the ratio between the interclass and intraclass scatters of scores. Similarly, it can be reduced to maximize (µ P − µ N ) 2 /(Var P + Var N ) in our 1-D case, where µ and Var are the mean and variance of each score distribution. Unfortunately, the optimality of Fisher-like criteria relies on the assumption that the data of each class is of a Gaussian distribution, which is obviously not satisfied in our case. Also, a high cost O(m 2 ) is entailed to compute the Fisher loss in a mini-batch with m samples by computing all the pairwise distances.</p><p>Consequently, we propose a faster-to-compute loss function that approximately maximizes the margin between the positive and negative similarity distributions without making any assumption about the distribution's shape or pattern. Specifically, we retrieve one hard quadruplet from a random batch during each forward pass. Please see the illustration in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. The quadruplet consists of the most dissimilar positive sample pair in the batch (î,ĵ) = argmin (i,j)∈P S i,j , which means their similarity score is most likely to cross the "safe margin" towards the negative similarity distribution in this local range. Next, we build a similarity neighborhood graph that links the chosen positive pair with their respective negative neighbors in the batch, and choose the hard negatives as the other two quadruplet membersk = argmax (î,k)∈N Sî ,k , andl = argmax (ĵ,l)∈N Sĵ ,l . Using this hard quadruplet (î,ĵ,k,l), we can now locally approximate the inter-distribution margin as min(Sî ,ĵ − Sî ,k , Sî ,ĵ − Sĵ ,l ) in a robust manner. This makes us immediately come to a double-header hinge loss E m to discriminate the target similarity distributions under the large margin criterion:</p><formula xml:id="formula_7">min E m = î ,ĵ (εî ,ĵ + τî ,ĵ ),<label>(3)</label></formula><formula xml:id="formula_8">s.t. : ∀(î,ĵ), max 0, α + Sî ,k − Sî ,ĵ ≤ εî ,ĵ , max 0, α + Sĵ ,l − Sî ,ĵ ≤ τî ,ĵ , (î,ĵ) = argmin (i,j)∈P S i,j ,k = argmax (î,k)∈N Sî ,k ,l = argmax (ĵ,l)∈N Sĵ ,l , εî ,ĵ ≥ 0, τî ,ĵ ≥ 0,</formula><p>where εî ,ĵ , τî ,ĵ are the slack variables, and α is the enforced margin.</p><p>The discriminative loss has four main benefits: 1) The discrimination of similarity distributions is assumption-free. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint metric and embedding optimization</head><p>Given the learned PDDM and mined hard samples in a mini-batch, we can use them to solve for a better, local similarity-aware feature embedding at the same time. For computational efficiency, we reuse the hard quadruplet's features for metric optimization (Eq. (3)) in the same forward pass. What follows is to use the double-header hinge loss again, but to constrain the deep features this time, see <ref type="figure" target="#fig_2">Figure 2</ref>. The objective is to ensure the Euclidean distance between hard negative features (Dî ,k or Dĵ ,l ) is larger than that between hard positive features Dî ,ĵ by a large margin. Combining the embedding loss E e and metric loss E m (Eq. (3)) gives our final joint loss function:</p><formula xml:id="formula_9">min E m + λE e + γ W 2 , where E e = î ,ĵ (oî ,ĵ + ρî ,ĵ ),<label>(4)</label></formula><formula xml:id="formula_10">s.t. : ∀(î,ĵ), max 0, β + Dî ,ĵ − Dî ,k ≤ oî ,ĵ , max 0, β + Dî ,ĵ − Dĵ ,l ≤ ρî ,ĵ , Dî ,ĵ = f (xî) − f (xĵ) 2 , oî ,ĵ ≥ 0, ρî ,ĵ ≥ 0,</formula><p>where W are the CNN parameters for both the PDDM and feature embedding, and oî ,ĵ , ρî ,ĵ and β are the slack variables and enforced margin for E e , and λ, γ are the regularization parameters. Since all features are 2 -normalized (see <ref type="figure" target="#fig_2">Figure 2</ref>), we have β+Dî ,ĵ −Dî ,k = β−2f (xî)f (xĵ)+2f (xî)f (xk), and can conveniently derive the gradients as those in triplet-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>This joint objective provides effective supervision in two domains, respectively at the score level and feature level that are mutually informed. Although the score level supervision by E m alone is already capable of optimizing both our metric and feature embedding, we will show the benefits of adding the feature level supervision by E e in experiments. Note we can still enforce the large margin relations of quadruple features in E e using the simple Euclidean metric. This is because the quadruple features are selected by our PDDM that is learned in the local Euclidean space as well.</p><formula xml:id="formula_11">( ) i f x ( ) j f x Class1 Class2 Class3 Class4 Contrastive embedding ( ) a f x ( ) n f x Class1 Class2 Class3 Class4 Triplet embedding ( ) p f x ( ) i f x ( ) k f x Class1 Class2 Class3 Class4</formula><p>Lifted structured embedding</p><formula xml:id="formula_12">( ) j f x( ) i f x( ) j f x Class1 Class2 Class3 Class4</formula><p>Local similarity-aware embeddinĝ <ref type="figure">Figure 3</ref>: Illustrative comparison of different feature embeddings. Pairwise similarities in classes 1-3 are effortlessly distinguishable in a heterogeneous feature space because there is always a relative safe margin between any two involved classes w.r.t. their class bounds. However, it is not the case for class 4. The contrastive <ref type="bibr" target="#b0">[1]</ref>, triplet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> and lifted structured <ref type="bibr" target="#b28">[29]</ref> embeddings select hard samples by the Euclidean distance that is not adaptive to the local feature structure. They may thus select inappropriate hard samples and the negative pairs get misled towards the wrong gradient direction (red arrow). In contrast, our local similarity-aware embedding is correctly updated by the genuinely hard examples in class 4. <ref type="figure">Figure 3</ref> compares our local similarity-aware feature embedding with existing works. Contrastive <ref type="bibr" target="#b0">[1]</ref> embedding is trained on pairwise data {(x i , x j , y i,j )}, and tries to minimize the distance between the positive feature pair and penalize the distance between negative feature pair for being smaller than a margin α. Triplet embedding <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> samples the triplet data {(x a , x p , x n )} where x a is an anchor point and x p , x n are from the same and different class, respectively. The objective is to separate the intraclass distance between (f (x a ), f (x p )) and interclass distance between (f (x a ), f (x n )) by margin α. While lifted structured embedding <ref type="bibr" target="#b28">[29]</ref> considers all the positive feature pairs (e.g., (f (x i ), f (x j )) in <ref type="figure">Figure 3</ref>) and all their linked negative pairs (e.g., (f (x i ), f (x k )), (f (x j ), f (x l )) and so on), and enforces a margin α between positive and negative distances.</p><formula xml:id="formula_13">( ) k f x ( ) l f x ... ...( ) l f x</formula><p>The common drawback of the above-mentioned embedding methods is that they sample pairwise or triplet (i.e., anchor) data randomly and rely on simplistic Euclidean metric. They are thus very likely to update from inappropriate hard samples and push the negative pairs towards the already well-separated embedding space (see the red arrow in <ref type="figure">Figure 3</ref>). While our method can use PDDM to find the genuinely hard feature quadruplet (f (xî), f (xĵ), f (xk), f (xl)), thus can update feature embedding in the correct direction. Also, our method is more efficient than the lifted structured embedding <ref type="bibr" target="#b28">[29]</ref> that requires computing dense pairwise distances within a mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>We use GoogLeNet <ref type="bibr" target="#b30">[31]</ref> (feature dimension d = 128) and CaffeNet <ref type="bibr" target="#b15">[16]</ref> (d = 4096) as our base network architectures for retrieval and transfer learning tasks respectively. They are initialized with their pretrained parameters on ImageNet classification. The fully-connected layers of our PDDM unit are initialized with random weights and followed by dropout <ref type="bibr" target="#b29">[30]</ref> with p = 0.5. For all experiments, we choose by grid search the mini-batch size m = 64, initial learning rate 1 × 10 −4 , momentum 0.9, margin parameters α = 0.5, β = 1 in Eqs. <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, and regularization parameters λ = 0.5, γ = 5 × 10 −4 (λ balances the metric loss E m against the embedding loss E e ). To find meaningful hard positives in our hard quadruplets, we ensure that any one class in a mini-batch has at least 4 samples. And, we always scale Sî ,ĵ into the range [0, 1] by the similarity graph in the batch. The entire network is trained for a maximum of 400 epochs until convergence.</p><p>4 Results Image retrieval. The task of image retrieval is a perfect testbed for our method, where both the learned PDDM and feature embedding (under the Euclidean feature distance) can be used to find similar images for a query. Ideally, a good similarity metric should be query-adapted (i.e., positiondependent), and both the metric and features should be able to generalize. We test these properties of our method on two popular fine-grained datasets with complex feature distribution. We deliberately make the evaluation more challenging by preparing training and testing sets that are disjoint in terms of class labels. Specifically, we use the CUB-200-2011 <ref type="bibr" target="#b31">[32]</ref> dataset with 200 bird classes and 11,788  images. We employ the first 100 classes (5,864 images) for training, and the remaining 100 classes (5,924 images) for testing. Another used dataset is CARS196 <ref type="bibr" target="#b14">[15]</ref> with 196 car classes and 16,185 images. The first 98 classes (8,054 images) are used for training, and the other 98 classes are retained for testing <ref type="bibr">(8,131 images)</ref>. We use the standard Recall@K as the retrieval evaluation metric.  shows that the proposed PDDM leads to 2× faster convergence in 200 epochs (28 hours on a Titan X GPU) and lower converged loss than the regular Euclidean metric, when both are used to mine hard quadruplets for embedding learning. Note the two resulting approaches both incur lower computational costs than <ref type="bibr" target="#b28">[29]</ref>, with a near linear rather than quadratic <ref type="bibr" target="#b28">[29]</ref> complexity in mini-batches. As observed from the retrieval results and their feature distributions in <ref type="figure" target="#fig_4">Figure 4</ref>-(bottom), our PDDM copes comfortably with large intraclass variations, and generates stable similarity scores for those differently scattered features positioned around a particular query. These results also demonstrate the successful generalization of PDDM on a test set with disjoint class labels. <ref type="table" target="#tab_0">Table 1</ref> quantifies the advantages of both of our similarity metric (PDDM) and similarity-aware feature embedding (dubbed 'PDDM+Quadruplet' for short). In the middle rows, we compare the results from using the metrics of Large Margin Deep Metric (LMDM) and our PDDM, both jointly trained with our quadruplet embedding. The LMDM is implemented by deeply regressing the similarity score from the feature difference only, without using the absolute feature position. Although it is also optimized under the large margin rule, it performs worse than our PDDM due to the lack of position information for metric adaptation. In the bottom rows, we test using the learned features under the Euclidean distance. We observed PDDM significantly improves the performance of both triplet and quadruplet embeddings. In particular, our full 'PDDM+Quadruplet' method yields large gains (8%+ Recall@K=1) over previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> all using the Euclidean distance for hard sample mining. Indeed, as visualized in <ref type="figure" target="#fig_4">Figure 4</ref>, our learned features are typically well-clustered, with sharp boundaries and large margins between many classes. Discussion. We previously mentioned that our PDDM and feature embedding can be learned by only optimizing the metric loss E m in Eq. (3). Here we experimentally prove the necessity of extra supervision from the embedding loss E e in Eq. (4). Without it, the Recall@K=1 of image retrieval by our 'PDDM score' and 'PDDM+Quadruplet' methods drop by 3.4%+ and 6.5%+, respectively. Another important parameter is the batch size m. When we set it to be smaller than 64, say 32, Recall@K=1 on CUB-200-2011 drops to 55.7% and worse with even smaller m. This is because the chosen hard quadruplet from a small batch makes little sense for learning. When we use large m=132, we have marginal gains but need many more epochs (than 400) to use enough training quadruplets.</p><p>Transfer learning. Considering the good performance of our fully learned features, here we evaluate their generalization ability under the scenarios of transfer learning and zero-shot learning. Transfer learning aims to transfer knowledge from the source classes to new ones. Existing methods explored the knowledge of part detectors <ref type="bibr" target="#b5">[6]</ref> or attribute classifiers <ref type="bibr" target="#b16">[17]</ref> across classes. Zero-shot learning is an extreme case of transfer learning, but differs in that for a new class only a description rather than labeled training samples is provided. The description can be in terms of attributes <ref type="bibr" target="#b16">[17]</ref>, WordNet hierarchy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, semantic class label graph <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>, or text data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. These learning scenarios are also related to the open set one <ref type="bibr" target="#b1">[2]</ref> where new classes grow continuously.</p><p>For transfer learning, we follow <ref type="bibr" target="#b20">[21]</ref> to train our feature embeddings and a Nearest Class Mean (NCM) classifier <ref type="bibr" target="#b20">[21]</ref> on the large-scale ImageNet 2010 1 dataset, which contains 1,000 classes and more than 1.2 million images. Then we apply the NCM classifier to the larger ImageNet-10K <ref type="bibr" target="#b4">[5]</ref> dataset with 10,000 classes, thus do not use any auxiliary knowledge such as parts and attributes. We use the standard flat top-1 accuracy as the classification evaluation metric. <ref type="table" target="#tab_1">Table 2</ref> shows that our features outperform state-of-the-art methods by a large margin, including the deep feature-based ones <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. We attribute this advantage to our end-to-end feature embedding learning and its large margin nature, which directly translates to good generalization ability.</p><p>For zero-shot learning, we follow the standard settings in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref> on ImageNet 2010: we learn our feature embeddings on 800 classes, and test on the remaining 200 classes. For simplicity, we also use the ImageNet hierarchy to estimate the mean of new testing classes from the means of related training classes. The flat top-5 accuracy is used as the classification evaluation metric. As can be seen from <ref type="table" target="#tab_1">Table 2</ref>, our features achieve top results again among many competing deep CNN-based methods. Considering our PDDM and local similarity-aware feature embedding are both well learned with safe margins between classes, in this zero-shot task, they would be naturally resistent to class boundary confusion between known and unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we developed a method of learning local similarity-aware deep feature embeddings in an end-to-end manner. The PDDM is proposed to adaptively measure the local feature similarity in a heterogeneous space, thus it is valuable for high quality online hard sample mining that can better guide the embedding learning. The double-header hinge loss on both the similarity metric and feature embedding is optimized under the large margin criterion. Experiments show the efficacy of our learned feature embedding in challenging image retrieval tasks, and point to its potential of generalizing to new classes in the large and open set scenarios such as transfer learning and zero-shot learning. In the future, it is interesting to study the generalization performance when using the shared attributes or visual-semantic embedding instead of ImageNet hierarchy for zero-shot learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) 2-D feature embedding (by t-SNE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) The overall network architecture. All CNNs have shared architectures and parameters. (b) The PDDM unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 )</head><label>2</label><figDesc>Hard samples are simultaneously found during the loss minimization. 3) The loss function incurs a low computational cost and encourages faster convergence. Specifically, the searching cost of the hard positive pair (î,ĵ) is very small since the positive pair setP is usually much smaller than the negative pair setN in an m-sized mini-batch. While the hard negative mining only incurs an O(m) complexity. 4) Eqs. (2, 3) can be easily optimized through the standard stochastic gradient descent to adjust the CNN parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Top: a comparison of the training convergence curves of our method with Euclidean-and PDDM-based hard quadruplet mining on the test sets of CUB-200-2011<ref type="bibr" target="#b31">[32]</ref> and CARS196<ref type="bibr" target="#b14">[15]</ref> datasets. Bottom: top 8 images retrieved by PDDM (similarity score and feature distance are shown underneath) and the corresponding feature embeddings (black dots) on CUB-200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="13">Recall@K (%) on the test sets of CUB-200-2011 [32] and CARS196 [15] datasets.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CUB-200-2011</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CARS196</cell><cell></cell><cell></cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Contrastive [1]</cell><cell cols="12">26.4 37.7 49.8 62.3 76.4 85.3 21.7 32.3 46.1 58.9 72.2 83.4</cell></row><row><cell>Triplet [27, 33]</cell><cell cols="12">36.1 48.6 59.3 70.0 80.2 88.4 39.1 50.4 63.3 74.5 84.1 89.8</cell></row><row><cell>LiftedStruct [29]</cell><cell cols="12">47.2 58.9 70.2 80.2 89.3 93.2 49.0 60.3 72.1 81.5 89.2 92.8</cell></row><row><cell>LMDM score</cell><cell cols="12">49.5 61.1 72.1 81.8 90.5 94.1 50.9 61.9 73.5 82.5 89.8 93.1</cell></row><row><cell>PDDM score</cell><cell cols="12">55.0 67.1 77.4 86.9 92.2 95.0 55.2 66.5 78.0 88.2 91.5 94.3</cell></row><row><cell>PDDM+Triplet</cell><cell cols="12">50.9 62.1 73.2 82.5 91.1 94.4 46.4 58.2 70.3 80.1 88.6 92.6</cell></row><row><cell cols="13">PDDM+Quadruplet 58.3 69.2 79.0 88.4 93.1 95.7 57.4 68.6 80.1 89.4 92.3 94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The flat top-1 accuracy (%) of transfer learning on ImageNet-10K [5] and flat top-5 accuracy (%) of zero-shot learning on ImageNet 2010. Ours ConSE [22] DeViSE [8] PST [24] [25] [21] AMP [10] Ours 6.4 16.7 18.1 19.2 21.9 28.4</figDesc><table><row><cell>Transfer learning on ImageNet-10K</cell><cell cols="3">Zero-shot learning on ImageNet 2010</cell><cell></cell></row><row><cell>[5] [26] [23] [18] [21] 28.5</cell><cell>31.8</cell><cell>34.0</cell><cell>34.8 35.7 41.0</cell><cell>48.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See http://www.image-net.org/challenges/LSVRC/2010/index.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported by SenseTime Group Limited and the Hong Kong Innovation and Technology Support Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="98" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What does classifying more than 10,000 image categories tell us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning globally-consistent local distance functions for shape-based image retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning distance metrics with contextual constraints for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of discriminative attributes and visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Visualizing high-dimensional data using t-SNE. JMLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards good practice in large-scale learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6537v2</idno>
		<title level="m">Fracking deep convolutional image descriptors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Random forests for metric learning with implicit pairwise position dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
