<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07">July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University and Microsoft Research Asia YANG LIU</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia YU-XIAO GUO</orgName>
								<orgName type="institution" key="instit3">University of Electronic Science and Technology of China and Microsoft Research Asia CHUN-YU SUN</orgName>
								<orgName type="institution" key="instit4">Tsinghua University and Microsoft Research Asia XIN TONG</orgName>
								<orgName type="institution" key="instit5">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University and Microsoft Research Asia YANG LIU</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia YU-XIAO GUO</orgName>
								<orgName type="institution" key="instit3">University of Electronic Science and Technology of China and Microsoft Research Asia CHUN-YU SUN</orgName>
								<orgName type="institution" key="instit4">Tsinghua University and Microsoft Research Asia XIN TONG</orgName>
								<orgName type="institution" key="instit5">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University and Microsoft Research Asia YANG LIU</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia YU-XIAO GUO</orgName>
								<orgName type="institution" key="instit3">University of Electronic Science and Technology of China and Microsoft Research Asia CHUN-YU SUN</orgName>
								<orgName type="institution" key="instit4">Tsinghua University and Microsoft Research Asia XIN TONG</orgName>
								<orgName type="institution" key="instit5">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University and Microsoft Research Asia YANG LIU</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia YU-XIAO GUO</orgName>
								<orgName type="institution" key="instit3">University of Electronic Science and Technology of China and Microsoft Research Asia CHUN-YU SUN</orgName>
								<orgName type="institution" key="instit4">Tsinghua University and Microsoft Research Asia XIN TONG</orgName>
								<orgName type="institution" key="instit5">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University and Microsoft Research Asia YANG LIU</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia YU-XIAO GUO</orgName>
								<orgName type="institution" key="instit3">University of Electronic Science and Technology of China and Microsoft Research Asia CHUN-YU SUN</orgName>
								<orgName type="institution" key="instit4">Tsinghua University and Microsoft Research Asia XIN TONG</orgName>
								<orgName type="institution" key="instit5">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University and Microsoft Research Asia YANG LIU</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia YU-XIAO GUO</orgName>
								<orgName type="institution" key="instit3">University of Electronic Science and Technology of China and Microsoft Research Asia CHUN-YU SUN</orgName>
								<orgName type="institution" key="instit4">Tsinghua University and Microsoft Research Asia XIN TONG</orgName>
								<orgName type="institution" key="instit5">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Trans. Graph</title>
						<imprint>
							<biblScope unit="volume">36</biblScope>
							<date type="published" when="2017-07">July 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3072959.3073608</idno>
					<note>This is the author&apos;s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Transactions on Graphics, https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Mesh models</term>
					<term>Point- based models</term>
					<term>Neural networks</term>
					<term>Additional Key Words and Phrases: octree, convolutional neural network, object classification, shape retrieval, shape segmentation ACM Reference format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>normal field octree input (d-depth)    ... convolution pooling convolution pooling d d-1 3 2 ...</p><p>... ... <ref type="figure">Fig. 1</ref>. An illustration of our octree-based convolutional neural network (O-CNN). Our method represents the input shape with an octree and feeds the averaged normal vectors stored in the finest leaf octants to the CNN as input. All the CNN operations are efficiently executed on the GPU and the resulting features are stored in the octree structure. Numbers inside the blue dashed square denote the depth of the octants involved in computation.</p><p>We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. An illustration of our octree-based convolutional neural network (O-CNN). Our method represents the input shape with an octree and feeds the averaged normal vectors stored in the finest leaf octants to the CNN as input. All the CNN operations are efficiently executed on the GPU and the resulting features are stored in the octree structure. Numbers inside the blue dashed square denote the depth of the octants involved in computation.</p><p>We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.</p><p>CCS Concepts: • Computing methodologies → Mesh models; Pointbased models; Neural networks;</p><p>Additional Key Words and Phrases: octree, convolutional neural network, object classification, shape retrieval, shape segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With recent advances in low-cost 3D acquisition devices and 3D modeling tools, the amount of 3D models created by end users has been increasing quickly. Analyzing and understanding these 3D shapes, as for classification, segmentation, and retrieval, have become more and more important for many graphics and vision applications. A key technique for these shape analysis tasks is to extract features of 3D models that can sufficiently characterize their shapes and parts.</p><p>In the computer vision field, convolutional neural networks (CNNs) are widely used for image feature extraction and have demonstrated their advantages over manually-crafted solutions in most image analysis and understanding tasks. However, it is a non-trivial task to adapt a CNN designed for regularly sampled 2D images to 3D shapes modeled by irregular triangle meshes or point clouds. A set of methods convert the 3D shapes to regularly sampled representations and apply a CNN to them. Voxel-based methods <ref type="bibr" target="#b19">[Maturana and Scherer 2015;</ref><ref type="bibr" target="#b31">Wu et al. 2015</ref>] rasterize a 3D shape as an indicator function or distance function sampled over dense voxels and apply a 3D CNN over the entire 3D volume. Since the memory and computation cost grow cubically as the voxel resolution increases, these methods become prohibitively expensive for high-resolution voxels. Manifold-based methods <ref type="bibr" target="#b2">[Boscaini et al. , 2016</ref><ref type="bibr" target="#b27">Sinha et al. 2016]</ref> perform CNN computations over the features defined on a 3D mesh manifold. These methods require smooth manifold surfaces as input and are sensitive to noise and large distortion, which makes them unsuitable for the non-manifold 72:2 • Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong 3D models in many 3D shape repositories. Multiple-view based approaches <ref type="bibr" target="#b26">Shi et al. 2015;</ref>] render the 3D shape into a set of 2D images observed from different views and feed the stacked images to the CNN. However, it is unclear how to determine the view positions to cover full 3D shapes and avoid self-occlusions.</p><p>We present an octree-based convolutional neural network, named O-CNN, for 3D shape analysis. The key idea of our method is to represent the 3D shapes with octrees and perform 3D CNN operations only on the sparse octants occupied by the boundary surfaces of 3D shapes. To this end, the O-CNN takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and computes features for the finest level octants. After pooling, the features are down-sampled to the parent octants in the next coarser level and are fed into the next O-CNN layer. This process is repeated until all O-CNN layers are evaluated.</p><p>The main technical challenge of the O-CNN is to parallelize the O-CNN computations defined on the sparse octants so that they can be efficiently executed on the GPU. To this end, we design a novel octree structure that stores the features and associated octant information into the graphics memory for supporting all CNN operations on the GPU. In particular, we pack the features and data of sparse octants at each depth as continuous arrays. A label buffer is introduced to find the correspondence between the features at different levels for efficient convolution and pooling operations. To efficiently compute 3D convolutions with an arbitrary kernel size, we build a hash table to quickly construct the local neighborhood volume of eight sibling octants and compute the 3D convolutions of these octants in parallel. With the help of this octree structure, the entire training and evaluation process can be efficiently executed on the GPU.</p><p>The O-CNN provides a generic and efficient CNN solution for 3D shape analysis. It supports various CNN structures and works for 3D shapes in different representations. By restraining the CNN computations and features on sparse octants of the 3D shape boundaries, the memory and computation costs of O-CNN grow quadratically as the octree depth increases, which makes it efficient for analyzing high-resolution 3D models. To demonstrate the efficiency of the O-CNN, we construct an O-CNN with basic CNN layers as shown in <ref type="figure">Figure 1</ref>. We train this O-CNN model with 3D shape datasets and refine the O-CNN models with different back-ends for three shape analysis tasks, including object classification, shape retrieval, and shape segmentation. Compared to existing 3D CNN solutions, our method achieves comparable or better accuracy with much less computational and memory costs in all three shape analysis tasks. We also evaluate the performance of the O-CNN with different octree depths in object classification and demonstrate the efficiency and efficacy of the O-CNN for analyzing high-resolution 3D shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first review existing CNN approaches and other deep learning methods for 3D shape analysis. Then we discuss the GPU based octree structures used in different graphics applications.</p><p>CNNs for 3D shape analysis. A set of CNN methods have been presented for 3D shape analysis. We classify these approaches into several classes according to the 3D shape representation used in each solution.</p><p>Voxel-based methods. model the 3D shape as a function sampled on voxels and define a 3D CNN over voxels for shape analysis. <ref type="bibr" target="#b31">Wu et al. [2015]</ref> proposed 3D ShapeNets for object recognition and shape completion. <ref type="bibr" target="#b19">Maturana and Scherer [2015]</ref> improve 3D ShapeNets with fewer input parameters defined in each voxel. These full-voxelbased methods are limited to low resolutions like 30 3 due to the high memory and computational cost.</p><p>To reduce the computational cost of full-voxel based methods, <ref type="bibr" target="#b8">Graham [2015]</ref> proposes the 3D sparse CNNs that apply CNN operations to active voxels and activate only the neighboring voxels inside the convolution kernel. However, the method quickly becomes less efficient as the number of convolution layers between the pooling layers increases. For a CNN with deep layers and a large kernel size, the computational and memory cost of this method is still high. <ref type="bibr" target="#b24">Riegler et al. [2017]</ref> combine the octree and a grid structure to support high-resolution 3D CNNs. Their method limits the 3D CNN to the interior volume of 3D shapes and becomes less efficient than the full-voxel-based solution when the volume resolution is lower than 64 3 . Our method limits the 3D CNN to the octants of the 3D shape boundaries and leverages a novel octree structure for efficiently training and evaluating the O-CNN on the GPU.</p><p>Manifold-based methods. perform CNN operations over the geometric features defined on a 3D mesh manifold. Some methods parameterize the 3D surfaces to 2D patches <ref type="bibr" target="#b2">[Boscaini et al. , 2016</ref><ref type="bibr" target="#b27">Sinha et al. 2016]</ref> or geometry images and feed the regularly sampled feature images into a 2D CNN for shape analysis. Other methods extend the CNN to the graphs defined by irregular triangle meshes <ref type="bibr" target="#b4">[Bronstein et al. 2017]</ref>. Although these methods are robust to isometric deformation of 3D shapes, they are constrained to smooth manifold meshes. The local features used in these methods are always computationally expensive. A good survey of these techniques can be found in <ref type="bibr" target="#b4">[Bronstein et al. 2017</ref>].</p><p>Multiview-based methods. represent the 3D shape with a set of images rendered from different views and take the image stacks as the input of a 2D CNN for shape analysis . Although these methods can directly exploit the image-based CNNs for 3D shape analysis and handle high-resolution inputs, it is unclear how to determine the number of views and distribute the views to cover the 3D shape while avoiding self-occlusions. Our method is based on the octree representation and avoids the view selection issue. It can also handle high-resolution inputs and achieve similar performance and accuracy to multiview-based methods.</p><p>Deep learning for 3D shape analysis. Besides the CNN, other deep learning methods have also been proposed for 3D shape analysis. For shape segmentation, <ref type="bibr" target="#b9">Guo et al. [2015]</ref> extract low-level feature vectors on each facet and pack them as images for training. <ref type="bibr" target="#b32">Li et al. [2016]</ref> introduce a probing filter that can efficiently extract features and work for a higher resolution like 64 3 . However, this approach cannot extract fine structures of shapes; therefore, it is not suitable for tasks like shape segmentation. <ref type="bibr" target="#b22">Qi et al. [2017]</ref> propose a </p><formula xml:id="formula_0">L 1 [0] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [1] L 1 [2] (a) (b) (c) (d) T 1 T 2 S 0 S 1 S 2 L 0 L 1 L 2 Input signal Fig. 2.</formula><p>A 2D quadtree illustration of our octree structure. (a) Given an input 2D shape marked in red, we construct a 2-depth quadtree. The squares that are not occupied by the shape are empty nodes. The numbers inside quad nodes are their shuffled keys. (b) The shuffle key vectors S l , l = 0 . . . 2,, each of which stores the shuffled keys of the quad nodes at the l -th depth. (c) The label vectors L l , l = 0 . . . 2, each of which stores the labels of the quad nodes at the l -th depth. For empty nodes, their labels are set to zero. For non-empty nodes, the label p of a node indicates it is the p-th non-empty node at this depth. The label buffer is used to find the correspondence from the parent node to its child nodes. (d) The results of CNN convolutions over the input signal are stored in a feature map T 2 . When T 2 is down-sampled by a CNN operation, such as pooling, the downsampled results are assigned to the first, the second and the third entries of T 1 . The label vector L 1 at the first depth is used to find the correspondence between the nodes at two depths.</p><p>neural network based on an unordered point cloud that can achieve good performance on shape classification and segmentation.</p><p>GPU-based octree structures. The octree structure <ref type="bibr" target="#b20">[Meagher 1982</ref>] is widely used in computer graphics for various tasks including rendering, modeling and collision detection. <ref type="bibr" target="#b35">Zhou et al. [2011]</ref> propose a GPU-based octree construction method and use the constructed octree for reconstructing surfaces on the GPU. Different from the octree structure that is optimized for surface reconstruction, the octree structure designed in our method is optimized for CNN training and evaluation. For this purpose, we discard the pointers from parent octants to children octants and introduce a label array for finding correspondence of octants at different depths for downsampling. Instead of computing the neighborhood of a single octant, we construct the neighborhoods for eight children nodes of a parent node for fast convolution computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D CNNS ON OCTREE BASED 3D SHAPES</head><p>Given an oriented 3D model (e.g. an oriented triangle mesh or a point cloud with oriented normals), our method first constructs the octree of the input 3D model and packs the information needed for CNN operations in the octree (Section 3.1). With the help of this octree structure, all the CNN operations can be efficiently executed on the GPU (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Octree for 3D CNNs</head><p>Octree construction. To construct an octree for an input 3D model, we first uniformly scale the 3D shape into an axis-aligned unit 3D bounding cube and then recursively subdivide the bounding cube of the 3D shape in breadth-first order. In each step, we traverse all non-empty octants occupied by the 3D shape boundary at the current depth l and subdivide each of them to eight child octants at the next depth l + 1. We repeat this process until the pre-defined octree depth d is reached. <ref type="figure">Figure 2</ref>(a) illustrates a 2-depth quadtree constructed for a 2D shape (marked in red).</p><p>After the octree is constructed, we collect a set of properties required for CNN operations at each octant and store their values in the octree. Specifically, we compute a shuffle key <ref type="bibr" target="#b30">[Wilhelms and Van Gelder 1992]</ref> and a label for each octant in the octree. Meanwhile, our method extracts the input signal of the CNN from the 3D shape stored in the finest leaf nodes and records the resulting CNN features at each octant. As shown in <ref type="figure">Figure 2</ref>, we organize the data stored in the octree in a depth order. At each depth, we sort the octants according to the ascending order of their shuffle keys and pack their property values into a set of 1D property vectors. All the property vectors share the same index, and the length of the vectors is the number of octants at the current depth. In the following, we describe the definition and implementation details of each property defined in our octree structure.</p><p>Shuffle key. The shuffle key of an octant O at depth l encodes its position in 3D space with an unique 3 l-bit string key(O) :=</p><p>x 1 y 1 z 1 x 2 y 2 z 2 . . . x l y l z l <ref type="bibr" target="#b30">[Wilhelms and Van Gelder 1992]</ref>, where each three-bit group x i , y i , z i ∈ {0, 1} defines its relative position in the 3D cube of its parent octant. The integer coordinates</p><formula xml:id="formula_1">(x, y, z) of an octant O are determined by x = (x 1 x 2 . . . x l ), y = (y 1 y 2 . . . y l ), z = (z 1 z 2 . . . z l )</formula><p>. We sort all the octants by their shuffle keys according to the ascending order and store the sorted shuffle keys of all octants at the l-th depth in a shuffle key vector S l , which is used later for constructing the neighborhood of an octant for 3D convolution. In our implementation, each shuffle key is stored in a 32 bit integer. <ref type="figure">Figure 2</ref> Label. In CNN computations, the pooling operation is frequently used to downsample the features computed at the l-th depth octants to their parent octants at the (l − 1)-th depth. As a result, we need to quickly find the parent-child relationship of the octants at the adjacent depths. To this end, we assign a label p for a non-empty octant at the l-th depth, which indicates that it is the p-th non-empty octant in the sorted octant list of the l-th depth. For empty octants, we simply set its label as zero. The labels of all octants at the l-th depth are stored in a label vector L l . <ref type="figure">Figure 2</ref> vector for each depth of a quadtree. For the non-empty quad node marked in blue, its label is 3 (L 1 [2] = 3) because it is the third nonempty node at the first depth, while the node with the shuffle key 0 is the first one and the node with the shuffle key 1 is the second one.</p><p>Given a non-empty node with index j at the l-th depth, we compute the index k of its first child octant at the (l + 1)-th depth by</p><formula xml:id="formula_2">k = 8 × (L l [j] − 1)</formula><p>. This is based on two observations. First, only the non-empty octants at the l-th depth are subdivided. Second, since we sort the octants according to the ascending order of their shuffle keys, the eight children of an octant are sequentially stored. Moreover, the child octants and their non-empty parents follow the same order in their own property vectors. As shown in <ref type="figure">Figure 2</ref>(c), the last four nodes at the second depth are created by the third non-empty node at the first depth (marked in blue).</p><p>Input signal. We use the averaged normal vectors computed at the finest leaf octants as the input signal of the CNN. For empty leaf octants, we simply assign a zero vector as the input signal. For non-empty leaf octants, we sample the 3D shape surface embedded in the leaf octant with a set of points and average the normals of all sampled points as the input signal at this leaf octant. We store the input signals of all leaf octants into an input signal vector. The size of the vector is the number of the finest leaf octants in the octree.</p><p>Compared to the binary indicator function used in many voxelbased CNN methods <ref type="bibr" target="#b19">[Maturana and Scherer 2015;</ref><ref type="bibr" target="#b31">Wu et al. 2015]</ref>, the normal signal is very sparse, i.e. only non-zero on the surface, and the averaged normals sampled in the finest leaf octants better represent the orientation of the local 3D shapes and provide more faithful 3D shape information to the CNN. <ref type="figure">Figure 3</ref> compares a voxelized 3D model and an octree representation of the same 3D shape rendered by oriented disks sampled at leaf octants, where the size of the leaf octant is the same as the voxel size. As shown in the figure, the octree representation (on the right) is more faithful to the ground truth 3D shape (on the left) than the voxel based representation (in the middle).</p><p>CNN features. For each 3D convolution kernel defined at the l-th depth, we record the convolution results on all the octants at the l-th depth in a feature map vector T l .</p><p>Mini-batch of 3D models. For 3D objects in a mini-batch used in the CNN training, their octrees are not the same. To support efficient CNN training on the GPU, we merge these octrees into one superoctree. For each octree depth l, we concatenate the property vectors (S l , L l and T l ) of all 3D objects to S * l , L * l and T * l of the super-octree. After that, we update the shuffle keys in S * l by using the highest 8 bits of each shuffle key to store the object index. We also update the label vector L * l in the super-octree to represent the index of each non-empty octant in the whole super-octree. After that, the super-octree can be directly used in the CNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN operations on the Octree</head><p>The most common operations in a CNN are convolution, pooling, and the inverse deconvolution and unpooling. With the help of our octree data structure, all these CNN operations can be efficiently implemented on the GPU.</p><p>3D Convolution. For applying the convolution operator to an octant, one needs to pick its neighboring octants at the same octree depth. To compute the convolution efficiently, we write the convolution operator Φ c in the unrolled form:</p><formula xml:id="formula_3">Φ c (O) = n i j k W (n) i jk · T (n) (O i jk ).</formula><p>Here O i jk represents a neighboring octant of O and T (·) represents the feature vector associated with O i jk . And T (n) (·) represents the n-th channel of the feature vector, and W (n)</p><p>i jk are the weights of the convolution operation. If O i jk does not exist in the octree, T (O i jk ) is set to the zero vector. In this form, the convolution operation can be converted to a matrix product <ref type="bibr" target="#b6">[Chellapilla et al. 2006;</ref><ref type="bibr" target="#b11">Jia et al. 2014]</ref> and computed efficiently on the GPU.</p><p>The convolution operator with kernel size K requires the access of K 3 − 1 neighboring octants of an octant. A possible solution is to pre-compute and store neighboring information. However, a CNN is normally trained on a batch of shapes. When K is very large, this will cause a significant amount of I/O processing and a large memory footprint. We build a hash table: H : key(O) → index(O) to facilitate the search, where index(O) records the position of O in S. Because the amortized time complexity of a hash table is constant, this choice can be regarded as a balance of computation cost and memory cost for the CNN. Given the shuffled key stored in the vector S l , the integer coordinates (x, y, z) of the octant can be restored, then the coordinates of the neighboring octants can be computed easily in constant time, as can their corresponding shuffled keys. Given the shuffled keys of neighboring octants, the hash table is efficiently searched in parallel to get their indices, according to which the neighboring data information is gathered and then the convolution operation can be applied.</p><p>If the stride of convolution is 1, the above operation is applied to all existing octants at the current octree depth. Naïvely, for each octant the hash table will be searched K 3 − 1 times. However, the neighborhood of eight sibling octants under the same parent have a lot of overlap, and there are only (K +1) 3 individual octants including the eight octants. The neighborhood search can be further sped up by just searching the (K + 1) 3 − 8 neighboring octants for each of the eight sibling octants. Concretely, if the kernel size is 3, then this optimization can accelerate the neighboring search operation by more than 2 times. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates this efficient neighborhood search on a 2D quadtree. Since the neighborhood of 4 sibling nodes under the same parent have a lot of overlap, we only access the union of these neighbors whose total number is 16 as shown on the right.</p><p>It is also very easy to perform a convolution with a stride of 2. Specifically, for the 8 sibling octants under the same parent, the convolution can be applied to the first sibling octant while ignoring the other siblings, which is equivalent to down-sampling the resolution of the feature map by a factor of 2. As for the convolution with a stride of 2 r (r &gt; 1), the operation can be applied to the first octant belonging to each sub-tree of height r , then the feature map will be downsampled by a factor of 2 r (r &gt; 1). Because of the special hierarchical structure of an octree, the stride of convolution is constrained to be an integer power of 2. However, convolution with an arbitrary stride is uncommon in the CNN literature. According to our experiments, the performance of our method will not be harmed by this stride limitation.</p><p>When performing the convolution operation with a stride larger than 1, down-sampling occurs and the length of the data vector T is shortened. The data flows from bottom to top. Then information stored in L l can be used to get the correspondence. Take the example shown in <ref type="figure">Figure 2</ref> as an illustration, the initial length of T 2 is 12. When down-sampling occurs the length of the vector will be 3. But there are 4 octants at the first depth of the octree, and the length of T 1 should be 4. Combining the information stored in L 1 and the down-sampled vector, T 1 can be renewed easily.</p><p>Pooling. The main functionality of pooling is to progressively condense the spatial size of the representation. The pooling layer operates independently on every channel of the feature map and resizes it spatially. The most common form is the max-pooling layer with filters of kernel size 2 applied with a stride of 2. It is very convenient to apply pooling on our octree structure. Since every 8 sibling octants under the same parent are stored consecutively, applying the max-pooling operator on an octree reduces to picking out the max elements from every 8 contiguous elements, which can be implemented efficiently on GPU devices. Then the resolution of the feature map is down-sampled by a factor of 2, and the information from the parent octants can be used to guide further operations.</p><p>Since pooling can be regarded as a special kind of convolution in practice, we follow the approach presented before for general pooling operations with other kernel sizes or stride sizes, i.e. finding the corresponding neighboring octants and applying the specified operation, such as max-pooling or average-pooling.</p><p>Unpooling. The unpooling operation is the reverse operation of pooling and performs up-sampling, which is widely used in CNN visualization [Zeiler and Fergus 2014] and image segmentation <ref type="bibr" target="#b21">[Noh et al. 2015]</ref>. The max-unpooling operation is often utilized together with the max-pooling operation. After applying the max-pooling operation, the locations of the maxima within each pooling region can be recorded in a set of switch variables stored in a continuous array. The corresponding max-unpooling operation makes use of these switches to place the signal of the current feature map into appropriate locations of the up-sampled feature map. <ref type="figure" target="#fig_4">Figure 5</ref> shows how max-unpooling works on a quadtree. Thanks to the contiguous storage of octants, we can reuse the efficient unpooling implementation developed for image-based CNNs.</p><p>Deconvolution. The deconvolution operator, also called transposed convolution and backwards convolution <ref type="bibr" target="#b17">[Long et al. 2015;</ref><ref type="bibr" target="#b34">Zeiler and Fergus 2014]</ref>, can be used to enlarge and densify the activation map, which can be implemented by just reversing the forward and backward passes of convolution. Based on the convolution on octrees presented previously, the deconvolution operation can be implemented accordingly.</p><p>Remark. Different from full-voxel-based CNNs that perform CNN operations over all the space including empty regions, CNN operations in our method are applied to octants only. That is to say, where there is an octant, there is CNN computation. Our understanding here is that propagating information to empty regions and exchanging information via empty regions are not necessary and would require more memory and computations. By restricting the information propagation in the octree, the shape information can be exchanged more effectively along the shape. Although there is not a theoretical proof on this point, we demonstrate the advantages of our method in Section 5.</p><p>By restricting the data storage and CNN computations in octants, the memory and computation cost of the octree based CNN is O(n 2 ), where n is the voxel resolution in each dimension at the finest level. On the contrary, the memory and computational cost of a full-voxel based solution is O(n 3 ). Furthermore, since all the data is stored contiguously in memory, O-CNN shares the same high performance GPU computations with 2D and 3D CNNs defined on the regular grids. The hash table and neighbor information which are needed in the k-th iteration during the training can be pre-computed in the (k −1)-th iteration in a separate thread, which incurs no computation time latency. A detailed evaluation and comparison are performed in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NETWORK STRUCTURE</head><p>The network structure of CNNs has evolved rapidly in recent years. Deeper and wider networks have shown their superiority in accomplishing many tasks. Existing 3D CNNs have used different networks to enhance their capabilities. However, this makes it hard to distinguish where the main benefit of their approach comes from. To clearly demonstrate the advantages of our octree-based representation, we design a simple network by following the concept of LeNet <ref type="bibr" target="#b14">[Lecun et al. 1998</ref>].</p><p>O-CNN . Our O-CNN is simple: we repeatedly apply convolution and pooling on the octree data structure from bottom to top. We use the ReLU function (f : x ∈ R → max(0, x)) to activate the output and use batch normalization (BN) to reduce the internal-covariateshift <ref type="bibr" target="#b16">[Loffe and Szegedy 2015]</ref>. We call the operation sequence "convolution + BN + ReLU + pooling" a basic unit and denote it by U l if the convolution is applied to the l-th depth octants. The number of channels of the feature map for U l is set to 2 max <ref type="bibr">(1,9−l )</ref> and the convolution kernel size is 3. Our O-CNN is defined by the following form:</p><formula xml:id="formula_4">input → U d → U d −1 → · · · → U 2</formula><p>and we call it O-CNN(d). To align all features from different octree structures, we enforce all the 2nd-depth octants to exist and use zero vector padding on the empty octants at the 2nd depth.</p><p>O-CNN for shape analysis. In our work, we apply our O-CNN to three shape analysis tasks: object classification, shape retrieval, and shape part segmentation.</p><p>For object classification, we add two fully connected (FC) layers, a softmax layer, and two Dropout layers <ref type="bibr" target="#b28">[Srivastava et al. 2014</ref></p><formula xml:id="formula_5">] after O-CNN(d), i.e. O-CNN(d) → Dropout → FC(128) → Dropout → FC(N c ) → softmax → output.</formula><p>Here 128 is the number of neurons in FC and N c is the number of classification categories. Dropout is used to avoid overfitting.</p><p>For shape retrieval, we use the output from the object classification as the key to search for the most similar shapes to the query.</p><p>For shape part segmentation, we adopt the state-of-the-art image semantic segmentation network DeconvNet <ref type="bibr" target="#b21">[Noh et al. 2015]</ref>, which cascades a deconvolution network after a convolution network for dense predictions. The convolution network is set as our O-CNN(d). The deconvolution network is the mirror of O-CNN(d) where the convolution and pooling operators are replaced by deconvolution and unpooling operators. We define "unpooling + deconvolution + BN + ReLU" as a basic unit and denote it by DU l if the unpooling is applied to the l-depth octants. The network structure for shape segmentation is</p><formula xml:id="formula_6">O-CNN(d) → DU 2 → DU 3 → · · · → DU d .</formula><p>The details of O-CNN for the above tasks and the experiments are presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND DISCUSSION</head><p>For demonstrating the efficiency and efficacy of our O-CNN, we conduct three shape analysis tasks on a desktop machine with an Intel Core I7-6900K CPU (3.2 GHz) and a GeForce 1080 GPU (8GB memory). Our GPU implementation of O-CNN uses the Caffe framework <ref type="bibr" target="#b11">[Jia et al. 2014]</ref> and is available at http://wang-ps.github. io/O-CNN.</p><p>Training details. We optimize the O-CNNs by stochastic gradient descent (SGD) with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 32. The dropout ratio is 0.5. The initial learning rate is 0.1, and decreased by a factor of 10 after every 10 epochs. The optimization stops after about 40 epochs. The hyper-parameters of the network are fixed in the shape classification and retrieval experiments. They are fine-tuned in object segmentation for the categories with a small number of shapes.</p><p>Octree data preparation. For the tasks we perform, the 3D training datasets are mainly from ModelNet40 <ref type="bibr" target="#b31">[Wu et al. 2015]</ref> and ShapeNe-tCore55 <ref type="bibr" target="#b5">[Chang et al. 2015]</ref> which contain a large number of triangle meshes with various shapes. We find that many meshes in ModelNet contain a lot of artifacts: flipped normals, non-manifold structures, and overlapped triangles. Thus to build the octree data structure with correct normal information, we first use the ray shooting algorithm to sample dense points with oriented normals from the shapes. Specifically, we place 14 virtual cameras on the face centers of the truncated bounding cube of the object, uniformly shoot 16k parallel rays towards the object from each direction, calculate the intersections of the rays and the surface, and orient the normals of the surface points towards the camera. The points on the invisible part of the shapes are discarded. We then build an octree structure on the point cloud and compute the average normal vectors of the points inside the leaf octants. The octree structures of all the shapes are stored in a database and saved on a hard disk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Object classification</head><p>The goal of object classification is to assign category information to every object, which is an essential and fundamental task in understanding 3D shapes.</p><p>Dataset. We use the ModelNet40 dataset <ref type="bibr" target="#b31">[Wu et al. 2015]</ref> for training and testing, which contains 12,311 CAD models from 40 categories, and is well annotated with multi-class labels. The training and testing sets are available in the dataset, in which 9,843 models are used for training, and 2,468 models for testing. The upright orientation of the models in the dataset is known. We augment the dataset by rotating each model along the upright direction uniformly to generate 12 poses for each model.  Orientation pooling. Since each model is rotated to 12 poses, in the testing phase the activations of the output layer for each pose can be pooled together to increase the accuracy of predictions. In this case, the orientation pooling is reduced to the voting approach, which has been adopted by <ref type="bibr" target="#b19">[Maturana and Scherer 2015]</ref>. More effectively, one can also pool the activations of the last convolution layers and then fine-tune the last two FC layers. This strategy has been adopted by . But the disadvantage of this approach is that it requires the training of one more neural network. For simplicity, we choose the voting strategy for classification. In <ref type="table" target="#tab_0">Table 1</ref>, we provide results with and without voting.</p><p>Comparisons and discussion. We did a comparison on classification accuracy with state-of-the-art 3D CNNs: VoxNet <ref type="bibr" target="#b19">[Maturana and Scherer 2015]</ref>, SubVolSup , FPNN , PointNet <ref type="bibr" target="#b22">[Qi et al. 2017</ref>], Geometry image <ref type="bibr" target="#b27">[Sinha et al. 2016]</ref>, and VRN <ref type="bibr" target="#b3">[Brock et al. 2016]</ref>. For fair comparison, we only consider the performance of a single CNN, and omit results from an ensemble of CNNs.</p><p>From <ref type="table" target="#tab_0">Table 1</ref>, we find that our O-CNN has a significant advantage over VoxNet and FPNN. Our O-CNN(4) whose resolution is 16 3 is already better than FPNN(64 3 ) and slightly worse than FPNN+normal(64 3 ) which utilizes both the distance field and the normal field information. Compared with non voxel-based methods, O-CNN(4) is only worse than PointNet. When we increase the resolution, O-CNN(5) beats all other methods. With the voting strategy, O-CNN is only worse than VRN which uses 24 rotation copies for training and voting.</p><p>It is also interesting to see that O-CNN(3) already has good accuracy, which is above 85%. This fact is consistent with human recognition: people can recognize the type of 3D shape easily from far away. The result indicates that our octree with shape normal information is very informative.</p><p>We observe that the accuracy of our O-CNN increases gradually with the resolution. But there are small accuracy drops when the resolution exceeds 64 3 . This is probably because the ModelNet40 data structure input signal without voting full voxel binary 87.9% full voxel normal 88.7% octree binary 87.3% octree normal 89.6% <ref type="table">Table 2</ref>. Representation comparison on the ModelNet40 dataset with the same network architecture. The representation resolution is 32 3 . The number shown in the table is the accuracy of classification without voting.</p><p>dataset is still not big enough and when the networks go deeper there are some overfitting risks during training.</p><p>Representation comparison. To further verify the superiority of using the octree structure with the normal signals, we have done experiments with the same network architecture as O-CNN(5) while using different input representations. The representation variation includes: (1) full voxel with the binary signal; (2) full voxel with the normal signal with zero vectors in empty voxels; (3) octree with the normal signal, i.e. the representation for our O-CNN; (4) octree with the binary signal, i.e. replacing the normal signal with occupying bits. The second variation can be regarded a generalized version of octree with the normal signal. For CNNs with the full voxel representation in our tests, we adapt the implementation of VoxNet <ref type="bibr" target="#b19">[Maturana and Scherer 2015]</ref> for our network architecture. We train and test the network on the ModelNet40 dataset, with the results summarized in <ref type="table">Table 2</ref>.</p><p>Note that the normal signal helps to achieve better performance on both octree and full voxel structure, which verifies our claim in Section 3.1 that the normal signal preserves more information of the original shape and is superior over the binary signal.</p><p>Moreover, the octree with the normal signal provides the highest level of accuracy among all methods, while the full voxel with the normal signal that performs the computation everywhere does not yield a better result. This indicates that restricting the computation on the octants only is a reasonable strategy that results in the good performance of the O-CNN. We leave a rigorous theoretical analysis for future studies.</p><p>Finally, we found that the octree with the binary signal has the worse performance than full voxel structure with binary signal. This is because the indicator function that represents the original shape is defined in a volume, while our octree is built from the point cloud. After replacing the normal signal as the occupying bits, it is equivalent to discarding the inside portion of the indicator function, which causes information loss compared with the full voxel representation and makes it hard to distinguish the inside and outside of the object.</p><p>Comparisons of memory and computation efficiency. We compare the memory and computational costs of our O-CNN with the costs of full-voxel-based CNNs. Similar to the testing done in representation comparison, we use our network structure for full-voxel-based CNNs. Note that in our network structure, the channel number of the feature maps at each depth used in O-CNNs decreases by a factor of 2 as the octree depth increases, the memory cost of O-CNN(d) can be reduced to be O(n), which enables O-CNN to analyze the 3D shapes in high resolutions.   We run 1000 forward-backward iterations, including all CPU-GPU communications, calculate the average time per iteration, and record the peak GPU memory consumption. The statistics of memory and time cost are summarized in <ref type="table" target="#tab_2">Tables 3 and 4</ref>. It is clear that the O-CNN runs much faster under all resolutions and occupies less memory when the resolution is greater than 16 3 (i.e. d ≤ 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN visualization.</head><p>With respect to CNNs for image understanding, it is known that the output of the learned convolution filters <ref type="bibr" target="#b7">[Goodfellow et al. 2016</ref>] is activated when important image features appear. We also observe this phenomenon on our O-CNN and it helps to better understand O-CNN.</p><p>In <ref type="figure">Figure 6</ref>, we illustrate some filters in U 5 , U 4 , U 3 of O-CNN(5) by color-coding the responses to the input shape on the corresponding octants. We find that the filters in U 5 capture low-level geometry features of the shape, while the filters in U 4 and U 3 capture highlevel shape features. In U 5 , filter A tends to capture forward planar regions, filter B prefers large round regions. In U 4 , filters C and D capture more global shape features. In U 3 , filters E and F are more sensitive to the shape category and they have little response when the category of the shape does not fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Shape retrieval</head><p>Nowadays 3D shapes are widely available, to manage and analyze them, a 3D shape retrieval method is essential. We test our O-CNN on the large-scale dataset ShapeNet Core55 and compare with the state-of-the-art methods in the 3D shape retrieval contest -SHREC16 <ref type="bibr" target="#b25">[Savva et al. 2016]</ref>.</p><p>Dataset. The ShapeNet Core55 dataset contains a total of 51190 3D models with 55 categories and 204 subcategories. The models are normalized to a unit length cube and have a consistent upright orientation. 70% of the dataset is used for training, 10% for validation, and 20% for testing. We use the same method as object classification to perform data augmentation.</p><p>Retrieval. The key to shape retrieval is to generate a compact and informative feature for each shape, with which the most similar shape can be retrieved. We train an O-CNN as the feature extractor, and the network structure is the same as the one used for classification. In the training stage, the cross-entropy loss function is minimized with only the category information. The subcategory information in the dataset is discarded for simplicity. The O-CNN output is the category probability of the input shape, which is used as the feature vector of each shape. Since each object is rotated to 12 poses, correspondingly there are 12 feature vectors, with which the orientation pooling presented previously is used to generate one feature vector for each shape. For each query shape, the label can be predicted from this feature vector. The retrieval set of a query shape is constructed by collecting all shapes that have the same label, and then sorting them according to the feature vector distance between the query shape and the retrieved shape. <ref type="figure" target="#fig_6">Fig. 7</ref> shows top-5 retrieval results of three models by O-CNN(6).</p><p>Performance comparison. For each query in the test set, a retrieval list is returned. Five metrics are used to evaluate the quality of results: precision, recall, mAP, F-score, and NDCG. The precision at an entry is the fraction of retrieved instances that are relevant up to this entry, while the recall is the fraction of relevant instances up to this entry. Roughly, the recall increases along with the length of the retrieval list, while precision decreases. mAP is the mean average precision and F-score is defined as the harmonic mean of precision and recall, which can be regarded as a summary of precision and recall. NDCG measures ranking quality, and the subcategory similarity between shapes is considered when computing this metric. The average performance across all query shapes on these four metrics are calculated with the evaluation software officially provided by <ref type="bibr" target="#b25">[Savva et al. 2016]</ref>, and summarized in <ref type="table" target="#tab_5">Table 5</ref>. The precision recall curve is shown in <ref type="figure" target="#fig_7">Figure 8</ref>.</p><p>We compare our O-CNN with five state-of-the-art methods in SHREC16 which are all based on multi-view based CNNs [Bai et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Top-5 retrieval results   2016; <ref type="bibr" target="#b25">Savva et al. 2016;</ref>]. As we can see from <ref type="table" target="#tab_5">Table 5</ref>, our O-CNN(5) is comparable to state-of-the-art results, and O-CNN(6) yields the best results among all tested methods. Moreover, though the subcategory information is discarded when training our network, we also get the best score on NDCG, which shows that with our octree representation the learned feature is very discriminative and can distinguish similar shapes very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Object part segmentation</head><p>Given a 3D object represented by a point cloud or a triangle mesh, the goal of part segmentation is to assign part category information to each point or triangle face. Compared with object classification, part segmentation is more challenging since the prediction is finegrained and dense.</p><p>Dataset. We conduct an experiment on a large-scale shape part annotation dataset introduced by <ref type="bibr" target="#b32">[Yi et al. 2016]</ref>, which augments a subset of the ShapeNet models with semantic part annotations. The dataset contains 16 categories of shapes, with 2 to 6 parts per category. In total there are 16,881 models with part annotations. However, the models in this dataset are represented as sparse point clouds, with only about 3k points for each model, and the point normals are missing. We align the point cloud with the corresponding 3D mesh, and project the point back to the triangle faces. Then we assign the normal of the triangle face to the point, and condense the point cloud by uniformly re-sampling the triangle faces. Based on this pre-processed point cloud, the octree structure is built. Similar to other tasks, every model has 12 copies rotated around the upright axis. For comparison with <ref type="bibr" target="#b22">[Qi et al. 2017;</ref><ref type="bibr" target="#b33">Yi et al. 2017]</ref>, we use the same training/test split.</p><p>Training. Compared with the dataset used in the retrieval and classification task, the dataset for segmentation is still limited, so training this network from scratch is challenging. Instead, we reuse the weights trained by the retrieval task. Specifically, in the training stage, the convolution part is initialized with the weight trained on ShapeNet and fixed during optimization, while the weight of the deconvolution part is randomly initialized and then evolves according to the optimization process.  CRF refinement. After training, very promising part category predictions can already be achieved according to the output of the deconvolution network. However, there are still some inconsistencies among adjacent segmented parts as shown in <ref type="figure" target="#fig_8">Fig. 9(b)</ref> because the deconvolution network makes the prediction for each point separately. We adopt the dense conditional random field (CRF) technique <ref type="bibr" target="#b12">[Krähenbühl and Koltun 2011]</ref> to refine results.</p><p>For the point cloud {p i } N i=1 with the corresponding normal {n i } N i=1 , we denote the predicted label as {x i } N i=1 . The following energy function is minimized to obtain the refined output:</p><formula xml:id="formula_7">E(x) = i ϕ u (x i ) + i &lt;j ϕ p (x i , x j )</formula><p>where i and j range from 1 to N . ϕ u (x i ) is the unary energy, and is defined as ϕ u (x i ) = − log(p(x i )), which is used to constrain the final output CRF to be similar with our neural network, where p(x i ) is the label probability produced by the neural network. ϕ p (x i , x j ) is the pairwise energy to incorporate neighbor information to refine the output:</p><formula xml:id="formula_8">ϕ p (x i , x j ) =µ(x i , x j ) ω 1 W θ 1 (∥p i − p j ∥)+ ω 2 W θ 2 (∥p i − p j ∥)W θ 3 (∥n i − n j ∥)</formula><p>where W θ i is the Gaussian function with a standard deviation θ i . The label compatibility function µ(x i , x j ), and hyper-parameters ω i and θ i are learned on the training set with the implementation provided by <ref type="bibr" target="#b13">[Krähenbühl and Koltun 2013]</ref>.</p><p>With CRF refinement, the inconsistencies between segmented parts can be significantly reduced. <ref type="figure" target="#fig_8">Fig. 9(c)</ref> shows the refinement result of <ref type="figure" target="#fig_8">Fig. 9(b)</ref>.</p><p>Performance comparison. The comparison is conducted with a learning-based technique <ref type="bibr" target="#b33">[Yi et al. 2017</ref>] that leverages per-point local geometric features and correspondences between shapes, and two recent deep learning based methods <ref type="bibr" target="#b22">[Qi et al. 2017;</ref><ref type="bibr" target="#b33">Yi et al. 2017]</ref>. The evaluation metric is the intersection over union (IoU) of the part class. For each shape category, all the part class IoUs are averaged together to obtain the category IoU. <ref type="table">Table 6</ref> reports our results. O-CNN(5) and O-CNN(6) perform better or comparable to other methods. O-CNN(6) is also better than O-CNN(5) in many cases and sufficiently demonstrates the benefits of using high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose octree-based convolutional neural networks (O-CNN) that take advantage of the sparseness of the octree representation and the local orientation of the shape to enable compact storage and fast computation, achieving better or comparable performance to existing work. The experiments on three shape analysis tasks demonstrate the efficacy and efficiency of O-CNNs. We expect that O-CNN will stimulate more work on 3D understanding and processing.</p><p>In the future, we would like to use our O-CNN to solve more shape analysis and processing challenges, especially on fine-grained tasks where O-CNNs with high resolutions are essential, like shape denoising, shape correspondence, object generation, and scene analysis. There are also many directions for improving O-CNNs, such as the following:</p><p>Adaptive octree. In our octree construction, we do not consider the geometry change of the shape. Actually, for nearly flat regions, it is fine to use a bigger octant to represent them without subdivision. By constructing the octree adaptively according to the local geometry, it is possible to further improve the computation and memory efficiency of O-CNN.</p><p>General lattices. In our O-CNN, we organize the 3D data and computation in the octree data structure, which can be regarded as a hierarchical sparse grid lattice. It is possible to build similar hierarchical structures based on other lattices, such as the tetrahedral lattice <ref type="bibr" target="#b8">[Graham 2015</ref>] and the permutohedral lattice <ref type="bibr" target="#b10">[Jampani et al. 2016]</ref>, the latter of which could be used for higher-dimension CNNs. We leave the generalization of these lattice structures for future studies.</p><p>Network structure. Although the structure of CNNs plays an important role in improving performance, we have not experimented with advanced structures, like deep residual networks or recurrent neural networks. In the future, we would like to integrate our O-CNNs with these advanced network structures to accomplish more challenging tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) illustrates the shuffle keys of all quadtree nodes and Figure 2(b) demonstrates the corresponding shuffle key array at each depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(c) illustrates the labelFig. 3. Left: the original 3D shape. Middle: the voxelized 3D shape. Right: the octree representation with normals sampled at the finest leaf octants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Neighbor access in convolution in 2D. A naïve implementation would pick 36 neighbors for the 4 sibling nodes (A, B, C, D) as shown on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>An example of max-pooling/unpooling on a quadtree structure ofFig. 2. Since every 4 sibling nodes under the same parent are stored contiguously, applying the max-pooling operator on a quadtree reduces to picking out the max element for each of 4 contiguous elements in an array. After down-sampling, the intermediate results stored in a temporary arrayT 1 as shown by the dashed box. Then the label vector L 1 is used to construct T 1 , i.e. assigningT 1 [L 1 [i] − 1] to T 1 [i] when L 1 [i] &gt; 0, and 0 otherwise. The unpooling operation is the reverse operation of pooling. The red arrows represent the switch variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Training details. Using the augmented dataset, we train our O-CNN(d) as shown in Section 4. To observe the behavior of O-CNN under different resolutions, we train six networks: O-CNN(3), O-CNN(4), O-CNN(5), O-CNN(6), O-CNN(7), O-CNN(8), i.e. the resolutions of leaf octants are 8 3 , 16 3 , 32 3 , 64 3 , 128 3 , 256 3 , respectively. The loss function is modeled as the cross-entropy, which is commonly used for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Top-5 retrieval results of O-CNN(6) on three models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Precision and recall curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>The effect of CRF refinement. (a) The ground truth part segmentation of a chair. (b) The result of O-CNN(6) without CRF refinement. As highlighted by the boxes, the boundaries of segmentation results are noisy and jagged. (c) The result of O-CNN(6) with CRF refinement. The inconsistencies on the segmentation boundaries are greatly reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Object classification results on ModelNet40 dataset. The number shown in the table is the accuracy of object recognition. The second and third columns show the results of the network with and without voting. Numbers in parentheses are the resolutions of voxels. A number in boldface emphasizes the best result.</figDesc><table><row><cell>Network</cell><cell cols="2">without voting with voting</cell></row><row><cell>VoxNet (32 3 ) Geometry image SubVolSup (32 3 ) FPNN (64 3 ) FPNN+normal(64 3 ) PointNet VRN (32 3 )</cell><cell>82.0% 83.9% 87.2% 87.5% 88.4% 89.2% 89.0%</cell><cell>83.0% -89.2% ---91.3%</cell></row><row><cell>O-CNN(3) O-CNN(4) O-CNN(5) O-CNN(6) O-CNN(7) O-CNN(8)</cell><cell>85.5% 88.3% 89.6% 89.9% 89.5% 89.6%</cell><cell>87.1% 89.3% 90.4% 90.6% 90.1% 90.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons on GPU-memory consumption. The batch size is 32.</figDesc><table><row><cell>Method</cell><cell>16 3</cell><cell>32 3</cell><cell>64 3</cell><cell>128 3</cell><cell>256 3</cell></row><row><cell cols="4">O-CNN full voxel+binary 59ms 425ms 1648ms 17ms 33ms 90ms full voxel+normal 75ms 510ms 4654ms</cell><cell cols="2">327ms 1265ms ----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Timings of one backward and forward operation in milliseconds.The batch size is 32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Retrieval results. The upper five methods are from the teams that submitted results to SHREC16.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>mean plane bag cap car chair e.ph. guitar knife lamp laptop motor mug pistol rocket skate table</figDesc><table><row><cell># shapes</cell><cell></cell><cell cols="2">2690 76</cell><cell>55</cell><cell>898</cell><cell cols="2">3758 69</cell><cell>787</cell><cell>392</cell><cell cols="2">1547 451</cell><cell>202</cell><cell>184</cell><cell>283</cell><cell>66</cell><cell>152</cell><cell>5271</cell></row><row><cell cols="2">[Yi et al. 2016] PointNet [Qi et al. 2017] 83.7 81.4 SpecCNN [Yi et al. 2017] 84.7</cell><cell>81.0 83.4 81.6</cell><cell cols="4">78.4 77.7 75.7 87.6 78.7 82.5 74.9 89.6 81.7 81.9 75.2 90.2</cell><cell cols="2">61.9 92.0 73.0 91.5 74.9 93.0</cell><cell>85.4 85.9 86.1</cell><cell cols="2">82.5 80.8 84.7 95.6 95.7 95.3</cell><cell>70.6 65.2 66.7</cell><cell cols="2">91.9 85.9 93.0 81.2 92.7 81.6</cell><cell>53.1 57.9 60.6</cell><cell>69.8 72.8 82.9 82.1 75.3 80.6</cell></row><row><cell>O-CNN(5) O-CNN(6)</cell><cell>85.2 85.9</cell><cell>84.2 85.5</cell><cell cols="6">86.9 84.6 74.1 90.8 87.1 84.7 77.0 91.1 85.1 91.9 81.4 91.3</cell><cell cols="2">87.0 87.4 83.3 82.5</cell><cell>94.9 95.4</cell><cell>59.0 56.9</cell><cell cols="2">94.9 79.7 96.2 81.6</cell><cell>55.2 53.5</cell><cell>69.4 74.1</cell><cell>84.2 84.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Table 6. Object part segmentation results.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) Ground truth</cell><cell cols="3">(b) Without refinement</cell><cell></cell><cell cols="3">(c) With refinement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Transactions on Graphics, Vol. 36, No. 4, Article 72. Publication date: July 2017.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We wish to thank the authors of <ref type="bibr" target="#b5">[Chang et al. 2015;</ref><ref type="bibr" target="#b31">Wu et al. 2015]</ref> for sharing their 3D model datasets with the public, the authors of <ref type="bibr" target="#b22">[Qi et al. 2017;</ref><ref type="bibr" target="#b32">Yi et al. 2016]</ref> for providing their evaluation details, Stephen Lin for proofreading the paper, and the anonymous reviewers for their constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GIFT: A real-time and scalable 3D shape search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D deep learning workshop (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: an information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High performance convolutional neural networks for document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidd</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3D convolutional neural networks. In British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D mesh labeling via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: image filtering, dense CRFs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Caffe: convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FPNN: field probing neural networks for 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch Normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Loffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VoxNet: a 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric modeling using octree encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Meagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="129" to="147" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">SHREC&apos;16 Track -Large-scale 3D shape retrieval from ShapeNet Core55</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Eurographics Workshop on 3D Object Retrieval</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepPano: deep panoramic representation for 3-D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning 3D shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Octrees for faster isosurface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Wilhelms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Van Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="201" to="227" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH ASIA)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data-parallel octrees for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. T. Vis. Comput. Gr</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="669" to="681" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
