<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All about Structure: Adapting Structural Information across Domains for Boosting Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
							<email>wpeng@nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">All about Structure: Adapting Structural Information across Domains for Boosting Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we tackle the problem of unsupervised domain adaptation for the task of semantic segmentation, where we attempt to transfer the knowledge learned upon synthetic datasets with ground-truth labels to real-world images without any annotation. With the hypothesis that the structural content of images is the most informative and decisive factor to semantic segmentation and can be readily shared across domains, we propose a Domain Invariant Structure Extraction (DISE) framework to disentangle images into domain-invariant structure and domain-specific texture representations, which can further realize imagetranslation across domains and enable label transfer to improve segmentation performance. Extensive experiments verify the effectiveness of our proposed DISE model and demonstrate its superiority over several state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is to predict pixel-level semantic labels for an image. It is considered one of the most challenging tasks in computer vision. Due to the renaissance of deep learning in recent years, we witness a great leap brought to this task. Since the inception of Fully Convolutional Network (FCN), which is built upon pre-trained classification models (e.g. VGG <ref type="bibr" target="#b20">[21]</ref> and ResNet <ref type="bibr" target="#b6">[7]</ref>) and deconvolutional layers, numerous techniques have been proposed to advance semantic segmentation, such as enlarging receptive fields <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> and better preserving contextual information <ref type="bibr" target="#b27">[28]</ref>, to name a few. However, these approaches rely largely on supervised learning, thereby calling for expensive pixel-level annotations.</p><p>To circumvent this issue, one solution is to train segmentation models on synthetic data. The computer graphics technology nowadays is able to synthesize high-quality, photo-realistic images for a virtual scene. It is thus possi-*Both authors contribute equally ble to build up a dataset for supervised semantic segmentation (e.g. GTA5 <ref type="bibr" target="#b16">[17]</ref> and SYNTHIA <ref type="bibr" target="#b17">[18]</ref>) based on these synthetic images. During the rendering process, their pixellevel semantic labels are readily available. Nevertheless, segmentation models trained on synthetic datasets often have difficulty achieving satisfactory performance in realworld scenes due to a phenomenon known as domain shift -i.e. synthetic and real-world images can still exhibit considerable difference in their low-level texture appearance. Domain adaptation is thus proposed to transfer the knowledge learned from a source domain (e.g. synthetic images) to another target domain (e.g. real images). One common approach is to learn a domain-invariant feature space across domains by matching their feature distributions, where different matching criteria have been explored, e.g. minimizing the second order statistics <ref type="bibr" target="#b22">[23]</ref> and domain adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> . There is also a recent research work <ref type="bibr" target="#b23">[24]</ref> which introduces distribution alignment directly in the structural output space for the task of semantic segmentation. However, these approaches are all driven by a strong assumption that the entire feature or output space of two domains can be well aligned (see <ref type="figure" target="#fig_0">Figure  1</ref> (a)) to yield a domain-invariant representation that is also discriminative for the tasks in question.</p><p>In this paper, we propose a Domain Invariant Structure Extraction (DISE) framework to address unsupervised domain adaptation for semantic segmentation. We hypothesize that the high-level structure information of an image would be the most effective for its segmentation prediction. Thus, our DISE aims to discover a domain-invariant structure feature by learning to disentangle domain-invariant structure information of an image from its domain-specific texture information, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> </p><formula xml:id="formula_0">(b).</formula><p>Our method distinguishes from similar prior works in (1) learning an image representation comprising explicitly a domain-invariant structure component and a domainspecific texture component, (2) making only the structure component domain invariant, and (3) allowing image-toimage translation across domains which further enables label transfer, with all achieved within one single framework. Although DISE shares some parallels with domain separation networks <ref type="bibr" target="#b0">[1]</ref> and DRIT <ref type="bibr" target="#b12">[13]</ref>, its emphasis on the separation of structure and texture information and the ability to translate images across domains and meanwhile maintain structures clearly highlight the novelties. Extensive experiments on standardized datasets confirm its superiority over several state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In comparison to image classification where there exist many prior works addressing the domain adaptation problem, semantic segmentation is considered a much more challenging task to apply domain adaptation, since its output is a segmentation map full of highly structured and contextual semantic information. We review several related works here and categorize them according to the use of three widely utilized strategies: distribution alignment, image translation, and label transfer. Different works may differ in their choice and conducting order of these strategies, as contrasted in <ref type="table">Table 1</ref>.</p><p>Firstly, similar to the case of domain adaptation for image classification, different criteria may be applied to match distributions across domains in the feature space (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>) or in the output space. The representative work of the latter is proposed by Tsai et al. <ref type="bibr" target="#b23">[24]</ref>, where adversarial learning is applied on segmentation maps, based on spatial contextual similarities between the source and target domains . However, the assumption that the whole feature or output space of the two domains can be well aligned often proves impractical, considering the substantial difference in appearance (namely, texture) between synthetic and <ref type="table">Table 1</ref>. Different strategies adopted by prior works on domain adaptation for semantic segmentation. IT, DA, LT stand for Image Translation, Distribution Alignment, and Label Transfer, respectively. Order denotes the order in which these strategies are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>IT DA LT Order Sankaranarayanan et al. <ref type="bibr" target="#b19">[20]</ref> IT→DA Hong et al. <ref type="bibr" target="#b8">[9]</ref> --Wu et al. <ref type="bibr" target="#b25">[26]</ref> IT→DA→LT Tsai et al. <ref type="bibr" target="#b23">[24]</ref> --Chen et al. <ref type="bibr" target="#b2">[3]</ref> --Hoffman et al. <ref type="bibr" target="#b7">[8]</ref> IT→LT, DA Zhu et al. <ref type="bibr" target="#b29">[30]</ref> IT→DA Our DISE DA→IT→LT real-world images in some applications. Secondly, the recent advance in image-to-image translation and style transfer <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref> has motivated the translation of source images to gain texture appearance of target images, or vice versa. On the one hand, this translation process allows segmentation models to use translated images as augmented training data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>; on the other hand, the common feature space learned in the course of image translation can facilitate learning a domain-invariant segmentation model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Finally, the image-to-image translation makes possible the transfer of labels from the source domain to the target domain, providing additional supervised signals to learn a model applicable to target-domain images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>. However, the direct image-translation may be harmful to learning, due to the risk of carrying over source specific information to the target domain.</p><p>Our proposed DISE makes use of all three strategies but differs from these prior works in several significant ways. We hypothesize that the high-level structure information of an image would be the most informative for its semantic segmentation. Thus, the DISE is to disentangle high-level, domain-invariant structure information of an image from its low-level, domain-specific texture information through a set of common and private encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this paper, we propose a Domain Invariant Structure Extraction (DISE) framework to address the problem of unsupervised domain adaptation for semantic segmentation. The emphasis on explicitly regularizing the common and private encoders towards capturing structure and texture information, along with the ability to translate images from one domain to another for label transfer, underlines the novelties of our method. The following gives a formal treatment of the DISE. We begin by an overview of its frame-  work. Next, we present in detail the loss functions used, followed by a description of implementation details.</p><formula xml:id="formula_1">0 &amp; 1 0 '2' 1 0 &amp;2&amp; ( % &amp; ( % ' ℒ 3*% &amp; ℒ 3*% ' ( $ &amp; ( % ' 1 0 '2&amp; " ℒ '3-4&amp;_-./ '2&amp; ℒ '3-4&amp;_&amp;'3 '2&amp; ℒ '3-4&amp;_'*5 '2&amp; Domain Invariant Structure Extraction (DISE) Notation # % 1 0 &amp;2' ! ( % &amp; ( $ ' " ℒ '3-4&amp;_-./ &amp;2' ℒ '3-4&amp;_&amp;'3 &amp;2' ℒ '3-4&amp;_'*5 &amp;2' ℒ &amp;*+ &amp;2' (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Domain Invariant Structure Extraction</head><p>The DISE aims to learn an image representation comprising a domain-invariant structure component and a domain-specific texture component. The setting assumes access to N s annotated source-domain images</p><formula xml:id="formula_2">X s = {(x s i , y s i )} Ns i=1 , with each image x s i ∈ R H×W ×3 having height H, width W and C-way per-pixel label of object categories y s i ∈ {0, 1} H×W ×C , and N t unannotated target- domain images X t = {x t i } Nt i=1</formula><p>. As shown in <ref type="figure" target="#fig_2">Figure 2</ref> (a), there are five sub-networks in DISE, namely, the common encoder E c shared across domains, the domain-specific private encoders E s p , E t p , the shared decoder D, and the pixel-wise classifier T . They are parameterized by θ c , θ s p , θ t p , θ d and θ t , respectively. Given a source-domain image x s as input, the common encoder E c produces z s c = E c (x s ; θ c ) to characterize its domain-invariant, high-level structure information while the source-specific private encoder E s p generates z s p = E s p (x s ; θ s p ) for capturing its remnant aspects that are largely related to domain-specific, low-level texture information. These two components z s c , z s p are complementary to each other; when combined together, they allow the decoder D to minimize a reconstruction loss L s rec between the input x s and its reconstructionx s2s = D(z s c , z s p ; θ d ). Likewise, a target-domain image x t can be encoded and decoded sim-</p><formula xml:id="formula_3">ilarly to minimize L t rec , yielding z t c = E c (x t ; θ c ), z t p = E t p (x t ; θ t p ) and x t ≈x t2t = D(z t c , z t p ; θ d ),</formula><p>where the private encoder E t p , like its counterpart E s p , extracts the targetspecific texture information. It is the structure components z s c , z t c that will be used by classifier T to predict segmentation maps,ŷ s = T (z s c , θ t ),ŷ t = T (z t c , θ t ) in source and target domains accordingly.</p><p>The disentanglement between structure and texture information is realized by the regularization coming from image translation with domain adversarial training <ref type="bibr" target="#b13">[14]</ref> and perceptual loss minimization <ref type="bibr" target="#b11">[12]</ref>. As illustrated in </p><formula xml:id="formula_4">imagesx s2t = D(z s c , z t p ; θ d ) andx t2s = D(z t c , z s p ; θ d ).</formula><p>If the common and private encoders behave as we expect them to capture the structure and texture information, respectively, the translated imagê x s2t (respectively,x t2s ) should hold the high-level structure the same as x s (respectively, x t ) while exhibiting similar low-level texture appearance to x t (respectively, x s ). To this end, we train our networks by imposing domain adversarial losses L t2s trans adv , L s2t trans adv <ref type="bibr" target="#b13">[14]</ref> and perceptual losses L t2s trans str , L t2s trans tex , L s2t trans str , L s2t trans tex <ref type="bibr" target="#b11">[12]</ref> at the output of the decoder D in order to ensure the domain and perceptual similarities between these translated images and their counterparts in the source or target domains. This image translation functionality of DISE further allows the transfer of ground-truth labels from the source domain to the target domain. More specifically, since the target-domain-like imagesx s2t share the same structure component as x s , we consider the ground-truth labels y s of x s to be the pseudo labels forx s2t on grounds of our hypothesis that the segmentation prediction for an image depends solely on its structure information.</p><p>Finally, we make the structure components z s c , z t c invariant to the domain from which they are extracted by minimizing another domain adversarial loss L seg adv at the output of the classifier T , as well as the negative log-likelihood functions of the ground-truth labels y s with respect to x s andx s2t , i.e. L s seg and L s2t seg (see <ref type="figure" target="#fig_2">Figure 2</ref> (d)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning</head><p>The training of the proposed DISE is to minimize a weighted combination of the aforementioned loss functions with respect to the parameters {θ c , θ s p , θ t p , θ d θ t } of the five sub-networks:</p><p>L =λ s seg L s seg + λ seg adv L seg adv + λ rec L rec + λ trans str L trans str + λ trans tex L trans tex + λ trans adv L trans adv + λ s2t seg L s2t seg ,</p><p>where the combination weights λ's are chosen empirically to strike a balance among the model capacity, reconstruction/translation quality, and prediction accuracy. In the following, we elaborate on each of these loss functions.</p><p>Segmentation Loss. The segmentation loss L s seg (θ c , θ t ) given by the typical cross-entropy based on the sourcedomain ground truths y s is to train supervisedly the common encoder E c and the classifier T in order to predict segmentation mapsŷ s for source-domain images x s .</p><p>Output Space Adversarial Loss. Inspired by Tsai et al. <ref type="bibr" target="#b23">[24]</ref>, we introduce an adversarial loss L seg adv (θ c , θ t ) at the output of the classifier T , in the hopes of making the common encoder E c and the classifier T generalize well on target-domain images. Specifically, we first train a discriminator D seg adv to distinguish between the source predictionŷ s and the target predictionŷ t at the patch level <ref type="bibr" target="#b10">[11]</ref> by minimizing a supervised domain loss (i.e. D seg adv should ideally output 1 for each patch in the source predictionŷ s and 0 for that in the target predictionŷ t ). We then update the common encoder E c and the classifier T to fool the discriminator D seg adv by inverting its output forŷ t from 0 to 1, that is, by minimizing</p><formula xml:id="formula_6">L seg adv (θ c , θ t ) = − 1 H W h ,w log(D seg adv (ŷ t ) h ,w ),</formula><p>(2) where h , w are patch coordinates and H = H/16, W = W/16 with the factor 16 accounting for the downsampling in the discriminator D seg adv . Reconstruction Loss.</p><p>The reconstruction loss L rec (θ c , θ s p , θ t p , θ d ) is to ensure that the two domaininvariant and domain-specific components z c , z p of an image representation together form a nearly complete summary of the image. To encourage the reconstruction to be perceptually similar to the input image, we follow the notion of perceptual loss <ref type="bibr" target="#b11">[12]</ref> to define our quality metric L perc (x, y; w) as a weighted sum of L1 differences between feature representations extracted from a pre-trained VGG network <ref type="bibr" target="#b21">[22]</ref>. In symbols, we have</p><formula xml:id="formula_7">L perc (x, y; w) = l∈L w (l) N (l) ψ (l) (x) − ψ (l) (y) 1 ,<label>(3)</label></formula><p>where ψ (l) (x) (respectively, ψ (l) (y)) is the activations of the l-th layer of the pre-trained VGG network for input x (respectively, y), N (l) is the number of activations in layer l, w (l) gives a separate weighting to the loss in layer l, and L refers to {relu1 1, relu2 1, relu3 1, relu4 1, relu5 1} of the VGG network. As pointed out in <ref type="bibr" target="#b11">[12]</ref>, the higher layers of VGG network tend to represent the high-level structure content of an image while the lower layers generally describe its low-level texture appearance. Equation 3 is then used to regularize the reconstruction of both source-and target-domain images by minimizing the sum of their respective perceptual losses:</p><formula xml:id="formula_8">L rec (θ c , θ s p , θ t p , θ d ) = L s rec + L t rec = L perc (x s2s , x s ; w rec ) + L perc (x t2t , x t ; w rec ),<label>(4)</label></formula><p>where the weighting w rec is set to weight more on higher layers.</p><p>Translation Structure Loss. As motivated previously in Section 3.1, an image produced by translation across domains should keep its structure unchanged. The translation structure loss L trans str (θ c , θ s p , θ t p , θ d ) as defined in Equation 5 measures the differences in high-level structure between the translated imagex s2t and the image x s from which the structure component ofx s2t is derived, and likewise, betweenx t2s and x t . This is achieved by choosing for the perceptual metric a weighting w str that again stresses on the feature reconstruction losses in higher layers of the pretrained VGG network. Our goal is to penalize the translated images which differ significantly in structure from the images with which they share the same structure component z c , thereby getting z c to encode explicitly the structure aspect of an image.</p><formula xml:id="formula_9">L trans str (θ c , θ s p , θ t p , θ d ) = L s2t trans str + L t2s trans str = L perc (x s2t , x s ; w str ) + L perc (x t2s , x t ; w str )<label>(5)</label></formula><p>Translation Texture Loss. The translation texture loss L trans tex (θ c , θ s p , θ t p , θ d ) further requires that the translated imagex s2t (respectively,x t2s ) should resemble closely in texture the image x t (respectively, x s ), since they share the same texture component z p . In doing so, z p has to encode explicitly the texture aspect of an image. Inspired by the work of AdaIN <ref type="bibr" target="#b9">[10]</ref>, we propose a weighted metric L tex (x, y; w) to measure channel-wisely the difference in the mean value of their activations extracted from a pretrained VGG network:</p><formula xml:id="formula_10">L tex (x, y; w) = l∈L w (l) C (l) c µ c (ψ (l) (x)) − µ c (ψ (l) (y)) 1 ,<label>(6)</label></formula><p>where C (l) is the number of channels in layer l of the VGG network, w (l) specifies the weighting given to layer l, and µ c (·) returns the mean activation of channel c. Like the translation structure loss, the translation texture loss also involves the two types of translation:</p><formula xml:id="formula_11">L trans tex (θ c , θ s p , θ t p , θ d ) = L s2t trans tex + L t2s trans tex = L tex (x s2t , x t ; w tex ) + L tex (x t2s , x s ; w tex ),<label>(7)</label></formula><p>where the weighting w tex of the perceptual metric is now chosen to emphasize more on early layers.</p><p>Translation Adversarial Loss. In addition to the aforementioned perceptual losses, we also employ adversarial losses L trans adv (θ c , θ s p , θ t p , θ d ) to adapt the translated imagesx s2t andx t2s to appear as if they were images out of the target and source domains, respectively. To this end, we adopt LSGAN <ref type="bibr" target="#b15">[16]</ref> and Patch Discriminator <ref type="bibr" target="#b10">[11]</ref>.</p><p>Label Transfer Loss. The label transfer loss L s2t seg (θ c , θ t ) is given by a typical cross-entropy loss that trains supervisedly the common encoder E c and the classifer T on translated imagesx s2t with pseudo labels y s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>Networks. For experiments, we use a base model, referring collectively to the common encoder E c and the pixelwise classifier T , similar to the segmentation network in <ref type="bibr" target="#b23">[24]</ref>, which is built on DeepLab-v2 <ref type="bibr" target="#b1">[2]</ref> with ResNet-101 <ref type="bibr" target="#b6">[7]</ref>. We obtain initial weights by pre-training on PASCAL VOC <ref type="bibr" target="#b4">[5]</ref> dataset, and at training time, reuse the pre-trained batchnorm layer. The common encoder E c outputs the feature maps of the last residual layer (layer4) as z c . For the private encoders E s p , E t p , we adopt a convolutional neural network containing 4 convolution blocks, followed by one global pooling layer and one fully-connected layer. The output of the private encoder E s p (respectively, E t p ) is an 8-dimensional representation z s p (respectively, z t p ). For the shared decoder D, we use three residual blocks and three deconvolution layers. The input to the decoder is a concatenation of the private code z p , the feature maps z c , and a flag indicating the domain of the private code.</p><p>Training Details. We implement DISE with Pytorch on a single Tesla V100 with 16 GB memory. The full training takes 88 GPU hours. Due to limited memory, at training time, we resize input images to 512×1024 and perform random cropping with a crop size of 256×512. However, at test time, the input images are of size 512×1024. For fair comparison, we follow Tsai et al. <ref type="bibr" target="#b23">[24]</ref> and resize the output predictions from 512×1024 to 1024×2048 at evaluation time. We train our model for 250,000 iterations with a batch size of 2. We use the SGD solver with an initial learning rate of 2.5 × 10 −4 for the common encoder E c and the classifier T ; the Adam solver with an initial learning rate of 1.0 × 10 −3 for the decoder D; and the Adam solver with an initial learning rate of 1.0 × 10 −4 for the others. All the learning rates decrease according to the polynomial decay policy. The momentum is set to 0.9 and 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we perform experiments on typical datasets for semantic segmentation. We compare the performance of our proposed method with several state-of-the-art baselines and conduct an ablation study to understand the effect of various combinations of loss functions on segmentation performance. The code and pre-trained models are available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For experiments, we follow the common protocol adopted by most prior works; that is, taking synthetic dataset GTA5 <ref type="bibr" target="#b16">[17]</ref> or SYNTHIA <ref type="bibr" target="#b17">[18]</ref> with ground-truth annotations as the source domain, and Cityscapes dataset <ref type="bibr" target="#b3">[4]</ref> as the target domain where no annotation is available during training. At test time, the evaluation is conducted on the validation set of Cityscapes. The details of these datasets are described as follows.</p><p>Cityscapes <ref type="bibr" target="#b3">[4]</ref> is a real-world dataset composed of streetview images captured in 50 different cities. Its data split includes 2975 training images and 500 validation images, with each having a spatial resolution of 2048 × 1024 and 19 semantic labels at the pixel level. Note again that no ground-truth label is used in model training.</p><p>GTA5 <ref type="bibr" target="#b16">[17]</ref> is a synthetic dataset containing 24996 images of size 1914 × 1052. These images are collected from computer game Grand Theft Auto V (GTAV) and come with pixel-level semantic labels that are fully compatible with Cityscapes <ref type="bibr" target="#b3">[4]</ref>.</p><p>SYNTHIA is another synthetic dataset composed of 9400 annotated synthetic images with the resolution 1280 × 960. Like GTA5, it has semantically compatible annotations with Cityscapes <ref type="bibr" target="#b3">[4]</ref>. Following the prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>, we use the SYNTHIA-RAND-CITYSCAPE subset <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>We compare the performance of our method against several baselines, including the models of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. Of these, the works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref> are representative of the conventional adaptation that matches distributions of feature or output spaces across domains based on adversarial training; the works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> are typical of those that map sourcedomain images to the target domain at the pixel level by image translation or style transfer; and Saleh et al. <ref type="bibr" target="#b18">[19]</ref> stands out from the others by object detection-based method for foreground instances. More details of these works can be found in Section 2.</p><p>GTA5 to Cityscapes. <ref type="table" target="#tab_0">Table 2</ref> shows that as compared to the baselines, our method achieves the state-of-the-art performance of 45.4 in mean intersection-over-union (mIoU). A breakdown analysis further reveals that it outperforms most of the baselines by a large margin in predicting "Road", "Sidewalk, "Wall", "Fence", "Building", and "Sky" classes. These are classes that often appear concurrently in an image and tend to be spatially connected. Moreover, some of them, e.g. "Road" and "Sidewalk", exhibit highly similar texture appearance. We thus attribute the good performance of our scheme to its ability to filter out the domain-specific texture information in forming a domain-invariant structure representation for semantic segmentation.</p><p>In <ref type="figure" target="#fig_4">Figure 3</ref>, we show qualitative results comparing our method against "Source Only" (i.e. no adaptation) and "Conventional Adaptation" (i.e. without disentanglement of structure and texture). For the latter, we present results of <ref type="bibr" target="#b23">[24]</ref>. It is clear that the segmentation predictions made by our method look most similar to the ground truths. On closer examination, we see that our model can better discern the difference between "Sidewalk" and "Road" as compared to the baselines. It also does a good job at identifying rare classes such as "Pole" and "Traffic Sign". These obser-vations suggest that our structure-based representations are indeed more discriminative than other representations that may have encoded both structure and texture information as with the "Conventional Adaptation".</p><p>SYNTHIA to Cityscapes. We also evaluate all models on the more challenging SYNTHIA dataset. Specifically, we follow <ref type="bibr" target="#b23">[24]</ref> to compare results based on semantic predictions for only 16 classes. <ref type="table" target="#tab_1">Table 3</ref> presents quantitative results in terms of per-class IoU and mIoU. It is seen that most of the aforementioned discussions made with GTA5 dataset can be carried over to SYNTHIA. Although the prior work <ref type="bibr" target="#b8">[9]</ref> performs closely to our model in terms of mIoU, the superiority of our method in classes like "Road", "Sidewalk", "Building", "Sky" still remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The following presents a study of four variants of our model by comparing their performance with four distinct training objectives:</p><p>• Source Only: Training with annotated GTA5 dataset <ref type="bibr" target="#b16">[17]</ref> by minimizing L s seg only, i.e. without any domain adaptation.</p><p>• Seg-map Adaptation: Training with annotated GTA5 dataset <ref type="bibr" target="#b16">[17]</ref> together with domain adaptation at the output space by minimizing L s seg and L seg adv . This corresponds to the method in <ref type="bibr" target="#b23">[24]</ref>, which aligns segmentation predictions across domains.</p><p>• DISE w/o Label Transfer: Training with all loss functions except label transfer loss, i.e. the setting for seg-map adaptation plus disentanglement of structure and texture components.</p><p>• DISE: Training with all loss functions. <ref type="table" target="#tab_2">Table 4</ref> compares the performance of these settings in terms of mIoU. As expected, without any domain adaptation, "Source Only" shows the worst performance with a 39.8 mIoU. The performance improves by 2.8 with Segmap Adaptation", arriving at a 42.6 mIoU, when introducing domain adaptation at the output space. An even higher gain of 4.3 over "Source Only" is seen for the setting of "DISE w/o Label Transfer", confirming the benefit of disentangling the structure and texture components. Finally, with additional augmented data due to label transfer, the DISE achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image-to-Image Translation</head><p>In <ref type="figure" target="#fig_5">Figure 4</ref>, we show qualitative results of image-toimage translation with DISE for two settings, S2T and T2S. With S2T (respectively, T2S), we combine the structure content of images in GTA5 (respectively, Cityscapes) in column (a) with the texture appearance of images in Cityscapes   (respectively, GTA5) in columns (b) and (d) to produce translated images in columns (c) and (e), respectively. We see that DISE is very effective in translating images from one domain to another with high quality. In all cases, the translated images preserve well the structure content while producing the desired texture appearance. This also validates our use of the ground-truth labels of the sourcedomain images as pseudo labels for their translated images with texture appearance similar to target-domain images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we hypothesize that the high-level structure information of an image is most decisive to semantic segmentation and can be made invariant across domains. Based on this hypothesis, we propose a novel framework, Domain Invariant Structure Extraction (DISE), to disentangle the representation of an image into a domain-invariant structure component and a domain-specific texture component, where the former is used to advance domain adaptation for semantic segmentation. The DISE also allows transfer of ground-truth labels from the source domain to the target domain, providing additional supervision for learning a segmentation network suitable for target-domain images. Extensive simulation results on typical datasets confirms the superiority of DISE over several state-of-the-art methods, justifying our initial hypothesis.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of the conventional domain adaptation for semantic segmentation and our proposed method. Instead of making the entire feature representation domain invariant, we align only the distributions of the structure component across domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the proposed domain-invariant structure extraction (DISE) framework for semantic segmentation. The DISE framework is composed of a common encoder Ec shared across domains, two domain-specific private encoders, E s p , E t p , a pixel-wise classifier T , and a shared decoder D. It encodes an image, source-domain or target-domain, into a domain-specific texture component zp and a domain-invariant structure component zc, as shown in part (a). With this disentanglement, it can translate an image x s (respectively, x t ) in one domain to another imagex s2t (respectively,x t2s ) in the other domain by combining the structure content of x s (respectively, x t ) with the texture appearance of x t (respectively, x s ), as shown in parts (b) and (c). This further enables the transfer of ground-truth labels from the source domain to the target domain, as illustrated in part (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2 (b) and (c), we consider any pair of source-and target-domain images with their respective representations x s = {z s c , z s p } and x t = {z t c , z t p }. We first interchange their domain-specific components, and then decode them into two unseen, translated</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>(a) Target Image (b) Ground Truth (c) Source Only (d) Conventional Adapt. (e) DISE (ours) Segmentation results on Cityscapes when adapted from GTA5. From left to right, (a) Target Image, (b) Ground Truth, (c) Source Only, (d) Conventional Adaptation [24], (e) and DISE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Sample results of translated images. S2T: the structure content of GTA5 images in (a) are combined with the texture appearance of Cityscapes images in (b) and (d) to output translated images in (c) and (e), respectively. T2S: the structure content of Cityscapes images in (a) are combined with the texture appearance of GTA5 images in (b) and (d) to output translated images in (c) and (e), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparison results on Cityscapes when adapted from GTA5 in terms of per-class IoU and mIoU over 19 classes. 49.0 70.7 13.5 10.9 38.5 29.4 33.7 77.9 37.6 65.8 75.1 32.4 77.8 39.2 45.2 0.0 25.5 35.4 44.5 Chen et al. [3] PSPNet [28] 76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6 39.4 Wu et al. [26] PSPNet [28] 85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.5 26.9 11.6 41.7 Chen et al. 47.5 82.5 31.3 25.6 33.0 33.7 25.8 82.7 28.8 82.7 62.4 30.8 85.2 27.7 34.5 6.4 25.2 24.4 45.4</figDesc><table><row><cell>Methods</cell><cell>Base Model</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Traffic Light</cell><cell>Traffic Sign</cell><cell>Vegetation</cell><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>Motorbike</cell><cell>Bicycle</cell><cell>mIoU</cell></row><row><cell>Sankaranarayanan et al. [20]</cell><cell>FCN8s [15]</cell><cell cols="20">88.0 30.5 78.6 25.2 23.5 16.7 23.5 11.3 78.7 27.2 71.9 51.3 19.5 80.4 19.8 18.3 0.9 20.8 18.4 37.1</cell></row><row><cell>Wu et al. [26]</cell><cell>FCN8s [15]</cell><cell cols="20">88.5 37.4 79.3 24.8 16.5 21.3 26.3 17.4 80.8 30.9 77.6 50.2 19.2 77.7 21.6 27.1 2.7 14.3 18.1 38.5</cell></row><row><cell cols="22">Hong et al. [9] 89.2 [3] FCN8s [15] Deeplab v2 [2] 85.4 31.2 78.6 27.9 22.2 21.9 23.7 11.4 80.7 29.3 68.9 48.5 14.1 78.0 19.1 23.8 9.4 8.3 0.0 35.9</cell></row><row><cell>Tsai et al. [24]</cell><cell>Deeplab v2 [2]</cell><cell cols="20">86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4</cell></row><row><cell>Saleh et al. [19]</cell><cell>Deeplab v2 [2]</cell><cell cols="20">79.8 29.3 77.8 24.2 21.6 6.9 23.5 44.2 80.5 38.0 76.2 52.7 22.2 83.0 32.3 41.3 27.0 19.3 27.7 42.5</cell></row><row><cell>Ours</cell><cell>Deeplab v2 [2]</cell><cell>91.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison results on Cityscapes when adapted from SYNTHIA in terms of per-class IoU and mIoU over 16 classes. 77.1 2.5 0.2 27.1 6.2 7.6 78.4 81.2 55.8 19.2 82.3 30.3 17.1 34.3 41.5</figDesc><table><row><cell>Methods</cell><cell>Base Model</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Traffic Light</cell><cell>Traffic Sign</cell><cell>Vegetation</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Bus</cell><cell>Motorbike</cell><cell>Bicycle</cell><cell>mIoU</cell></row><row><cell>Sankaranarayanan et al. [20]</cell><cell>FCN8s [15]</cell><cell cols="17">80.1 29.1 77.5 2.8 0.4 26.8 11.1 18.0 78.1 76.7 48.2 15.2 70.5 17.4 8.7 16.7 36.1</cell></row><row><cell>Wu et al. [26]</cell><cell>FCN8s [15]</cell><cell cols="17">81.5 33.4 72.4 7.9 0.2 20.0 8.6 10.5 71.0 68.7 51.5 18.7 75.3 22.7 12.8 28.1 36.5</cell></row><row><cell>Hong et al. [9]</cell><cell>FCN8s [15]</cell><cell cols="17">85.0 25.8 73.5 3.4 3.0 31.5 19.5 21.3 67.4 69.4 68.5 25.0 76.5 41.6 17.9 29.5 41.2</cell></row><row><cell>Wu et al. [26]</cell><cell>PSPNet [28]</cell><cell cols="17">82.8 36.4 75.7 5.1 0.1 25.8 8.04 18.7 74.7 76.9 51.1 15.9 77.7 24.8 4.1 37.3 38.4</cell></row><row><cell>Chen et al. [3]</cell><cell>Deeplab v2 [2]</cell><cell cols="17">77.7 30.0 77.5 9.6 0.3 25.8 10.3 15.6 77.6 79.8 44.5 16.6 67.8 14.5 7.0 23.8 36.2</cell></row><row><cell>Tsai et al. [24]</cell><cell>Deeplab v2 [2]</cell><cell cols="17">84.3 42.7 77.5 9.3 0.2 22.9 4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3 40.0</cell></row><row><cell>Ours</cell><cell>Deeplab v2 [2]</cell><cell cols="2">91.7 53.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation study results on Cityscapes when adapted from GTA5 in terms of mIoU. We present results for no adaptation (Source Only), adaptation at the output space only (Seg-map Adaptation), adaptation at the output space together with structure and texture disentanglement (DISE w/o Label Transfer), and adaptation with all losses considered (DISE). L seg adv C: L rec + L trans str + L trans tex + L trans adv D: L s2t seg</figDesc><table><row><cell>Method</cell><cell>A B C D mIoU</cell></row><row><cell>Source Only</cell><cell>39.8</cell></row><row><cell>Seg-map Adaptation</cell><cell>42.6</cell></row><row><cell>DISE w/o Label Transfer</cell><cell>44.1</cell></row><row><cell>DISE</cell><cell>45.4</cell></row><row><cell>A: L s seg</cell><cell></cell></row><row><cell>B:</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/a514514772/ DISE-Domain-Invariant-Structure-Extraction</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project is supported by MOST-108-2634-F-009-013 and MOST-108-2636-E-009-001 and we are grateful to the National Center for Highperformance Computing for computer time and facilities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
